{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff35afe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "os.makedirs(\"plots\", exist_ok=True)\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28f54e4",
   "metadata": {},
   "source": [
    "## DATASET CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866c9de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DEAPDataset(Dataset):\n",
    "    def __init__(self, feature_path, label_path, exclude_subject=None, only_subject=None, normalize=True, train=False, noise_std=0.05):\n",
    "        self.features = np.load(feature_path)  # shape: (32, 40, 40, 5, 63)\n",
    "        self.labels = np.load(label_path)      # shape: (32, 40, 63)\n",
    "        self.train = train\n",
    "        self.noise_std = noise_std\n",
    "\n",
    "        if normalize:\n",
    "            all_features = []\n",
    "            for subj in range(32):\n",
    "                if exclude_subject is not None and subj == exclude_subject:\n",
    "                    continue\n",
    "                if only_subject is not None and subj != only_subject:\n",
    "                    continue\n",
    "\n",
    "                subj_features = self.features[subj].transpose(0, 1, 3, 2).reshape(-1, 40, 5)\n",
    "                all_features.append(subj_features)\n",
    "\n",
    "            if all_features:\n",
    "                all_features = np.concatenate(all_features, axis=0)\n",
    "                self.mean = np.mean(all_features, axis=0)\n",
    "                self.std = np.std(all_features, axis=0) + 1e-8\n",
    "            else:\n",
    "                self.mean = 0\n",
    "                self.std = 1\n",
    "        else:\n",
    "            self.mean = 0\n",
    "            self.std = 1\n",
    "\n",
    "        self.samples = []\n",
    "        for subj in range(32):\n",
    "            if exclude_subject is not None and subj == exclude_subject:\n",
    "                continue\n",
    "            if only_subject is not None and subj != only_subject:\n",
    "                continue\n",
    "\n",
    "            for trial in range(40):\n",
    "                for win in range(63):\n",
    "                    x = self.features[subj, trial, :, :, win]\n",
    "                    y = self.labels[subj, trial, win]\n",
    "                    self.samples.append((x, y, subj))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, y, subj = self.samples[idx]\n",
    "        x = (x - self.mean) / self.std\n",
    "\n",
    "        if self.train:\n",
    "            x = torch.tensor(x, dtype=torch.float32)\n",
    "\n",
    "            # âœ… Add Gaussian jitter\n",
    "            x += torch.randn_like(x) * 0.01\n",
    "\n",
    "            # âœ… Channel-wise dropout (spatial masking)\n",
    "            if np.random.rand() < 0.3:\n",
    "                mask = torch.rand(x.shape[1]) > 0.2\n",
    "                x[:, ~mask] = 0\n",
    "        else:\n",
    "            x = torch.tensor(x, dtype=torch.float32)\n",
    "\n",
    "        return x, int(y)\n",
    "\n",
    "    def get_subject_data(self, subject):\n",
    "        return [(torch.tensor((x - self.mean) / self.std, dtype=torch.float32), int(y))\n",
    "                for x, y, subj in self.samples if subj == subject]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b979d6e9",
   "metadata": {},
   "source": [
    "## MODEL ARCHITECTURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2aff90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommonFeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Add convolutional layers before GRU to capture spatial patterns\n",
    "        self.spatial_conv = nn.Conv1d(40, 32, kernel_size=1)\n",
    "        \n",
    "        # Increase GRU complexity with more layers\n",
    "        self.gru = nn.GRU(input_size=32, hidden_size=128, num_layers=2, \n",
    "                          batch_first=True, bidirectional=True, dropout=0.2)\n",
    "        \n",
    "        self.attention = nn.MultiheadAttention(embed_dim=256, num_heads=4)\n",
    "        self.fc = nn.Linear(256, 128)  # Wider representation\n",
    "        self.fc2 = nn.Linear(128, 64)  # Additional layer for deeper representation\n",
    "        self.bn = nn.BatchNorm1d(64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = x.view(batch_size, 40, 5)  # Reshape to (batch, channels, features)\n",
    "        x_spatial = x\n",
    "        # Apply spatial convolution\n",
    "        # x_spatial = x.transpose(1, 2)  # (batch, 5, 40)\n",
    "        x_spatial = self.spatial_conv(x_spatial)  # (batch, 32, 40)\n",
    "        x = x_spatial.transpose(1, 2)  # (batch, 40, 32)\n",
    "        \n",
    "        # GRU processing\n",
    "        output, h = self.gru(x)  # output: (batch, time, hidden*2)\n",
    "        \n",
    "        # Self-attention on sequence output\n",
    "        attn_output, _ = self.attention(output.transpose(0, 1), \n",
    "                                        output.transpose(0, 1), \n",
    "                                        output.transpose(0, 1))\n",
    "        attn_output = attn_output.transpose(0, 1)\n",
    "        \n",
    "        # Global average pooling on attention output\n",
    "        x = torch.mean(attn_output, dim=1)  # (batch, hidden*2)\n",
    "        \n",
    "        # MLP projection head\n",
    "        x = F.relu(self.fc(x))\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn(x)\n",
    "        x = F.normalize(x, p=2, dim=1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, in_channels, reduction=4):  # Increased reduction ratio for more capacity\n",
    "        super(SEBlock, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool1d(1)  # Adding max pooling for better feature selection\n",
    "        \n",
    "        # Two-branch excitation network\n",
    "        self.fc1 = nn.Linear(in_channels * 2, in_channels // reduction)\n",
    "        self.bn1 = nn.BatchNorm1d(in_channels // reduction)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.2)  # Adding dropout for regularization\n",
    "        self.fc2 = nn.Linear(in_channels // reduction, in_channels)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, channels)\n",
    "        # b, c = x.size()\n",
    "        \n",
    "        # Both average and max pooling for better feature aggregation\n",
    "        avg_feat = x.mean(dim=1, keepdim=True)   \n",
    "        max_feat = x.max(dim=1, keepdim=True)[0]\n",
    "        \n",
    "        avg_scaled = avg_feat.expand_as(x)  # [B, C]\n",
    "        max_scaled = max_feat.expand_as(x)  # [B, C]\n",
    "        \n",
    "        # Concatenate both features\n",
    "        features = torch.cat([avg_scaled, max_scaled], dim=1)\n",
    "        \n",
    "        # Excitation\n",
    "        scale = self.fc1(features)\n",
    "        scale = self.bn1(scale)\n",
    "        scale = self.relu(scale)\n",
    "        scale = self.dropout(scale)\n",
    "        scale = self.fc2(scale)\n",
    "        scale = self.sigmoid(scale)\n",
    "        \n",
    "        return x * scale\n",
    "    \n",
    "class SubjectSpecificMapper(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.25):\n",
    "        super().__init__()\n",
    "        # Wider architecture with residual connection\n",
    "        self.fc1 = nn.Linear(64, 64)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.act1 = nn.LeakyReLU(negative_slope=0.2)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.bn2 = nn.BatchNorm1d(32)\n",
    "        self.act2 = nn.LeakyReLU(negative_slope=0.2)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.se = SEBlock(32, reduction=4)\n",
    "        \n",
    "        # Add a frequency attention mechanism\n",
    "        self.freq_attn = nn.Sequential(\n",
    "            nn.Linear(32, 5),  # 5 frequency bands\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First block with residual\n",
    "        identity = x\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        # Add residual if dimensions match\n",
    "        if x.shape == identity.shape:\n",
    "            x = x + identity\n",
    "            \n",
    "        # Second block\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.act2(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        # Apply SE block\n",
    "        x = self.se(x)\n",
    "        \n",
    "        # Generate frequency attention weights and apply\n",
    "        freq_weights = self.freq_attn(x)\n",
    "        \n",
    "        # L2 normalization\n",
    "        x = F.normalize(x, p=2, dim=1)\n",
    "        return x, freq_weights\n",
    "    \n",
    "class SubjectSpecificClassifier(nn.Module):\n",
    "    def __init__(self, temperature=0.5):  # Lower temperature for sharper logits\n",
    "        super().__init__()\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        \n",
    "        # Add more capacity with an extra layer\n",
    "        self.fc1 = nn.Linear(32, 32)\n",
    "        self.act = nn.GELU()  # GELU often works better than ReLU for classification\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.bn2 = nn.BatchNorm1d(32)\n",
    "        \n",
    "        self.fc2 = nn.Linear(32, 4)  # Final output for 4 emotions\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.bn1(x)\n",
    "        \n",
    "        # Extra layer for more capacity\n",
    "        identity = x\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.bn2(x)\n",
    "        \n",
    "        # Residual connection\n",
    "        x = x + identity\n",
    "        \n",
    "        # Classification layer\n",
    "        logits = self.fc2(x)\n",
    "        \n",
    "        # Apply temperature scaling\n",
    "        return logits / self.temperature\n",
    "    \n",
    "class CrossSubjectAlignmentModule(nn.Module):\n",
    "    def __init__(self, feature_dim=64):\n",
    "        super().__init__()\n",
    "        self.subject_encoder = nn.Sequential(\n",
    "            nn.Linear(feature_dim, 128),\n",
    "            # nn.BatchNorm1d(128),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(128, feature_dim)\n",
    "        )\n",
    "        \n",
    "        self.feature_projector = nn.Sequential(\n",
    "            nn.Linear(feature_dim, feature_dim),\n",
    "            # nn.BatchNorm1d(feature_dim),\n",
    "            nn.LayerNorm(feature_dim),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(feature_dim, feature_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, features, subject_ids):\n",
    "        if subject_ids is None:\n",
    "            return F.normalize(features, p=2, dim=1)\n",
    "\n",
    "        batch_size = features.size(0)\n",
    "        \n",
    "        # Get unique subjects in batch\n",
    "        unique_subjects = torch.unique(subject_ids)\n",
    "        \n",
    "        # For each subject, compute a centroid\n",
    "        centroids = []\n",
    "        for subj in unique_subjects:\n",
    "            mask = (subject_ids == subj)\n",
    "            if mask.sum() > 0:\n",
    "                subj_feats = features[mask]\n",
    "                centroid = subj_feats.mean(dim=0, keepdim=True)\n",
    "                centroids.append((subj, self.subject_encoder(centroid)))\n",
    "        \n",
    "        # Align features to their respective centroids\n",
    "        aligned_features = torch.zeros_like(features)\n",
    "        for i in range(batch_size):\n",
    "            subj = subject_ids[i]\n",
    "            for s, centroid in centroids:\n",
    "                if s == subj:\n",
    "                    # Project feature towards subject centroid\n",
    "                    feat = features[i:i+1]\n",
    "                    projected = self.feature_projector(feat)\n",
    "                    aligned_features[i] = projected + centroid.squeeze(0)\n",
    "                    break\n",
    "                    \n",
    "        return F.normalize(aligned_features, p=2, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a366f7f7",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c3cf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionRecognitionModel(nn.Module):\n",
    "    def __init__(self, num_subjects=32):\n",
    "        super().__init__()\n",
    "        # Feature extraction\n",
    "        self.common_feature_extractor = CommonFeatureExtractor()\n",
    "        \n",
    "        # Subject-specific processing\n",
    "        self.subject_mapper = SubjectSpecificMapper()\n",
    "        \n",
    "        # Cross-subject alignment\n",
    "        self.cross_subject_aligner = CrossSubjectAlignmentModule()\n",
    "        \n",
    "        # Classification\n",
    "        self.classifier = SubjectSpecificClassifier()\n",
    "        \n",
    "    def forward(self, x, subject_ids=None, return_features=False):\n",
    "        # Extract common features\n",
    "        common_features = self.common_feature_extractor(x)\n",
    "        \n",
    "        # Map to subject-specific space\n",
    "        subject_features, freq_weights = self.subject_mapper(common_features)\n",
    "        \n",
    "        # Apply cross-subject alignment if subject IDs provided\n",
    "        if subject_ids is not None:\n",
    "            aligned_features = self.cross_subject_aligner(subject_features, subject_ids)\n",
    "        else:\n",
    "            aligned_features = subject_features\n",
    "            \n",
    "        # Classification\n",
    "        logits = self.classifier(aligned_features)\n",
    "        \n",
    "        if return_features:\n",
    "            return logits, common_features, subject_features, aligned_features, freq_weights\n",
    "        else:\n",
    "            return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60e56a1",
   "metadata": {},
   "source": [
    "## LOSS FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b006579",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupervisedContrastiveLoss(nn.Module):\n",
    "    def __init__(self, temperature=0.05):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, features, labels):\n",
    "        features = F.normalize(features, dim=1)\n",
    "        batch_size = features.shape[0]\n",
    "\n",
    "        # Handle case where batch size is 1\n",
    "        if batch_size <= 1:\n",
    "            return torch.tensor(0.0, device=features.device, requires_grad=True)\n",
    "\n",
    "        sim_matrix = torch.matmul(features, features.T) / self.temperature\n",
    "        labels = labels.contiguous().view(-1, 1)\n",
    "        mask = torch.eq(labels, labels.T).float().to(features.device)\n",
    "        logits_mask = torch.ones_like(mask) - torch.eye(batch_size).to(features.device)\n",
    "        mask = mask * logits_mask\n",
    "\n",
    "        # Handle case where there are no positive pairs\n",
    "        if mask.sum() == 0:\n",
    "            return torch.tensor(0.0, device=features.device, requires_grad=True)\n",
    "\n",
    "        exp_sim = torch.exp(sim_matrix) * logits_mask\n",
    "        log_prob = sim_matrix - torch.log(exp_sim.sum(dim=1, keepdim=True) + 1e-9)\n",
    "        loss = - (mask * log_prob).sum() / (mask.sum() + 1e-9)\n",
    "        return loss\n",
    "\n",
    "class MMDLoss(nn.Module):\n",
    "    def __init__(self, kernel_mul=2.0, num_kernels=5):\n",
    "        super().__init__()\n",
    "        self.kernel_mul = kernel_mul\n",
    "        self.num_kernels = num_kernels\n",
    "\n",
    "    def gaussian_kernel(self, source, target):\n",
    "        total = torch.cat([source, target], dim=0)\n",
    "\n",
    "        # Handle small batch sizes\n",
    "        if total.shape[0] <= 1:\n",
    "            return torch.tensor(0.0, device=source.device, requires_grad=True)\n",
    "\n",
    "        total0 = total.unsqueeze(0)\n",
    "        total1 = total.unsqueeze(1)\n",
    "        L2_distance = ((total0 - total1) ** 2).sum(2)\n",
    "\n",
    "        # Prevent division by zero\n",
    "        bandwidth = torch.mean(L2_distance.detach()) + 1e-8\n",
    "        bandwidth_list = [bandwidth * (self.kernel_mul ** i) for i in range(self.num_kernels)]\n",
    "        kernels = [torch.exp(-L2_distance / bw) for bw in bandwidth_list]\n",
    "        return sum(kernels) / len(kernels)\n",
    "\n",
    "    def forward(self, source, target):\n",
    "        source = source.view(source.size(0), -1)\n",
    "        target = target.view(target.size(0), -1)\n",
    "\n",
    "        # Handle empty batches\n",
    "        if source.shape[0] == 0 or target.shape[0] == 0:\n",
    "            return torch.tensor(0.0, device=source.device, requires_grad=True)\n",
    "\n",
    "        kernels = self.gaussian_kernel(source, target)\n",
    "        batch_size = source.size(0)\n",
    "        XX = kernels[:batch_size, :batch_size]\n",
    "        YY = kernels[batch_size:, batch_size:]\n",
    "        XY = kernels[:batch_size, batch_size:]\n",
    "        YX = kernels[batch_size:, :batch_size]\n",
    "        return torch.mean(XX + YY - XY - YX)\n",
    "\n",
    "class ContrastiveLossLcon2(nn.Module):\n",
    "    def __init__(self, feature_dim=32, num_classes=4, tau=0.1, gamma=0.5, queue_size=1024):\n",
    "        super().__init__()\n",
    "        self.tau = tau\n",
    "        self.gamma = gamma\n",
    "        self.prototypes = nn.Parameter(torch.randn(num_classes, feature_dim))\n",
    "        self.register_buffer(\"queue\", torch.randn(queue_size, feature_dim))\n",
    "        self.queue = F.normalize(self.queue, dim=-1)\n",
    "\n",
    "    def forward(self, z_t, pseudo_labels):\n",
    "        # Handle empty batches\n",
    "        if z_t.shape[0] == 0:\n",
    "            return torch.tensor(0.0, device=z_t.device, requires_grad=True)\n",
    "\n",
    "        z_t = F.normalize(z_t, dim=-1)\n",
    "        device = z_t.device\n",
    "        pseudo_labels = pseudo_labels.to(device)\n",
    "\n",
    "        pos_proto = self.prototypes.to(device)[pseudo_labels]\n",
    "\n",
    "        # Compute positive and negative logits\n",
    "        pos_logits = torch.sum(z_t * pos_proto, dim=-1) / self.tau\n",
    "\n",
    "        # Handle case where queue is empty\n",
    "        if self.queue.shape[0] == 0:\n",
    "            return self.gamma * F.cross_entropy(pos_logits.unsqueeze(1), torch.zeros(z_t.size(0), dtype=torch.long, device=device))\n",
    "\n",
    "        neg_logits = torch.matmul(z_t, self.queue.to(device).T) / self.tau\n",
    "        logits = torch.cat([pos_logits.unsqueeze(1), neg_logits], dim=1)\n",
    "        labels = torch.zeros(z_t.size(0), dtype=torch.long).to(device)\n",
    "\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        self._dequeue_and_enqueue(z_t)\n",
    "        return self.gamma * loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _dequeue_and_enqueue(self, embeddings):\n",
    "        embeddings = embeddings.detach().to(self.queue.device)\n",
    "        batch_size = embeddings.size(0)\n",
    "        queue_size = self.queue.size(0)\n",
    "\n",
    "        if batch_size >= queue_size:\n",
    "            self.queue = embeddings[-queue_size:]\n",
    "        else:\n",
    "            self.queue = torch.cat([self.queue[batch_size:], embeddings], dim=0)\n",
    "\n",
    "class GeneralizedCrossEntropy(nn.Module):\n",
    "    def __init__(self, q=0.7, weight=None):\n",
    "        super().__init__()\n",
    "        self.q = q\n",
    "        self.weight = weight  # class weights (tensor)\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "\n",
    "        # Handle empty batches\n",
    "        if targets.shape[0] == 0:\n",
    "            return torch.tensor(0.0, device=logits.device, requires_grad=True)\n",
    "\n",
    "        targets_onehot = F.one_hot(targets, num_classes=probs.shape[1]).float()\n",
    "        probs = torch.sum(probs * targets_onehot, dim=1)\n",
    "\n",
    "        if self.weight is not None:\n",
    "            weights = self.weight[targets]\n",
    "            loss = (1 - probs ** self.q) / self.q\n",
    "            return (weights * loss).mean()\n",
    "        else:\n",
    "            return ((1 - probs ** self.q) / self.q).mean()\n",
    "\n",
    "class FocalLossWithSmoothing(nn.Module):\n",
    "    def __init__(self, gamma=2.0, smoothing=0.1, weight=None, reduction='mean'):\n",
    "        \"\"\"\n",
    "        Focal Loss with optional label smoothing.\n",
    "        Args:\n",
    "            gamma (float): focusing parameter for modulating factor (1 - p_t)\n",
    "            smoothing (float): label smoothing factor\n",
    "            weight (torch.Tensor): class weights\n",
    "            reduction (str): 'mean' or 'sum'\n",
    "        \"\"\"\n",
    "        super(FocalLossWithSmoothing, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.smoothing = smoothing\n",
    "        self.weight = weight\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        num_classes = logits.size(1)\n",
    "\n",
    "        # Convert to one-hot with label smoothing\n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(logits)\n",
    "            true_dist.fill_(self.smoothing / (num_classes - 1))\n",
    "            true_dist.scatter_(1, targets.data.unsqueeze(1), 1.0 - self.smoothing)\n",
    "\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        probs = torch.clamp(probs, 1e-6, 1.0)  # avoid log(0)\n",
    "\n",
    "        # Focal loss component\n",
    "        log_probs = torch.log(probs)\n",
    "        focal_weight = (1 - probs) ** self.gamma\n",
    "\n",
    "        loss = -true_dist * focal_weight * log_probs\n",
    "\n",
    "        # Apply class weights\n",
    "        if self.weight is not None:\n",
    "            weight = self.weight.unsqueeze(0)  # (1, num_classes)\n",
    "            loss = loss * weight\n",
    "\n",
    "        # loss = loss.sum(dim=1)  # sum over classes\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return loss.mean()\n",
    "        # elif self.reduction == 'sum':\n",
    "        #     return loss.sum()\n",
    "        else:\n",
    "            return loss.sum()  # no reduction\n",
    "\n",
    "class PrototypeContrastiveLoss(nn.Module):\n",
    "    def __init__(self, feature_dim=32, num_classes=4, tau=0.1):\n",
    "        super().__init__()\n",
    "        self.feature_dim = feature_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.tau = tau\n",
    "        # Initialize class prototypes\n",
    "        self.register_buffer('prototypes', torch.zeros(num_classes, feature_dim))\n",
    "        self.register_buffer('prototype_counts', torch.zeros(num_classes))\n",
    "        \n",
    "    def forward(self, features, labels):\n",
    "        # Update prototypes with moving average\n",
    "        for c in range(self.num_classes):\n",
    "            class_mask = (labels == c)\n",
    "            if class_mask.sum() > 0:\n",
    "                class_features = features[class_mask]\n",
    "                class_mean = class_features.mean(0)\n",
    "                \n",
    "                # Update prototype with momentum\n",
    "                momentum = 0.9\n",
    "                self.prototypes[c] = momentum * self.prototypes[c] + (1 - momentum) * class_mean\n",
    "                self.prototype_counts[c] += 1\n",
    "        \n",
    "        # Normalize prototypes\n",
    "        valid_prototypes = self.prototype_counts > 0\n",
    "        if valid_prototypes.sum() > 0:\n",
    "            self.prototypes[valid_prototypes] = F.normalize(self.prototypes[valid_prototypes], dim=1)\n",
    "        \n",
    "        # Compute distances to prototypes\n",
    "        features_norm = F.normalize(features, dim=1)\n",
    "        logits = features_norm @ self.prototypes.t() / self.tau\n",
    "        \n",
    "        # Compute contrastive loss\n",
    "        labels_onehot = F.one_hot(labels, num_classes=self.num_classes).float()\n",
    "        loss = -torch.sum(labels_onehot * F.log_softmax(logits, dim=1)) / labels.size(0)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "class DynamicWeightedLoss(nn.Module):\n",
    "    \"\"\"Loss function with dynamically adjusted class weights based on performance\"\"\"\n",
    "    def __init__(self, num_classes=4, initial_weights=None, momentum=0.9):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.focal_loss = FocalLossWithSmoothing(gamma=2.0, smoothing=0.1)\n",
    "        self.register_buffer('weights', torch.ones(num_classes) if initial_weights is None else initial_weights)\n",
    "        self.register_buffer('class_accuracies', torch.ones(num_classes))\n",
    "        self.momentum = momentum\n",
    "        \n",
    "    def update_weights(self, logits, targets):\n",
    "        \"\"\"Update weights based on per-class accuracy\"\"\"\n",
    "        with torch.no_grad():\n",
    "            preds = logits.argmax(dim=1)\n",
    "            for c in range(self.num_classes):\n",
    "                # Find samples of this class\n",
    "                class_mask = (targets == c)\n",
    "                if class_mask.sum() > 0:\n",
    "                    # Calculate accuracy for this class\n",
    "                    correct = (preds[class_mask] == targets[class_mask]).float().mean()\n",
    "                    # Update running average of class accuracy\n",
    "                    self.class_accuracies[c] = self.momentum * self.class_accuracies[c] + (1 - self.momentum) * correct\n",
    "            \n",
    "            # Inverse of accuracy as weight (lower accuracy = higher weight)\n",
    "            new_weights = 1.0 / (self.class_accuracies + 1e-5)\n",
    "            # Normalize weights\n",
    "            new_weights = new_weights / new_weights.sum() * self.num_classes\n",
    "            self.weights = new_weights\n",
    "            \n",
    "    def forward(self, logits, targets):\n",
    "        # Update weights based on current batch performance\n",
    "        self.update_weights(logits, targets)\n",
    "        # Apply weights to focal loss\n",
    "        self.focal_loss.weight = self.weights\n",
    "        return self.focal_loss(logits, targets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9afad78",
   "metadata": {},
   "source": [
    "## COMBINED LOSS FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645c5af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionRecognitionLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.6, beta=0.3, gamma=0.1, temperature=0.07):\n",
    "        super().__init__()\n",
    "        # Classification loss with dynamic weighting\n",
    "        self.cls_loss = DynamicWeightedLoss(num_classes=4)\n",
    "        \n",
    "        # Enhanced contrastive loss for better representation learning\n",
    "        self.contrastive_loss = SupervisedContrastiveLoss(temperature=temperature)\n",
    "        \n",
    "        # Domain alignment loss to reduce subject-specific variations\n",
    "        self.domain_loss = MMDLoss(kernel_mul=2.0, num_kernels=5)\n",
    "        \n",
    "        # Prototype contrastive loss for better class separation\n",
    "        self.proto_loss = PrototypeContrastiveLoss(feature_dim=32, num_classes=4, tau=0.1)\n",
    "        \n",
    "        # Loss weights\n",
    "        self.alpha = alpha  # weight for classification loss\n",
    "        self.beta = beta    # weight for contrastive loss\n",
    "        self.gamma = gamma  # weight for domain alignment loss\n",
    "    \n",
    "    def forward(self, logits, common_features, subject_features, aligned_features, \n",
    "                labels, subject_ids=None):\n",
    "        \"\"\"\n",
    "        Multi-objective loss calculation\n",
    "        \n",
    "        Args:\n",
    "            logits: Classification outputs\n",
    "            common_features: Features from CommonFeatureExtractor\n",
    "            subject_features: Features from SubjectSpecificMapper\n",
    "            aligned_features: Features after cross-subject alignment\n",
    "            labels: Emotion labels\n",
    "            subject_ids: Subject identifiers\n",
    "        \"\"\"\n",
    "        # Classification loss\n",
    "        cls_loss = self.cls_loss(logits, labels)\n",
    "        \n",
    "        # Contrastive loss on aligned features\n",
    "        cont_loss = self.contrastive_loss(aligned_features, labels)\n",
    "        \n",
    "        # Prototype loss for better emotion clustering\n",
    "        proto_loss = self.proto_loss(subject_features, labels)\n",
    "        \n",
    "        # Subject domain alignment loss (if subject IDs are provided)\n",
    "        domain_loss = torch.tensor(0.0, device=logits.device)\n",
    "        if subject_ids is not None:\n",
    "            # Group features by subject\n",
    "            \n",
    "            unique_subjects = torch.unique(subject_ids)\n",
    "            if len(unique_subjects) > 1:  # Only compute if multiple subjects\n",
    "                for i in range(len(unique_subjects)):\n",
    "                    for j in range(i+1, len(unique_subjects)):\n",
    "                        s1, s2 = unique_subjects[i], unique_subjects[j]\n",
    "                        s1_feats = common_features[subject_ids == s1]\n",
    "                        s2_feats = common_features[subject_ids == s2]\n",
    "                        if len(s1_feats) > 0 and len(s2_feats) > 0:\n",
    "                            domain_loss += self.domain_loss(s1_feats, s2_feats)\n",
    "        \n",
    "        # Weighted combination\n",
    "        total_loss = (self.alpha * cls_loss + \n",
    "                      self.beta * (cont_loss + proto_loss) + \n",
    "                      self.gamma * domain_loss)\n",
    "        \n",
    "        return total_loss, {\n",
    "            'cls_loss': cls_loss.item(),\n",
    "            'cont_loss': cont_loss.item(),\n",
    "            'proto_loss': proto_loss.item(),\n",
    "            'domain_loss': domain_loss.item(),\n",
    "            'total_loss': total_loss.item()\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758dc9e3",
   "metadata": {},
   "source": [
    "## TRAINING LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd21e028",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_batch(dataset, exclude, batch_size=32):\n",
    "    \"\"\"Get a batch of data from target subjects\"\"\"\n",
    "    for subj in range(32):\n",
    "        if subj != exclude:\n",
    "            data = dataset.get_subject_data(subj)\n",
    "            if len(data) >= batch_size:\n",
    "                indices = torch.randperm(len(data))[:batch_size]\n",
    "                x, y = zip(*[data[i] for i in indices])\n",
    "                return torch.stack(x), torch.tensor(y)\n",
    "\n",
    "    # Fallback: Return a small batch if no subject has enough data\n",
    "    all_data = []\n",
    "    for subj in range(32):\n",
    "        if subj != exclude:\n",
    "            all_data.extend(dataset.get_subject_data(subj))\n",
    "\n",
    "    if len(all_data) == 0:\n",
    "        # Empty tensor with correct shape as a fallback\n",
    "        empty_sample = next(iter(dataset))\n",
    "        return torch.zeros((0, *empty_sample[0].shape), dtype=torch.float32), torch.zeros(0, dtype=torch.long)\n",
    "\n",
    "    indices = torch.randperm(len(all_data))[:min(batch_size, len(all_data))]\n",
    "    x, y = zip(*[all_data[i] for i in indices])\n",
    "    return torch.stack(x), torch.tensor(y)\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, subject_idx):\n",
    "    \"\"\"Create and save confusion matrix plot\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1, 2, 3])\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'Confusion Matrix - Subject {subject_idx+1}')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.savefig(f'plots/new_new_confmat_subject{subject_idx+1}.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbe0ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Subset, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "os.makedirs(\"plots\", exist_ok=True)\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2929e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_eeg(x, p=0.5):\n",
    "    orig_x = x.clone()  # Keep original for mixup\n",
    "    \n",
    "    # Existing augmentations with refinements\n",
    "    if torch.rand(1, device=x.device) < p:\n",
    "        # Gaussian noise with adaptive magnitude\n",
    "        noise_level = 0.01 + 0.02 * torch.rand(1, device=x.device)\n",
    "        noise = torch.randn_like(x) * noise_level\n",
    "        x = x + noise\n",
    "        \n",
    "    if torch.rand(1, device=x.device) < p:\n",
    "        # Random scaling with more variation\n",
    "        scale = 0.9 + 0.2 * torch.rand(1, device=x.device)\n",
    "        x = x * scale\n",
    "        \n",
    "    if torch.rand(1, device=x.device) < p:\n",
    "        # Improved channel dropout (spatially correlated)\n",
    "        # More realistic - neighboring channels often affected together\n",
    "        num_channels = x.size(1)\n",
    "        center_channel = torch.randint(0, num_channels, (1,)).item()\n",
    "        width = torch.randint(1, max(2, num_channels // 4), (1,)).item()\n",
    "        \n",
    "        # Create a Gaussian window centered on the selected channel\n",
    "        channel_weights = torch.ones(num_channels, device=x.device)\n",
    "        for ch in range(num_channels):\n",
    "            dist = min(abs(ch - center_channel), num_channels - abs(ch - center_channel))\n",
    "            if dist < width:\n",
    "                channel_weights[ch] = 0.2  # Reduce amplitude of nearby channels\n",
    "        \n",
    "        x = x * channel_weights.unsqueeze(0).unsqueeze(-1)\n",
    "        \n",
    "    if torch.rand(1, device=x.device) < p:\n",
    "        # Frequency-selective noise (target specific frequency bands)\n",
    "        # Simple approximation - in practice you might use FFT\n",
    "        batch_size = x.size(0)\n",
    "        for i in range(batch_size):\n",
    "            segment_length = torch.randint(3, 10, (1,)).item()\n",
    "            for j in range(0, x.size(-1), segment_length):\n",
    "                if j + segment_length <= x.size(-1) and torch.rand(1) < 0.3:\n",
    "                    noise_segment = torch.randn(x.size(1), segment_length, device=x.device) * 0.05\n",
    "                    if j + segment_length <= x.size(-1):\n",
    "                        x[i, :, j:j+segment_length] += noise_segment\n",
    "    \n",
    "    # Additional EEG-specific augmentations\n",
    "    if torch.rand(1, device=x.device) < p:\n",
    "        # Simulate baseline drift\n",
    "        end_value = (torch.rand(1, device=x.device) * 0.1).item()\n",
    "        drift = torch.linspace(0, end_value, x.size(-1), device=x.device)\n",
    "        drift = drift.unsqueeze(0).unsqueeze(1).expand_as(x)\n",
    "        x = x + drift\n",
    "    \n",
    "    # Mixup augmentation (blend two samples)\n",
    "    if torch.rand(1, device=x.device) < 0.3:  # 30% chance of mixup\n",
    "        # Shuffle the batch to create pairs\n",
    "        idx = torch.randperm(x.size(0), device=x.device)\n",
    "        mixed_x = x.clone()\n",
    "        lam = torch.distributions.Beta(0.4, 0.4).sample().to(x.device)\n",
    "        mixed_x = lam * x + (1 - lam) * x[idx]\n",
    "        \n",
    "        # Return the mixed samples along with mixing coefficient\n",
    "        # Your training loop will need to be modified to handle this\n",
    "        return mixed_x, idx, lam\n",
    "        \n",
    "    return x, None, None  # No mixup performed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1164e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_subject_mmd_loss(features, subject_ids, mmd_loss_fn):\n",
    "    domain_loss = torch.tensor(0.0, device=features.device)\n",
    "    unique_subjects = torch.unique(subject_ids)\n",
    "\n",
    "    if len(unique_subjects) <= 1:\n",
    "        return domain_loss\n",
    "\n",
    "    for i in range(len(unique_subjects)):\n",
    "        for j in range(i + 1, len(unique_subjects)):\n",
    "            s1, s2 = unique_subjects[i], unique_subjects[j]\n",
    "            s1_feats = features[subject_ids == s1]\n",
    "            s2_feats = features[subject_ids == s2]\n",
    "            if s1_feats.size(0) > 1 and s2_feats.size(0) > 1:\n",
    "                domain_loss += mmd_loss_fn(s1_feats, s2_feats)\n",
    "\n",
    "    return domain_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c739a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ Training Function ------------------\n",
    "def train_all_subjects(feature_path, label_path, num_epochs=500, batch_size=64, warmup_epochs=150):\n",
    "    print(\"\\nðŸš€ Training on all subjects together with contrastive + domain adaptation\")\n",
    "    \n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    val_losses = []\n",
    "    val_accs = []\n",
    "\n",
    "    dataset = DEAPDataset(feature_path, label_path, normalize=True, train=True)\n",
    "    indices = list(range(len(dataset)))\n",
    "    targets = [label for _, label in dataset]\n",
    "    train_idx, test_idx, _, _ = train_test_split(\n",
    "        indices, targets, test_size=0.2, stratify=targets, random_state=42\n",
    "    )\n",
    "\n",
    "    train_set = Subset(dataset, train_idx)\n",
    "    test_set = Subset(DEAPDataset(feature_path,label_path,normalize=True,train=False), test_idx)\n",
    "\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Init models\n",
    "    cfe = CommonFeatureExtractor().to(device)\n",
    "    sfe = SubjectSpecificMapper().to(device)\n",
    "    ssc = SubjectSpecificClassifier(temperature = 2.0).to(device)\n",
    "    # cs_align = CrossSubjectAlignmentModule(feature_dim = 64).to(device)\n",
    "\n",
    "    # Init losses\n",
    "\n",
    "    labels_np = np.array([label for _, label in dataset])\n",
    "    # class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(labels_np), y=labels_np)\n",
    "    # class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "    \n",
    "    class_counts = np.bincount(labels_np)\n",
    "    print(f\"Classs distribution: {class_counts}\")\n",
    "    \n",
    "    initial_weights = compute_class_weight('balanced', classes=np.unique(labels_np), y=labels_np)\n",
    "    class_weights = torch.tensor(initial_weights, dtype=torch.float32).to(device)\n",
    "    print(f\"Class weights: {class_weights}\")\n",
    "    \n",
    "    \n",
    "\n",
    "    loss_con1 = SupervisedContrastiveLoss(temperature=0.07).to(device)\n",
    "    loss_mmd = MMDLoss().to(device)\n",
    "    loss_con2 = ContrastiveLossLcon2().to(device)\n",
    "    # loss_cls = GeneralizedCrossEntropy(q=0.5, weight=class_weights).to(device)\n",
    "    # loss_cls = FocalLossWithSmoothing(gamma=2.0, smoothing=0.1, weight=class_weights).to(device)\n",
    "    loss_cls = DynamicWeightedLoss(num_classes=4, initial_weights=class_weights).to(device)\n",
    "    loss_proto = PrototypeContrastiveLoss(feature_dim=32 , num_classes=4 , tau=0.1).to(device)\n",
    "\n",
    "\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        list(cfe.parameters()) + list(sfe.parameters()) + list(ssc.parameters()),\n",
    "        lr=2e-3, weight_decay=1e-3\n",
    "    )\n",
    "    \n",
    "\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5, verbose=True , min_lr=1e-6 )\n",
    "\n",
    "    pateince = 10\n",
    "    early_stop_counter = 0\n",
    "    best_acc = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        cfe.train()\n",
    "        sfe.train()\n",
    "        ssc.train()\n",
    "        # cs_align.train()\n",
    "\n",
    "        epoch_loss, correct, total = 0, 0, 0\n",
    "        total_loss, total_correct, total = 0, 0, 0\n",
    "        mmd_weight = max(0.2*(1 - epoch / num_epochs), 0.05)  # Decrease MMD weight over epochs\n",
    "        \n",
    "        class_correct = torch.zeros(4).to(device)\n",
    "        class_total = torch.zeros(4).to(device)\n",
    "        \n",
    "        delta =0.05\n",
    "        alpha = min(epoch / num_epochs *2 , 1.0)\n",
    "        beta = 0.1\n",
    "\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            subject_ids = xb[:, 0, 0].long()\n",
    "            \n",
    "            xb_aug, _, _ = augment_eeg(xb.clone())\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            z_common_org = cfe(xb)\n",
    "            # z_common_org = cs_align(z_common_org , subject_ids)\n",
    "            \n",
    "            z_common_aug = cfe(xb_aug)\n",
    "            # z_common_aug = cs_align(z_common_aug , subject_ids)\n",
    "            \n",
    "            z_subject, _ = sfe(z_common_org)\n",
    "            logits = ssc(z_subject)\n",
    "\n",
    "            if epoch < warmup_epochs:\n",
    "                loss = 0.5 * loss_con1(z_common_org, yb) + loss_cls(logits, yb)\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    \n",
    "                    z_tmp = cfe(xb)\n",
    "                    z_tmp , _ = sfe(z_tmp)\n",
    "                    pseudo_logits = ssc(z_tmp)\n",
    "                    pseudo_probs = F.softmax(pseudo_logits, dim=1)\n",
    "                    pseudo_entropy = -torch.sum(pseudo_probs * torch.log(pseudo_probs + 1e-9), dim=1)\n",
    "                    \n",
    "                    probs = F.softmax(logits, dim=1)\n",
    "                    confidence = torch.max(probs, dim=1)[0]\n",
    "                    penalty = confidence.mean()\n",
    "                    # pseudo_labels = pseudo_logits.argmax(dim=1)\n",
    "                \n",
    "                    # Adaptive threshold: starts strict, loosens\n",
    "                entropy_threshold = max(0.9, 1.5 - epoch / num_epochs)\n",
    "                \n",
    "                confident_mask = pseudo_entropy < entropy_threshold\n",
    "                \n",
    "                z_subject_filtered = z_subject[confident_mask].detach()\n",
    "                pseudo_labels_filtered = pseudo_logits.argmax(dim=1)[confident_mask].detach()\n",
    "                \n",
    "                if z_subject_filtered.size(0) > 0:\n",
    "                    # contrastive_loss = loss_con2(z_subject_filtered , pseudo_labels_filtered)\n",
    "                    contrastive_loss = loss_proto(z_subject_filtered , pseudo_labels_filtered)\n",
    "                else:\n",
    "                    contrastive_loss = torch.tensor(0.0 , device=device)\n",
    "                    \n",
    "                mmd_loss_aug = loss_mmd(z_common_org, z_common_aug)\n",
    "                # mmd_loss_subject = compute_subject_mmd_loss(z_common_org, subject_ids, loss_mmd)\n",
    "                          \n",
    "                loss = 1.0 * loss_cls(logits, yb) + \\\n",
    "                       alpha * (0.5 * contrastive_loss) + \\\n",
    "                       alpha * (mmd_weight * mmd_loss_aug ) + \\\n",
    "                       beta * penalty\n",
    "                    #    delta * mmd_loss_subject + \\\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(list(cfe.parameters()) + list(sfe.parameters()) + list(ssc.parameters()), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            preds = logits.argmax(1)\n",
    "            correct += (logits.argmax(1) == yb).sum().item()\n",
    "            total += yb.size(0)\n",
    "            \n",
    "            for c in range(4):\n",
    "                class_mask = (yb == c)\n",
    "                if class_mask.sum() > 0:\n",
    "                    class_correct[c] += (preds[class_mask] == yb[class_mask]).sum().item()\n",
    "                    class_total[c] += class_mask.sum().item()\n",
    "        \n",
    "        print(\"\\nPer-class accuracy:\")\n",
    "        for c in range(4):\n",
    "            if class_total[c] > 0:\n",
    "                class_acc = class_correct[c] / class_total[c]\n",
    "                print(f\"Class {c}: {class_acc:.4f} ({int(class_total[c])} / {int(class_total[c])})\")\n",
    "            else:\n",
    "                print(f\"Class {c}: No samples\")\n",
    "        \n",
    "        print(f\"Current class weights: {loss_cls.weights.cpu().numpy()}\")\n",
    "            \n",
    "        train_loss = epoch_loss/ len(train_loader)\n",
    "        train_acc = correct / total\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] - Loss: {train_loss:.4f} - Train Acc: {train_acc:.4f}\")\n",
    "\n",
    "        # Validation\n",
    "        if (epoch + 1) % 5 == 0 or epoch == num_epochs - 1:\n",
    "            cfe.eval(); sfe.eval(); ssc.eval(); \n",
    "            val_loss = 0\n",
    "            all_preds, all_labels = [], []\n",
    "            with torch.no_grad():\n",
    "                for xb, yb in test_loader:\n",
    "                    yb = yb.to(device)\n",
    "                    xb = xb.to(device)\n",
    "                    z = cfe(xb)\n",
    "                    z, _ = sfe(z)\n",
    "                    logits = ssc(z)\n",
    "                    \n",
    "                    val_loss += loss_cls(logits, yb).item()\n",
    "                    \n",
    "                    all_preds.extend(logits.argmax(1).cpu().numpy())\n",
    "                    all_labels.extend(yb.cpu().numpy())\n",
    "            \n",
    "            val_loss = val_loss/len(test_loader)\n",
    "            val_acc = accuracy_score(all_labels, all_preds)\n",
    "            \n",
    "            val_losses.append(val_loss)\n",
    "            val_accs.append(val_acc)\n",
    "            \n",
    "            print(f\"ðŸ“Š Val Loss: {val_loss:.4f} - Val Acc: {val_acc:.4f}\")\n",
    "            scheduler.step(val_acc)\n",
    "            \n",
    "            if val_acc > best_acc:\n",
    "                best_acc = val_acc\n",
    "                early_stop_counter = 0\n",
    "                torch.save({\n",
    "                    'cfe': cfe.state_dict(),\n",
    "                    'sfe': sfe.state_dict(),\n",
    "                    'ssc': ssc.state_dict(),\n",
    "                    # 'cs_align': cs_align.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'epoch': epoch + 1\n",
    "                }, f\"checkpoints/new_new_best_model.pt\")\n",
    "                print(f\"ðŸ† New best accuracy: {best_acc:.4f}\")\n",
    "            else:\n",
    "                early_stop_counter = 0\n",
    "                print(f\"Early stopping counter: {early_stop_counter}\")\n",
    "                if early_stop_counter >= pateince:\n",
    "                    print(f\"Early stopping triggered after {pateince} epochs without improvement.\")\n",
    "                    break\n",
    "\n",
    "    print(\"\\nâœ… Training complete.\")\n",
    "    print(f\"Best Val Accuracy: {best_acc:.4f}\")\n",
    "\n",
    "    # Final Confusion Matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds, labels=[0, 1, 2, 3])\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix - All Subjects')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.savefig('plots/new_new_confmat_all_subjects.png')\n",
    "    plt.close()\n",
    "    \n",
    "    plot_training_curves(train_losses, train_accs, val_losses, val_accs, save_path=\"plots/new_new_final_training_curves.png\")\n",
    "\n",
    "    return best_acc, all_labels, all_preds , train_losses , train_accs , val_losses , val_accs\n",
    "\n",
    "def plot_training_curves(train_losses, train_accs, val_losses, val_accs, save_path=None):\n",
    "    \"\"\"Plot training and validation curves\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot losses\n",
    "    ax1.plot(train_losses, label='Training Loss', color='blue')\n",
    "    if val_losses:\n",
    "        # Plot validation points where we have them\n",
    "        val_indices = np.linspace(0, len(train_losses)-1, len(val_losses)).astype(int)\n",
    "        ax1.plot(val_indices, val_losses, label='Validation Loss', color='red', marker='o')\n",
    "    ax1.set_title('Loss Curves')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "\n",
    "    # Plot accuracies\n",
    "    ax2.plot(train_accs, label='Training Accuracy', color='blue')\n",
    "    if val_accs:\n",
    "        # Plot validation points where we have them\n",
    "        val_indices = np.linspace(0, len(train_accs)-1, len(val_accs)).astype(int)\n",
    "        ax2.plot(val_indices, val_accs, label='Validation Accuracy', color='red', marker='o')\n",
    "    ax2.set_title('Accuracy Curves')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e072563d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    feature_path = r\"E:\\FYP\\Finalise Fyp\\EEg-based-Emotion-Recognition\\de_features.npy\"\n",
    "    label_path = r\"E:\\FYP\\Finalise Fyp\\EEg-based-Emotion-Recognition\\de_labels.npy\"\n",
    "    train_all_subjects(feature_path, label_path, num_epochs=400, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc5b0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_trained_model(checkpoint_path=\"E:\\FYP\\Finalise Fyp\\EEg-based-Emotion-Recognition\\FYP_2\\Hamza\\checkpoints/new_new_best_model.pt\"):\n",
    "    cfe = CommonFeatureExtractor().to(device)\n",
    "    sfe = SubjectSpecificMapper().to(device)\n",
    "    ssc = SubjectSpecificClassifier().to(device)\n",
    "\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    cfe.load_state_dict(checkpoint['cfe'])\n",
    "    sfe.load_state_dict(checkpoint['sfe'])\n",
    "    ssc.load_state_dict(checkpoint['ssc'])\n",
    "\n",
    "    cfe.eval()\n",
    "    sfe.eval()\n",
    "    ssc.eval()\n",
    "    return cfe, sfe, ssc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5008f3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "def test_model(feature_path, label_path, checkpoint_path=\"E:/FYP/Finalise Fyp/EEg-based-Emotion-Recognition/FYP_2/Hamza/checkpoints/new_new_best_model.pt\", batch_size=64):\n",
    "    print(\"ðŸ” Testing saved model...\")\n",
    "\n",
    "    # Load the model\n",
    "    cfe, sfe, ssc = load_trained_model(checkpoint_path)\n",
    "\n",
    "    # Prepare dataset\n",
    "    dataset = DEAPDataset(feature_path, label_path, normalize=True)\n",
    "    indices = list(range(len(dataset)))\n",
    "    labels = [label for _, label in dataset]\n",
    "    _, test_idx, _, _ = train_test_split(indices, labels, test_size=0.2, stratify=labels, random_state=42)\n",
    "    test_set = Subset(dataset, test_idx)\n",
    "    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in test_loader:\n",
    "            xb = xb.to(device)\n",
    "            z,_ = sfe(cfe(xb))\n",
    "            logits = ssc(z)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(yb.numpy())\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    print(f\"âœ… Test Accuracy: {acc:.4f}\")\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds, labels=[0, 1, 2, 3])\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Test Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.savefig(\"plots/new_new_test_confmat.png\")\n",
    "    plt.close()\n",
    "    \n",
    "\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(all_labels, all_preds, average=None)\n",
    "    for i, (p, r, f) in enumerate(zip(precision, recall, f1)):\n",
    "        print(f\"Class {i}: Precision={p:.4f}, Recall={r:.4f}, F1={f:.4f}\")\n",
    "\n",
    "\n",
    "    return acc, all_labels, all_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb44b51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    feature_path = \"E:/FYP/Finalise Fyp/EEg-based-Emotion-Recognition/de_features.npy\"\n",
    "    label_path = \"E:/FYP/Finalise Fyp/EEg-based-Emotion-Recognition/de_labels.npy\"\n",
    "    test_model(feature_path, label_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c7232d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def plot_tsne_features(cfe, sfe, ssc, feature_path, label_path, layer=\"sfe\", num_samples=2000):\n",
    "    dataset = DEAPDataset(feature_path, label_path, normalize=True)\n",
    "    indices = list(range(len(dataset)))\n",
    "    labels = [label for _, label in dataset]\n",
    "    _, test_idx, _, _ = train_test_split(indices, labels, test_size=0.2, stratify=labels, random_state=42)\n",
    "    test_set = Subset(dataset, test_idx)\n",
    "    test_loader = DataLoader(test_set, batch_size=128, shuffle=True)\n",
    "\n",
    "    all_feats, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in test_loader:\n",
    "            xb = xb.to(device)\n",
    "            z_cfe= cfe(xb)                     # 64-d\n",
    "            z_sfe,_ = sfe(z_cfe)                  # 32-d\n",
    "            feats = z_sfe if layer == \"sfe\" else z_cfe\n",
    "            all_feats.append(feats.cpu())\n",
    "            all_labels.append(yb)\n",
    "\n",
    "            if len(torch.cat(all_feats)) >= num_samples:\n",
    "                break\n",
    "\n",
    "    all_feats = torch.cat(all_feats)[:num_samples]\n",
    "    all_labels = torch.cat(all_labels)[:num_samples]\n",
    "\n",
    "    tsne = TSNE(n_components=2, perplexity=30, learning_rate=200, n_iter=1000)\n",
    "    tsne_feats = tsne.fit_transform(all_feats)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    scatter = plt.scatter(tsne_feats[:, 0], tsne_feats[:, 1], c=all_labels, cmap='tab10', alpha=0.7)\n",
    "    plt.legend(*scatter.legend_elements(), title=\"Classes\")\n",
    "    plt.title(f\"t-SNE of {'SFE' if layer == 'sfe' else 'CFE'} features\")\n",
    "    plt.savefig(f\"plots/new_new_tsne_{layer}_features.png\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca7b54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    feature_path = \"E:/FYP/Finalise Fyp/EEg-based-Emotion-Recognition/de_features.npy\"\n",
    "    label_path = \"E:/FYP/Finalise Fyp/EEg-based-Emotion-Recognition/de_labels.npy\"\n",
    "\n",
    "    # Load the model\n",
    "    cfe, sfe, ssc = load_trained_model(\"E:/FYP/Finalise Fyp/EEg-based-Emotion-Recognition/FYP_2/Hamza/checkpoints/new_new_best_model.pt\")\n",
    "\n",
    "    # Plot t-SNE from either 'cfe' or 'sfe'\n",
    "    plot_tsne_features(cfe, sfe, ssc, feature_path, label_path, layer=\"sfe\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9079623e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tsne_features(cfe, sfe, ssc, feature_path, label_path, layer=\"cfe\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d40fcd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
