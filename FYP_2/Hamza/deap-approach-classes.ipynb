{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-03-09T18:07:15.828610Z",
     "iopub.status.busy": "2025-03-09T18:07:15.828273Z",
     "iopub.status.idle": "2025-03-09T18:07:20.032608Z",
     "shell.execute_reply": "2025-03-09T18:07:20.031665Z",
     "shell.execute_reply.started": "2025-03-09T18:07:15.828582Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import os\n",
    "import scipy.io\n",
    "from scipy.signal import butter, lfilter\n",
    "import scipy.signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T18:07:20.034229Z",
     "iopub.status.busy": "2025-03-09T18:07:20.033782Z",
     "iopub.status.idle": "2025-03-09T18:07:59.470680Z",
     "shell.execute_reply": "2025-03-09T18:07:59.469917Z",
     "shell.execute_reply.started": "2025-03-09T18:07:20.034198Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EEG Data Shape: (32, 40, 40, 8064)\n",
      "Labels Shape: (32, 40, 4)\n"
     ]
    }
   ],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "\n",
    "data_path = \"E:\\\\FYP\\\\Finalise Fyp\\\\EEg-based-Emotion-Recognition\\\\data_preprocessed_matlab\\\\\"\n",
    "\n",
    "# Initialize arrays\n",
    "subject_data = np.zeros((32, 40, 40, 8064), dtype=np.float32)\n",
    "subject_labels = np.zeros((32, 40, 4), dtype=np.float32)\n",
    "\n",
    "# Load all 32 subjects\n",
    "for i in range(1, 33):\n",
    "    mat = scipy.io.loadmat(f\"{data_path}s{i:02d}.mat\")\n",
    "    subject_data[i-1] = mat[\"data\"]\n",
    "    subject_labels[i-1] = mat[\"labels\"]\n",
    "\n",
    "# Print shapes to confirm\n",
    "print(\"EEG Data Shape:\", subject_data.shape)  \n",
    "print(\"Labels Shape:\", subject_labels.shape)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Normalizing EEG data (by Hamza)\n",
    "\n",
    "subject_data = (subject_data - np.mean(subject_data, axis=-1, keepdims=True)) / np.std(subject_data, axis=-1, keepdims=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T18:07:59.472418Z",
     "iopub.status.busy": "2025-03-09T18:07:59.472170Z",
     "iopub.status.idle": "2025-03-09T18:13:03.280860Z",
     "shell.execute_reply": "2025-03-09T18:13:03.280053Z",
     "shell.execute_reply.started": "2025-03-09T18:07:59.472398Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DE Feature Shape: (32, 40, 40, 5, 63)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.signal\n",
    "# Frequency band definitions\n",
    "freq_bands = {\n",
    "    \"delta\": (1, 4),\n",
    "    \"theta\": (4, 8),\n",
    "    \"alpha\": (8, 14),\n",
    "    \"beta\": (14, 30),\n",
    "    \"gamma\": (31, 50),\n",
    "}\n",
    "\n",
    "# Define function to compute Differential Entropy (DE)\n",
    "def compute_de(signal):\n",
    "    \"\"\"Compute Differential Entropy (DE) for a given EEG segment\"\"\"\n",
    "    variance = np.var(signal, axis=-1, keepdims=True)  # Compute variance\n",
    "    de = 0.5 * np.log(2 * np.pi * np.e * variance)  # Apply DE formula\n",
    "    return de.squeeze()  # Remove extra dimensions\n",
    "\n",
    "# Define function to extract DE features\n",
    "def extract_de_features(subject_data, fs=128, window_size=128):\n",
    "    \"\"\"\n",
    "    Extract DE features from EEG data.\n",
    "    - subject_data: EEG data of shape (32, 40, 40, 8064)\n",
    "    - fs: Sampling frequency (128 Hz)\n",
    "    - window_size: 1 second (128 samples)\n",
    "    Returns: DE feature array of shape (32, 40, 40, 5, 63)\n",
    "    \"\"\"\n",
    "    num_subjects, num_trials, num_channels, num_samples = subject_data.shape\n",
    "    num_bands = len(freq_bands)\n",
    "    num_windows = num_samples // window_size  # 8064 / 128 = 63 windows\n",
    "\n",
    "    # Initialize DE feature array\n",
    "    de_features = np.zeros((num_subjects, num_trials, num_channels, num_bands, num_windows))\n",
    "\n",
    "    # Loop through subjects, trials, and channels\n",
    "    for subj in range(num_subjects):\n",
    "        for trial in range(num_trials):\n",
    "            for ch in range(num_channels):\n",
    "                # Extract single-channel EEG data for this trial\n",
    "                signal = subject_data[subj, trial, ch, :]\n",
    "\n",
    "                # Apply bandpass filters and compute DE for each frequency band\n",
    "                for b_idx, (band, (low, high)) in enumerate(freq_bands.items()):\n",
    "                    # Bandpass filter\n",
    "                    sos = scipy.signal.butter(4, [low, high], btype=\"bandpass\", fs=fs, output=\"sos\")\n",
    "                    filtered_signal = scipy.signal.sosfiltfilt(sos, signal) ## sosfiltfilt (by hamza)\n",
    "\n",
    "                    # Segment into 1-second windows (128 samples each)\n",
    "                    segmented = np.array(np.split(filtered_signal, num_windows, axis=-1))\n",
    "\n",
    "                    # Compute DE for each window\n",
    "                    de_features[subj, trial, ch, b_idx, :] = compute_de(segmented)\n",
    "\n",
    "    return de_features\n",
    "\n",
    "# Extract DE features\n",
    "de_features = extract_de_features(subject_data)\n",
    "\n",
    "# Print shape to confirm\n",
    "print(\"DE Feature Shape:\", de_features.shape)  # Expected: (32, 40, 40, 5, 63)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## by (Hamza)\n",
    "de_features = (de_features - np.mean(de_features, axis=(1, 2, 4), keepdims=True)) / np.std(de_features, axis=(1, 2, 4), keepdims=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T18:13:03.282884Z",
     "iopub.status.busy": "2025-03-09T18:13:03.282596Z",
     "iopub.status.idle": "2025-03-09T18:13:03.289075Z",
     "shell.execute_reply": "2025-03-09T18:13:03.288257Z",
     "shell.execute_reply.started": "2025-03-09T18:13:03.282861Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#1. Common Feature Extractor (CFE)\n",
    "class CommonFeatureExtractor(nn.Module):\n",
    "    def __init__(self, input_dim=200, output_dim=64):\n",
    "        super(CommonFeatureExtractor, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 256)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.fc3 = nn.Linear(128, output_dim)\n",
    "        self.activation = nn.LeakyReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.activation(self.bn2(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T18:13:03.290205Z",
     "iopub.status.busy": "2025-03-09T18:13:03.289980Z",
     "iopub.status.idle": "2025-03-09T18:13:03.311295Z",
     "shell.execute_reply": "2025-03-09T18:13:03.310611Z",
     "shell.execute_reply.started": "2025-03-09T18:13:03.290187Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ContrastiveLossLcon1(nn.Module):\n",
    "    def __init__(self, tau=0.2, chunk_size=128):\n",
    "        super(ContrastiveLossLcon1, self).__init__()\n",
    "        self.tau = tau\n",
    "        self.chunk_size = chunk_size  # Controls mini-batch size for similarity computation\n",
    "\n",
    "    def forward(self, q, labels=None):\n",
    "        batch_size = q.shape[0]\n",
    "        q = F.normalize(q, dim=-1)  # Normalize embeddings\n",
    "\n",
    "        loss = 0.0\n",
    "        num_chunks = (batch_size + self.chunk_size - 1) // self.chunk_size  # Compute number of chunks\n",
    "\n",
    "        for i in range(num_chunks):\n",
    "            start = i * self.chunk_size\n",
    "            end = min((i + 1) * self.chunk_size, batch_size)\n",
    "\n",
    "            q_chunk = q[start:end]  # Extract chunk of embeddings\n",
    "            sim_matrix = torch.mm(q_chunk, q.T)  # Compute similarity (128, 80640)\n",
    "\n",
    "            # Create a mask for self-similarity **only within the chunk**\n",
    "            mask = torch.eye(q_chunk.shape[0], dtype=torch.bool, device=q.device)  # (128, 128)\n",
    "            sim_matrix[:, start:end].masked_fill_(mask, float('-inf'))  # Only mask current chunk\n",
    "\n",
    "            exp_sim = torch.exp(sim_matrix / self.tau)  # Compute softmax scores\n",
    "\n",
    "            if labels is None:  # Unsupervised Contrastive Loss\n",
    "                loss += -torch.log(exp_sim[:, 0] / (exp_sim.sum(dim=1) + 1e-9)).mean()\n",
    "            else:  # Supervised Contrastive Loss\n",
    "                mask_same_class = labels[start:end].unsqueeze(1) == labels.unsqueeze(0)\n",
    "                exp_sim_same = exp_sim * mask_same_class.float()\n",
    "                loss += -torch.log(exp_sim_same.sum(dim=-1) / (exp_sim.sum(dim=-1) + 1e-9)).mean()\n",
    "\n",
    "        return loss / num_chunks  # Normalize loss across chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T18:13:03.312375Z",
     "iopub.status.busy": "2025-03-09T18:13:03.312157Z",
     "iopub.status.idle": "2025-03-09T18:13:03.332832Z",
     "shell.execute_reply": "2025-03-09T18:13:03.332153Z",
     "shell.execute_reply.started": "2025-03-09T18:13:03.312356Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 3. Subject-Specific Feature Extractor (SFE)\n",
    "\n",
    "class SubjectSpecificFeatureExtractor(nn.Module):\n",
    "    def __init__(self, input_dim=64, output_dim=32):\n",
    "        super(SubjectSpecificFeatureExtractor, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "        self.activation = nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.normalize(x, dim=-1) ## changes by (Hamza)\n",
    "        return self.activation(self.fc(x))  # Output shape: (batch_size, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T18:13:03.333717Z",
     "iopub.status.busy": "2025-03-09T18:13:03.333467Z",
     "iopub.status.idle": "2025-03-09T18:13:03.350170Z",
     "shell.execute_reply": "2025-03-09T18:13:03.349362Z",
     "shell.execute_reply.started": "2025-03-09T18:13:03.333674Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#4. Maximum Mean Discrepancy (MMD) Loss\n",
    "def mmd_loss(source_features, target_features):\n",
    "    source_mean = source_features.mean(dim=0)\n",
    "    target_mean = target_features.mean(dim=0)\n",
    "    loss = torch.norm(source_mean - target_mean, p=2) ** 2  \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T18:13:03.352444Z",
     "iopub.status.busy": "2025-03-09T18:13:03.352197Z",
     "iopub.status.idle": "2025-03-09T18:13:03.366637Z",
     "shell.execute_reply": "2025-03-09T18:13:03.365816Z",
     "shell.execute_reply.started": "2025-03-09T18:13:03.352415Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 5. Contrastive Loss L_con2 with Class Prototypes\n",
    "class ContrastiveLossLcon2(nn.Module):\n",
    "    def __init__(self, feature_dim=32, num_classes=4, tau=0.3, gamma=0.5, queue_size=1024):\n",
    "        super(ContrastiveLossLcon2, self).__init__()\n",
    "        self.tau = tau\n",
    "        self.gamma = gamma\n",
    "        self.num_classes = num_classes\n",
    "        self.queue_size = queue_size\n",
    "\n",
    "        # Initialize class prototypes (μ_c)\n",
    "        self.prototypes = nn.Parameter(torch.randn(num_classes, feature_dim))  # Shape: (4, 32)\n",
    "        self.prototypes.data = F.normalize(self.prototypes.data, dim=-1)  # Normalize prototypes ##by(Hamza)\n",
    "\n",
    "        # Initialize memory queue for negative samples\n",
    "        self.queue = torch.randn(queue_size, feature_dim)  # Shape: (1024, 32)\n",
    "        self.queue = F.normalize(self.queue, dim=-1)  # Normalize queue embeddings\n",
    "\n",
    "    def forward(self, z_t, pseudo_labels):\n",
    "        \"\"\"\n",
    "        Compute contrastive loss L_con2 for inter-domain alignment.\n",
    "        - z_t: Target domain features (batch_size, trials, time_windows, 32)\n",
    "        - pseudo_labels: Pseudo-labels for target samples (batch_size, trials, time_windows)\n",
    "\n",
    "        Returns:\n",
    "            Contrastive loss scalar\n",
    "        \"\"\"\n",
    "        batch_size, trials, time_windows, feature_dim = z_t.shape\n",
    "\n",
    "        # Flatten input for processing\n",
    "        z_t = z_t.view(-1, feature_dim)  # Shape: (batch_size * trials * time_windows, 32)\n",
    "        pseudo_labels = pseudo_labels.view(-1)  # Shape: (batch_size * trials * time_windows)\n",
    "\n",
    "        # Normalize embeddings\n",
    "        z_t = F.normalize(z_t, dim=-1)  # Normalize target embeddings\n",
    "        self.prototypes.data = F.normalize(self.prototypes.data, dim=-1)  # Normalize prototypes\n",
    "\n",
    "        # Compute similarity to class prototypes\n",
    "        similarity = torch.mm(z_t, self.prototypes.T)  # Shape: (batch_size * trials * time_windows, 4)\n",
    "        \n",
    "        # Select correct class prototype based on pseudo-labels\n",
    "        proto_sim = similarity.gather(1, pseudo_labels.unsqueeze(1))  # Shape: (batch_size * trials * time_windows, 1)\n",
    "\n",
    "        # Compute softmax denominator (all possible embeddings)\n",
    "        queue_sim = torch.mm(z_t, self.queue.T)  # Shape: (batch_size * trials * time_windows, queue_size)\n",
    "        exp_sim = torch.cat([proto_sim, queue_sim], dim=1)  # Concatenate prototypes & queue\n",
    "        exp_sim = torch.exp(exp_sim / self.tau)  # Apply temperature scaling\n",
    "\n",
    "        # Contrastive loss computation\n",
    "        loss = -torch.log(exp_sim[:, 0] / exp_sim.sum(dim=1))  # Only consider prototype similarity\n",
    "        loss = loss.mean()\n",
    "\n",
    "        # Update prototypes with momentum (γ)\n",
    "        for i in range(self.num_classes):\n",
    "            class_mask = (pseudo_labels == i).float().unsqueeze(1)  # Mask for samples of class i\n",
    "            if class_mask.sum() > 0: ## changes by (Hamza)\n",
    "                class_mean = (class_mask * z_t).sum(dim=0) / (class_mask.sum() + 1e-9)  # Compute class mean\n",
    "                self.prototypes.data[i] = F.normalize(self.gamma * self.prototypes.data[i] + (1 - self.gamma) * class_mean, dim=-1)\n",
    "\n",
    "        # Update memory queue (FIFO replacement)\n",
    "        self.queue = torch.cat([self.queue[batch_size:], z_t.detach()], dim=0)  # Remove old, add new\n",
    "        self.queue = F.normalize(self.queue, dim=-1) ## changes by (Hamza)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T18:13:03.367637Z",
     "iopub.status.busy": "2025-03-09T18:13:03.367445Z",
     "iopub.status.idle": "2025-03-09T18:13:03.384639Z",
     "shell.execute_reply": "2025-03-09T18:13:03.383911Z",
     "shell.execute_reply.started": "2025-03-09T18:13:03.367620Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 6. Subject-Specific Classifier (SSC)\n",
    "class SubjectSpecificClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=32, num_classes=4):\n",
    "        super(SubjectSpecificClassifier, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T18:13:03.385402Z",
     "iopub.status.busy": "2025-03-09T18:13:03.385179Z",
     "iopub.status.idle": "2025-03-09T18:13:03.399962Z",
     "shell.execute_reply": "2025-03-09T18:13:03.399219Z",
     "shell.execute_reply.started": "2025-03-09T18:13:03.385384Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 7. Generalized Cross-Entropy (GCE) Loss\n",
    "class GCELoss(nn.Module):\n",
    "    def __init__(self, q=0.55):\n",
    "        super(GCELoss, self).__init__()\n",
    "        self.q = q\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        true_probs = probs.gather(1, targets.unsqueeze(1)).squeeze()\n",
    "        loss = (1 - true_probs ** self.q) / self.q\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Shape: torch.Size([32, 40, 63, 200])\n"
     ]
    }
   ],
   "source": [
    "de_features = torch.tensor(de_features, dtype=torch.float32)  # (32, 40, 40, 5, 63)\n",
    "num_subjects, num_trials, num_channels, num_bands, num_windows = de_features.shape\n",
    "de_features = de_features.view(num_subjects, num_trials, num_windows, num_channels * num_bands)  # (32, 40, 63, 200)\n",
    "print(\"Final Shape:\", de_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_features = (de_features - de_features.mean(dim=(0, 1), keepdims=True)) / (de_features.std(dim=(0, 1), keepdims=True) + 1e-9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T18:21:21.504270Z",
     "iopub.status.busy": "2025-03-09T18:21:21.503964Z",
     "iopub.status.idle": "2025-03-09T18:21:21.556129Z",
     "shell.execute_reply": "2025-03-09T18:21:21.555116Z",
     "shell.execute_reply.started": "2025-03-09T18:21:21.504244Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Shape: torch.Size([80640, 200])\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 40.00 MiB. GPU 0 has a total capacity of 2.00 GiB of which 0 bytes is free. Of the allocated memory 9.19 GiB is allocated by PyTorch, and 18.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 34\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Forward pass through CFE\u001b[39;00m\n\u001b[0;32m     33\u001b[0m shared_features \u001b[38;5;241m=\u001b[39m cfe(reshaped_features)  \u001b[38;5;66;03m# Shape: (80640, 64)\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m common_loss \u001b[38;5;241m=\u001b[39m \u001b[43mcontrastive_loss_lcon1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshared_features\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Keep on GPU\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Forward pass through SFE\u001b[39;00m\n\u001b[0;32m     37\u001b[0m subject_features \u001b[38;5;241m=\u001b[39m sfe(shared_features)  \u001b[38;5;66;03m# Shape: (80640, 32)\u001b[39;00m\n",
      "File \u001b[1;32me:\\FYP\\Finalise Fyp\\EEg-based-Emotion-Recognition\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\FYP\\Finalise Fyp\\EEg-based-Emotion-Recognition\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[7], line 29\u001b[0m, in \u001b[0;36mContrastiveLossLcon1.forward\u001b[1;34m(self, q, labels)\u001b[0m\n\u001b[0;32m     26\u001b[0m mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meye(q_chunk\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbool, device\u001b[38;5;241m=\u001b[39mq\u001b[38;5;241m.\u001b[39mdevice)  \u001b[38;5;66;03m# (128, 128)\u001b[39;00m\n\u001b[0;32m     27\u001b[0m sim_matrix[:, start:end]\u001b[38;5;241m.\u001b[39mmasked_fill_(mask, \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-inf\u001b[39m\u001b[38;5;124m'\u001b[39m))  \u001b[38;5;66;03m# Only mask current chunk\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m exp_sim \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexp\u001b[49m\u001b[43m(\u001b[49m\u001b[43msim_matrix\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtau\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Compute softmax scores\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# Unsupervised Contrastive Loss\u001b[39;00m\n\u001b[0;32m     32\u001b[0m     loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlog(exp_sim[:, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m/\u001b[39m (exp_sim\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-9\u001b[39m))\u001b[38;5;241m.\u001b[39mmean()\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 40.00 MiB. GPU 0 has a total capacity of 2.00 GiB of which 0 bytes is free. Of the allocated memory 9.19 GiB is allocated by PyTorch, and 18.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize models\n",
    "cfe = CommonFeatureExtractor().to(device)\n",
    "sfe = SubjectSpecificFeatureExtractor().to(device)\n",
    "ssc = SubjectSpecificClassifier().to(device)\n",
    "contrastive_loss_lcon1 = ContrastiveLossLcon1().to(device)\n",
    "contrastive_loss_lcon2 = ContrastiveLossLcon2().to(device)\n",
    "gce_loss_fn = GCELoss().to(device)\n",
    "\n",
    "# Projection layer for MMD alignment (outside training loop)\n",
    "feature_projection = torch.nn.Linear(64, 32).to(device)  # Projects CFE output to 32-dim\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(list(cfe.parameters()) + list(sfe.parameters()) + list(ssc.parameters()), lr=0.01)\n",
    "\n",
    "# Training settings\n",
    "epochs = 50\n",
    "batch_size = 32  # Matches subjects in dataset\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Reshape DE features for CFE input\n",
    "    num_subjects, num_trials, num_windows, feature_dim = de_features.shape  # (32, 40, 63, 200)\n",
    "\n",
    "    # Reshape into (batch_size * trials * time_windows, 200)\n",
    "    reshaped_features = de_features.view(-1, feature_dim).to(device)  # Shape: (80640, 200)\n",
    "\n",
    "    print(\"Final Shape:\", reshaped_features.shape)  # Expected: (80640, 200)\n",
    "\n",
    "    # Forward pass through CFE\n",
    "    shared_features = cfe(reshaped_features)  # Shape: (80640, 64)\n",
    "    common_loss = contrastive_loss_lcon1(shared_features)  # Keep on GPU\n",
    "\n",
    "    # Forward pass through SFE\n",
    "    subject_features = sfe(shared_features)  # Shape: (80640, 32)\n",
    "\n",
    "    # Forward pass through SSC\n",
    "    predictions = ssc(subject_features)  # Shape: (80640, 4)\n",
    "\n",
    "    # Apply projection before MMD computation (using pre-defined layer)\n",
    "    shared_features_projected = feature_projection(shared_features)  # Shape: (80640, 32)\n",
    "\n",
    "    # Compute MMD loss (Only in Early Epochs)\n",
    "    if epoch < 25:  # Matches MSCL training strategy\n",
    "        mmd_value = mmd_loss(shared_features_projected, subject_features)\n",
    "    else:\n",
    "        mmd_value = torch.tensor(0.0, device=device)\n",
    "\n",
    "    # Compute GCE Loss (Using actual DEAP labels)\n",
    "    labels = subject_labels.view(-1).to(device)  # Shape: (80640,)\n",
    "    gce_loss = gce_loss_fn(predictions, labels)\n",
    "\n",
    "    # Reshape subject_features for L_con2\n",
    "    subject_features = subject_features.view(num_subjects, num_trials, num_windows, -1)  # Shape: (32, 40, 63, 32)\n",
    "\n",
    "    # Compute L_con2 (Using Pseudo Labels)\n",
    "    pseudo_labels = torch.randint(0, 4, (num_subjects, num_trials, num_windows), device=device)  # Shape: (32, 40, 63)\n",
    "    lcon2_value = contrastive_loss_lcon2(subject_features, pseudo_labels)\n",
    "\n",
    "    # Final Loss\n",
    "    total_loss = gce_loss + mmd_value + common_loss + lcon2_value\n",
    "\n",
    "    # Backpropagation\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Total Loss: {total_loss.item():.4f}\")\n",
    "\n",
    "print(\"✅ MSCL Training Complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6735639,
     "sourceId": 10845560,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
