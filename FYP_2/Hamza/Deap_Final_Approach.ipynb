{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d166b1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import os\n",
    "import scipy.io\n",
    "from scipy.signal import butter, lfilter\n",
    "import scipy.signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d65e3b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Raw EEG Shape: (32, 40, 40, 8064)\n",
      "âœ… Emotion Labels Shape: (32, 40)\n",
      "âœ… DE features saved to: E:/FYP/Finalise Fyp/EEg-based-Emotion-Recognition/de_features.npy\n",
      "âœ… Labels saved to: E:/FYP/Finalise Fyp/EEg-based-Emotion-Recognition/de_labels.npy\n",
      "ðŸŽ‰ All processing done! DE features and labels are now saved and ready for training.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.signal\n",
    "import scipy.io\n",
    "import os\n",
    "\n",
    "# ==== CONFIG ====\n",
    "data_path = \"E:/FYP/Finalise Fyp/EEg-based-Emotion-Recognition/data_preprocessed_matlab/\"\n",
    "save_features = \"E:/FYP/Finalise Fyp/EEg-based-Emotion-Recognition/de_features.npy\"\n",
    "save_labels = \"E:/FYP/Finalise Fyp/EEg-based-Emotion-Recognition/de_labels.npy\"\n",
    "\n",
    "# ==== Frequency bands ====\n",
    "freq_bands = {\n",
    "    \"delta\": (1, 4),\n",
    "    \"theta\": (4, 8),\n",
    "    \"alpha\": (8, 14),\n",
    "    \"beta\": (14, 30),\n",
    "    \"gamma\": (31, 50),\n",
    "}\n",
    "\n",
    "# Differential Entropy formula\n",
    "def compute_de(signal):\n",
    "    variance = np.var(signal, axis=-1, keepdims=True)\n",
    "    de = 0.5 * np.log(2 * np.pi * np.e * variance)\n",
    "    return de.squeeze()\n",
    "\n",
    "# Feature extraction and save\n",
    "def extract_and_save_de_features(subject_data, subject_labels, feature_path, label_path, fs=128, window_size=128):\n",
    "    num_subjects, num_trials, num_channels, num_samples = subject_data.shape\n",
    "    num_bands = len(freq_bands)\n",
    "    num_windows = num_samples // window_size\n",
    "\n",
    "    de_features = np.zeros((num_subjects, num_trials, num_channels, num_bands, num_windows))\n",
    "\n",
    "    for subj in range(num_subjects):\n",
    "        for trial in range(num_trials):\n",
    "            for ch in range(num_channels):\n",
    "                signal = subject_data[subj, trial, ch, :]\n",
    "                for b_idx, (band, (low, high)) in enumerate(freq_bands.items()):\n",
    "                    sos = scipy.signal.butter(4, [low, high], btype=\"bandpass\", fs=fs, output=\"sos\")\n",
    "                    filtered_signal = scipy.signal.sosfilt(sos, signal)\n",
    "\n",
    "                    segmented = np.array(np.split(filtered_signal, num_windows, axis=-1))\n",
    "                    de_features[subj, trial, ch, b_idx, :] = compute_de(segmented)\n",
    "\n",
    "    np.save(feature_path, de_features)\n",
    "    print(f\"âœ… DE features saved to: {feature_path}\")\n",
    "\n",
    "    # Expand labels to match window dimension\n",
    "    expanded_labels = np.repeat(subject_labels[:, :, np.newaxis], num_windows, axis=2)  # Shape: (32, 40, 63)\n",
    "    np.save(label_path, expanded_labels)\n",
    "    print(f\"âœ… Labels saved to: {label_path}\")\n",
    "\n",
    "# ==== Loading raw EEG data ====\n",
    "subject_data = []\n",
    "subject_labels = []\n",
    "\n",
    "for i in range(1, 33):\n",
    "    mat = scipy.io.loadmat(f\"{data_path}s{i:02d}.mat\")\n",
    "    eeg = mat[\"data\"]  # Shape: (40, 40, 8064)\n",
    "    labels = mat[\"labels\"]  # Shape: (40, 4)\n",
    "\n",
    "    # Map labels to 4 emotion classes (Valence + Arousal)\n",
    "    valence = labels[:, 0]\n",
    "    arousal = labels[:, 1]\n",
    "    emotion_class = np.zeros_like(valence, dtype=int)\n",
    "    emotion_class[(valence >= 5) & (arousal >= 5)] = 3\n",
    "    emotion_class[(valence < 5) & (arousal >= 5)] = 1\n",
    "    emotion_class[(valence >= 5) & (arousal < 5)] = 2\n",
    "    emotion_class[(valence < 5) & (arousal < 5)] = 0\n",
    "\n",
    "    subject_data.append(eeg)\n",
    "    subject_labels.append(emotion_class)\n",
    "\n",
    "subject_data = np.array(subject_data)      # Shape: (32, 40, 40, 8064)\n",
    "subject_labels = np.array(subject_labels)  # Shape: (32, 40)\n",
    "\n",
    "print(\"âœ… Raw EEG Shape:\", subject_data.shape)\n",
    "print(\"âœ… Emotion Labels Shape:\", subject_labels.shape)\n",
    "\n",
    "# ==== Extract and save DE features + labels ====\n",
    "extract_and_save_de_features(subject_data, subject_labels, save_features, save_labels)\n",
    "\n",
    "print(\"ðŸŽ‰ All processing done! DE features and labels are now saved and ready for training.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e27ae55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… DE features loaded: (32, 40, 40, 5, 63)\n",
      "âœ… DE labels loaded: (32, 40, 63)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "de_features = np.load(save_features)\n",
    "de_labels = np.load(save_labels) \n",
    "\n",
    "print(\"âœ… DE features loaded:\", de_features.shape)\n",
    "print(\"âœ… DE labels loaded:\", de_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "461e90a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§  Class distribution (after segmentation):\n",
      "Class 0: 16380 samples\n",
      "Class 1: 18648 samples\n",
      "Class 2: 16758 samples\n",
      "Class 3: 28854 samples\n",
      "\n",
      "ðŸ’¡ Total samples: 80640\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load your saved labels\n",
    "de_labels = np.load(\"E:/FYP/Finalise Fyp/EEg-based-Emotion-Recognition/de_labels.npy\")\n",
    "\n",
    "# Flatten labels to 1D array\n",
    "flat_labels = de_labels.flatten()\n",
    "\n",
    "# Count each class\n",
    "classes, counts = np.unique(flat_labels, return_counts=True)\n",
    "\n",
    "print(\"ðŸ§  Class distribution (after segmentation):\")\n",
    "for cls, count in zip(classes, counts):\n",
    "    print(f\"Class {cls}: {count} samples\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Total samples:\", np.sum(counts))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782258d0",
   "metadata": {},
   "source": [
    "## DEAP DATASET CLASS FOR FEATURES & LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8232331",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class DEAPDataset(Dataset):\n",
    "    def __init__(self, feature_path, label_path, transform=None):\n",
    "        self.features = np.load(feature_path)  # Shape: (32, 40, 40, 5, 63)\n",
    "        self.labels = np.load(label_path)      # Shape: (32, 40, 63)\n",
    "        self.transform = transform\n",
    "\n",
    "        self.samples = []\n",
    "        num_subjects, num_trials, _, _, num_windows = self.features.shape\n",
    "\n",
    "        for subj in range(num_subjects):\n",
    "            for trial in range(num_trials):\n",
    "                for win in range(num_windows):\n",
    "                    label = self.labels[subj, trial, win]\n",
    "                    self.samples.append((subj, trial, win, label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        subj, trial, win, label = self.samples[idx]\n",
    "        feature = self.features[subj, trial, :, :, win]  # Shape: (40 channels, 5 bands)\n",
    "\n",
    "        if self.transform:\n",
    "            feature = self.transform(feature)\n",
    "\n",
    "        return torch.tensor(feature, dtype=torch.float32), int(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0671a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Balanced DataLoader ready for training.\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "\n",
    "# Instantiate dataset\n",
    "dataset = DEAPDataset('E:/FYP/Finalise Fyp/EEg-based-Emotion-Recognition/de_features.npy',\n",
    "                      'E:/FYP/Finalise Fyp/EEg-based-Emotion-Recognition/de_labels.npy')\n",
    "\n",
    "# Get all labels for computing class weights\n",
    "all_labels = np.array([label for _, _, _, label in dataset.samples])\n",
    "classes, counts = np.unique(all_labels, return_counts=True)\n",
    "\n",
    "# Inverse frequency as weight\n",
    "class_weights = 1. / counts\n",
    "sample_weights = [class_weights[label] for label in all_labels]\n",
    "\n",
    "# Create the sampler\n",
    "sampler = WeightedRandomSampler(weights=sample_weights,\n",
    "                                 num_samples=len(sample_weights),\n",
    "                                 replacement=True)\n",
    "\n",
    "# Create DataLoader\n",
    "train_loader = DataLoader(dataset, batch_size=64, sampler=sampler)\n",
    "\n",
    "print(\"âœ… Balanced DataLoader ready for training.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78c62f5",
   "metadata": {},
   "source": [
    "## MODEL ARCHITECTURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18103030",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CommonFeatureExtractor(nn.Module):\n",
    "    def __init__(self, input_dim=200):  # DEAP: 40 channels Ã— 5 bands\n",
    "        super(CommonFeatureExtractor, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.act = nn.LeakyReLU()  # As per paper, not ReLU\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)         # Flatten input (40, 5) â†’ [200]\n",
    "        x = self.act(self.fc1(x))         # Input â†’ 256\n",
    "        x = self.act(self.fc2(x))         # 256 â†’ 128\n",
    "        x = self.act(self.fc3(x))         # 128 â†’ 64 (final embedding)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3afc5356",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubjectSpecificMapper(nn.Module):\n",
    "    def __init__(self, input_dim=64, output_dim=32):\n",
    "        super(SubjectSpecificMapper, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "        self.act = nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(self.fc(x))  # 64 â†’ 32 with LeakyReLU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2140658b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubjectSpecificClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=32, num_classes=4):  # You use 4 classes (DEAP)\n",
    "        super(SubjectSpecificClassifier, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)  # 32 â†’ 4, raw logits (no activation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c60c30e",
   "metadata": {},
   "source": [
    "## Contrastive Loss 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36a17ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SupervisedContrastiveLoss(nn.Module):\n",
    "    def __init__(self, temperature=0.5):\n",
    "        super(SupervisedContrastiveLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, features, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            features: Tensor of shape [batch_size, feature_dim]\n",
    "            labels:   Tensor of shape [batch_size] (int class labels)\n",
    "        Returns:\n",
    "            loss: scalar\n",
    "        \"\"\"\n",
    "        device = features.device\n",
    "        features = F.normalize(features, dim=1)\n",
    "        batch_size = features.shape[0]\n",
    "\n",
    "        # Cosine similarity matrix\n",
    "        sim_matrix = torch.matmul(features, features.T) / self.temperature\n",
    "\n",
    "        # Mask: same-label entries â†’ 1, others â†’ 0\n",
    "        labels = labels.contiguous().view(-1, 1)\n",
    "        mask = torch.eq(labels, labels.T).float().to(device)\n",
    "\n",
    "        # Remove self-similarity from denominator\n",
    "        logits_mask = torch.ones_like(mask) - torch.eye(batch_size).to(device)\n",
    "        mask = mask * logits_mask\n",
    "\n",
    "        # Numerator: exp(similarity with positives)\n",
    "        exp_sim = torch.exp(sim_matrix) * logits_mask\n",
    "        log_prob = sim_matrix - torch.log(exp_sim.sum(dim=1, keepdim=True) + 1e-9)\n",
    "\n",
    "        # Final loss: average only over positive pairs\n",
    "        loss = - (mask * log_prob).sum() / (mask.sum() + 1e-9)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93673316",
   "metadata": {},
   "source": [
    "## MMD LOSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b2b2b7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MMDLoss(nn.Module):\n",
    "    def __init__(self, kernel_type='rbf', kernel_mul=2.0, num_kernels=5):\n",
    "        super(MMDLoss, self).__init__()\n",
    "        self.kernel_type = kernel_type\n",
    "        self.kernel_mul = kernel_mul\n",
    "        self.num_kernels = num_kernels\n",
    "\n",
    "    def gaussian_kernel(self, source, target):\n",
    "        total = torch.cat([source, target], dim=0)  # [n + m, d]\n",
    "        total0 = total.unsqueeze(0)  # [1, n+m, d]\n",
    "        total1 = total.unsqueeze(1)  # [n+m, 1, d]\n",
    "        L2_distance = ((total0 - total1) ** 2).sum(2)  # [n+m, n+m]\n",
    "\n",
    "        # Use multiple Gaussian kernels\n",
    "        bandwidth = torch.mean(L2_distance).detach()\n",
    "        bandwidth_list = [bandwidth * (self.kernel_mul ** i) for i in range(self.num_kernels)]\n",
    "        kernels = [torch.exp(-L2_distance / bw) for bw in bandwidth_list]\n",
    "        return sum(kernels) / len(kernels)\n",
    "\n",
    "    def forward(self, source, target):\n",
    "        source = source.view(source.size(0), -1)  # [bs, d]\n",
    "        target = target.view(target.size(0), -1)\n",
    "        kernels = self.gaussian_kernel(source, target)\n",
    "\n",
    "        batch_size = source.size(0)\n",
    "        XX = kernels[:batch_size, :batch_size]  # source-source\n",
    "        YY = kernels[batch_size:, batch_size:]  # target-target\n",
    "        XY = kernels[:batch_size, batch_size:]  # source-target\n",
    "        YX = kernels[batch_size:, :batch_size]  # target-source\n",
    "\n",
    "        loss = torch.mean(XX + YY - XY - YX)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea01d76",
   "metadata": {},
   "source": [
    "## Contrastive Loss 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "961113b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ContrastiveLossLcon2(nn.Module):\n",
    "    def __init__(self, feature_dim=32, num_classes=4, tau=0.3, gamma=0.5, queue_size=1024):\n",
    "        super(ContrastiveLossLcon2, self).__init__()\n",
    "        self.tau = tau\n",
    "        self.gamma = gamma\n",
    "        self.num_classes = num_classes\n",
    "        self.queue_size = queue_size\n",
    "\n",
    "        # Initialize class prototypes (Î¼_c)\n",
    "        self.prototypes = nn.Parameter(torch.randn(num_classes, feature_dim))  # Shape: [4, 32]\n",
    "\n",
    "        # Initialize memory queue for negative samples\n",
    "        self.register_buffer(\"queue\", torch.randn(queue_size, feature_dim))  # Shape: [1024, 32]\n",
    "        self.queue = F.normalize(self.queue, dim=-1)\n",
    "\n",
    "    def forward(self, z_t, pseudo_labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            z_t: target embeddings from SFE (B, 32)\n",
    "            pseudo_labels: class indices (B,)\n",
    "        \"\"\"\n",
    "        z_t = F.normalize(z_t, dim=-1)  # Normalize subject-specific features\n",
    "\n",
    "        # 1. Positive logits (with prototypes)\n",
    "        pos_proto = self.prototypes[pseudo_labels]  # (B, 32)\n",
    "        pos_logits = torch.sum(z_t * pos_proto, dim=-1) / self.tau  # cosine similarity / tau\n",
    "\n",
    "        # 2. Negative logits (with queue)\n",
    "        neg_logits = torch.matmul(z_t, self.queue.T) / self.tau  # (B, Q)\n",
    "\n",
    "        # 3. Combine positives and negatives into logits\n",
    "        logits = torch.cat([pos_logits.unsqueeze(1), neg_logits], dim=1)  # (B, 1+Q)\n",
    "        labels = torch.zeros(z_t.size(0), dtype=torch.long).to(z_t.device)  # Positives at index 0\n",
    "\n",
    "        # 4. Compute loss\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "        # 5. Update the queue\n",
    "        self._dequeue_and_enqueue(z_t)\n",
    "\n",
    "        return self.gamma * loss  # Î³-scaled\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _dequeue_and_enqueue(self, embeddings):\n",
    "        # Detach to avoid gradients\n",
    "        embeddings = embeddings.detach()\n",
    "\n",
    "        batch_size = embeddings.size(0)\n",
    "        queue = self.queue\n",
    "\n",
    "        if batch_size >= self.queue_size:\n",
    "            self.queue = embeddings[-self.queue_size:]\n",
    "        else:\n",
    "            self.queue = torch.cat([queue[batch_size:], embeddings], dim=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc705eb",
   "metadata": {},
   "source": [
    "## GCE LOSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9bbb5e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GeneralizedCrossEntropy(nn.Module):\n",
    "    def __init__(self, q=0.7):\n",
    "        super(GeneralizedCrossEntropy, self).__init__()\n",
    "        self.q = q\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        \"\"\"\n",
    "        logits: [B, C]\n",
    "        targets: [B] (class indices)\n",
    "        \"\"\"\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        probs_true = probs[torch.arange(logits.size(0)), targets]  # pick p_y\n",
    "\n",
    "        if self.q == 1.0:\n",
    "            loss = 1.0 - probs_true\n",
    "        else:\n",
    "            loss = (1.0 - probs_true.pow(self.q)) / self.q\n",
    "\n",
    "        return loss.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e856821",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
