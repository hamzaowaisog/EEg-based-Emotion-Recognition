{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6c86871c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "os.makedirs(\"plots\", exist_ok=True)\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ab5b2b",
   "metadata": {},
   "source": [
    "## DATASET CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aa3ce7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DEAPDataset(Dataset):\n",
    "    def __init__(self, feature_path, label_path, exclude_subject=None, only_subject=None, normalize=True, train=False, noise_std=0.05):\n",
    "        self.features = np.load(feature_path)  # shape: (32, 40, 40, 5, 63)\n",
    "        self.labels = np.load(label_path)      # shape: (32, 40, 63)\n",
    "        self.train = train\n",
    "        self.noise_std = noise_std\n",
    "\n",
    "        if normalize:\n",
    "            all_features = []\n",
    "            self.mean_std_by_subject = {}\n",
    "            for subj in range(32):\n",
    "                if exclude_subject is not None and subj == exclude_subject:\n",
    "                    continue\n",
    "                if only_subject is not None and subj != only_subject:\n",
    "                    continue\n",
    "\n",
    "                subj_features = self.features[subj].transpose(0, 1, 3, 2).reshape(-1, 40, 5)\n",
    "                mean = np.mean(subj_features, axis=0)\n",
    "                std = np.std(subj_features, axis=0) + 1e-8\n",
    "                self.mean_std_by_subject[subj] = (mean, std)\n",
    "                all_features.append(subj_features)\n",
    "\n",
    "            if all_features:\n",
    "                all_features = np.concatenate(all_features, axis=0)\n",
    "                self.mean = np.mean(all_features, axis=0)\n",
    "                self.std = np.std(all_features, axis=0) + 1e-8\n",
    "            else:\n",
    "                self.mean = 0\n",
    "                self.std = 1\n",
    "        else:\n",
    "            self.mean = 0\n",
    "            self.std = 1\n",
    "\n",
    "        self.samples = []\n",
    "        for subj in range(32):\n",
    "            if exclude_subject is not None and subj == exclude_subject:\n",
    "                continue\n",
    "            if only_subject is not None and subj != only_subject:\n",
    "                continue\n",
    "\n",
    "            for trial in range(40):\n",
    "                for win in range(63):\n",
    "                    x = self.features[subj, trial, :, :, win]\n",
    "                    y = self.labels[subj, trial, win]\n",
    "                    self.samples.append((x, y, subj))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, y, subj = self.samples[idx]\n",
    "        mean , std = self.mean_std_by_subject[subj]\n",
    "        x = (x - mean) / std\n",
    "\n",
    "        if self.train:\n",
    "            x = torch.tensor(x, dtype=torch.float32)\n",
    "\n",
    "            # ✅ Add Gaussian jitter\n",
    "            x += torch.randn_like(x) * 0.01\n",
    "\n",
    "            # ✅ Channel-wise dropout (spatial masking)\n",
    "            if np.random.rand() < 0.3:\n",
    "                mask = torch.rand(x.shape[1]) > 0.2\n",
    "                x[:, ~mask] = 0\n",
    "        else:\n",
    "            x = torch.tensor(x, dtype=torch.float32)\n",
    "\n",
    "        return x, int(y)\n",
    "\n",
    "    def get_subject_data(self, subject):\n",
    "        mean , std = self.mean_std_by_subject[subject]\n",
    "        return [(torch.tensor((x - mean) / std, dtype=torch.float32), int(y))\n",
    "                for x, y, subj in self.samples if subj == subject]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7449ae",
   "metadata": {},
   "source": [
    "## MODEL ARCHITECTURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "13a672ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommonFeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(input_size=5, hidden_size=128, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(128 * 2, 64)  # Bidirectional output\n",
    "        self.bn = nn.BatchNorm1d(64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), 40, 5)  # Reshape to (batch, time, features)\n",
    "        _, h = self.gru(x)  # h: (2, batch, 128)\n",
    "        h = torch.cat([h[0], h[1]], dim=1)  # (batch, 256)\n",
    "        out = self.fc(h)  # (batch, 64)\n",
    "        out = self.bn(out)\n",
    "        out = F.normalize(out, p=2 ,dim=1)\n",
    "        return out\n",
    "\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, in_channels, reduction=8):\n",
    "        super(SEBlock, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_channels, in_channels // reduction)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(in_channels // reduction, in_channels)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, channels)\n",
    "        scale = self.fc1(x)\n",
    "        scale = self.relu(scale)\n",
    "        scale = self.fc2(scale)\n",
    "        scale = self.sigmoid(scale)\n",
    "        return x * scale\n",
    "\n",
    "class SubjectSpecificMapper(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(64, 32)\n",
    "        self.bn = nn.BatchNorm1d(32)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.act = nn.LeakyReLU()\n",
    "        self.se = SEBlock(32)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.act(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.se(x)\n",
    "        x = F.normalize(x, p=2 ,dim=1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SubjectSpecificClassifier(nn.Module):\n",
    "    def __init__(self , temperature=1.0):\n",
    "        super().__init__()\n",
    "        self.bn = nn.BatchNorm1d(32)\n",
    "        self.fc = nn.Linear(32, 4)\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.bn(x)\n",
    "        logits = self.fc(x)\n",
    "        return logits / self.temperature\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c0862f",
   "metadata": {},
   "source": [
    "## LOSS FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e607998d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupervisedContrastiveLoss(nn.Module):\n",
    "    def __init__(self, temperature=0.07):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, features, labels):\n",
    "        features = F.normalize(features, dim=1)\n",
    "        batch_size = features.shape[0]\n",
    "\n",
    "        # Handle case where batch size is 1\n",
    "        if batch_size <= 1:\n",
    "            return torch.tensor(0.0, device=features.device, requires_grad=True)\n",
    "\n",
    "        sim_matrix = torch.matmul(features, features.T) / self.temperature\n",
    "        labels = labels.contiguous().view(-1, 1)\n",
    "        mask = torch.eq(labels, labels.T).float().to(features.device)\n",
    "        logits_mask = torch.ones_like(mask) - torch.eye(batch_size).to(features.device)\n",
    "        mask = mask * logits_mask\n",
    "\n",
    "        # Handle case where there are no positive pairs\n",
    "        if mask.sum() == 0:\n",
    "            return torch.tensor(0.0, device=features.device, requires_grad=True)\n",
    "\n",
    "        exp_sim = torch.exp(sim_matrix) * logits_mask\n",
    "        log_prob = sim_matrix - torch.log(exp_sim.sum(dim=1, keepdim=True) + 1e-9)\n",
    "        loss = - (mask * log_prob).sum() / (mask.sum() + 1e-9)\n",
    "        return loss\n",
    "\n",
    "class MMDLoss(nn.Module):\n",
    "    def __init__(self, kernel_mul=2.0, num_kernels=5):\n",
    "        super().__init__()\n",
    "        self.kernel_mul = kernel_mul\n",
    "        self.num_kernels = num_kernels\n",
    "\n",
    "    def gaussian_kernel(self, source, target):\n",
    "        total = torch.cat([source, target], dim=0)\n",
    "\n",
    "        # Handle small batch sizes\n",
    "        if total.shape[0] <= 1:\n",
    "            return torch.tensor(0.0, device=source.device, requires_grad=True)\n",
    "\n",
    "        total0 = total.unsqueeze(0)\n",
    "        total1 = total.unsqueeze(1)\n",
    "        L2_distance = ((total0 - total1) ** 2).sum(2)\n",
    "\n",
    "        # Prevent division by zero\n",
    "        bandwidth = torch.mean(L2_distance.detach()) + 1e-8\n",
    "        bandwidth_list = [bandwidth * (self.kernel_mul ** i) for i in range(self.num_kernels)]\n",
    "        kernels = [torch.exp(-L2_distance / bw) for bw in bandwidth_list]\n",
    "        return sum(kernels) / len(kernels)\n",
    "\n",
    "    def forward(self, source, target):\n",
    "        source = source.view(source.size(0), -1)\n",
    "        target = target.view(target.size(0), -1)\n",
    "\n",
    "        # Handle empty batches\n",
    "        if source.shape[0] == 0 or target.shape[0] == 0:\n",
    "            return torch.tensor(0.0, device=source.device, requires_grad=True)\n",
    "\n",
    "        kernels = self.gaussian_kernel(source, target)\n",
    "        batch_size = source.size(0)\n",
    "        XX = kernels[:batch_size, :batch_size]\n",
    "        YY = kernels[batch_size:, batch_size:]\n",
    "        XY = kernels[:batch_size, batch_size:]\n",
    "        YX = kernels[batch_size:, :batch_size]\n",
    "        return torch.mean(XX + YY - XY - YX)\n",
    "\n",
    "class ContrastiveLossLcon2(nn.Module):\n",
    "    def __init__(self, feature_dim=32, num_classes=4, tau=0.1, gamma=0.5, queue_size=1024):\n",
    "        super().__init__()\n",
    "        self.tau = tau\n",
    "        self.gamma = gamma\n",
    "        self.prototypes = nn.Parameter(torch.randn(num_classes, feature_dim))\n",
    "        self.register_buffer(\"queue\", torch.randn(queue_size, feature_dim))\n",
    "        self.queue = F.normalize(self.queue, dim=-1)\n",
    "\n",
    "    def forward(self, z_t, pseudo_labels):\n",
    "        # Handle empty batches\n",
    "        if z_t.shape[0] == 0:\n",
    "            return torch.tensor(0.0, device=z_t.device, requires_grad=True)\n",
    "\n",
    "        z_t = F.normalize(z_t, dim=-1)\n",
    "        device = z_t.device\n",
    "        pseudo_labels = pseudo_labels.to(device)\n",
    "\n",
    "        pos_proto = self.prototypes.to(device)[pseudo_labels]\n",
    "\n",
    "        # Compute positive and negative logits\n",
    "        pos_logits = torch.sum(z_t * pos_proto, dim=-1) / self.tau\n",
    "\n",
    "        # Handle case where queue is empty\n",
    "        if self.queue.shape[0] == 0:\n",
    "            return self.gamma * F.cross_entropy(pos_logits.unsqueeze(1), torch.zeros(z_t.size(0), dtype=torch.long, device=device))\n",
    "\n",
    "        neg_logits = torch.matmul(z_t, self.queue.to(device).T) / self.tau\n",
    "        logits = torch.cat([pos_logits.unsqueeze(1), neg_logits], dim=1)\n",
    "        labels = torch.zeros(z_t.size(0), dtype=torch.long).to(device)\n",
    "\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        self._dequeue_and_enqueue(z_t)\n",
    "        return self.gamma * loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _dequeue_and_enqueue(self, embeddings):\n",
    "        embeddings = embeddings.detach().to(self.queue.device)\n",
    "        batch_size = embeddings.size(0)\n",
    "        queue_size = self.queue.size(0)\n",
    "\n",
    "        if batch_size >= queue_size:\n",
    "            self.queue = embeddings[-queue_size:]\n",
    "        else:\n",
    "            self.queue = torch.cat([self.queue[batch_size:], embeddings], dim=0)\n",
    "\n",
    "class GeneralizedCrossEntropy(nn.Module):\n",
    "    def __init__(self, q=0.7, weight=None):\n",
    "        super().__init__()\n",
    "        self.q = q\n",
    "        self.weight = weight  # class weights (tensor)\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "\n",
    "        # Handle empty batches\n",
    "        if targets.shape[0] == 0:\n",
    "            return torch.tensor(0.0, device=logits.device, requires_grad=True)\n",
    "\n",
    "        targets_onehot = F.one_hot(targets, num_classes=probs.shape[1]).float()\n",
    "        probs = torch.sum(probs * targets_onehot, dim=1)\n",
    "\n",
    "        if self.weight is not None:\n",
    "            weights = self.weight[targets]\n",
    "            loss = (1 - probs ** self.q) / self.q\n",
    "            return (weights * loss).mean()\n",
    "        else:\n",
    "            return ((1 - probs ** self.q) / self.q).mean()\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FocalLossWithSmoothing(nn.Module):\n",
    "    def __init__(self, gamma=2.0, smoothing=0.1, weight=None, reduction='mean'):\n",
    "        \"\"\"\n",
    "        Focal Loss with optional label smoothing.\n",
    "        Args:\n",
    "            gamma (float): focusing parameter for modulating factor (1 - p_t)\n",
    "            smoothing (float): label smoothing factor\n",
    "            weight (torch.Tensor): class weights\n",
    "            reduction (str): 'mean' or 'sum'\n",
    "        \"\"\"\n",
    "        super(FocalLossWithSmoothing, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.smoothing = smoothing\n",
    "        self.weight = weight\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        num_classes = logits.size(1)\n",
    "\n",
    "        # Convert to one-hot with label smoothing\n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(logits)\n",
    "            true_dist.fill_(self.smoothing / (num_classes - 1))\n",
    "            true_dist.scatter_(1, targets.data.unsqueeze(1), 1.0 - self.smoothing)\n",
    "\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        probs = torch.clamp(probs, 1e-6, 1.0)  # avoid log(0)\n",
    "\n",
    "        # Focal loss component\n",
    "        log_probs = torch.log(probs)\n",
    "        focal_weight = (1 - probs) ** self.gamma\n",
    "\n",
    "        loss = -true_dist * focal_weight * log_probs\n",
    "\n",
    "        # Apply class weights\n",
    "        if self.weight is not None:\n",
    "            weight = self.weight.unsqueeze(0)  # (1, num_classes)\n",
    "            loss = loss * weight\n",
    "\n",
    "        # loss = loss.sum(dim=1)  # sum over classes\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return loss.mean()\n",
    "        # elif self.reduction == 'sum':\n",
    "        #     return loss.sum()\n",
    "        else:\n",
    "            return loss.sum()  # no reduction\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PrototypeContrastiveLoss(nn.Module):\n",
    "    def __init__(self, feature_dim=32, num_classes=4, tau=0.1):\n",
    "        super().__init__()\n",
    "        self.feature_dim = feature_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.tau = tau\n",
    "        # Initialize class prototypes\n",
    "        self.register_buffer('prototypes', torch.zeros(num_classes, feature_dim))\n",
    "        self.register_buffer('prototype_counts', torch.zeros(num_classes))\n",
    "        \n",
    "    def forward(self, features, labels):\n",
    "        # Update prototypes with moving average\n",
    "        for c in range(self.num_classes):\n",
    "            class_mask = (labels == c)\n",
    "            if class_mask.sum() > 0:\n",
    "                class_features = features[class_mask]\n",
    "                class_mean = class_features.mean(0)\n",
    "                \n",
    "                # Update prototype with momentum\n",
    "                momentum = 0.9\n",
    "                self.prototypes[c] = momentum * self.prototypes[c] + (1 - momentum) * class_mean\n",
    "                self.prototype_counts[c] += 1\n",
    "        \n",
    "        # Normalize prototypes\n",
    "        valid_prototypes = self.prototype_counts > 0\n",
    "        if valid_prototypes.sum() > 0:\n",
    "            self.prototypes[valid_prototypes] = F.normalize(self.prototypes[valid_prototypes], dim=1)\n",
    "        \n",
    "        # Compute distances to prototypes\n",
    "        features_norm = F.normalize(features, dim=1)\n",
    "        logits = features_norm @ self.prototypes.t() / self.tau\n",
    "        \n",
    "        # Compute contrastive loss\n",
    "        labels_onehot = F.one_hot(labels, num_classes=self.num_classes).float()\n",
    "        loss = -torch.sum(labels_onehot * F.log_softmax(logits, dim=1)) / labels.size(0)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835c9ccc",
   "metadata": {},
   "source": [
    "## TRAINING LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "51518fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_batch(dataset, exclude, batch_size=64):\n",
    "    \"\"\"Get a batch of data from target subjects\"\"\"\n",
    "    for subj in range(32):\n",
    "        if subj != exclude:\n",
    "            data = dataset.get_subject_data(subj)\n",
    "            if len(data) >= batch_size:\n",
    "                indices = torch.randperm(len(data))[:batch_size]\n",
    "                x, y = zip(*[data[i] for i in indices])\n",
    "                return torch.stack(x), torch.tensor(y)\n",
    "\n",
    "    # Fallback: Return a small batch if no subject has enough data\n",
    "    all_data = []\n",
    "    for subj in range(32):\n",
    "        if subj != exclude:\n",
    "            all_data.extend(dataset.get_subject_data(subj))\n",
    "\n",
    "    if len(all_data) == 0:\n",
    "        # Empty tensor with correct shape as a fallback\n",
    "        empty_sample = next(iter(dataset))\n",
    "        return torch.zeros((0, *empty_sample[0].shape), dtype=torch.float32), torch.zeros(0, dtype=torch.long)\n",
    "\n",
    "    indices = torch.randperm(len(all_data))[:min(batch_size, len(all_data))]\n",
    "    x, y = zip(*[all_data[i] for i in indices])\n",
    "    return torch.stack(x), torch.tensor(y)\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, subject_idx):\n",
    "    \"\"\"Create and save confusion matrix plot\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1, 2, 3])\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'Confusion Matrix - Subject {subject_idx+1}')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.savefig(f'plots/new_confmat_subject{subject_idx+1}.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624187e8",
   "metadata": {},
   "source": [
    "## Weighted Class Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "144deea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicWeightedLoss(nn.Module):\n",
    "    \"\"\"Loss function with dynamically adjusted class weights based on performance\"\"\"\n",
    "    def __init__(self, num_classes=4, initial_weights=None, momentum=0.9):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.focal_loss = FocalLossWithSmoothing(gamma=2.0, smoothing=0.1)\n",
    "        self.register_buffer('weights', torch.ones(num_classes) if initial_weights is None else initial_weights)\n",
    "        self.register_buffer('class_accuracies', torch.ones(num_classes))\n",
    "        self.momentum = momentum\n",
    "        \n",
    "    def update_weights(self, logits, targets):\n",
    "        \"\"\"Update weights based on per-class accuracy\"\"\"\n",
    "        with torch.no_grad():\n",
    "            preds = logits.argmax(dim=1)\n",
    "            for c in range(self.num_classes):\n",
    "                # Find samples of this class\n",
    "                class_mask = (targets == c)\n",
    "                if class_mask.sum() > 0:\n",
    "                    # Calculate accuracy for this class\n",
    "                    correct = (preds[class_mask] == targets[class_mask]).float().mean()\n",
    "                    # Update running average of class accuracy\n",
    "                    self.class_accuracies[c] = self.momentum * self.class_accuracies[c] + (1 - self.momentum) * correct\n",
    "            \n",
    "            # Inverse of accuracy as weight (lower accuracy = higher weight)\n",
    "            new_weights = 1.0 / (self.class_accuracies + 1e-5)\n",
    "            # Normalize weights\n",
    "            new_weights = new_weights / new_weights.sum() * self.num_classes\n",
    "            self.weights = new_weights\n",
    "            \n",
    "    def forward(self, logits, targets):\n",
    "        # Update weights based on current batch performance\n",
    "        self.update_weights(logits, targets)\n",
    "        # Apply weights to focal loss\n",
    "        self.focal_loss.weight = self.weights\n",
    "        return self.focal_loss(logits, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b95b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "🚀 Training on all subjects together with contrastive + domain adaptation\n",
      "Classs distribution: [16380 18648 16758 28854]\n",
      "Class weights: tensor([1.2308, 1.0811, 1.2030, 0.6987], device='cuda:0')\n",
      "Warmup factor: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\FYP\\Finalise Fyp\\EEg-based-Emotion-Recognition\\.venv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Per-class accuracy:\n",
      "Class 0: 0.3311 (13104 / 13104)\n",
      "Class 1: 0.2131 (14918 / 14918)\n",
      "Class 2: 0.2534 (13407 / 13407)\n",
      "Class 3: 0.2069 (23083 / 23083)\n",
      "Current class weights: [1.2307693  1.081081   1.2030075  0.69868994]\n",
      "Epoch [1/250] - Loss: 5.1970 - Train Acc: 0.2432\n",
      "Warmup factor: 0.0000\n",
      "\n",
      "Per-class accuracy:\n",
      "Class 0: 0.3716 (13104 / 13104)\n",
      "Class 1: 0.2450 (14918 / 14918)\n",
      "Class 2: 0.2394 (13407 / 13407)\n",
      "Class 3: 0.2206 (23083 / 23083)\n",
      "Current class weights: [1.2307693  1.081081   1.2030075  0.69868994]\n",
      "Epoch [2/250] - Loss: 4.1413 - Train Acc: 0.2608\n",
      "Warmup factor: 0.0000\n",
      "\n",
      "Per-class accuracy:\n",
      "Class 0: 0.3892 (13104 / 13104)\n",
      "Class 1: 0.2538 (14918 / 14918)\n",
      "Class 2: 0.2549 (13407 / 13407)\n",
      "Class 3: 0.1807 (23083 / 23083)\n",
      "Current class weights: [1.2307693  1.081081   1.2030075  0.69868994]\n",
      "Epoch [3/250] - Loss: 4.1398 - Train Acc: 0.2554\n",
      "Warmup factor: 0.0000\n",
      "\n",
      "Per-class accuracy:\n",
      "Class 0: 0.4089 (13104 / 13104)\n",
      "Class 1: 0.2824 (14918 / 14918)\n",
      "Class 2: 0.2460 (13407 / 13407)\n",
      "Class 3: 0.2014 (23083 / 23083)\n",
      "Current class weights: [1.2307693  1.081081   1.2030075  0.69868994]\n",
      "Epoch [4/250] - Loss: 4.1366 - Train Acc: 0.2715\n",
      "Warmup factor: 0.0000\n",
      "\n",
      "Per-class accuracy:\n",
      "Class 0: 0.4375 (13104 / 13104)\n",
      "Class 1: 0.3193 (14918 / 14918)\n",
      "Class 2: 0.2346 (13407 / 13407)\n",
      "Class 3: 0.1849 (23083 / 23083)\n",
      "Current class weights: [1.2307693  1.081081   1.2030075  0.69868994]\n",
      "Epoch [5/250] - Loss: 4.1316 - Train Acc: 0.2776\n",
      "📊 Val Acc: 0.2881\n",
      "Warmup factor: 0.0000\n",
      "\n",
      "Per-class accuracy:\n",
      "Class 0: 0.4374 (13104 / 13104)\n",
      "Class 1: 0.3271 (14918 / 14918)\n",
      "Class 2: 0.2158 (13407 / 13407)\n",
      "Class 3: 0.2309 (23083 / 23083)\n",
      "Current class weights: [1.2307693  1.081081   1.2030075  0.69868994]\n",
      "Epoch [6/250] - Loss: 4.1247 - Train Acc: 0.2920\n",
      "Warmup factor: 0.0000\n",
      "\n",
      "Per-class accuracy:\n",
      "Class 0: 0.4755 (13104 / 13104)\n",
      "Class 1: 0.3099 (14918 / 14918)\n",
      "Class 2: 0.2262 (13407 / 13407)\n",
      "Class 3: 0.2317 (23083 / 23083)\n",
      "Current class weights: [1.2307693  1.081081   1.2030075  0.69868994]\n",
      "Epoch [7/250] - Loss: 4.1146 - Train Acc: 0.2982\n",
      "Warmup factor: 0.0000\n",
      "\n",
      "Per-class accuracy:\n",
      "Class 0: 0.4803 (13104 / 13104)\n",
      "Class 1: 0.2912 (14918 / 14918)\n",
      "Class 2: 0.2100 (13407 / 13407)\n",
      "Class 3: 0.2397 (23083 / 23083)\n",
      "Current class weights: [1.2307693  1.081081   1.2030075  0.69868994]\n",
      "Epoch [8/250] - Loss: 4.1037 - Train Acc: 0.2943\n",
      "Warmup factor: 0.0000\n",
      "\n",
      "Per-class accuracy:\n",
      "Class 0: 0.5071 (13104 / 13104)\n",
      "Class 1: 0.2726 (14918 / 14918)\n",
      "Class 2: 0.2159 (13407 / 13407)\n",
      "Class 3: 0.2324 (23083 / 23083)\n",
      "Current class weights: [1.2307693  1.081081   1.2030075  0.69868994]\n",
      "Epoch [9/250] - Loss: 4.0924 - Train Acc: 0.2941\n",
      "Warmup factor: 0.0000\n",
      "\n",
      "Per-class accuracy:\n",
      "Class 0: 0.5117 (13104 / 13104)\n",
      "Class 1: 0.2473 (14918 / 14918)\n",
      "Class 2: 0.2032 (13407 / 13407)\n",
      "Class 3: 0.2368 (23083 / 23083)\n",
      "Current class weights: [1.2307693  1.081081   1.2030075  0.69868994]\n",
      "Epoch [10/250] - Loss: 4.0809 - Train Acc: 0.2881\n",
      "📊 Val Acc: 0.2853\n",
      "Warmup factor: 0.0000\n",
      "\n",
      "Per-class accuracy:\n",
      "Class 0: 0.5087 (13104 / 13104)\n",
      "Class 1: 0.2531 (14918 / 14918)\n",
      "Class 2: 0.1888 (13407 / 13407)\n",
      "Class 3: 0.2335 (23083 / 23083)\n",
      "Current class weights: [1.2307693  1.081081   1.2030075  0.69868994]\n",
      "Epoch [11/250] - Loss: 4.0694 - Train Acc: 0.2846\n",
      "Warmup factor: 0.0000\n",
      "\n",
      "Per-class accuracy:\n",
      "Class 0: 0.5082 (13104 / 13104)\n",
      "Class 1: 0.2494 (14918 / 14918)\n",
      "Class 2: 0.1708 (13407 / 13407)\n",
      "Class 3: 0.2418 (23083 / 23083)\n",
      "Current class weights: [1.2307693  1.081081   1.2030075  0.69868994]\n",
      "Epoch [12/250] - Loss: 4.0581 - Train Acc: 0.2829\n",
      "Warmup factor: 0.0000\n",
      "\n",
      "Per-class accuracy:\n",
      "Class 0: 0.5237 (13104 / 13104)\n",
      "Class 1: 0.2512 (14918 / 14918)\n",
      "Class 2: 0.1701 (13407 / 13407)\n",
      "Class 3: 0.2300 (23083 / 23083)\n",
      "Current class weights: [1.2307693  1.081081   1.2030075  0.69868994]\n",
      "Epoch [13/250] - Loss: 4.0456 - Train Acc: 0.2821\n",
      "Warmup factor: 0.0000\n",
      "\n",
      "Per-class accuracy:\n",
      "Class 0: 0.5368 (13104 / 13104)\n",
      "Class 1: 0.2361 (14918 / 14918)\n",
      "Class 2: 0.1723 (13407 / 13407)\n",
      "Class 3: 0.2176 (23083 / 23083)\n",
      "Current class weights: [1.2307693  1.081081   1.2030075  0.69868994]\n",
      "Epoch [14/250] - Loss: 4.0373 - Train Acc: 0.2773\n",
      "Warmup factor: 0.0000\n",
      "\n",
      "Per-class accuracy:\n",
      "Class 0: 0.5268 (13104 / 13104)\n",
      "Class 1: 0.2522 (14918 / 14918)\n",
      "Class 2: 0.1573 (13407 / 13407)\n",
      "Class 3: 0.2193 (23083 / 23083)\n",
      "Current class weights: [1.2307693  1.081081   1.2030075  0.69868994]\n",
      "Epoch [15/250] - Loss: 4.0278 - Train Acc: 0.2765\n",
      "📊 Val Acc: 0.2801\n",
      "Warmup factor: 0.0000\n",
      "\n",
      "Per-class accuracy:\n",
      "Class 0: 0.5269 (13104 / 13104)\n",
      "Class 1: 0.2343 (14918 / 14918)\n",
      "Class 2: 0.1609 (13407 / 13407)\n",
      "Class 3: 0.2158 (23083 / 23083)\n",
      "Current class weights: [1.2307693  1.081081   1.2030075  0.69868994]\n",
      "Epoch [16/250] - Loss: 4.0182 - Train Acc: 0.2719\n",
      "Warmup factor: 0.0000\n",
      "\n",
      "Per-class accuracy:\n",
      "Class 0: 0.5225 (13104 / 13104)\n",
      "Class 1: 0.2368 (14918 / 14918)\n",
      "Class 2: 0.1749 (13407 / 13407)\n",
      "Class 3: 0.2179 (23083 / 23083)\n",
      "Current class weights: [1.2307693  1.081081   1.2030075  0.69868994]\n",
      "Epoch [17/250] - Loss: 4.0115 - Train Acc: 0.2752\n",
      "Warmup factor: 0.0000\n",
      "\n",
      "Per-class accuracy:\n",
      "Class 0: 0.5090 (13104 / 13104)\n",
      "Class 1: 0.2392 (14918 / 14918)\n",
      "Class 2: 0.1750 (13407 / 13407)\n",
      "Class 3: 0.2172 (23083 / 23083)\n",
      "Current class weights: [1.2307693  1.081081   1.2030075  0.69868994]\n",
      "Epoch [18/250] - Loss: 4.0049 - Train Acc: 0.2728\n",
      "Warmup factor: 0.0000\n",
      "\n",
      "Per-class accuracy:\n",
      "Class 0: 0.5188 (13104 / 13104)\n",
      "Class 1: 0.2243 (14918 / 14918)\n",
      "Class 2: 0.1635 (13407 / 13407)\n",
      "Class 3: 0.2038 (23083 / 23083)\n",
      "Current class weights: [1.2307693  1.081081   1.2030075  0.69868994]\n",
      "Epoch [19/250] - Loss: 4.0002 - Train Acc: 0.2642\n",
      "Warmup factor: 0.0000\n",
      "\n",
      "Per-class accuracy:\n",
      "Class 0: 0.5280 (13104 / 13104)\n",
      "Class 1: 0.2258 (14918 / 14918)\n",
      "Class 2: 0.1564 (13407 / 13407)\n",
      "Class 3: 0.2131 (23083 / 23083)\n",
      "Current class weights: [1.2307693  1.081081   1.2030075  0.69868994]\n",
      "Epoch [20/250] - Loss: 3.9904 - Train Acc: 0.2682\n",
      "📊 Val Acc: 0.2599\n",
      "Warmup factor: 0.0000\n",
      "\n",
      "Per-class accuracy:\n",
      "Class 0: 0.5272 (13104 / 13104)\n",
      "Class 1: 0.2158 (14918 / 14918)\n",
      "Class 2: 0.1679 (13407 / 13407)\n",
      "Class 3: 0.2035 (23083 / 23083)\n",
      "Current class weights: [1.2307693  1.081081   1.2030075  0.69868994]\n",
      "Epoch [21/250] - Loss: 3.9858 - Train Acc: 0.2647\n",
      "Warmup factor: 0.0000\n",
      "\n",
      "Per-class accuracy:\n",
      "Class 0: 0.5426 (13104 / 13104)\n",
      "Class 1: 0.2301 (14918 / 14918)\n",
      "Class 2: 0.1462 (13407 / 13407)\n",
      "Class 3: 0.2196 (23083 / 23083)\n",
      "Current class weights: [1.2307693  1.081081   1.2030075  0.69868994]\n",
      "Epoch [22/250] - Loss: 3.9789 - Train Acc: 0.2724\n",
      "Warmup factor: 0.0000\n",
      "\n",
      "Per-class accuracy:\n",
      "Class 0: 0.5348 (13104 / 13104)\n",
      "Class 1: 0.2276 (14918 / 14918)\n",
      "Class 2: 0.1555 (13407 / 13407)\n",
      "Class 3: 0.2101 (23083 / 23083)\n",
      "Current class weights: [1.2307693  1.081081   1.2030075  0.69868994]\n",
      "Epoch [23/250] - Loss: 3.9769 - Train Acc: 0.2688\n",
      "Warmup factor: 0.0000\n",
      "\n",
      "Per-class accuracy:\n",
      "Class 0: 0.5319 (13104 / 13104)\n",
      "Class 1: 0.2212 (14918 / 14918)\n",
      "Class 2: 0.1622 (13407 / 13407)\n",
      "Class 3: 0.2069 (23083 / 23083)\n",
      "Current class weights: [1.2307693  1.081081   1.2030075  0.69868994]\n",
      "Epoch [24/250] - Loss: 3.9700 - Train Acc: 0.2669\n",
      "Warmup factor: 0.0000\n",
      "\n",
      "Per-class accuracy:\n",
      "Class 0: 0.5285 (13104 / 13104)\n",
      "Class 1: 0.2194 (14918 / 14918)\n",
      "Class 2: 0.1644 (13407 / 13407)\n",
      "Class 3: 0.2079 (23083 / 23083)\n",
      "Current class weights: [1.2307693  1.081081   1.2030075  0.69868994]\n",
      "Epoch [25/250] - Loss: 3.9637 - Train Acc: 0.2667\n",
      "📊 Val Acc: 0.2681\n",
      "Warmup factor: 0.0000\n",
      "\n",
      "Per-class accuracy:\n",
      "Class 0: 0.5226 (13104 / 13104)\n",
      "Class 1: 0.2296 (14918 / 14918)\n",
      "Class 2: 0.1610 (13407 / 13407)\n",
      "Class 3: 0.2094 (23083 / 23083)\n",
      "Current class weights: [1.2307693  1.081081   1.2030075  0.69868994]\n",
      "Epoch [26/250] - Loss: 3.9611 - Train Acc: 0.2676\n",
      "Warmup factor: 0.0000\n",
      "\n",
      "Per-class accuracy:\n",
      "Class 0: 0.5174 (13104 / 13104)\n",
      "Class 1: 0.2225 (14918 / 14918)\n",
      "Class 2: 0.1689 (13407 / 13407)\n",
      "Class 3: 0.2051 (23083 / 23083)\n",
      "Current class weights: [1.2307693  1.081081   1.2030075  0.69868994]\n",
      "Epoch [27/250] - Loss: 3.9575 - Train Acc: 0.2651\n",
      "Warmup factor: 0.0000\n",
      "\n",
      "Per-class accuracy:\n",
      "Class 0: 0.5354 (13104 / 13104)\n",
      "Class 1: 0.2109 (14918 / 14918)\n",
      "Class 2: 0.1818 (13407 / 13407)\n",
      "Class 3: 0.2038 (23083 / 23083)\n",
      "Current class weights: [1.2307693  1.081081   1.2030075  0.69868994]\n",
      "Epoch [28/250] - Loss: 3.9517 - Train Acc: 0.2682\n",
      "Warmup factor: 0.0000\n",
      "\n",
      "Per-class accuracy:\n",
      "Class 0: 0.5319 (13104 / 13104)\n",
      "Class 1: 0.2106 (14918 / 14918)\n",
      "Class 2: 0.1704 (13407 / 13407)\n",
      "Class 3: 0.2038 (23083 / 23083)\n",
      "Current class weights: [1.2307693  1.081081   1.2030075  0.69868994]\n",
      "Epoch [29/250] - Loss: 3.9482 - Train Acc: 0.2651\n",
      "Warmup factor: 0.0000\n",
      "\n",
      "Per-class accuracy:\n",
      "Class 0: 0.5414 (13104 / 13104)\n",
      "Class 1: 0.2114 (14918 / 14918)\n",
      "Class 2: 0.1898 (13407 / 13407)\n",
      "Class 3: 0.2058 (23083 / 23083)\n",
      "Current class weights: [1.2307693  1.081081   1.2030075  0.69868994]\n",
      "Epoch [30/250] - Loss: 3.9463 - Train Acc: 0.2719\n",
      "📊 Val Acc: 0.2830\n",
      "Warmup factor: 0.0000\n",
      "\n",
      "Per-class accuracy:\n",
      "Class 0: 0.5294 (13104 / 13104)\n",
      "Class 1: 0.2375 (14918 / 14918)\n",
      "Class 2: 0.1854 (13407 / 13407)\n",
      "Class 3: 0.2097 (23083 / 23083)\n",
      "Current class weights: [1.2307693  1.081081   1.2030075  0.69868994]\n",
      "Epoch [31/250] - Loss: 3.9398 - Train Acc: 0.2760\n",
      "Warmup factor: 0.0000\n",
      "\n",
      "Per-class accuracy:\n",
      "Class 0: 0.5296 (13104 / 13104)\n",
      "Class 1: 0.2256 (14918 / 14918)\n",
      "Class 2: 0.1918 (13407 / 13407)\n",
      "Class 3: 0.2019 (23083 / 23083)\n",
      "Current class weights: [1.2307693  1.081081   1.2030075  0.69868994]\n",
      "Epoch [32/250] - Loss: 3.9398 - Train Acc: 0.2718\n",
      "Warmup factor: 0.0000\n",
      "\n",
      "Per-class accuracy:\n",
      "Class 0: 0.5269 (13104 / 13104)\n",
      "Class 1: 0.2190 (14918 / 14918)\n",
      "Class 2: 0.1854 (13407 / 13407)\n",
      "Class 3: 0.1933 (23083 / 23083)\n",
      "Current class weights: [1.2307693  1.081081   1.2030075  0.69868994]\n",
      "Epoch [33/250] - Loss: 3.9382 - Train Acc: 0.2654\n",
      "Warmup factor: 0.0000\n",
      "\n",
      "Per-class accuracy:\n",
      "Class 0: 0.5095 (13104 / 13104)\n",
      "Class 1: 0.2172 (14918 / 14918)\n",
      "Class 2: 0.1864 (13407 / 13407)\n",
      "Class 3: 0.1902 (23083 / 23083)\n",
      "Current class weights: [1.2307693  1.081081   1.2030075  0.69868994]\n",
      "Epoch [34/250] - Loss: 3.9324 - Train Acc: 0.2605\n",
      "Warmup factor: 0.0000\n",
      "\n",
      "Per-class accuracy:\n",
      "Class 0: 0.5070 (13104 / 13104)\n",
      "Class 1: 0.2329 (14918 / 14918)\n",
      "Class 2: 0.1736 (13407 / 13407)\n",
      "Class 3: 0.1908 (23083 / 23083)\n",
      "Current class weights: [1.2307693  1.081081   1.2030075  0.69868994]\n",
      "Epoch [35/250] - Loss: 3.9330 - Train Acc: 0.2612\n",
      "📊 Val Acc: 0.2599\n",
      "Warmup factor: 0.0000\n",
      "\n",
      "Per-class accuracy:\n",
      "Class 0: 0.5192 (13104 / 13104)\n",
      "Class 1: 0.2209 (14918 / 14918)\n",
      "Class 2: 0.1852 (13407 / 13407)\n",
      "Class 3: 0.1860 (23083 / 23083)\n",
      "Current class weights: [1.2307693  1.081081   1.2030075  0.69868994]\n",
      "Epoch [36/250] - Loss: 3.8842 - Train Acc: 0.2616\n",
      "Warmup factor: 0.0000\n",
      "\n",
      "Per-class accuracy:\n",
      "Class 0: 0.5298 (13104 / 13104)\n",
      "Class 1: 0.2188 (14918 / 14918)\n",
      "Class 2: 0.1835 (13407 / 13407)\n",
      "Class 3: 0.1888 (23083 / 23083)\n",
      "Current class weights: [1.2307693  1.081081   1.2030075  0.69868994]\n",
      "Epoch [37/250] - Loss: 3.8599 - Train Acc: 0.2639\n",
      "Warmup factor: 0.0000\n",
      "\n",
      "Per-class accuracy:\n",
      "Class 0: 0.5430 (13104 / 13104)\n",
      "Class 1: 0.2334 (14918 / 14918)\n",
      "Class 2: 0.1876 (13407 / 13407)\n",
      "Class 3: 0.1873 (23083 / 23083)\n",
      "Current class weights: [1.2307693  1.081081   1.2030075  0.69868994]\n",
      "Epoch [38/250] - Loss: 3.8407 - Train Acc: 0.2703\n",
      "Warmup factor: 0.0000\n",
      "\n",
      "Per-class accuracy:\n",
      "Class 0: 0.5452 (13104 / 13104)\n",
      "Class 1: 0.2295 (14918 / 14918)\n",
      "Class 2: 0.1836 (13407 / 13407)\n",
      "Class 3: 0.1871 (23083 / 23083)\n",
      "Current class weights: [1.2307693  1.081081   1.2030075  0.69868994]\n",
      "Epoch [39/250] - Loss: 3.8328 - Train Acc: 0.2689\n",
      "Warmup factor: 0.0000\n",
      "\n",
      "Per-class accuracy:\n",
      "Class 0: 0.5377 (13104 / 13104)\n",
      "Class 1: 0.2366 (14918 / 14918)\n",
      "Class 2: 0.1863 (13407 / 13407)\n",
      "Class 3: 0.1931 (23083 / 23083)\n",
      "Current class weights: [1.2307693  1.081081   1.2030075  0.69868994]\n",
      "Epoch [40/250] - Loss: 3.8243 - Train Acc: 0.2717\n",
      "📊 Val Acc: 0.2699\n",
      "Warmup factor: 0.0000\n",
      "\n",
      "Per-class accuracy:\n",
      "Class 0: 0.5308 (13104 / 13104)\n",
      "Class 1: 0.2333 (14918 / 14918)\n",
      "Class 2: 0.1865 (13407 / 13407)\n",
      "Class 3: 0.1859 (23083 / 23083)\n",
      "Current class weights: [1.2307693  1.081081   1.2030075  0.69868994]\n",
      "Epoch [41/250] - Loss: 3.8134 - Train Acc: 0.2670\n",
      "Warmup factor: 0.0000\n",
      "\n",
      "Per-class accuracy:\n",
      "Class 0: 0.5420 (13104 / 13104)\n",
      "Class 1: 0.2323 (14918 / 14918)\n",
      "Class 2: 0.1939 (13407 / 13407)\n",
      "Class 3: 0.1891 (23083 / 23083)\n",
      "Current class weights: [1.2307693  1.081081   1.2030075  0.69868994]\n",
      "Epoch [42/250] - Loss: 3.8046 - Train Acc: 0.2718\n",
      "Warmup factor: 0.0000\n",
      "\n",
      "Per-class accuracy:\n",
      "Class 0: 0.5498 (13104 / 13104)\n",
      "Class 1: 0.2316 (14918 / 14918)\n",
      "Class 2: 0.1915 (13407 / 13407)\n",
      "Class 3: 0.1887 (23083 / 23083)\n",
      "Current class weights: [1.2307693  1.081081   1.2030075  0.69868994]\n",
      "Epoch [43/250] - Loss: 3.7969 - Train Acc: 0.2726\n",
      "Warmup factor: 0.0000\n",
      "\n",
      "Per-class accuracy:\n",
      "Class 0: 0.5646 (13104 / 13104)\n",
      "Class 1: 0.2189 (14918 / 14918)\n",
      "Class 2: 0.2019 (13407 / 13407)\n",
      "Class 3: 0.1853 (23083 / 23083)\n",
      "Current class weights: [1.2307693  1.081081   1.2030075  0.69868994]\n",
      "Epoch [44/250] - Loss: 3.7906 - Train Acc: 0.2736\n",
      "Warmup factor: 0.0000\n",
      "\n",
      "Per-class accuracy:\n",
      "Class 0: 0.5601 (13104 / 13104)\n",
      "Class 1: 0.2175 (14918 / 14918)\n",
      "Class 2: 0.1982 (13407 / 13407)\n",
      "Class 3: 0.1794 (23083 / 23083)\n",
      "Current class weights: [1.2307693  1.081081   1.2030075  0.69868994]\n",
      "Epoch [45/250] - Loss: 3.7810 - Train Acc: 0.2694\n",
      "📊 Val Acc: 0.2835\n",
      "Warmup factor: 0.0000\n",
      "\n",
      "Per-class accuracy:\n",
      "Class 0: 0.5675 (13104 / 13104)\n",
      "Class 1: 0.2088 (14918 / 14918)\n",
      "Class 2: 0.2088 (13407 / 13407)\n",
      "Class 3: 0.1815 (23083 / 23083)\n",
      "Current class weights: [1.2307693  1.081081   1.2030075  0.69868994]\n",
      "Epoch [46/250] - Loss: 3.7770 - Train Acc: 0.2719\n",
      "Warmup factor: 0.0000\n",
      "\n",
      "Per-class accuracy:\n",
      "Class 0: 0.5617 (13104 / 13104)\n",
      "Class 1: 0.2356 (14918 / 14918)\n",
      "Class 2: 0.2047 (13407 / 13407)\n",
      "Class 3: 0.1788 (23083 / 23083)\n",
      "Current class weights: [1.2307693  1.081081   1.2030075  0.69868994]\n",
      "Epoch [47/250] - Loss: 3.7707 - Train Acc: 0.2751\n",
      "Warmup factor: 0.0000\n",
      "\n",
      "Per-class accuracy:\n",
      "Class 0: 0.5636 (13104 / 13104)\n",
      "Class 1: 0.2341 (14918 / 14918)\n",
      "Class 2: 0.2026 (13407 / 13407)\n",
      "Class 3: 0.1793 (23083 / 23083)\n",
      "Current class weights: [1.2307693  1.081081   1.2030075  0.69868994]\n",
      "Epoch [48/250] - Loss: 3.7636 - Train Acc: 0.2749\n",
      "Warmup factor: 0.0000\n",
      "\n",
      "Per-class accuracy:\n",
      "Class 0: 0.5806 (13104 / 13104)\n",
      "Class 1: 0.2428 (14918 / 14918)\n",
      "Class 2: 0.2118 (13407 / 13407)\n",
      "Class 3: 0.1799 (23083 / 23083)\n",
      "Current class weights: [1.2307693  1.081081   1.2030075  0.69868994]\n",
      "Epoch [49/250] - Loss: 3.7566 - Train Acc: 0.2825\n",
      "Warmup factor: 0.0000\n",
      "\n",
      "Per-class accuracy:\n",
      "Class 0: 0.5716 (13104 / 13104)\n",
      "Class 1: 0.2456 (14918 / 14918)\n",
      "Class 2: 0.2146 (13407 / 13407)\n",
      "Class 3: 0.1859 (23083 / 23083)\n",
      "Current class weights: [1.2307693  1.081081   1.2030075  0.69868994]\n",
      "Epoch [50/250] - Loss: 3.7509 - Train Acc: 0.2840\n",
      "📊 Val Acc: 0.2758\n",
      "Warmup factor: 0.0000\n",
      "\n",
      "Per-class accuracy:\n",
      "Class 0: 0.5679 (13104 / 13104)\n",
      "Class 1: 0.2281 (14918 / 14918)\n",
      "Class 2: 0.2200 (13407 / 13407)\n",
      "Class 3: 0.1788 (23083 / 23083)\n",
      "Current class weights: [1.2307693  1.081081   1.2030075  0.69868994]\n",
      "Epoch [51/250] - Loss: 3.7476 - Train Acc: 0.2778\n",
      "Warmup factor: 0.0000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Subset, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "os.makedirs(\"plots\", exist_ok=True)\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def augment_eeg(x, p=0.5):\n",
    "    # More sophisticated augmentation techniques\n",
    "    if torch.rand(1, device=x.device) < p:\n",
    "        # Gaussian noise with adaptive magnitude\n",
    "        noise_level = 0.01 + 0.02 * torch.rand(1, device=x.device)\n",
    "        noise = torch.randn_like(x) * noise_level\n",
    "        x = x + noise\n",
    "        \n",
    "    if torch.rand(1, device=x.device) < p:\n",
    "        # Random scaling with more variation\n",
    "        scale = 0.9 + 0.2 * torch.rand(1, device=x.device)\n",
    "        x = x * scale\n",
    "        \n",
    "    if torch.rand(1, device=x.device) < p:\n",
    "        # Channel dropout (randomly zero out some channels)\n",
    "        mask = torch.bernoulli(torch.ones_like(x) * 0.9).to(x.device)\n",
    "        x = x * mask\n",
    "        \n",
    "    if torch.rand(1, device=x.device) < p:\n",
    "        # Time masking (simulate artifacts)\n",
    "        batch_size = x.size(0)\n",
    "        feature_dim = x.size(1)\n",
    "        mask_length = int(feature_dim * 0.1)\n",
    "        start = torch.randint(0, feature_dim - mask_length, (batch_size,), device=x.device)\n",
    "        \n",
    "        mask = torch.ones_like(x)\n",
    "        for i in range(batch_size):\n",
    "            mask[i, start[i]:start[i]+mask_length] = 0\n",
    "        x = x * mask\n",
    "        \n",
    "    return x\n",
    "# ------------------ Training Function ------------------\n",
    "def train_all_subjects(feature_path, label_path, num_epochs=100, batch_size=64, warmup_epochs=75):\n",
    "    print(\"\\n🚀 Training on all subjects together with contrastive + domain adaptation\")\n",
    "    \n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    val_losses = []\n",
    "    val_accs = []\n",
    "\n",
    "    dataset = DEAPDataset(feature_path, label_path, normalize=True, train=True)\n",
    "    indices = list(range(len(dataset)))\n",
    "    targets = [label for _, label in dataset]\n",
    "    train_idx, test_idx, _, _ = train_test_split(\n",
    "        indices, targets, test_size=0.2, stratify=targets, random_state=42\n",
    "    )\n",
    "\n",
    "    train_set = Subset(dataset, train_idx)\n",
    "    test_set = Subset(DEAPDataset(feature_path,label_path,normalize=True,train=False), test_idx)\n",
    "\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Init models\n",
    "    cfe = CommonFeatureExtractor().to(device)\n",
    "    sfe = SubjectSpecificMapper().to(device)\n",
    "    ssc = SubjectSpecificClassifier(temperature = 2.0).to(device)\n",
    "\n",
    "    # Init losses\n",
    "\n",
    "    labels_np = np.array([label for _, label in dataset])\n",
    "    # class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(labels_np), y=labels_np)\n",
    "    # class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "    \n",
    "    class_counts = np.bincount(labels_np)\n",
    "    print(f\"Classs distribution: {class_counts}\")\n",
    "    \n",
    "    initial_weights = compute_class_weight('balanced', classes=np.unique(labels_np), y=labels_np)\n",
    "    class_weights = torch.tensor(initial_weights, dtype=torch.float32).to(device)\n",
    "    print(f\"Class weights: {class_weights}\")\n",
    "    \n",
    "    \n",
    "\n",
    "    loss_con1 = SupervisedContrastiveLoss(temperature=0.07).to(device)\n",
    "    loss_mmd = MMDLoss().to(device)\n",
    "    loss_con2 = ContrastiveLossLcon2().to(device)\n",
    "    # loss_cls = GeneralizedCrossEntropy(q=0.5, weight=class_weights).to(device)\n",
    "    # loss_cls = FocalLossWithSmoothing(gamma=2.0, smoothing=0.1, weight=class_weights).to(device)\n",
    "    loss_cls = DynamicWeightedLoss(num_classes=4, initial_weights=class_weights).to(device)\n",
    "    loss_proto = PrototypeContrastiveLoss(feature_dim=32 , num_classes=4 , tau=0.1).to(device)\n",
    "\n",
    "\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        list(cfe.parameters()) + list(sfe.parameters()) + list(ssc.parameters()),\n",
    "        lr=2e-3, weight_decay=1e-3\n",
    "    )\n",
    "    \n",
    "\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5, verbose=True , min_lr=1e-6 )\n",
    "\n",
    "    best_acc = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        cfe.train()\n",
    "        sfe.train()\n",
    "        ssc.train()\n",
    "\n",
    "        epoch_loss, correct, total = 0, 0, 0\n",
    "        mmd_weight = max(0.1*(1 - epoch / num_epochs), 0.01)  # Decrease MMD weight over epochs\n",
    "        \n",
    "        class_correct = torch.zeros(4).to(device)\n",
    "        class_total = torch.zeros(4).to(device)\n",
    "        \n",
    "        if epoch < warmup_epochs * 0.8:\n",
    "            warmup_factor = 0.0\n",
    "        elif epoch < warmup_epochs:\n",
    "            progress = (epoch - warmup_epochs * 0.7) / (warmup_epochs - warmup_epochs * 0.3)\n",
    "            warmup_factor = progress\n",
    "        else:\n",
    "            warmup_factor = 1.0\n",
    "            \n",
    "        print(f\"Warmup factor: {warmup_factor:.4f}\")\n",
    "\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            \n",
    "            xb_aug = augment_eeg(xb.clone())\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            z_common_org = cfe(xb)\n",
    "            z_common_aug = cfe(xb_aug)\n",
    "            \n",
    "            z_subject = sfe(z_common_org)\n",
    "            logits = ssc(z_subject)\n",
    "\n",
    "            loss = loss_con1(z_common_org, yb)\n",
    "\n",
    "            if warmup_factor > 0.7:\n",
    "                with torch.no_grad():\n",
    "                    \n",
    "                    pseudo_logits = ssc(sfe(cfe(xb)))\n",
    "                    pseudo_probs = F.softmax(pseudo_logits, dim=1)\n",
    "                    pseudo_entropy = -torch.sum(pseudo_probs * torch.log(pseudo_probs + 1e-9), dim=1)\n",
    "                    \n",
    "                    probs = F.softmax(logits, dim=1)\n",
    "                    confidence = torch.max(probs, dim=1)[0]\n",
    "                    penalty = confidence.mean()\n",
    "                    # pseudo_labels = pseudo_logits.argmax(dim=1)\n",
    "                \n",
    "                    # Adaptive threshold: starts strict, loosens\n",
    "                entropy_threshold = max(0.9, 1.5 - epoch / num_epochs)\n",
    "                \n",
    "                confident_mask = pseudo_entropy < entropy_threshold\n",
    "                \n",
    "                z_subject_filtered = z_subject[confident_mask].detach()\n",
    "                pseudo_labels_filtered = pseudo_logits.argmax(dim=1)[confident_mask].detach()\n",
    "                \n",
    "                if z_subject_filtered.size(0) > 0:\n",
    "                    # contrastive_loss = loss_con2(z_subject_filtered , pseudo_labels_filtered)\n",
    "                    contrastive_loss = loss_proto(z_subject_filtered , pseudo_labels_filtered)\n",
    "                else:\n",
    "                    contrastive_loss = torch.tensor(0.0 , device=device)\n",
    "                    \n",
    "                alpha = min(epoch / num_epochs *2 , 1.0)\n",
    "                                    \n",
    "                beta = 0.1\n",
    "                loss = ((1-warmup_factor)*1.0 * loss_cls(logits, yb)) + \\\n",
    "                       (warmup_factor * (alpha * (0.5 * contrastive_loss) + \\\n",
    "                       alpha * (mmd_weight * loss_mmd(z_common_org, z_common_aug)) + \\\n",
    "                       beta * penalty))\n",
    "            else :\n",
    "                loss = loss_con1(z_common_org, yb)    \n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(list(cfe.parameters()) + list(sfe.parameters()) + list(ssc.parameters()), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            preds = logits.argmax(1)\n",
    "            correct += (logits.argmax(1) == yb).sum().item()\n",
    "            total += yb.size(0)\n",
    "            \n",
    "            for c in range(4):\n",
    "                class_mask = (yb == c)\n",
    "                if class_mask.sum() > 0:\n",
    "                    class_correct[c] += (preds[class_mask] == yb[class_mask]).sum().item()\n",
    "                    class_total[c] += class_mask.sum().item()\n",
    "        \n",
    "        print(\"\\nPer-class accuracy:\")\n",
    "        for c in range(4):\n",
    "            if class_total[c] > 0:\n",
    "                class_acc = class_correct[c] / class_total[c]\n",
    "                print(f\"Class {c}: {class_acc:.4f} ({int(class_total[c])} / {int(class_total[c])})\")\n",
    "            else:\n",
    "                print(f\"Class {c}: No samples\")\n",
    "        \n",
    "        print(f\"Current class weights: {loss_cls.weights.cpu().numpy()}\")\n",
    "            \n",
    "        train_loss = epoch_loss/ len(train_loader)\n",
    "        train_acc = correct / total\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] - Loss: {train_loss:.4f} - Train Acc: {train_acc:.4f}\")\n",
    "\n",
    "        # Validation\n",
    "        if (epoch + 1) % 5 == 0 or epoch == num_epochs - 1:\n",
    "            cfe.eval(); sfe.eval(); ssc.eval()\n",
    "            val_loss = 0\n",
    "            all_preds, all_labels = [], []\n",
    "            with torch.no_grad():\n",
    "                for xb, yb in test_loader:\n",
    "                    xb = xb.to(device)\n",
    "                    z = sfe(cfe(xb))\n",
    "                    logits = ssc(z)\n",
    "                    all_preds.extend(logits.argmax(1).cpu().numpy())\n",
    "                    all_labels.extend(yb.numpy())\n",
    "            \n",
    "            val_loss = val_loss/len(test_loader)\n",
    "            val_acc = accuracy_score(all_labels, all_preds)\n",
    "            \n",
    "            val_losses.append(val_loss)\n",
    "            val_accs.append(val_acc)\n",
    "            \n",
    "            print(f\"📊 Val Acc: {val_acc:.4f}\")\n",
    "            scheduler.step(val_acc)\n",
    "            if val_acc > best_acc:\n",
    "                best_acc = val_acc\n",
    "                torch.save({\n",
    "                    'cfe': cfe.state_dict(),\n",
    "                    'sfe': sfe.state_dict(),\n",
    "                    'ssc': ssc.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'epoch': epoch + 1\n",
    "                }, f\"checkpoints/new_best_model.pt\")\n",
    "\n",
    "    print(\"\\n✅ Training complete.\")\n",
    "    print(f\"Best Val Accuracy: {best_acc:.4f}\")\n",
    "\n",
    "    # Final Confusion Matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds, labels=[0, 1, 2, 3])\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix - All Subjects')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.savefig('plots/new_confmat_all_subjects.png')\n",
    "    plt.close()\n",
    "    \n",
    "    plot_training_curves(train_losses, train_accs, val_losses, val_accs, save_path=\"plots/new_final_training_curves.png\")\n",
    "\n",
    "    return best_acc, all_labels, all_preds , train_losses , train_accs , val_losses , val_accs\n",
    "\n",
    "def plot_training_curves(train_losses, train_accs, val_losses, val_accs, save_path=None):\n",
    "    \"\"\"Plot training and validation curves\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot losses\n",
    "    ax1.plot(train_losses, label='Training Loss', color='blue')\n",
    "    if val_losses:\n",
    "        # Plot validation points where we have them\n",
    "        val_indices = np.linspace(0, len(train_losses)-1, len(val_losses)).astype(int)\n",
    "        ax1.plot(val_indices, val_losses, label='Validation Loss', color='red', marker='o')\n",
    "    ax1.set_title('Loss Curves')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "\n",
    "    # Plot accuracies\n",
    "    ax2.plot(train_accs, label='Training Accuracy', color='blue')\n",
    "    if val_accs:\n",
    "        # Plot validation points where we have them\n",
    "        val_indices = np.linspace(0, len(train_accs)-1, len(val_accs)).astype(int)\n",
    "        ax2.plot(val_indices, val_accs, label='Validation Accuracy', color='red', marker='o')\n",
    "    ax2.set_title('Accuracy Curves')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "# ------------------ Main ------------------\n",
    "if __name__ == \"__main__\":\n",
    "    feature_path = r\"E:\\FYP\\Finalise Fyp\\EEg-based-Emotion-Recognition\\de_features.npy\"\n",
    "    label_path = r\"E:\\FYP\\Finalise Fyp\\EEg-based-Emotion-Recognition\\de_labels.npy\"\n",
    "    train_all_subjects(feature_path, label_path, num_epochs=250, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78699af",
   "metadata": {},
   "source": [
    "## TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3517b950",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_trained_model(checkpoint_path=\"E:\\FYP\\Finalise Fyp\\EEg-based-Emotion-Recognition\\FYP_2\\Hamza\\checkpoints/new_best_model.pt\"):\n",
    "    cfe = CommonFeatureExtractor().to(device)\n",
    "    sfe = SubjectSpecificMapper().to(device)\n",
    "    ssc = SubjectSpecificClassifier().to(device)\n",
    "\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    cfe.load_state_dict(checkpoint['cfe'])\n",
    "    sfe.load_state_dict(checkpoint['sfe'])\n",
    "    ssc.load_state_dict(checkpoint['ssc'])\n",
    "\n",
    "    cfe.eval()\n",
    "    sfe.eval()\n",
    "    ssc.eval()\n",
    "    return cfe, sfe, ssc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45dc0386",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(feature_path, label_path, checkpoint_path=\"E:/FYP/Finalise Fyp/EEg-based-Emotion-Recognition/FYP_2/Hamza/checkpoints/new_best_model.pt\", batch_size=64):\n",
    "    print(\"🔍 Testing saved model...\")\n",
    "\n",
    "    # Load the model\n",
    "    cfe, sfe, ssc = load_trained_model(checkpoint_path)\n",
    "\n",
    "    # Prepare dataset\n",
    "    dataset = DEAPDataset(feature_path, label_path, normalize=True)\n",
    "    indices = list(range(len(dataset)))\n",
    "    labels = [label for _, label in dataset]\n",
    "    _, test_idx, _, _ = train_test_split(indices, labels, test_size=0.2, stratify=labels, random_state=42)\n",
    "    test_set = Subset(dataset, test_idx)\n",
    "    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in test_loader:\n",
    "            xb = xb.to(device)\n",
    "            z = sfe(cfe(xb))\n",
    "            logits = ssc(z)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(yb.numpy())\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    print(f\"✅ Test Accuracy: {acc:.4f}\")\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds, labels=[0, 1, 2, 3])\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Test Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.savefig(\"plots/new_test_confmat.png\")\n",
    "    plt.close()\n",
    "\n",
    "    return acc, all_labels, all_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef09769",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    feature_path = \"E:/FYP/Finalise Fyp/EEg-based-Emotion-Recognition/de_features.npy\"\n",
    "    label_path = \"E:/FYP/Finalise Fyp/EEg-based-Emotion-Recognition/de_labels.npy\"\n",
    "    test_model(feature_path, label_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f118c01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def plot_tsne_features(cfe, sfe, ssc, feature_path, label_path, layer=\"sfe\", num_samples=2000):\n",
    "    dataset = DEAPDataset(feature_path, label_path, normalize=True)\n",
    "    indices = list(range(len(dataset)))\n",
    "    labels = [label for _, label in dataset]\n",
    "    _, test_idx, _, _ = train_test_split(indices, labels, test_size=0.2, stratify=labels, random_state=42)\n",
    "    test_set = Subset(dataset, test_idx)\n",
    "    test_loader = DataLoader(test_set, batch_size=128, shuffle=True)\n",
    "\n",
    "    all_feats, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in test_loader:\n",
    "            xb = xb.to(device)\n",
    "            z_cfe = cfe(xb)                     # 64-d\n",
    "            z_sfe = sfe(z_cfe)                  # 32-d\n",
    "            feats = z_sfe if layer == \"sfe\" else z_cfe\n",
    "            all_feats.append(feats.cpu())\n",
    "            all_labels.append(yb)\n",
    "\n",
    "            if len(torch.cat(all_feats)) >= num_samples:\n",
    "                break\n",
    "\n",
    "    all_feats = torch.cat(all_feats)[:num_samples]\n",
    "    all_labels = torch.cat(all_labels)[:num_samples]\n",
    "\n",
    "    tsne = TSNE(n_components=2, perplexity=30, learning_rate=200, n_iter=1000)\n",
    "    tsne_feats = tsne.fit_transform(all_feats)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    scatter = plt.scatter(tsne_feats[:, 0], tsne_feats[:, 1], c=all_labels, cmap='tab10', alpha=0.7)\n",
    "    plt.legend(*scatter.legend_elements(), title=\"Classes\")\n",
    "    plt.title(f\"t-SNE of {'SFE' if layer == 'sfe' else 'CFE'} features\")\n",
    "    plt.savefig(f\"plots/new_tsne_{layer}_features.png\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4e47db",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    feature_path = \"E:/FYP/Finalise Fyp/EEg-based-Emotion-Recognition/de_features.npy\"\n",
    "    label_path = \"E:/FYP/Finalise Fyp/EEg-based-Emotion-Recognition/de_labels.npy\"\n",
    "\n",
    "    # Load the model\n",
    "    cfe, sfe, ssc = load_trained_model(\"E:/FYP/Finalise Fyp/EEg-based-Emotion-Recognition/FYP_2/Hamza/checkpoints/new_best_model.pt\")\n",
    "\n",
    "    # Plot t-SNE from either 'cfe' or 'sfe'\n",
    "    plot_tsne_features(cfe, sfe, ssc, feature_path, label_path, layer=\"sfe\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b95caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tsne_features(cfe, sfe, ssc, feature_path, label_path, layer=\"cfe\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95540406",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
