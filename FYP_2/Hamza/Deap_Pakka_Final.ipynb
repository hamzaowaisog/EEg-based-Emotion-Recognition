{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2848cb5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "os.makedirs(\"plots\", exist_ok=True)\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4ef7841",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DEAPDataset(Dataset):\n",
    "    def __init__(self, feature_path, label_path, exclude_subject=None, only_subject=None, normalize=True):\n",
    "        self.features = np.load(feature_path)  # shape: (32, 40, 40, 5, 63)\n",
    "        self.labels = np.load(label_path)      # shape: (32, 40, 63)\n",
    "        \n",
    "        # Compute statistics for normalization if requested\n",
    "        if normalize:\n",
    "            all_features = []\n",
    "            for subj in range(32):\n",
    "                if exclude_subject is not None and subj == exclude_subject:\n",
    "                    continue\n",
    "                if only_subject is not None and subj != only_subject:\n",
    "                    continue\n",
    "                \n",
    "                # Reshape to get all features for this subject\n",
    "                subj_features = self.features[subj].transpose(0, 1, 3, 2).reshape(-1, 40, 5)  # Result: (N, 40, 5)\n",
    "                all_features.append(subj_features)\n",
    "            \n",
    "            if all_features:  # Check if list is not empty\n",
    "                all_features = np.concatenate(all_features, axis=0)\n",
    "                self.mean = np.mean(all_features, axis=0)\n",
    "                self.std = np.std(all_features, axis=0) + 1e-8\n",
    "            else:\n",
    "                # Default if no subjects selected\n",
    "                self.mean = 0\n",
    "                self.std = 1\n",
    "        else:\n",
    "            self.mean = 0\n",
    "            self.std = 1\n",
    "            \n",
    "        # Prepare samples\n",
    "        self.samples = []\n",
    "        for subj in range(32):\n",
    "            if exclude_subject is not None and subj == exclude_subject:\n",
    "                continue\n",
    "            if only_subject is not None and subj != only_subject:\n",
    "                continue\n",
    "                \n",
    "            for trial in range(40):\n",
    "                for win in range(63):\n",
    "                    x = self.features[subj, trial, :, :, win]\n",
    "                    y = self.labels[subj, trial, win]\n",
    "                    self.samples.append((x, y, subj))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, y, subj = self.samples[idx]\n",
    "        # Normalize the data\n",
    "        x = (x - self.mean) / self.std\n",
    "        return torch.tensor(x, dtype=torch.float32), int(y)\n",
    "\n",
    "    def get_subject_data(self, subject):\n",
    "        return [(torch.tensor((x - self.mean) / self.std, dtype=torch.float32), int(y))\n",
    "                for x, y, subj in self.samples if subj == subject]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1b208ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommonFeatureExtractor(nn.Module):\n",
    "    def __init__(self, input_dim=200):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 256)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.act = nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # [batch, 40, 5] → [batch, 200]\n",
    "        x = self.act(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.act(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.act(self.bn3(self.fc3(x)))\n",
    "        return x\n",
    "\n",
    "class SubjectSpecificMapper(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(64, 32)\n",
    "        self.bn = nn.BatchNorm1d(32)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.act = nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.act(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class SubjectSpecificClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(32, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31f9f1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupervisedContrastiveLoss(nn.Module):\n",
    "    def __init__(self, temperature=0.07):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, features, labels):\n",
    "        features = F.normalize(features, dim=1)\n",
    "        batch_size = features.shape[0]\n",
    "        \n",
    "        # Handle case where batch size is 1\n",
    "        if batch_size <= 1:\n",
    "            return torch.tensor(0.0, device=features.device, requires_grad=True)\n",
    "            \n",
    "        sim_matrix = torch.matmul(features, features.T) / self.temperature\n",
    "        labels = labels.contiguous().view(-1, 1)\n",
    "        mask = torch.eq(labels, labels.T).float().to(features.device)\n",
    "        logits_mask = torch.ones_like(mask) - torch.eye(batch_size).to(features.device)\n",
    "        mask = mask * logits_mask\n",
    "        \n",
    "        # Handle case where there are no positive pairs\n",
    "        if mask.sum() == 0:\n",
    "            return torch.tensor(0.0, device=features.device, requires_grad=True)\n",
    "            \n",
    "        exp_sim = torch.exp(sim_matrix) * logits_mask\n",
    "        log_prob = sim_matrix - torch.log(exp_sim.sum(dim=1, keepdim=True) + 1e-9)\n",
    "        loss = - (mask * log_prob).sum() / (mask.sum() + 1e-9)\n",
    "        return loss\n",
    "\n",
    "class MMDLoss(nn.Module):\n",
    "    def __init__(self, kernel_mul=2.0, num_kernels=5):\n",
    "        super().__init__()\n",
    "        self.kernel_mul = kernel_mul\n",
    "        self.num_kernels = num_kernels\n",
    "\n",
    "    def gaussian_kernel(self, source, target):\n",
    "        total = torch.cat([source, target], dim=0)\n",
    "        \n",
    "        # Handle small batch sizes\n",
    "        if total.shape[0] <= 1:\n",
    "            return torch.tensor(0.0, device=source.device, requires_grad=True)\n",
    "            \n",
    "        total0 = total.unsqueeze(0)\n",
    "        total1 = total.unsqueeze(1)\n",
    "        L2_distance = ((total0 - total1) ** 2).sum(2)\n",
    "        \n",
    "        # Prevent division by zero\n",
    "        bandwidth = torch.mean(L2_distance.detach()) + 1e-8\n",
    "        bandwidth_list = [bandwidth * (self.kernel_mul ** i) for i in range(self.num_kernels)]\n",
    "        kernels = [torch.exp(-L2_distance / bw) for bw in bandwidth_list]\n",
    "        return sum(kernels) / len(kernels)\n",
    "\n",
    "    def forward(self, source, target):\n",
    "        source = source.view(source.size(0), -1)\n",
    "        target = target.view(target.size(0), -1)\n",
    "        \n",
    "        # Handle empty batches\n",
    "        if source.shape[0] == 0 or target.shape[0] == 0:\n",
    "            return torch.tensor(0.0, device=source.device, requires_grad=True)\n",
    "            \n",
    "        kernels = self.gaussian_kernel(source, target)\n",
    "        batch_size = source.size(0)\n",
    "        XX = kernels[:batch_size, :batch_size]\n",
    "        YY = kernels[batch_size:, batch_size:]\n",
    "        XY = kernels[:batch_size, batch_size:]\n",
    "        YX = kernels[batch_size:, :batch_size]\n",
    "        return torch.mean(XX + YY - XY - YX)\n",
    "\n",
    "class ContrastiveLossLcon2(nn.Module):\n",
    "    def __init__(self, feature_dim=32, num_classes=4, tau=0.1, gamma=0.5, queue_size=1024):\n",
    "        super().__init__()\n",
    "        self.tau = tau\n",
    "        self.gamma = gamma\n",
    "        self.prototypes = nn.Parameter(torch.randn(num_classes, feature_dim))\n",
    "        self.register_buffer(\"queue\", torch.randn(queue_size, feature_dim))\n",
    "        self.queue = F.normalize(self.queue, dim=-1)\n",
    "\n",
    "    def forward(self, z_t, pseudo_labels):\n",
    "        # Handle empty batches\n",
    "        if z_t.shape[0] == 0:\n",
    "            return torch.tensor(0.0, device=z_t.device, requires_grad=True)\n",
    "            \n",
    "        z_t = F.normalize(z_t, dim=-1)\n",
    "        device = z_t.device\n",
    "        pseudo_labels = pseudo_labels.to(device)\n",
    "        \n",
    "        pos_proto = self.prototypes.to(device)[pseudo_labels]\n",
    "        \n",
    "        # Compute positive and negative logits\n",
    "        pos_logits = torch.sum(z_t * pos_proto, dim=-1) / self.tau\n",
    "        \n",
    "        # Handle case where queue is empty\n",
    "        if self.queue.shape[0] == 0:\n",
    "            return self.gamma * F.cross_entropy(pos_logits.unsqueeze(1), torch.zeros(z_t.size(0), dtype=torch.long, device=device))\n",
    "            \n",
    "        neg_logits = torch.matmul(z_t, self.queue.to(device).T) / self.tau\n",
    "        logits = torch.cat([pos_logits.unsqueeze(1), neg_logits], dim=1)\n",
    "        labels = torch.zeros(z_t.size(0), dtype=torch.long).to(device)\n",
    "        \n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        self._dequeue_and_enqueue(z_t)\n",
    "        return self.gamma * loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _dequeue_and_enqueue(self, embeddings):\n",
    "        embeddings = embeddings.detach().to(self.queue.device)\n",
    "        batch_size = embeddings.size(0)\n",
    "        queue_size = self.queue.size(0)\n",
    "        \n",
    "        if batch_size >= queue_size:\n",
    "            self.queue = embeddings[-queue_size:]\n",
    "        else:\n",
    "            self.queue = torch.cat([self.queue[batch_size:], embeddings], dim=0)\n",
    "\n",
    "class GeneralizedCrossEntropy(nn.Module):\n",
    "    def __init__(self, q=0.7, weight=None):\n",
    "        super().__init__()\n",
    "        self.q = q\n",
    "        self.weight = weight  # class weights (tensor)\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        \n",
    "        # Handle empty batches\n",
    "        if targets.shape[0] == 0:\n",
    "            return torch.tensor(0.0, device=logits.device, requires_grad=True)\n",
    "            \n",
    "        targets_onehot = F.one_hot(targets, num_classes=probs.shape[1]).float()\n",
    "        probs = torch.sum(probs * targets_onehot, dim=1)\n",
    "\n",
    "        if self.weight is not None:\n",
    "            weights = self.weight[targets]\n",
    "            loss = (1 - probs ** self.q) / self.q\n",
    "            return (weights * loss).mean()\n",
    "        else:\n",
    "            return ((1 - probs ** self.q) / self.q).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09d36043",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_batch(dataset, exclude, batch_size=64):\n",
    "    \"\"\"Get a batch of data from target subjects\"\"\"\n",
    "    for subj in range(32):\n",
    "        if subj != exclude:\n",
    "            data = dataset.get_subject_data(subj)\n",
    "            if len(data) >= batch_size:\n",
    "                indices = torch.randperm(len(data))[:batch_size]\n",
    "                x, y = zip(*[data[i] for i in indices])\n",
    "                return torch.stack(x), torch.tensor(y)\n",
    "    \n",
    "    # Fallback: Return a small batch if no subject has enough data\n",
    "    all_data = []\n",
    "    for subj in range(32):\n",
    "        if subj != exclude:\n",
    "            all_data.extend(dataset.get_subject_data(subj))\n",
    "    \n",
    "    if len(all_data) == 0:\n",
    "        # Empty tensor with correct shape as a fallback\n",
    "        empty_sample = next(iter(dataset))\n",
    "        return torch.zeros((0, *empty_sample[0].shape), dtype=torch.float32), torch.zeros(0, dtype=torch.long)\n",
    "        \n",
    "    indices = torch.randperm(len(all_data))[:min(batch_size, len(all_data))]\n",
    "    x, y = zip(*[all_data[i] for i in indices])\n",
    "    return torch.stack(x), torch.tensor(y)\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, subject_idx):\n",
    "    \"\"\"Create and save confusion matrix plot\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1, 2, 3])\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'Confusion Matrix - Subject {subject_idx+1}')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.savefig(f'plots/confmat_subject{subject_idx+1}.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3838163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 Starting Fold 1/32 - Test Subject: s01\n",
      "Epoch [1/100] - Loss: 6.1280 - Con1: 6.1280 - MMD: 0.0043 - Train Acc: 0.0000\n",
      "📊 Validation Accuracy on Subject 1: 0.3214\n",
      "Epoch [2/100] - Loss: 4.1410 - Con1: 4.1409 - MMD: 0.0056 - Train Acc: 0.0000\n",
      "Epoch [3/100] - Loss: 4.1368 - Con1: 4.1367 - MMD: 0.0048 - Train Acc: 0.0000\n",
      "Epoch [4/100] - Loss: 4.1323 - Con1: 4.1322 - MMD: 0.0043 - Train Acc: 0.0000\n",
      "Epoch [5/100] - Loss: 4.1277 - Con1: 4.1276 - MMD: 0.0044 - Train Acc: 0.0000\n",
      "Epoch [6/100] - Loss: 4.1241 - Con1: 4.1239 - MMD: 0.0041 - Train Acc: 0.0000\n",
      "📊 Validation Accuracy on Subject 1: 0.2833\n",
      "Epoch [7/100] - Loss: 4.1197 - Con1: 4.1195 - MMD: 0.0034 - Train Acc: 0.0000\n",
      "Epoch [8/100] - Loss: 4.1162 - Con1: 4.1160 - MMD: 0.0037 - Cls: 1.1618 - Train Acc: 0.2200\n",
      "🏆 New best accuracy for fold 1: 0.2200\n",
      "Epoch [9/100] - Loss: 3.7228 - Con1: 4.1138 - MMD: 0.0035 - Cls: 0.9848 - Train Acc: 0.4669\n",
      "🏆 New best accuracy for fold 1: 0.4669\n",
      "Epoch [10/100] - Loss: 3.3229 - Con1: 4.1131 - MMD: 0.0038 - Cls: 0.9512 - Train Acc: 0.4815\n",
      "🏆 New best accuracy for fold 1: 0.4815\n",
      "Epoch [11/100] - Loss: 2.9372 - Con1: 4.1191 - MMD: 0.0037 - Cls: 0.9667 - Train Acc: 0.4682\n",
      "📊 Validation Accuracy on Subject 1: 0.1587\n",
      "Epoch [12/100] - Loss: 2.5339 - Con1: 4.1167 - MMD: 0.0034 - Cls: 0.9505 - Train Acc: 0.4765\n",
      "Epoch [13/100] - Loss: 2.1286 - Con1: 4.1149 - MMD: 0.0028 - Cls: 0.9364 - Train Acc: 0.4873\n",
      "🏆 New best accuracy for fold 1: 0.4873\n",
      "Epoch [14/100] - Loss: 1.7206 - Con1: 4.1125 - MMD: 0.0028 - Cls: 0.9229 - Train Acc: 0.4950\n",
      "🏆 New best accuracy for fold 1: 0.4950\n",
      "Epoch [15/100] - Loss: 1.3049 - Con1: 4.1086 - MMD: 0.0026 - Cls: 0.9040 - Train Acc: 0.5070\n",
      "🏆 New best accuracy for fold 1: 0.5070\n",
      "Epoch [16/100] - Loss: 1.0991 - Con2: 0.2768 - MMD: 0.0027 - Cls: 0.9605 - Train Acc: 0.4730\n",
      "📊 Validation Accuracy on Subject 1: 0.1528\n",
      "Epoch [17/100] - Loss: 0.9496 - Con2: 0.0428 - MMD: 0.0030 - Cls: 0.9279 - Train Acc: 0.4927\n",
      "Epoch [18/100] - Loss: 0.9280 - Con2: 0.0319 - MMD: 0.0034 - Cls: 0.9117 - Train Acc: 0.5032\n",
      "Epoch [19/100] - Loss: 0.9117 - Con2: 0.0298 - MMD: 0.0032 - Cls: 0.8964 - Train Acc: 0.5165\n",
      "🏆 New best accuracy for fold 1: 0.5165\n",
      "Epoch [20/100] - Loss: 0.9014 - Con2: 0.0290 - MMD: 0.0036 - Cls: 0.8866 - Train Acc: 0.5201\n",
      "🏆 New best accuracy for fold 1: 0.5201\n",
      "Epoch [21/100] - Loss: 0.8905 - Con2: 0.0254 - MMD: 0.0034 - Cls: 0.8775 - Train Acc: 0.5272\n",
      "🏆 New best accuracy for fold 1: 0.5272\n",
      "📊 Validation Accuracy on Subject 1: 0.2659\n",
      "Epoch [22/100] - Loss: 0.8834 - Con2: 0.0239 - MMD: 0.0036 - Cls: 0.8710 - Train Acc: 0.5298\n",
      "🏆 New best accuracy for fold 1: 0.5298\n",
      "Epoch [23/100] - Loss: 0.8775 - Con2: 0.0212 - MMD: 0.0041 - Cls: 0.8665 - Train Acc: 0.5339\n",
      "🏆 New best accuracy for fold 1: 0.5339\n",
      "Epoch [24/100] - Loss: 0.8662 - Con2: 0.0210 - MMD: 0.0042 - Cls: 0.8553 - Train Acc: 0.5408\n",
      "🏆 New best accuracy for fold 1: 0.5408\n",
      "Epoch [25/100] - Loss: 0.8622 - Con2: 0.0203 - MMD: 0.0041 - Cls: 0.8516 - Train Acc: 0.5430\n",
      "🏆 New best accuracy for fold 1: 0.5430\n",
      "Epoch [26/100] - Loss: 0.8539 - Con2: 0.0192 - MMD: 0.0043 - Cls: 0.8439 - Train Acc: 0.5479\n",
      "🏆 New best accuracy for fold 1: 0.5479\n",
      "📊 Validation Accuracy on Subject 1: 0.2655\n",
      "Epoch [27/100] - Loss: 0.8514 - Con2: 0.0187 - MMD: 0.0045 - Cls: 0.8415 - Train Acc: 0.5506\n",
      "🏆 New best accuracy for fold 1: 0.5506\n",
      "Epoch [28/100] - Loss: 0.8491 - Con2: 0.0188 - MMD: 0.0044 - Cls: 0.8393 - Train Acc: 0.5511\n",
      "🏆 New best accuracy for fold 1: 0.5511\n",
      "Epoch [29/100] - Loss: 0.8457 - Con2: 0.0192 - MMD: 0.0044 - Cls: 0.8357 - Train Acc: 0.5533\n",
      "🏆 New best accuracy for fold 1: 0.5533\n",
      "Epoch [30/100] - Loss: 0.8440 - Con2: 0.0184 - MMD: 0.0044 - Cls: 0.8344 - Train Acc: 0.5545\n",
      "🏆 New best accuracy for fold 1: 0.5545\n",
      "Epoch [31/100] - Loss: 0.8861 - Con2: 0.0222 - MMD: 0.0043 - Cls: 0.8745 - Train Acc: 0.5282\n",
      "📊 Validation Accuracy on Subject 1: 0.1687\n",
      "Epoch [32/100] - Loss: 0.8872 - Con2: 0.0174 - MMD: 0.0047 - Cls: 0.8780 - Train Acc: 0.5279\n",
      "Epoch [33/100] - Loss: 0.8853 - Con2: 0.0164 - MMD: 0.0052 - Cls: 0.8765 - Train Acc: 0.5268\n",
      "Epoch [34/100] - Loss: 0.8798 - Con2: 0.0168 - MMD: 0.0051 - Cls: 0.8709 - Train Acc: 0.5283\n",
      "Epoch [35/100] - Loss: 0.8791 - Con2: 0.0180 - MMD: 0.0056 - Cls: 0.8696 - Train Acc: 0.5319\n",
      "Epoch [36/100] - Loss: 0.8687 - Con2: 0.0181 - MMD: 0.0057 - Cls: 0.8591 - Train Acc: 0.5355\n",
      "📊 Validation Accuracy on Subject 1: 0.2683\n",
      "Epoch [37/100] - Loss: 0.8673 - Con2: 0.0169 - MMD: 0.0055 - Cls: 0.8584 - Train Acc: 0.5370\n",
      "Epoch [38/100] - Loss: 0.8572 - Con2: 0.0169 - MMD: 0.0062 - Cls: 0.8481 - Train Acc: 0.5449\n",
      "Epoch [39/100] - Loss: 0.8540 - Con2: 0.0164 - MMD: 0.0057 - Cls: 0.8452 - Train Acc: 0.5463\n",
      "Epoch [40/100] - Loss: 0.8511 - Con2: 0.0182 - MMD: 0.0059 - Cls: 0.8414 - Train Acc: 0.5474\n",
      "Epoch [41/100] - Loss: 0.8491 - Con2: 0.0190 - MMD: 0.0060 - Cls: 0.8390 - Train Acc: 0.5504\n",
      "📊 Validation Accuracy on Subject 1: 0.2504\n",
      "Epoch [42/100] - Loss: 0.8425 - Con2: 0.0180 - MMD: 0.0062 - Cls: 0.8329 - Train Acc: 0.5536\n",
      "Epoch [43/100] - Loss: 0.8411 - Con2: 0.0179 - MMD: 0.0056 - Cls: 0.8316 - Train Acc: 0.5554\n",
      "🏆 New best accuracy for fold 1: 0.5554\n",
      "Epoch [44/100] - Loss: 0.8341 - Con2: 0.0177 - MMD: 0.0059 - Cls: 0.8247 - Train Acc: 0.5576\n",
      "🏆 New best accuracy for fold 1: 0.5576\n",
      "Epoch [45/100] - Loss: 0.8324 - Con2: 0.0169 - MMD: 0.0059 - Cls: 0.8234 - Train Acc: 0.5586\n",
      "🏆 New best accuracy for fold 1: 0.5586\n",
      "Epoch [46/100] - Loss: 0.8220 - Con2: 0.0161 - MMD: 0.0061 - Cls: 0.8133 - Train Acc: 0.5648\n",
      "🏆 New best accuracy for fold 1: 0.5648\n",
      "📊 Validation Accuracy on Subject 1: 0.2361\n",
      "Epoch [47/100] - Loss: 0.8179 - Con2: 0.0169 - MMD: 0.0065 - Cls: 0.8087 - Train Acc: 0.5678\n",
      "🏆 New best accuracy for fold 1: 0.5678\n",
      "Epoch [48/100] - Loss: 0.8177 - Con2: 0.0180 - MMD: 0.0069 - Cls: 0.8080 - Train Acc: 0.5695\n",
      "🏆 New best accuracy for fold 1: 0.5695\n",
      "Epoch [49/100] - Loss: 0.8105 - Con2: 0.0168 - MMD: 0.0069 - Cls: 0.8014 - Train Acc: 0.5739\n",
      "🏆 New best accuracy for fold 1: 0.5739\n",
      "Epoch [50/100] - Loss: 0.8050 - Con2: 0.0162 - MMD: 0.0070 - Cls: 0.7962 - Train Acc: 0.5757\n",
      "🏆 New best accuracy for fold 1: 0.5757\n",
      "Epoch [51/100] - Loss: 0.8028 - Con2: 0.0162 - MMD: 0.0071 - Cls: 0.7940 - Train Acc: 0.5776\n",
      "🏆 New best accuracy for fold 1: 0.5776\n",
      "📊 Validation Accuracy on Subject 1: 0.2040\n",
      "Epoch [52/100] - Loss: 0.7976 - Con2: 0.0166 - MMD: 0.0069 - Cls: 0.7886 - Train Acc: 0.5806\n",
      "🏆 New best accuracy for fold 1: 0.5806\n",
      "Epoch [53/100] - Loss: 0.7945 - Con2: 0.0171 - MMD: 0.0072 - Cls: 0.7852 - Train Acc: 0.5819\n",
      "🏆 New best accuracy for fold 1: 0.5819\n",
      "Epoch [54/100] - Loss: 0.7879 - Con2: 0.0169 - MMD: 0.0075 - Cls: 0.7787 - Train Acc: 0.5877\n",
      "🏆 New best accuracy for fold 1: 0.5877\n",
      "Epoch [55/100] - Loss: 0.7835 - Con2: 0.0157 - MMD: 0.0074 - Cls: 0.7749 - Train Acc: 0.5902\n",
      "🏆 New best accuracy for fold 1: 0.5902\n",
      "Epoch [56/100] - Loss: 0.7824 - Con2: 0.0162 - MMD: 0.0075 - Cls: 0.7735 - Train Acc: 0.5908\n",
      "🏆 New best accuracy for fold 1: 0.5908\n",
      "📊 Validation Accuracy on Subject 1: 0.1881\n",
      "Epoch [57/100] - Loss: 0.7804 - Con2: 0.0165 - MMD: 0.0075 - Cls: 0.7714 - Train Acc: 0.5920\n",
      "🏆 New best accuracy for fold 1: 0.5920\n",
      "Epoch [58/100] - Loss: 0.7731 - Con2: 0.0161 - MMD: 0.0075 - Cls: 0.7643 - Train Acc: 0.5949\n",
      "🏆 New best accuracy for fold 1: 0.5949\n",
      "Epoch [59/100] - Loss: 0.7770 - Con2: 0.0160 - MMD: 0.0077 - Cls: 0.7682 - Train Acc: 0.5933\n",
      "Epoch [60/100] - Loss: 0.7628 - Con2: 0.0159 - MMD: 0.0076 - Cls: 0.7541 - Train Acc: 0.6026\n",
      "🏆 New best accuracy for fold 1: 0.6026\n",
      "Epoch [61/100] - Loss: 0.7653 - Con2: 0.0154 - MMD: 0.0080 - Cls: 0.7568 - Train Acc: 0.6009\n",
      "📊 Validation Accuracy on Subject 1: 0.1964\n",
      "Epoch [62/100] - Loss: 0.7629 - Con2: 0.0154 - MMD: 0.0080 - Cls: 0.7545 - Train Acc: 0.6018\n",
      "Epoch [63/100] - Loss: 0.7611 - Con2: 0.0161 - MMD: 0.0080 - Cls: 0.7522 - Train Acc: 0.6027\n",
      "🏆 New best accuracy for fold 1: 0.6027\n",
      "Epoch [64/100] - Loss: 0.7622 - Con2: 0.0157 - MMD: 0.0079 - Cls: 0.7535 - Train Acc: 0.6032\n",
      "🏆 New best accuracy for fold 1: 0.6032\n",
      "Epoch [65/100] - Loss: 0.7593 - Con2: 0.0166 - MMD: 0.0082 - Cls: 0.7501 - Train Acc: 0.6048\n",
      "🏆 New best accuracy for fold 1: 0.6048\n",
      "Epoch [66/100] - Loss: 0.7492 - Con2: 0.0163 - MMD: 0.0083 - Cls: 0.7402 - Train Acc: 0.6097\n",
      "🏆 New best accuracy for fold 1: 0.6097\n",
      "📊 Validation Accuracy on Subject 1: 0.1893\n",
      "Epoch [67/100] - Loss: 0.7567 - Con2: 0.0170 - MMD: 0.0082 - Cls: 0.7474 - Train Acc: 0.6042\n",
      "Epoch [68/100] - Loss: 0.7563 - Con2: 0.0158 - MMD: 0.0083 - Cls: 0.7476 - Train Acc: 0.6037\n",
      "Epoch [69/100] - Loss: 0.7534 - Con2: 0.0158 - MMD: 0.0080 - Cls: 0.7447 - Train Acc: 0.6085\n",
      "Epoch [70/100] - Loss: 0.7564 - Con2: 0.0156 - MMD: 0.0082 - Cls: 0.7478 - Train Acc: 0.6080\n",
      "Epoch [71/100] - Loss: 0.8052 - Con2: 0.0193 - MMD: 0.0074 - Cls: 0.7948 - Train Acc: 0.5767\n",
      "📊 Validation Accuracy on Subject 1: 0.1782\n",
      "Epoch [72/100] - Loss: 0.8179 - Con2: 0.0188 - MMD: 0.0070 - Cls: 0.8078 - Train Acc: 0.5703\n",
      "Epoch [73/100] - Loss: 0.8160 - Con2: 0.0195 - MMD: 0.0065 - Cls: 0.8056 - Train Acc: 0.5718\n",
      "Epoch [74/100] - Loss: 0.8163 - Con2: 0.0173 - MMD: 0.0069 - Cls: 0.8070 - Train Acc: 0.5711\n",
      "Epoch [75/100] - Loss: 0.8112 - Con2: 0.0157 - MMD: 0.0070 - Cls: 0.8027 - Train Acc: 0.5725\n",
      "Epoch [76/100] - Loss: 0.8088 - Con2: 0.0160 - MMD: 0.0073 - Cls: 0.8000 - Train Acc: 0.5756\n",
      "📊 Validation Accuracy on Subject 1: 0.2536\n",
      "Epoch [77/100] - Loss: 0.8084 - Con2: 0.0181 - MMD: 0.0074 - Cls: 0.7986 - Train Acc: 0.5737\n",
      "Epoch [78/100] - Loss: 0.8031 - Con2: 0.0162 - MMD: 0.0074 - Cls: 0.7942 - Train Acc: 0.5779\n",
      "Epoch [79/100] - Loss: 0.8022 - Con2: 0.0170 - MMD: 0.0076 - Cls: 0.7929 - Train Acc: 0.5799\n",
      "Epoch [80/100] - Loss: 0.8033 - Con2: 0.0202 - MMD: 0.0081 - Cls: 0.7924 - Train Acc: 0.5785\n",
      "Epoch [81/100] - Loss: 0.7967 - Con2: 0.0168 - MMD: 0.0076 - Cls: 0.7876 - Train Acc: 0.5820\n",
      "📊 Validation Accuracy on Subject 1: 0.2909\n",
      "Epoch [82/100] - Loss: 0.7945 - Con2: 0.0178 - MMD: 0.0074 - Cls: 0.7848 - Train Acc: 0.5844\n",
      "Epoch [83/100] - Loss: 0.7954 - Con2: 0.0162 - MMD: 0.0074 - Cls: 0.7865 - Train Acc: 0.5815\n",
      "Epoch [84/100] - Loss: 0.7916 - Con2: 0.0170 - MMD: 0.0077 - Cls: 0.7823 - Train Acc: 0.5871\n",
      "Epoch [85/100] - Loss: 0.7934 - Con2: 0.0199 - MMD: 0.0085 - Cls: 0.7826 - Train Acc: 0.5858\n",
      "Epoch [86/100] - Loss: 0.7900 - Con2: 0.0184 - MMD: 0.0088 - Cls: 0.7799 - Train Acc: 0.5861\n",
      "📊 Validation Accuracy on Subject 1: 0.2004\n",
      "Epoch [87/100] - Loss: 0.7813 - Con2: 0.0164 - MMD: 0.0089 - Cls: 0.7722 - Train Acc: 0.5908\n",
      "Epoch [88/100] - Loss: 0.7845 - Con2: 0.0158 - MMD: 0.0088 - Cls: 0.7757 - Train Acc: 0.5889\n",
      "Epoch [89/100] - Loss: 0.7790 - Con2: 0.0169 - MMD: 0.0091 - Cls: 0.7696 - Train Acc: 0.5923\n",
      "Epoch [90/100] - Loss: 0.7808 - Con2: 0.0188 - MMD: 0.0095 - Cls: 0.7705 - Train Acc: 0.5922\n",
      "Epoch [91/100] - Loss: 0.7768 - Con2: 0.0180 - MMD: 0.0091 - Cls: 0.7669 - Train Acc: 0.5937\n",
      "📊 Validation Accuracy on Subject 1: 0.2242\n",
      "Epoch [92/100] - Loss: 0.7743 - Con2: 0.0207 - MMD: 0.0095 - Cls: 0.7630 - Train Acc: 0.5968\n",
      "Epoch [93/100] - Loss: 0.7702 - Con2: 0.0176 - MMD: 0.0101 - Cls: 0.7604 - Train Acc: 0.5982\n",
      "Epoch [94/100] - Loss: 0.7716 - Con2: 0.0167 - MMD: 0.0102 - Cls: 0.7622 - Train Acc: 0.5978\n",
      "Epoch [95/100] - Loss: 0.7664 - Con2: 0.0158 - MMD: 0.0099 - Cls: 0.7575 - Train Acc: 0.5989\n",
      "Epoch [96/100] - Loss: 0.7702 - Con2: 0.0184 - MMD: 0.0104 - Cls: 0.7599 - Train Acc: 0.5961\n",
      "📊 Validation Accuracy on Subject 1: 0.2024\n",
      "Epoch [97/100] - Loss: 0.7647 - Con2: 0.0197 - MMD: 0.0105 - Cls: 0.7538 - Train Acc: 0.6005\n",
      "Epoch [98/100] - Loss: 0.7664 - Con2: 0.0223 - MMD: 0.0107 - Cls: 0.7542 - Train Acc: 0.6035\n",
      "Epoch [99/100] - Loss: 0.7611 - Con2: 0.0182 - MMD: 0.0108 - Cls: 0.7509 - Train Acc: 0.6038\n",
      "Epoch [100/100] - Loss: 0.7535 - Con2: 0.0185 - MMD: 0.0118 - Cls: 0.7430 - Train Acc: 0.6084\n",
      "📊 Validation Accuracy on Subject 1: 0.2635\n",
      "🎯 Final Accuracy on Subject 1: 0.2635\n",
      "\n",
      "🚀 Starting Fold 2/32 - Test Subject: s02\n",
      "Epoch [1/100] - Loss: 6.1564 - Con1: 6.1564 - MMD: 0.0029 - Train Acc: 0.0000\n",
      "📊 Validation Accuracy on Subject 2: 0.2067\n",
      "Epoch [2/100] - Loss: 4.1412 - Con1: 4.1412 - MMD: 0.0018 - Train Acc: 0.0000\n",
      "Epoch [3/100] - Loss: 4.1364 - Con1: 4.1363 - MMD: 0.0023 - Train Acc: 0.0000\n",
      "Epoch [4/100] - Loss: 4.1316 - Con1: 4.1316 - MMD: 0.0029 - Train Acc: 0.0000\n",
      "Epoch [5/100] - Loss: 4.1282 - Con1: 4.1281 - MMD: 0.0033 - Train Acc: 0.0000\n",
      "Epoch [6/100] - Loss: 4.1240 - Con1: 4.1239 - MMD: 0.0031 - Train Acc: 0.0000\n",
      "📊 Validation Accuracy on Subject 2: 0.1889\n",
      "Epoch [7/100] - Loss: 4.1188 - Con1: 4.1187 - MMD: 0.0026 - Train Acc: 0.0000\n",
      "Epoch [8/100] - Loss: 4.1153 - Con1: 4.1152 - MMD: 0.0025 - Cls: 1.1678 - Train Acc: 0.2567\n",
      "🏆 New best accuracy for fold 2: 0.2567\n",
      "Epoch [9/100] - Loss: 3.7201 - Con1: 4.1126 - MMD: 0.0027 - Cls: 0.9713 - Train Acc: 0.4736\n",
      "🏆 New best accuracy for fold 2: 0.4736\n",
      "Epoch [10/100] - Loss: 3.3186 - Con1: 4.1104 - MMD: 0.0029 - Cls: 0.9427 - Train Acc: 0.4869\n",
      "🏆 New best accuracy for fold 2: 0.4869\n",
      "Epoch [11/100] - Loss: 2.9367 - Con1: 4.1187 - MMD: 0.0029 - Cls: 0.9660 - Train Acc: 0.4709\n",
      "📊 Validation Accuracy on Subject 2: 0.1532\n",
      "Epoch [12/100] - Loss: 2.5330 - Con1: 4.1168 - MMD: 0.0026 - Cls: 0.9489 - Train Acc: 0.4799\n",
      "Epoch [13/100] - Loss: 2.1253 - Con1: 4.1137 - MMD: 0.0026 - Cls: 0.9320 - Train Acc: 0.4908\n",
      "🏆 New best accuracy for fold 2: 0.4908\n",
      "Epoch [14/100] - Loss: 1.7152 - Con1: 4.1100 - MMD: 0.0025 - Cls: 0.9167 - Train Acc: 0.5018\n",
      "🏆 New best accuracy for fold 2: 0.5018\n",
      "Epoch [15/100] - Loss: 1.3050 - Con1: 4.1092 - MMD: 0.0026 - Cls: 0.9042 - Train Acc: 0.5065\n",
      "🏆 New best accuracy for fold 2: 0.5065\n",
      "Epoch [16/100] - Loss: 1.0957 - Con2: 0.3012 - MMD: 0.0075 - Cls: 0.9443 - Train Acc: 0.4865\n",
      "📊 Validation Accuracy on Subject 2: 0.1377\n",
      "Epoch [17/100] - Loss: 0.9551 - Con2: 0.0804 - MMD: 0.0133 - Cls: 0.9135 - Train Acc: 0.5023\n",
      "Epoch [18/100] - Loss: 0.9355 - Con2: 0.0730 - MMD: 0.0158 - Cls: 0.8974 - Train Acc: 0.5158\n",
      "🏆 New best accuracy for fold 2: 0.5158\n",
      "Epoch [19/100] - Loss: 0.9195 - Con2: 0.0578 - MMD: 0.0155 - Cls: 0.8890 - Train Acc: 0.5173\n",
      "🏆 New best accuracy for fold 2: 0.5173\n",
      "Epoch [20/100] - Loss: 0.9017 - Con2: 0.0484 - MMD: 0.0164 - Cls: 0.8759 - Train Acc: 0.5277\n",
      "🏆 New best accuracy for fold 2: 0.5277\n",
      "Epoch [21/100] - Loss: 0.8838 - Con2: 0.0350 - MMD: 0.0170 - Cls: 0.8647 - Train Acc: 0.5348\n",
      "🏆 New best accuracy for fold 2: 0.5348\n",
      "📊 Validation Accuracy on Subject 2: 0.1238\n",
      "Epoch [22/100] - Loss: 0.8699 - Con2: 0.0263 - MMD: 0.0174 - Cls: 0.8550 - Train Acc: 0.5431\n",
      "🏆 New best accuracy for fold 2: 0.5431\n",
      "Epoch [23/100] - Loss: 0.8624 - Con2: 0.0217 - MMD: 0.0160 - Cls: 0.8499 - Train Acc: 0.5443\n",
      "🏆 New best accuracy for fold 2: 0.5443\n",
      "Epoch [24/100] - Loss: 0.8508 - Con2: 0.0215 - MMD: 0.0173 - Cls: 0.8384 - Train Acc: 0.5539\n",
      "🏆 New best accuracy for fold 2: 0.5539\n",
      "Epoch [25/100] - Loss: 0.8418 - Con2: 0.0199 - MMD: 0.0171 - Cls: 0.8301 - Train Acc: 0.5573\n",
      "🏆 New best accuracy for fold 2: 0.5573\n",
      "Epoch [26/100] - Loss: 0.8404 - Con2: 0.0173 - MMD: 0.0172 - Cls: 0.8300 - Train Acc: 0.5562\n",
      "📊 Validation Accuracy on Subject 2: 0.1325\n",
      "Epoch [27/100] - Loss: 0.8352 - Con2: 0.0175 - MMD: 0.0169 - Cls: 0.8248 - Train Acc: 0.5601\n",
      "🏆 New best accuracy for fold 2: 0.5601\n",
      "Epoch [28/100] - Loss: 0.8340 - Con2: 0.0170 - MMD: 0.0168 - Cls: 0.8238 - Train Acc: 0.5610\n",
      "🏆 New best accuracy for fold 2: 0.5610\n",
      "Epoch [29/100] - Loss: 0.8310 - Con2: 0.0174 - MMD: 0.0166 - Cls: 0.8206 - Train Acc: 0.5621\n",
      "🏆 New best accuracy for fold 2: 0.5621\n",
      "Epoch [30/100] - Loss: 0.8265 - Con2: 0.0172 - MMD: 0.0164 - Cls: 0.8163 - Train Acc: 0.5657\n",
      "🏆 New best accuracy for fold 2: 0.5657\n",
      "Epoch [31/100] - Loss: 0.8694 - Con2: 0.0171 - MMD: 0.0137 - Cls: 0.8595 - Train Acc: 0.5384\n",
      "📊 Validation Accuracy on Subject 2: 0.1548\n",
      "Epoch [32/100] - Loss: 0.8691 - Con2: 0.0134 - MMD: 0.0118 - Cls: 0.8612 - Train Acc: 0.5376\n",
      "Epoch [33/100] - Loss: 0.8651 - Con2: 0.0136 - MMD: 0.0119 - Cls: 0.8571 - Train Acc: 0.5414\n",
      "Epoch [34/100] - Loss: 0.8587 - Con2: 0.0131 - MMD: 0.0116 - Cls: 0.8510 - Train Acc: 0.5442\n",
      "Epoch [35/100] - Loss: 0.8496 - Con2: 0.0118 - MMD: 0.0112 - Cls: 0.8426 - Train Acc: 0.5468\n",
      "Epoch [36/100] - Loss: 0.8470 - Con2: 0.0116 - MMD: 0.0108 - Cls: 0.8401 - Train Acc: 0.5496\n",
      "📊 Validation Accuracy on Subject 2: 0.1853\n",
      "Epoch [37/100] - Loss: 0.8407 - Con2: 0.0105 - MMD: 0.0099 - Cls: 0.8345 - Train Acc: 0.5528\n",
      "Epoch [38/100] - Loss: 0.8325 - Con2: 0.0112 - MMD: 0.0096 - Cls: 0.8259 - Train Acc: 0.5589\n",
      "Epoch [39/100] - Loss: 0.8300 - Con2: 0.0117 - MMD: 0.0092 - Cls: 0.8232 - Train Acc: 0.5608\n",
      "Epoch [40/100] - Loss: 0.8246 - Con2: 0.0117 - MMD: 0.0095 - Cls: 0.8178 - Train Acc: 0.5622\n",
      "Epoch [41/100] - Loss: 0.8199 - Con2: 0.0112 - MMD: 0.0094 - Cls: 0.8134 - Train Acc: 0.5652\n",
      "📊 Validation Accuracy on Subject 2: 0.1794\n",
      "Epoch [42/100] - Loss: 0.8152 - Con2: 0.0117 - MMD: 0.0091 - Cls: 0.8084 - Train Acc: 0.5703\n",
      "🏆 New best accuracy for fold 2: 0.5703\n",
      "Epoch [43/100] - Loss: 0.8082 - Con2: 0.0113 - MMD: 0.0086 - Cls: 0.8017 - Train Acc: 0.5749\n",
      "🏆 New best accuracy for fold 2: 0.5749\n",
      "Epoch [44/100] - Loss: 0.7987 - Con2: 0.0123 - MMD: 0.0095 - Cls: 0.7916 - Train Acc: 0.5814\n",
      "🏆 New best accuracy for fold 2: 0.5814\n",
      "Epoch [45/100] - Loss: 0.7983 - Con2: 0.0112 - MMD: 0.0088 - Cls: 0.7919 - Train Acc: 0.5805\n",
      "Epoch [46/100] - Loss: 0.7939 - Con2: 0.0104 - MMD: 0.0089 - Cls: 0.7878 - Train Acc: 0.5823\n",
      "🏆 New best accuracy for fold 2: 0.5823\n",
      "📊 Validation Accuracy on Subject 2: 0.1794\n",
      "Epoch [47/100] - Loss: 0.7900 - Con2: 0.0107 - MMD: 0.0091 - Cls: 0.7838 - Train Acc: 0.5852\n",
      "🏆 New best accuracy for fold 2: 0.5852\n",
      "Epoch [48/100] - Loss: 0.7857 - Con2: 0.0117 - MMD: 0.0091 - Cls: 0.7789 - Train Acc: 0.5885\n",
      "🏆 New best accuracy for fold 2: 0.5885\n",
      "Epoch [49/100] - Loss: 0.7825 - Con2: 0.0116 - MMD: 0.0094 - Cls: 0.7758 - Train Acc: 0.5898\n",
      "🏆 New best accuracy for fold 2: 0.5898\n",
      "Epoch [50/100] - Loss: 0.7746 - Con2: 0.0116 - MMD: 0.0087 - Cls: 0.7679 - Train Acc: 0.5941\n",
      "🏆 New best accuracy for fold 2: 0.5941\n",
      "Epoch [51/100] - Loss: 0.7706 - Con2: 0.0124 - MMD: 0.0088 - Cls: 0.7635 - Train Acc: 0.5964\n",
      "🏆 New best accuracy for fold 2: 0.5964\n",
      "📊 Validation Accuracy on Subject 2: 0.2000\n",
      "Epoch [52/100] - Loss: 0.7688 - Con2: 0.0111 - MMD: 0.0083 - Cls: 0.7624 - Train Acc: 0.5984\n",
      "🏆 New best accuracy for fold 2: 0.5984\n",
      "Epoch [53/100] - Loss: 0.7638 - Con2: 0.0111 - MMD: 0.0086 - Cls: 0.7574 - Train Acc: 0.6012\n",
      "🏆 New best accuracy for fold 2: 0.6012\n",
      "Epoch [54/100] - Loss: 0.7593 - Con2: 0.0107 - MMD: 0.0085 - Cls: 0.7530 - Train Acc: 0.6029\n",
      "🏆 New best accuracy for fold 2: 0.6029\n",
      "Epoch [55/100] - Loss: 0.7531 - Con2: 0.0110 - MMD: 0.0090 - Cls: 0.7467 - Train Acc: 0.6082\n",
      "🏆 New best accuracy for fold 2: 0.6082\n",
      "Epoch [56/100] - Loss: 0.7467 - Con2: 0.0109 - MMD: 0.0084 - Cls: 0.7404 - Train Acc: 0.6115\n",
      "🏆 New best accuracy for fold 2: 0.6115\n",
      "📊 Validation Accuracy on Subject 2: 0.1917\n",
      "Epoch [57/100] - Loss: 0.7449 - Con2: 0.0109 - MMD: 0.0085 - Cls: 0.7386 - Train Acc: 0.6126\n",
      "🏆 New best accuracy for fold 2: 0.6126\n",
      "Epoch [58/100] - Loss: 0.7416 - Con2: 0.0108 - MMD: 0.0086 - Cls: 0.7354 - Train Acc: 0.6143\n",
      "🏆 New best accuracy for fold 2: 0.6143\n",
      "Epoch [59/100] - Loss: 0.7366 - Con2: 0.0105 - MMD: 0.0087 - Cls: 0.7304 - Train Acc: 0.6176\n",
      "🏆 New best accuracy for fold 2: 0.6176\n",
      "Epoch [60/100] - Loss: 0.7403 - Con2: 0.0105 - MMD: 0.0085 - Cls: 0.7342 - Train Acc: 0.6143\n",
      "Epoch [61/100] - Loss: 0.7356 - Con2: 0.0105 - MMD: 0.0086 - Cls: 0.7294 - Train Acc: 0.6181\n",
      "🏆 New best accuracy for fold 2: 0.6181\n",
      "📊 Validation Accuracy on Subject 2: 0.1837\n",
      "Epoch [62/100] - Loss: 0.7289 - Con2: 0.0103 - MMD: 0.0084 - Cls: 0.7230 - Train Acc: 0.6200\n",
      "🏆 New best accuracy for fold 2: 0.6200\n",
      "Epoch [63/100] - Loss: 0.7264 - Con2: 0.0103 - MMD: 0.0085 - Cls: 0.7204 - Train Acc: 0.6199\n",
      "Epoch [64/100] - Loss: 0.7240 - Con2: 0.0099 - MMD: 0.0084 - Cls: 0.7182 - Train Acc: 0.6251\n",
      "🏆 New best accuracy for fold 2: 0.6251\n",
      "Epoch [65/100] - Loss: 0.7283 - Con2: 0.0104 - MMD: 0.0085 - Cls: 0.7222 - Train Acc: 0.6224\n",
      "Epoch [66/100] - Loss: 0.7235 - Con2: 0.0104 - MMD: 0.0086 - Cls: 0.7175 - Train Acc: 0.6254\n",
      "🏆 New best accuracy for fold 2: 0.6254\n",
      "📊 Validation Accuracy on Subject 2: 0.1865\n",
      "Epoch [67/100] - Loss: 0.7244 - Con2: 0.0100 - MMD: 0.0086 - Cls: 0.7186 - Train Acc: 0.6224\n",
      "Epoch [68/100] - Loss: 0.7210 - Con2: 0.0115 - MMD: 0.0083 - Cls: 0.7144 - Train Acc: 0.6263\n",
      "🏆 New best accuracy for fold 2: 0.6263\n",
      "Epoch [69/100] - Loss: 0.7236 - Con2: 0.0100 - MMD: 0.0086 - Cls: 0.7177 - Train Acc: 0.6244\n",
      "Epoch [70/100] - Loss: 0.7183 - Con2: 0.0105 - MMD: 0.0084 - Cls: 0.7122 - Train Acc: 0.6284\n",
      "🏆 New best accuracy for fold 2: 0.6284\n",
      "Epoch [71/100] - Loss: 0.7769 - Con2: 0.0125 - MMD: 0.0090 - Cls: 0.7697 - Train Acc: 0.5939\n",
      "📊 Validation Accuracy on Subject 2: 0.2004\n",
      "Epoch [72/100] - Loss: 0.7838 - Con2: 0.0121 - MMD: 0.0093 - Cls: 0.7768 - Train Acc: 0.5899\n",
      "Epoch [73/100] - Loss: 0.7830 - Con2: 0.0118 - MMD: 0.0096 - Cls: 0.7761 - Train Acc: 0.5913\n",
      "Epoch [74/100] - Loss: 0.7816 - Con2: 0.0110 - MMD: 0.0097 - Cls: 0.7751 - Train Acc: 0.5896\n",
      "Epoch [75/100] - Loss: 0.7814 - Con2: 0.0123 - MMD: 0.0095 - Cls: 0.7743 - Train Acc: 0.5891\n",
      "Epoch [76/100] - Loss: 0.7804 - Con2: 0.0108 - MMD: 0.0091 - Cls: 0.7741 - Train Acc: 0.5910\n",
      "📊 Validation Accuracy on Subject 2: 0.2060\n",
      "Epoch [77/100] - Loss: 0.7764 - Con2: 0.0109 - MMD: 0.0094 - Cls: 0.7700 - Train Acc: 0.5943\n",
      "Epoch [78/100] - Loss: 0.7762 - Con2: 0.0122 - MMD: 0.0100 - Cls: 0.7691 - Train Acc: 0.5936\n",
      "Epoch [79/100] - Loss: 0.7742 - Con2: 0.0112 - MMD: 0.0101 - Cls: 0.7676 - Train Acc: 0.5939\n",
      "Epoch [80/100] - Loss: 0.7658 - Con2: 0.0102 - MMD: 0.0099 - Cls: 0.7598 - Train Acc: 0.6015\n",
      "Epoch [81/100] - Loss: 0.7642 - Con2: 0.0110 - MMD: 0.0099 - Cls: 0.7577 - Train Acc: 0.6032\n",
      "📊 Validation Accuracy on Subject 2: 0.2087\n",
      "Epoch [82/100] - Loss: 0.7662 - Con2: 0.0109 - MMD: 0.0105 - Cls: 0.7597 - Train Acc: 0.6001\n",
      "Epoch [83/100] - Loss: 0.7679 - Con2: 0.0105 - MMD: 0.0103 - Cls: 0.7616 - Train Acc: 0.5994\n",
      "Epoch [84/100] - Loss: 0.7619 - Con2: 0.0096 - MMD: 0.0102 - Cls: 0.7561 - Train Acc: 0.6029\n",
      "Epoch [85/100] - Loss: 0.7558 - Con2: 0.0096 - MMD: 0.0100 - Cls: 0.7500 - Train Acc: 0.6044\n",
      "Epoch [86/100] - Loss: 0.7553 - Con2: 0.0103 - MMD: 0.0101 - Cls: 0.7491 - Train Acc: 0.6072\n",
      "📊 Validation Accuracy on Subject 2: 0.1813\n",
      "Epoch [87/100] - Loss: 0.7536 - Con2: 0.0112 - MMD: 0.0104 - Cls: 0.7470 - Train Acc: 0.6073\n",
      "Epoch [88/100] - Loss: 0.7495 - Con2: 0.0106 - MMD: 0.0100 - Cls: 0.7432 - Train Acc: 0.6089\n",
      "Epoch [89/100] - Loss: 0.7474 - Con2: 0.0107 - MMD: 0.0101 - Cls: 0.7410 - Train Acc: 0.6116\n",
      "Epoch [90/100] - Loss: 0.7449 - Con2: 0.0107 - MMD: 0.0107 - Cls: 0.7385 - Train Acc: 0.6139\n",
      "Epoch [91/100] - Loss: 0.7471 - Con2: 0.0105 - MMD: 0.0108 - Cls: 0.7408 - Train Acc: 0.6114\n",
      "📊 Validation Accuracy on Subject 2: 0.1861\n",
      "Epoch [92/100] - Loss: 0.7423 - Con2: 0.0107 - MMD: 0.0101 - Cls: 0.7360 - Train Acc: 0.6139\n",
      "Epoch [93/100] - Loss: 0.7403 - Con2: 0.0100 - MMD: 0.0105 - Cls: 0.7342 - Train Acc: 0.6143\n",
      "Epoch [94/100] - Loss: 0.7389 - Con2: 0.0099 - MMD: 0.0107 - Cls: 0.7329 - Train Acc: 0.6157\n",
      "Epoch [95/100] - Loss: 0.7313 - Con2: 0.0102 - MMD: 0.0105 - Cls: 0.7252 - Train Acc: 0.6220\n",
      "Epoch [96/100] - Loss: 0.7323 - Con2: 0.0094 - MMD: 0.0106 - Cls: 0.7265 - Train Acc: 0.6200\n",
      "📊 Validation Accuracy on Subject 2: 0.1996\n",
      "Epoch [97/100] - Loss: 0.7298 - Con2: 0.0101 - MMD: 0.0106 - Cls: 0.7237 - Train Acc: 0.6213\n",
      "Epoch [98/100] - Loss: 0.7250 - Con2: 0.0101 - MMD: 0.0104 - Cls: 0.7189 - Train Acc: 0.6233\n",
      "Epoch [99/100] - Loss: 0.7268 - Con2: 0.0107 - MMD: 0.0101 - Cls: 0.7204 - Train Acc: 0.6230\n",
      "Epoch [100/100] - Loss: 0.7220 - Con2: 0.0095 - MMD: 0.0107 - Cls: 0.7162 - Train Acc: 0.6245\n",
      "📊 Validation Accuracy on Subject 2: 0.1877\n",
      "🎯 Final Accuracy on Subject 2: 0.1877\n",
      "\n",
      "🚀 Starting Fold 3/32 - Test Subject: s03\n",
      "Epoch [1/100] - Loss: 6.1570 - Con1: 6.1570 - MMD: 0.0032 - Train Acc: 0.0000\n",
      "📊 Validation Accuracy on Subject 3: 0.1810\n",
      "Epoch [2/100] - Loss: 4.1417 - Con1: 4.1416 - MMD: 0.0026 - Train Acc: 0.0000\n",
      "Epoch [3/100] - Loss: 4.1378 - Con1: 4.1377 - MMD: 0.0031 - Train Acc: 0.0000\n",
      "Epoch [4/100] - Loss: 4.1335 - Con1: 4.1334 - MMD: 0.0033 - Train Acc: 0.0000\n",
      "Epoch [5/100] - Loss: 4.1294 - Con1: 4.1293 - MMD: 0.0036 - Train Acc: 0.0000\n",
      "Epoch [6/100] - Loss: 4.1261 - Con1: 4.1260 - MMD: 0.0035 - Train Acc: 0.0000\n",
      "📊 Validation Accuracy on Subject 3: 0.3313\n",
      "Epoch [7/100] - Loss: 4.1218 - Con1: 4.1216 - MMD: 0.0035 - Train Acc: 0.0000\n",
      "Epoch [8/100] - Loss: 4.1200 - Con1: 4.1199 - MMD: 0.0030 - Cls: 1.1745 - Train Acc: 0.2415\n",
      "🏆 New best accuracy for fold 3: 0.2415\n",
      "Epoch [9/100] - Loss: 3.7249 - Con1: 4.1158 - MMD: 0.0029 - Cls: 0.9867 - Train Acc: 0.4608\n",
      "🏆 New best accuracy for fold 3: 0.4608\n",
      "Epoch [10/100] - Loss: 3.3257 - Con1: 4.1148 - MMD: 0.0030 - Cls: 0.9578 - Train Acc: 0.4783\n",
      "🏆 New best accuracy for fold 3: 0.4783\n",
      "Epoch [11/100] - Loss: 2.9435 - Con1: 4.1223 - MMD: 0.0031 - Cls: 0.9784 - Train Acc: 0.4593\n",
      "📊 Validation Accuracy on Subject 3: 0.1540\n",
      "Epoch [12/100] - Loss: 2.5382 - Con1: 4.1187 - MMD: 0.0029 - Cls: 0.9572 - Train Acc: 0.4722\n",
      "Epoch [13/100] - Loss: 2.1337 - Con1: 4.1166 - MMD: 0.0031 - Cls: 0.9435 - Train Acc: 0.4830\n",
      "🏆 New best accuracy for fold 3: 0.4830\n",
      "Epoch [14/100] - Loss: 1.7222 - Con1: 4.1132 - MMD: 0.0027 - Cls: 0.9249 - Train Acc: 0.4960\n",
      "🏆 New best accuracy for fold 3: 0.4960\n",
      "Epoch [15/100] - Loss: 1.3158 - Con1: 4.1124 - MMD: 0.0029 - Cls: 0.9160 - Train Acc: 0.4999\n",
      "🏆 New best accuracy for fold 3: 0.4999\n",
      "Epoch [16/100] - Loss: 0.9881 - Con2: 0.1195 - MMD: 0.0042 - Cls: 0.9279 - Train Acc: 0.4959\n",
      "📊 Validation Accuracy on Subject 3: 0.1492\n",
      "Epoch [17/100] - Loss: 0.9149 - Con2: 0.0168 - MMD: 0.0051 - Cls: 0.9060 - Train Acc: 0.5114\n",
      "🏆 New best accuracy for fold 3: 0.5114\n",
      "Epoch [18/100] - Loss: 0.9042 - Con2: 0.0119 - MMD: 0.0051 - Cls: 0.8978 - Train Acc: 0.5155\n",
      "🏆 New best accuracy for fold 3: 0.5155\n",
      "Epoch [19/100] - Loss: 0.8933 - Con2: 0.0108 - MMD: 0.0055 - Cls: 0.8874 - Train Acc: 0.5213\n",
      "🏆 New best accuracy for fold 3: 0.5213\n",
      "Epoch [20/100] - Loss: 0.8779 - Con2: 0.0084 - MMD: 0.0060 - Cls: 0.8731 - Train Acc: 0.5293\n",
      "🏆 New best accuracy for fold 3: 0.5293\n",
      "Epoch [21/100] - Loss: 0.8710 - Con2: 0.0081 - MMD: 0.0060 - Cls: 0.8664 - Train Acc: 0.5334\n",
      "🏆 New best accuracy for fold 3: 0.5334\n",
      "📊 Validation Accuracy on Subject 3: 0.1603\n",
      "Epoch [22/100] - Loss: 0.8582 - Con2: 0.0072 - MMD: 0.0058 - Cls: 0.8541 - Train Acc: 0.5424\n",
      "🏆 New best accuracy for fold 3: 0.5424\n",
      "Epoch [23/100] - Loss: 0.8505 - Con2: 0.0063 - MMD: 0.0059 - Cls: 0.8467 - Train Acc: 0.5458\n",
      "🏆 New best accuracy for fold 3: 0.5458\n",
      "Epoch [24/100] - Loss: 0.8458 - Con2: 0.0064 - MMD: 0.0058 - Cls: 0.8420 - Train Acc: 0.5494\n",
      "🏆 New best accuracy for fold 3: 0.5494\n",
      "Epoch [25/100] - Loss: 0.8386 - Con2: 0.0063 - MMD: 0.0058 - Cls: 0.8349 - Train Acc: 0.5538\n",
      "🏆 New best accuracy for fold 3: 0.5538\n",
      "Epoch [26/100] - Loss: 0.8306 - Con2: 0.0066 - MMD: 0.0058 - Cls: 0.8267 - Train Acc: 0.5584\n",
      "🏆 New best accuracy for fold 3: 0.5584\n",
      "📊 Validation Accuracy on Subject 3: 0.0829\n",
      "Epoch [27/100] - Loss: 0.8285 - Con2: 0.0062 - MMD: 0.0059 - Cls: 0.8248 - Train Acc: 0.5600\n",
      "🏆 New best accuracy for fold 3: 0.5600\n",
      "Epoch [28/100] - Loss: 0.8264 - Con2: 0.0061 - MMD: 0.0057 - Cls: 0.8227 - Train Acc: 0.5616\n",
      "🏆 New best accuracy for fold 3: 0.5616\n",
      "Epoch [29/100] - Loss: 0.8241 - Con2: 0.0064 - MMD: 0.0057 - Cls: 0.8203 - Train Acc: 0.5605\n",
      "Epoch [30/100] - Loss: 0.8241 - Con2: 0.0061 - MMD: 0.0058 - Cls: 0.8205 - Train Acc: 0.5607\n",
      "Epoch [31/100] - Loss: 0.8600 - Con2: 0.0068 - MMD: 0.0060 - Cls: 0.8561 - Train Acc: 0.5391\n",
      "📊 Validation Accuracy on Subject 3: 0.1063\n",
      "Epoch [32/100] - Loss: 0.8658 - Con2: 0.0054 - MMD: 0.0061 - Cls: 0.8625 - Train Acc: 0.5360\n",
      "Epoch [33/100] - Loss: 0.8570 - Con2: 0.0056 - MMD: 0.0058 - Cls: 0.8536 - Train Acc: 0.5418\n",
      "Epoch [34/100] - Loss: 0.8541 - Con2: 0.0056 - MMD: 0.0056 - Cls: 0.8507 - Train Acc: 0.5450\n",
      "Epoch [35/100] - Loss: 0.8487 - Con2: 0.0056 - MMD: 0.0057 - Cls: 0.8453 - Train Acc: 0.5480\n",
      "Epoch [36/100] - Loss: 0.8403 - Con2: 0.0054 - MMD: 0.0061 - Cls: 0.8370 - Train Acc: 0.5538\n",
      "📊 Validation Accuracy on Subject 3: 0.1520\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 283\u001b[0m\n\u001b[0;32m    280\u001b[0m label_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mE:/FYP/Finalise Fyp/EEg-based-Emotion-Recognition/de_labels.npy\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    282\u001b[0m \u001b[38;5;66;03m# Run training\u001b[39;00m\n\u001b[1;32m--> 283\u001b[0m fold_accuracies, global_true, global_pred \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    284\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    285\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_subjects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    287\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    288\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwarmup_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    289\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\n\u001b[0;32m    290\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[14], line 71\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(feature_path, label_path, num_subjects, num_epochs, warmup_epochs, batch_size)\u001b[0m\n\u001b[0;32m     68\u001b[0m x_s, y_s \u001b[38;5;241m=\u001b[39m x_s\u001b[38;5;241m.\u001b[39mto(device), y_s\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# Get target batch\u001b[39;00m\n\u001b[1;32m---> 71\u001b[0m x_t, y_t \u001b[38;5;241m=\u001b[39m \u001b[43mget_target_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_subject\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     72\u001b[0m x_t, y_t \u001b[38;5;241m=\u001b[39m x_t\u001b[38;5;241m.\u001b[39mto(device), y_t\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# Skip this batch if either source or target is empty\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[12], line 5\u001b[0m, in \u001b[0;36mget_target_batch\u001b[1;34m(dataset, exclude, batch_size)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m subj \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m32\u001b[39m):\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m subj \u001b[38;5;241m!=\u001b[39m exclude:\n\u001b[1;32m----> 5\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_subject_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m batch_size:\n\u001b[0;32m      7\u001b[0m             indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandperm(\u001b[38;5;28mlen\u001b[39m(data))[:batch_size]\n",
      "Cell \u001b[1;32mIn[9], line 55\u001b[0m, in \u001b[0;36mDEAPDataset.get_subject_data\u001b[1;34m(self, subject)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_subject_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, subject):\n\u001b[1;32m---> 55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mint\u001b[39m(y))\n\u001b[0;32m     56\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m x, y, subj \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples \u001b[38;5;28;01mif\u001b[39;00m subj \u001b[38;5;241m==\u001b[39m subject]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train_model(feature_path, label_path, num_subjects=32, num_epochs=100, \n",
    "                warmup_epochs=15, batch_size=64):\n",
    "    \"\"\"Main training function for all folds\"\"\"\n",
    "    \n",
    "    # Hyperparameters\n",
    "    lambda_1 = 1.0  # For contrastive_loss_con1 during warmup\n",
    "    lambda_2 = 0.5  # For contrastive_loss_con2 after warmup\n",
    "    lambda_3_init = 0.1  # Initial value for MMD loss\n",
    "    \n",
    "    # Logging containers\n",
    "    fold_accuracies = []\n",
    "    global_true = []\n",
    "    global_pred = []\n",
    "    \n",
    "    for test_subject in range(num_subjects):\n",
    "        print(f\"\\n🚀 Starting Fold {test_subject+1}/{num_subjects} - Test Subject: s{test_subject+1:02d}\")\n",
    "        \n",
    "        # Create datasets and loaders\n",
    "        train_dataset = DEAPDataset(feature_path, label_path, exclude_subject=test_subject, normalize=True)\n",
    "        test_dataset = DEAPDataset(feature_path, label_path, only_subject=test_subject, normalize=True)\n",
    "        \n",
    "        # Create balanced sampler\n",
    "        all_labels = np.array([label for _, label in train_dataset])\n",
    "        class_sample_counts = np.bincount(all_labels)\n",
    "        class_weights = 1. / class_sample_counts\n",
    "        class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "        sample_weights = [class_weights[label] for label in all_labels]\n",
    "        sampler = WeightedRandomSampler(sample_weights, len(sample_weights), replacement=True)\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        # Initialize models\n",
    "        cfe = CommonFeatureExtractor().to(device)\n",
    "        sfe = SubjectSpecificMapper().to(device)\n",
    "        ssc = SubjectSpecificClassifier().to(device)\n",
    "        \n",
    "        # Initialize losses\n",
    "        contrastive_loss_con1 = SupervisedContrastiveLoss(temperature=0.03).to(device)\n",
    "        contrastive_loss_con2 = ContrastiveLossLcon2(tau=0.1).to(device)\n",
    "        mmd_loss = MMDLoss().to(device)\n",
    "        gce_loss = GeneralizedCrossEntropy(q=0.3, weight=class_weights_tensor).to(device)\n",
    "        \n",
    "        # Initialize optimizer and scheduler\n",
    "        all_params = list(cfe.parameters()) + list(sfe.parameters()) + list(ssc.parameters())\n",
    "        optimizer = torch.optim.AdamW(all_params, lr=1e-3, weight_decay=1e-4)\n",
    "        scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=1e-6)\n",
    "        \n",
    "        # Training tracking\n",
    "        best_acc = 0\n",
    "        transition_start = warmup_epochs // 2\n",
    "        \n",
    "        # Track loss components\n",
    "        loss_history = {\n",
    "            'total': [], 'con1': [], 'con2': [], 'mmd': [], 'cls': [],\n",
    "            'train_acc': [], 'val_acc': []\n",
    "        }\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(num_epochs):\n",
    "            cfe.train()\n",
    "            sfe.train()\n",
    "            ssc.train()\n",
    "            \n",
    "            epoch_losses = {'total': 0, 'con1': 0, 'con2': 0, 'mmd': 0, 'cls': 0}\n",
    "            total_correct, total_samples = 0, 0\n",
    "            \n",
    "            for x_s, y_s in train_loader:\n",
    "                x_s, y_s = x_s.to(device), y_s.to(device)\n",
    "                \n",
    "                # Get target batch\n",
    "                x_t, y_t = get_target_batch(train_dataset, exclude=test_subject, batch_size=batch_size)\n",
    "                x_t, y_t = x_t.to(device), y_t.to(device)\n",
    "                \n",
    "                # Skip this batch if either source or target is empty\n",
    "                if x_s.shape[0] == 0 or x_t.shape[0] == 0:\n",
    "                    continue\n",
    "                \n",
    "                # Match batch sizes\n",
    "                min_size = min(x_s.size(0), x_t.size(0))\n",
    "                x_s = x_s[:min_size]\n",
    "                y_s = y_s[:min_size]\n",
    "                x_t = x_t[:min_size]\n",
    "                y_t = y_t[:min_size]\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Extract features\n",
    "                z_s_common = cfe(x_s)\n",
    "                z_t_common = cfe(x_t)\n",
    "                \n",
    "                # Always extract subject-specific features\n",
    "                z_s_subject = sfe(z_s_common)\n",
    "                z_t_subject = sfe(z_t_common)\n",
    "                \n",
    "                # Domain adaptation loss (always applied)\n",
    "                loss_mmd = mmd_loss(z_s_common, z_t_common)\n",
    "                \n",
    "                # Adjusted MMD weight - gradually increase during warmup\n",
    "                lambda_3 = lambda_3_init\n",
    "                if epoch < warmup_epochs:\n",
    "                    lambda_3 = lambda_3_init * (epoch + 1) / warmup_epochs\n",
    "                \n",
    "                # Different training phases\n",
    "                if epoch < transition_start:\n",
    "                    # Pure contrastive phase\n",
    "                    loss_con1 = contrastive_loss_con1(z_s_common, y_s)\n",
    "                    loss = lambda_1 * loss_con1 + lambda_3 * loss_mmd\n",
    "                    \n",
    "                    # Track loss components\n",
    "                    epoch_losses['con1'] += loss_con1.item()\n",
    "                    epoch_losses['mmd'] += loss_mmd.item()\n",
    "                    \n",
    "                elif epoch < warmup_epochs:\n",
    "                    # Transition phase - gradually introduce classification\n",
    "                    transition_factor = (epoch - transition_start) / (warmup_epochs - transition_start)\n",
    "                    \n",
    "                    loss_con1 = contrastive_loss_con1(z_s_common, y_s)\n",
    "                    logits = ssc(z_s_subject)\n",
    "                    loss_cls = gce_loss(logits, y_s)\n",
    "                    \n",
    "                    loss = (1 - transition_factor) * (lambda_1 * loss_con1) + \\\n",
    "                           transition_factor * loss_cls + lambda_3 * loss_mmd\n",
    "                    \n",
    "                    # Track loss components\n",
    "                    epoch_losses['con1'] += loss_con1.item()\n",
    "                    epoch_losses['cls'] += loss_cls.item()\n",
    "                    epoch_losses['mmd'] += loss_mmd.item()\n",
    "                    \n",
    "                    # Calculate accuracy\n",
    "                    preds = torch.argmax(logits, dim=1)\n",
    "                    total_correct += (preds == y_s).sum().item()\n",
    "                    total_samples += y_s.size(0)\n",
    "                    \n",
    "                else:\n",
    "                    # Full classification phase\n",
    "                    logits = ssc(z_s_subject)\n",
    "                    loss_cls = gce_loss(logits, y_s)\n",
    "                    \n",
    "                    # Use pseudo labels for target domain\n",
    "                    with torch.no_grad():\n",
    "                        pseudo_logits = ssc(z_t_subject)\n",
    "                        pseudo_probs = F.softmax(pseudo_logits, dim=1)\n",
    "                        pseudo_conf, pseudo_labels = torch.max(pseudo_probs, dim=1)\n",
    "                        confident_mask = pseudo_conf > 0.85\n",
    "                        if confident_mask.sum() == 0:\n",
    "                            continue\n",
    "                        z_t_subject = z_t_subject[confident_mask]\n",
    "                        pseudo_labels = pseudo_labels[confident_mask]\n",
    "                    \n",
    "                    loss_con2 = contrastive_loss_con2(z_t_subject, pseudo_labels)\n",
    "                    loss = loss_cls + lambda_2 * loss_con2 + lambda_3 * loss_mmd\n",
    "                    \n",
    "                    # Track loss components\n",
    "                    epoch_losses['cls'] += loss_cls.item()\n",
    "                    epoch_losses['con2'] += loss_con2.item()\n",
    "                    epoch_losses['mmd'] += loss_mmd.item()\n",
    "                    \n",
    "                    # Calculate accuracy\n",
    "                    preds = torch.argmax(logits, dim=1)\n",
    "                    total_correct += (preds == y_s).sum().item()\n",
    "                    total_samples += y_s.size(0)\n",
    "                \n",
    "                # Update total loss\n",
    "                epoch_losses['total'] += loss.item()\n",
    "                \n",
    "                # Backpropagation with gradient clipping\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(all_params, max_norm=1.0)\n",
    "                optimizer.step()\n",
    "            \n",
    "            # Update scheduler\n",
    "            scheduler.step()\n",
    "            \n",
    "            # Calculate average losses\n",
    "            num_batches = len(train_loader)\n",
    "            avg_total_loss = epoch_losses['total'] / num_batches\n",
    "            avg_train_acc = total_correct / total_samples if total_samples > 0 else 0\n",
    "            \n",
    "            # Update loss history\n",
    "            loss_history['total'].append(avg_total_loss)\n",
    "            loss_history['train_acc'].append(avg_train_acc)\n",
    "            \n",
    "            for key in ['con1', 'con2', 'mmd', 'cls']:\n",
    "                if epoch_losses[key] > 0:\n",
    "                    loss_history[key].append(epoch_losses[key] / num_batches)\n",
    "                else:\n",
    "                    loss_history[key].append(0)\n",
    "            \n",
    "            # Print progress\n",
    "            progress_str = f\"Epoch [{epoch+1}/{num_epochs}] - Loss: {avg_total_loss:.4f}\"\n",
    "            if epoch < warmup_epochs:\n",
    "                progress_str += f\" - Con1: {loss_history['con1'][-1]:.4f}\"\n",
    "            else:\n",
    "                progress_str += f\" - Con2: {loss_history['con2'][-1]:.4f}\"\n",
    "            \n",
    "            progress_str += f\" - MMD: {loss_history['mmd'][-1]:.4f}\"\n",
    "            \n",
    "            if epoch >= transition_start:\n",
    "                progress_str += f\" - Cls: {loss_history['cls'][-1]:.4f}\"\n",
    "                \n",
    "            progress_str += f\" - Train Acc: {avg_train_acc:.4f}\"\n",
    "            print(progress_str)\n",
    "            \n",
    "            # Save checkpoint if accuracy improves\n",
    "            if avg_train_acc > best_acc:\n",
    "                best_acc = avg_train_acc\n",
    "                print(f\"🏆 New best accuracy for fold {test_subject+1}: {best_acc:.4f}\")\n",
    "                torch.save({\n",
    "                    'epoch': epoch + 1,\n",
    "                    'cfe_state_dict': cfe.state_dict(),\n",
    "                    'sfe_state_dict': sfe.state_dict(),\n",
    "                    'ssc_state_dict': ssc.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'scheduler_state_dict': scheduler.state_dict(),\n",
    "                    'loss': avg_total_loss,\n",
    "                    'accuracy': avg_train_acc\n",
    "                }, f\"checkpoints/fold{test_subject+1}_best.pt\")\n",
    "            \n",
    "            # Evaluation after each epoch\n",
    "            if epoch % 5 == 0 or epoch == num_epochs - 1:\n",
    "                cfe.eval()\n",
    "                sfe.eval()\n",
    "                ssc.eval()\n",
    "                \n",
    "                all_preds, all_labels = [], []\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    for x_test, y_test in test_loader:\n",
    "                        x_test = x_test.to(device)\n",
    "                        z_common = cfe(x_test)\n",
    "                        z_subject = sfe(z_common)\n",
    "                        logits = ssc(z_subject)\n",
    "                        preds = torch.argmax(logits, dim=1)\n",
    "                        \n",
    "                        all_preds.extend(preds.cpu().numpy())\n",
    "                        all_labels.extend(y_test.numpy())\n",
    "                \n",
    "                val_acc = accuracy_score(all_labels, all_preds)\n",
    "                loss_history['val_acc'].append(val_acc)\n",
    "                print(f\"📊 Validation Accuracy on Subject {test_subject+1}: {val_acc:.4f}\")\n",
    "        \n",
    "        # Final evaluation\n",
    "        cfe.eval()\n",
    "        sfe.eval()\n",
    "        ssc.eval()\n",
    "        \n",
    "        all_preds, all_labels = [], []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for x_test, y_test in test_loader:\n",
    "                x_test = x_test.to(device)\n",
    "                z_common = cfe(x_test)\n",
    "                z_subject = sfe(z_common)\n",
    "                logits = ssc(z_subject)\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "                \n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(y_test.numpy())\n",
    "        \n",
    "        final_acc = accuracy_score(all_labels, all_preds)\n",
    "        fold_accuracies.append(final_acc)\n",
    "        print(f\"🎯 Final Accuracy on Subject {test_subject+1}: {final_acc:.4f}\")\n",
    "        \n",
    "        # Plot confusion matrix\n",
    "        plot_confusion_matrix(all_labels, all_preds, test_subject)\n",
    "        global_true.extend(all_labels)\n",
    "        global_pred.extend(all_preds)\n",
    "        \n",
    "        # Save loss history for this fold\n",
    "        np.save(f\"logs/fold{test_subject+1}_loss_history.npy\", loss_history)\n",
    "    \n",
    "    # Final report\n",
    "    print(\"\\n✅ Training complete.\")\n",
    "    print(\"Subject-wise accuracies:\", fold_accuracies)\n",
    "    print(\"Average Accuracy:\", np.mean(fold_accuracies))\n",
    "    print(\"Overall Accuracy:\", accuracy_score(global_true, global_pred))\n",
    "    \n",
    "    # Plot overall confusion matrix\n",
    "    plot_confusion_matrix(global_true, global_pred, 32)  # Use index 32 for global\n",
    "    \n",
    "    return fold_accuracies, global_true, global_pred\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set paths to your data\n",
    "    feature_path = 'E:/FYP/Finalise Fyp/EEg-based-Emotion-Recognition/de_features.npy'\n",
    "    label_path = 'E:/FYP/Finalise Fyp/EEg-based-Emotion-Recognition/de_labels.npy'\n",
    "    \n",
    "    # Run training\n",
    "    fold_accuracies, global_true, global_pred = train_model(\n",
    "        feature_path=feature_path,\n",
    "        label_path=label_path,\n",
    "        num_subjects=32,\n",
    "        num_epochs=100,\n",
    "        warmup_epochs=15,\n",
    "        batch_size=64\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196c2ad8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
