{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2848cb5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "os.makedirs(\"plots\", exist_ok=True)\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4ef7841",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DEAPDataset(Dataset):\n",
    "    def __init__(self, feature_path, label_path, exclude_subject=None, only_subject=None, normalize=True):\n",
    "        self.features = np.load(feature_path)  # shape: (32, 40, 40, 5, 63)\n",
    "        self.labels = np.load(label_path)      # shape: (32, 40, 63)\n",
    "        \n",
    "        # Compute statistics for normalization if requested\n",
    "        if normalize:\n",
    "            all_features = []\n",
    "            for subj in range(32):\n",
    "                if exclude_subject is not None and subj == exclude_subject:\n",
    "                    continue\n",
    "                if only_subject is not None and subj != only_subject:\n",
    "                    continue\n",
    "                \n",
    "                # Reshape to get all features for this subject\n",
    "                subj_features = self.features[subj].transpose(0, 1, 3, 2).reshape(-1, 40, 5)  # Result: (N, 40, 5)\n",
    "                all_features.append(subj_features)\n",
    "            \n",
    "            if all_features:  # Check if list is not empty\n",
    "                all_features = np.concatenate(all_features, axis=0)\n",
    "                self.mean = np.mean(all_features, axis=0)\n",
    "                self.std = np.std(all_features, axis=0) + 1e-8\n",
    "            else:\n",
    "                # Default if no subjects selected\n",
    "                self.mean = 0\n",
    "                self.std = 1\n",
    "        else:\n",
    "            self.mean = 0\n",
    "            self.std = 1\n",
    "            \n",
    "        # Prepare samples\n",
    "        self.samples = []\n",
    "        for subj in range(32):\n",
    "            if exclude_subject is not None and subj == exclude_subject:\n",
    "                continue\n",
    "            if only_subject is not None and subj != only_subject:\n",
    "                continue\n",
    "                \n",
    "            for trial in range(40):\n",
    "                for win in range(63):\n",
    "                    x = self.features[subj, trial, :, :, win]\n",
    "                    y = self.labels[subj, trial, win]\n",
    "                    self.samples.append((x, y, subj))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, y, subj = self.samples[idx]\n",
    "        # Normalize the data\n",
    "        x = (x - self.mean) / self.std\n",
    "        return torch.tensor(x, dtype=torch.float32), int(y)\n",
    "\n",
    "    def get_subject_data(self, subject):\n",
    "        return [(torch.tensor((x - self.mean) / self.std, dtype=torch.float32), int(y))\n",
    "                for x, y, subj in self.samples if subj == subject]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1b208ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommonFeatureExtractor(nn.Module):\n",
    "    def __init__(self, input_dim=200):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 256)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.act = nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # [batch, 40, 5] → [batch, 200]\n",
    "        x = self.act(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.act(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.act(self.bn3(self.fc3(x)))\n",
    "        return x\n",
    "\n",
    "class SubjectSpecificMapper(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(64, 32)\n",
    "        self.bn = nn.BatchNorm1d(32)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.act = nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.act(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class SubjectSpecificClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(32, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31f9f1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupervisedContrastiveLoss(nn.Module):\n",
    "    def __init__(self, temperature=0.07):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, features, labels):\n",
    "        features = F.normalize(features, dim=1)\n",
    "        batch_size = features.shape[0]\n",
    "        \n",
    "        # Handle case where batch size is 1\n",
    "        if batch_size <= 1:\n",
    "            return torch.tensor(0.0, device=features.device, requires_grad=True)\n",
    "            \n",
    "        sim_matrix = torch.matmul(features, features.T) / self.temperature\n",
    "        labels = labels.contiguous().view(-1, 1)\n",
    "        mask = torch.eq(labels, labels.T).float().to(features.device)\n",
    "        logits_mask = torch.ones_like(mask) - torch.eye(batch_size).to(features.device)\n",
    "        mask = mask * logits_mask\n",
    "        \n",
    "        # Handle case where there are no positive pairs\n",
    "        if mask.sum() == 0:\n",
    "            return torch.tensor(0.0, device=features.device, requires_grad=True)\n",
    "            \n",
    "        exp_sim = torch.exp(sim_matrix) * logits_mask\n",
    "        log_prob = sim_matrix - torch.log(exp_sim.sum(dim=1, keepdim=True) + 1e-9)\n",
    "        loss = - (mask * log_prob).sum() / (mask.sum() + 1e-9)\n",
    "        return loss\n",
    "\n",
    "class MMDLoss(nn.Module):\n",
    "    def __init__(self, kernel_mul=2.0, num_kernels=5):\n",
    "        super().__init__()\n",
    "        self.kernel_mul = kernel_mul\n",
    "        self.num_kernels = num_kernels\n",
    "\n",
    "    def gaussian_kernel(self, source, target):\n",
    "        total = torch.cat([source, target], dim=0)\n",
    "        \n",
    "        # Handle small batch sizes\n",
    "        if total.shape[0] <= 1:\n",
    "            return torch.tensor(0.0, device=source.device, requires_grad=True)\n",
    "            \n",
    "        total0 = total.unsqueeze(0)\n",
    "        total1 = total.unsqueeze(1)\n",
    "        L2_distance = ((total0 - total1) ** 2).sum(2)\n",
    "        \n",
    "        # Prevent division by zero\n",
    "        bandwidth = torch.mean(L2_distance.detach()) + 1e-8\n",
    "        bandwidth_list = [bandwidth * (self.kernel_mul ** i) for i in range(self.num_kernels)]\n",
    "        kernels = [torch.exp(-L2_distance / bw) for bw in bandwidth_list]\n",
    "        return sum(kernels) / len(kernels)\n",
    "\n",
    "    def forward(self, source, target):\n",
    "        source = source.view(source.size(0), -1)\n",
    "        target = target.view(target.size(0), -1)\n",
    "        \n",
    "        # Handle empty batches\n",
    "        if source.shape[0] == 0 or target.shape[0] == 0:\n",
    "            return torch.tensor(0.0, device=source.device, requires_grad=True)\n",
    "            \n",
    "        kernels = self.gaussian_kernel(source, target)\n",
    "        batch_size = source.size(0)\n",
    "        XX = kernels[:batch_size, :batch_size]\n",
    "        YY = kernels[batch_size:, batch_size:]\n",
    "        XY = kernels[:batch_size, batch_size:]\n",
    "        YX = kernels[batch_size:, :batch_size]\n",
    "        return torch.mean(XX + YY - XY - YX)\n",
    "\n",
    "class ContrastiveLossLcon2(nn.Module):\n",
    "    def __init__(self, feature_dim=32, num_classes=4, tau=0.1, gamma=0.5, queue_size=1024):\n",
    "        super().__init__()\n",
    "        self.tau = tau\n",
    "        self.gamma = gamma\n",
    "        self.prototypes = nn.Parameter(torch.randn(num_classes, feature_dim))\n",
    "        self.register_buffer(\"queue\", torch.randn(queue_size, feature_dim))\n",
    "        self.queue = F.normalize(self.queue, dim=-1)\n",
    "\n",
    "    def forward(self, z_t, pseudo_labels):\n",
    "        # Handle empty batches\n",
    "        if z_t.shape[0] == 0:\n",
    "            return torch.tensor(0.0, device=z_t.device, requires_grad=True)\n",
    "            \n",
    "        z_t = F.normalize(z_t, dim=-1)\n",
    "        device = z_t.device\n",
    "        pseudo_labels = pseudo_labels.to(device)\n",
    "        \n",
    "        pos_proto = self.prototypes.to(device)[pseudo_labels]\n",
    "        \n",
    "        # Compute positive and negative logits\n",
    "        pos_logits = torch.sum(z_t * pos_proto, dim=-1) / self.tau\n",
    "        \n",
    "        # Handle case where queue is empty\n",
    "        if self.queue.shape[0] == 0:\n",
    "            return self.gamma * F.cross_entropy(pos_logits.unsqueeze(1), torch.zeros(z_t.size(0), dtype=torch.long, device=device))\n",
    "            \n",
    "        neg_logits = torch.matmul(z_t, self.queue.to(device).T) / self.tau\n",
    "        logits = torch.cat([pos_logits.unsqueeze(1), neg_logits], dim=1)\n",
    "        labels = torch.zeros(z_t.size(0), dtype=torch.long).to(device)\n",
    "        \n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        self._dequeue_and_enqueue(z_t)\n",
    "        return self.gamma * loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _dequeue_and_enqueue(self, embeddings):\n",
    "        embeddings = embeddings.detach().to(self.queue.device)\n",
    "        batch_size = embeddings.size(0)\n",
    "        queue_size = self.queue.size(0)\n",
    "        \n",
    "        if batch_size >= queue_size:\n",
    "            self.queue = embeddings[-queue_size:]\n",
    "        else:\n",
    "            self.queue = torch.cat([self.queue[batch_size:], embeddings], dim=0)\n",
    "\n",
    "class GeneralizedCrossEntropy(nn.Module):\n",
    "    def __init__(self, q=0.7, weight=None):\n",
    "        super().__init__()\n",
    "        self.q = q\n",
    "        self.weight = weight  # class weights (tensor)\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        \n",
    "        # Handle empty batches\n",
    "        if targets.shape[0] == 0:\n",
    "            return torch.tensor(0.0, device=logits.device, requires_grad=True)\n",
    "            \n",
    "        targets_onehot = F.one_hot(targets, num_classes=probs.shape[1]).float()\n",
    "        probs = torch.sum(probs * targets_onehot, dim=1)\n",
    "\n",
    "        if self.weight is not None:\n",
    "            weights = self.weight[targets]\n",
    "            loss = (1 - probs ** self.q) / self.q\n",
    "            return (weights * loss).mean()\n",
    "        else:\n",
    "            return ((1 - probs ** self.q) / self.q).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09d36043",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_batch(dataset, exclude, batch_size=64):\n",
    "    \"\"\"Get a batch of data from target subjects\"\"\"\n",
    "    for subj in range(32):\n",
    "        if subj != exclude:\n",
    "            data = dataset.get_subject_data(subj)\n",
    "            if len(data) >= batch_size:\n",
    "                indices = torch.randperm(len(data))[:batch_size]\n",
    "                x, y = zip(*[data[i] for i in indices])\n",
    "                return torch.stack(x), torch.tensor(y)\n",
    "    \n",
    "    # Fallback: Return a small batch if no subject has enough data\n",
    "    all_data = []\n",
    "    for subj in range(32):\n",
    "        if subj != exclude:\n",
    "            all_data.extend(dataset.get_subject_data(subj))\n",
    "    \n",
    "    if len(all_data) == 0:\n",
    "        # Empty tensor with correct shape as a fallback\n",
    "        empty_sample = next(iter(dataset))\n",
    "        return torch.zeros((0, *empty_sample[0].shape), dtype=torch.float32), torch.zeros(0, dtype=torch.long)\n",
    "        \n",
    "    indices = torch.randperm(len(all_data))[:min(batch_size, len(all_data))]\n",
    "    x, y = zip(*[all_data[i] for i in indices])\n",
    "    return torch.stack(x), torch.tensor(y)\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, subject_idx):\n",
    "    \"\"\"Create and save confusion matrix plot\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1, 2, 3])\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'Confusion Matrix - Subject {subject_idx+1}')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.savefig(f'plots/confmat_subject{subject_idx+1}.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3838163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 Starting Fold 1/32 - Test Subject: s01\n",
      "Epoch [1/100] - Loss: 4.5787 - Con1: 4.5787 - MMD: 0.0039 - Train Acc: 0.0000\n",
      "📊 Validation Accuracy on Subject 1: 0.3230\n",
      "Epoch [2/100] - Loss: 4.1389 - Con1: 4.1388 - MMD: 0.0053 - Train Acc: 0.0000\n",
      "Epoch [3/100] - Loss: 4.1335 - Con1: 4.1334 - MMD: 0.0044 - Train Acc: 0.0000\n",
      "Epoch [4/100] - Loss: 4.1283 - Con1: 4.1282 - MMD: 0.0042 - Train Acc: 0.0000\n",
      "Epoch [5/100] - Loss: 4.1245 - Con1: 4.1244 - MMD: 0.0037 - Train Acc: 0.0000\n",
      "Epoch [6/100] - Loss: 4.1188 - Con1: 4.1187 - MMD: 0.0031 - Train Acc: 0.0000\n",
      "📊 Validation Accuracy on Subject 1: 0.3206\n",
      "Epoch [7/100] - Loss: 4.1159 - Con1: 4.1158 - MMD: 0.0029 - Train Acc: 0.0000\n",
      "Epoch [8/100] - Loss: 4.1098 - Con1: 4.1096 - MMD: 0.0026 - Cls: 0.8992 - Train Acc: 0.2523\n",
      "🏆 New best accuracy for fold 1: 0.2523\n",
      "Epoch [9/100] - Loss: 3.6853 - Con1: 4.1083 - MMD: 0.0025 - Cls: 0.7226 - Train Acc: 0.4818\n",
      "🏆 New best accuracy for fold 1: 0.4818\n",
      "Epoch [10/100] - Loss: 3.2505 - Con1: 4.1064 - MMD: 0.0026 - Cls: 0.6822 - Train Acc: 0.4969\n",
      "🏆 New best accuracy for fold 1: 0.4969\n",
      "Epoch [11/100] - Loss: 2.8351 - Con1: 4.1164 - MMD: 0.0029 - Cls: 0.6991 - Train Acc: 0.4765\n",
      "📊 Validation Accuracy on Subject 1: 0.2052\n",
      "Epoch [12/100] - Loss: 2.4036 - Con1: 4.1149 - MMD: 0.0026 - Cls: 0.6918 - Train Acc: 0.4830\n",
      "Epoch [13/100] - Loss: 1.9682 - Con1: 4.1134 - MMD: 0.0025 - Cls: 0.6808 - Train Acc: 0.4919\n",
      "Epoch [14/100] - Loss: 1.5312 - Con1: 4.1106 - MMD: 0.0023 - Cls: 0.6711 - Train Acc: 0.4989\n",
      "🏆 New best accuracy for fold 1: 0.4989\n",
      "Epoch [15/100] - Loss: 1.0952 - Con1: 4.1094 - MMD: 0.0025 - Cls: 0.6643 - Train Acc: 0.5046\n",
      "🏆 New best accuracy for fold 1: 0.5046\n",
      "Epoch [16/100] - Loss: 0.7881 - Con2: 0.1770 - MMD: 0.0028 - Cls: 0.6993 - Train Acc: 0.4810\n",
      "📊 Validation Accuracy on Subject 1: 0.1722\n",
      "Epoch [17/100] - Loss: 0.6870 - Con2: 0.0286 - MMD: 0.0028 - Cls: 0.6724 - Train Acc: 0.5001\n",
      "Epoch [18/100] - Loss: 0.6744 - Con2: 0.0187 - MMD: 0.0030 - Cls: 0.6647 - Train Acc: 0.5081\n",
      "🏆 New best accuracy for fold 1: 0.5081\n",
      "Epoch [19/100] - Loss: 0.6688 - Con2: 0.0164 - MMD: 0.0030 - Cls: 0.6603 - Train Acc: 0.5109\n",
      "🏆 New best accuracy for fold 1: 0.5109\n",
      "Epoch [20/100] - Loss: 0.6549 - Con2: 0.0130 - MMD: 0.0035 - Cls: 0.6480 - Train Acc: 0.5194\n",
      "🏆 New best accuracy for fold 1: 0.5194\n",
      "Epoch [21/100] - Loss: 0.6462 - Con2: 0.0119 - MMD: 0.0035 - Cls: 0.6399 - Train Acc: 0.5268\n",
      "🏆 New best accuracy for fold 1: 0.5268\n",
      "📊 Validation Accuracy on Subject 1: 0.1968\n",
      "Epoch [22/100] - Loss: 0.6423 - Con2: 0.0116 - MMD: 0.0033 - Cls: 0.6362 - Train Acc: 0.5300\n",
      "🏆 New best accuracy for fold 1: 0.5300\n",
      "Epoch [23/100] - Loss: 0.6327 - Con2: 0.0110 - MMD: 0.0036 - Cls: 0.6268 - Train Acc: 0.5375\n",
      "🏆 New best accuracy for fold 1: 0.5375\n",
      "Epoch [24/100] - Loss: 0.6301 - Con2: 0.0106 - MMD: 0.0038 - Cls: 0.6244 - Train Acc: 0.5388\n",
      "🏆 New best accuracy for fold 1: 0.5388\n",
      "Epoch [25/100] - Loss: 0.6255 - Con2: 0.0102 - MMD: 0.0040 - Cls: 0.6200 - Train Acc: 0.5419\n",
      "🏆 New best accuracy for fold 1: 0.5419\n",
      "Epoch [26/100] - Loss: 0.6220 - Con2: 0.0101 - MMD: 0.0038 - Cls: 0.6166 - Train Acc: 0.5464\n",
      "🏆 New best accuracy for fold 1: 0.5464\n",
      "📊 Validation Accuracy on Subject 1: 0.2770\n",
      "Epoch [27/100] - Loss: 0.6162 - Con2: 0.0103 - MMD: 0.0039 - Cls: 0.6107 - Train Acc: 0.5497\n",
      "🏆 New best accuracy for fold 1: 0.5497\n",
      "Epoch [28/100] - Loss: 0.6124 - Con2: 0.0103 - MMD: 0.0042 - Cls: 0.6069 - Train Acc: 0.5532\n",
      "🏆 New best accuracy for fold 1: 0.5532\n",
      "Epoch [29/100] - Loss: 0.6093 - Con2: 0.0100 - MMD: 0.0042 - Cls: 0.6039 - Train Acc: 0.5547\n",
      "🏆 New best accuracy for fold 1: 0.5547\n",
      "Epoch [30/100] - Loss: 0.6132 - Con2: 0.0097 - MMD: 0.0043 - Cls: 0.6079 - Train Acc: 0.5512\n",
      "Epoch [31/100] - Loss: 0.6429 - Con2: 0.0105 - MMD: 0.0035 - Cls: 0.6373 - Train Acc: 0.5300\n",
      "📊 Validation Accuracy on Subject 1: 0.2821\n",
      "Epoch [32/100] - Loss: 0.6463 - Con2: 0.0100 - MMD: 0.0036 - Cls: 0.6409 - Train Acc: 0.5254\n",
      "Epoch [33/100] - Loss: 0.6472 - Con2: 0.0097 - MMD: 0.0038 - Cls: 0.6420 - Train Acc: 0.5247\n",
      "Epoch [34/100] - Loss: 0.6371 - Con2: 0.0092 - MMD: 0.0039 - Cls: 0.6321 - Train Acc: 0.5311\n",
      "Epoch [35/100] - Loss: 0.6392 - Con2: 0.0103 - MMD: 0.0038 - Cls: 0.6337 - Train Acc: 0.5317\n",
      "Epoch [36/100] - Loss: 0.6394 - Con2: 0.0100 - MMD: 0.0039 - Cls: 0.6340 - Train Acc: 0.5294\n",
      "📊 Validation Accuracy on Subject 1: 0.2357\n",
      "Epoch [37/100] - Loss: 0.6352 - Con2: 0.0100 - MMD: 0.0038 - Cls: 0.6298 - Train Acc: 0.5346\n",
      "Epoch [38/100] - Loss: 0.6275 - Con2: 0.0085 - MMD: 0.0040 - Cls: 0.6228 - Train Acc: 0.5396\n",
      "Epoch [39/100] - Loss: 0.6239 - Con2: 0.0079 - MMD: 0.0042 - Cls: 0.6196 - Train Acc: 0.5425\n",
      "Epoch [40/100] - Loss: 0.6175 - Con2: 0.0080 - MMD: 0.0041 - Cls: 0.6131 - Train Acc: 0.5458\n",
      "Epoch [41/100] - Loss: 0.6182 - Con2: 0.0078 - MMD: 0.0041 - Cls: 0.6139 - Train Acc: 0.5470\n",
      "📊 Validation Accuracy on Subject 1: 0.2238\n",
      "Epoch [42/100] - Loss: 0.6125 - Con2: 0.0075 - MMD: 0.0039 - Cls: 0.6083 - Train Acc: 0.5498\n",
      "Epoch [43/100] - Loss: 0.6067 - Con2: 0.0070 - MMD: 0.0042 - Cls: 0.6028 - Train Acc: 0.5551\n",
      "🏆 New best accuracy for fold 1: 0.5551\n",
      "Epoch [44/100] - Loss: 0.6053 - Con2: 0.0073 - MMD: 0.0042 - Cls: 0.6012 - Train Acc: 0.5552\n",
      "🏆 New best accuracy for fold 1: 0.5552\n",
      "Epoch [45/100] - Loss: 0.6012 - Con2: 0.0067 - MMD: 0.0044 - Cls: 0.5974 - Train Acc: 0.5581\n",
      "🏆 New best accuracy for fold 1: 0.5581\n",
      "Epoch [46/100] - Loss: 0.6010 - Con2: 0.0073 - MMD: 0.0045 - Cls: 0.5969 - Train Acc: 0.5600\n",
      "🏆 New best accuracy for fold 1: 0.5600\n",
      "📊 Validation Accuracy on Subject 1: 0.2639\n",
      "Epoch [47/100] - Loss: 0.5978 - Con2: 0.0079 - MMD: 0.0049 - Cls: 0.5933 - Train Acc: 0.5616\n",
      "🏆 New best accuracy for fold 1: 0.5616\n",
      "Epoch [48/100] - Loss: 0.5929 - Con2: 0.0073 - MMD: 0.0049 - Cls: 0.5888 - Train Acc: 0.5658\n",
      "🏆 New best accuracy for fold 1: 0.5658\n",
      "Epoch [49/100] - Loss: 0.5887 - Con2: 0.0067 - MMD: 0.0051 - Cls: 0.5848 - Train Acc: 0.5695\n",
      "🏆 New best accuracy for fold 1: 0.5695\n",
      "Epoch [50/100] - Loss: 0.5817 - Con2: 0.0063 - MMD: 0.0050 - Cls: 0.5780 - Train Acc: 0.5743\n",
      "🏆 New best accuracy for fold 1: 0.5743\n",
      "Epoch [51/100] - Loss: 0.5848 - Con2: 0.0068 - MMD: 0.0049 - Cls: 0.5809 - Train Acc: 0.5718\n",
      "📊 Validation Accuracy on Subject 1: 0.3095\n",
      "Epoch [52/100] - Loss: 0.5758 - Con2: 0.0066 - MMD: 0.0052 - Cls: 0.5720 - Train Acc: 0.5794\n",
      "🏆 New best accuracy for fold 1: 0.5794\n",
      "Epoch [53/100] - Loss: 0.5758 - Con2: 0.0066 - MMD: 0.0057 - Cls: 0.5720 - Train Acc: 0.5787\n",
      "Epoch [54/100] - Loss: 0.5724 - Con2: 0.0063 - MMD: 0.0055 - Cls: 0.5687 - Train Acc: 0.5822\n",
      "🏆 New best accuracy for fold 1: 0.5822\n",
      "Epoch [55/100] - Loss: 0.5663 - Con2: 0.0069 - MMD: 0.0060 - Cls: 0.5622 - Train Acc: 0.5859\n",
      "🏆 New best accuracy for fold 1: 0.5859\n",
      "Epoch [56/100] - Loss: 0.5676 - Con2: 0.0065 - MMD: 0.0058 - Cls: 0.5638 - Train Acc: 0.5852\n",
      "📊 Validation Accuracy on Subject 1: 0.2675\n",
      "Epoch [57/100] - Loss: 0.5608 - Con2: 0.0062 - MMD: 0.0061 - Cls: 0.5571 - Train Acc: 0.5909\n",
      "🏆 New best accuracy for fold 1: 0.5909\n",
      "Epoch [58/100] - Loss: 0.5634 - Con2: 0.0072 - MMD: 0.0062 - Cls: 0.5592 - Train Acc: 0.5894\n",
      "Epoch [59/100] - Loss: 0.5595 - Con2: 0.0063 - MMD: 0.0065 - Cls: 0.5557 - Train Acc: 0.5913\n",
      "🏆 New best accuracy for fold 1: 0.5913\n",
      "Epoch [60/100] - Loss: 0.5520 - Con2: 0.0064 - MMD: 0.0065 - Cls: 0.5481 - Train Acc: 0.5978\n",
      "🏆 New best accuracy for fold 1: 0.5978\n",
      "Epoch [61/100] - Loss: 0.5522 - Con2: 0.0060 - MMD: 0.0064 - Cls: 0.5486 - Train Acc: 0.5973\n",
      "📊 Validation Accuracy on Subject 1: 0.2639\n",
      "Epoch [62/100] - Loss: 0.5517 - Con2: 0.0054 - MMD: 0.0061 - Cls: 0.5483 - Train Acc: 0.5971\n",
      "Epoch [63/100] - Loss: 0.5511 - Con2: 0.0057 - MMD: 0.0064 - Cls: 0.5476 - Train Acc: 0.5988\n",
      "🏆 New best accuracy for fold 1: 0.5988\n",
      "Epoch [64/100] - Loss: 0.5501 - Con2: 0.0057 - MMD: 0.0063 - Cls: 0.5466 - Train Acc: 0.5975\n",
      "Epoch [65/100] - Loss: 0.5482 - Con2: 0.0055 - MMD: 0.0064 - Cls: 0.5448 - Train Acc: 0.5991\n",
      "🏆 New best accuracy for fold 1: 0.5991\n",
      "Epoch [66/100] - Loss: 0.5486 - Con2: 0.0055 - MMD: 0.0066 - Cls: 0.5452 - Train Acc: 0.6001\n",
      "🏆 New best accuracy for fold 1: 0.6001\n",
      "📊 Validation Accuracy on Subject 1: 0.2591\n",
      "Epoch [67/100] - Loss: 0.5449 - Con2: 0.0057 - MMD: 0.0064 - Cls: 0.5414 - Train Acc: 0.6014\n",
      "🏆 New best accuracy for fold 1: 0.6014\n",
      "Epoch [68/100] - Loss: 0.5485 - Con2: 0.0055 - MMD: 0.0064 - Cls: 0.5452 - Train Acc: 0.5990\n",
      "Epoch [69/100] - Loss: 0.5449 - Con2: 0.0055 - MMD: 0.0064 - Cls: 0.5416 - Train Acc: 0.6037\n",
      "🏆 New best accuracy for fold 1: 0.6037\n",
      "Epoch [70/100] - Loss: 0.5442 - Con2: 0.0051 - MMD: 0.0065 - Cls: 0.5409 - Train Acc: 0.6039\n",
      "🏆 New best accuracy for fold 1: 0.6039\n",
      "Epoch [71/100] - Loss: 0.5838 - Con2: 0.0064 - MMD: 0.0059 - Cls: 0.5800 - Train Acc: 0.5720\n",
      "📊 Validation Accuracy on Subject 1: 0.2365\n",
      "Epoch [72/100] - Loss: 0.5904 - Con2: 0.0060 - MMD: 0.0054 - Cls: 0.5869 - Train Acc: 0.5670\n",
      "Epoch [73/100] - Loss: 0.5921 - Con2: 0.0057 - MMD: 0.0054 - Cls: 0.5887 - Train Acc: 0.5661\n",
      "Epoch [74/100] - Loss: 0.5874 - Con2: 0.0060 - MMD: 0.0057 - Cls: 0.5838 - Train Acc: 0.5694\n",
      "Epoch [75/100] - Loss: 0.5857 - Con2: 0.0051 - MMD: 0.0053 - Cls: 0.5826 - Train Acc: 0.5701\n",
      "Epoch [76/100] - Loss: 0.5866 - Con2: 0.0053 - MMD: 0.0052 - Cls: 0.5834 - Train Acc: 0.5699\n",
      "📊 Validation Accuracy on Subject 1: 0.1980\n",
      "Epoch [77/100] - Loss: 0.5867 - Con2: 0.0048 - MMD: 0.0054 - Cls: 0.5838 - Train Acc: 0.5691\n",
      "Epoch [78/100] - Loss: 0.5859 - Con2: 0.0049 - MMD: 0.0057 - Cls: 0.5829 - Train Acc: 0.5699\n",
      "Epoch [79/100] - Loss: 0.5808 - Con2: 0.0053 - MMD: 0.0054 - Cls: 0.5776 - Train Acc: 0.5743\n",
      "Epoch [80/100] - Loss: 0.5786 - Con2: 0.0050 - MMD: 0.0056 - Cls: 0.5756 - Train Acc: 0.5768\n",
      "Epoch [81/100] - Loss: 0.5771 - Con2: 0.0050 - MMD: 0.0062 - Cls: 0.5740 - Train Acc: 0.5771\n",
      "📊 Validation Accuracy on Subject 1: 0.1675\n",
      "Epoch [82/100] - Loss: 0.5792 - Con2: 0.0046 - MMD: 0.0060 - Cls: 0.5763 - Train Acc: 0.5749\n",
      "Epoch [83/100] - Loss: 0.5723 - Con2: 0.0048 - MMD: 0.0053 - Cls: 0.5694 - Train Acc: 0.5815\n",
      "Epoch [84/100] - Loss: 0.5750 - Con2: 0.0044 - MMD: 0.0055 - Cls: 0.5723 - Train Acc: 0.5774\n",
      "Epoch [85/100] - Loss: 0.5673 - Con2: 0.0050 - MMD: 0.0053 - Cls: 0.5643 - Train Acc: 0.5839\n",
      "Epoch [86/100] - Loss: 0.5715 - Con2: 0.0051 - MMD: 0.0057 - Cls: 0.5684 - Train Acc: 0.5813\n",
      "📊 Validation Accuracy on Subject 1: 0.1917\n",
      "Epoch [87/100] - Loss: 0.5697 - Con2: 0.0047 - MMD: 0.0059 - Cls: 0.5667 - Train Acc: 0.5817\n",
      "Epoch [88/100] - Loss: 0.5633 - Con2: 0.0042 - MMD: 0.0057 - Cls: 0.5606 - Train Acc: 0.5890\n",
      "Epoch [89/100] - Loss: 0.5644 - Con2: 0.0046 - MMD: 0.0054 - Cls: 0.5615 - Train Acc: 0.5881\n",
      "Epoch [90/100] - Loss: 0.5647 - Con2: 0.0040 - MMD: 0.0051 - Cls: 0.5622 - Train Acc: 0.5851\n",
      "Epoch [91/100] - Loss: 0.5597 - Con2: 0.0044 - MMD: 0.0054 - Cls: 0.5569 - Train Acc: 0.5908\n",
      "📊 Validation Accuracy on Subject 1: 0.2071\n",
      "Epoch [92/100] - Loss: 0.5582 - Con2: 0.0042 - MMD: 0.0054 - Cls: 0.5556 - Train Acc: 0.5915\n",
      "Epoch [93/100] - Loss: 0.5606 - Con2: 0.0047 - MMD: 0.0052 - Cls: 0.5577 - Train Acc: 0.5901\n",
      "Epoch [94/100] - Loss: 0.5542 - Con2: 0.0044 - MMD: 0.0052 - Cls: 0.5515 - Train Acc: 0.5943\n",
      "Epoch [95/100] - Loss: 0.5494 - Con2: 0.0045 - MMD: 0.0055 - Cls: 0.5466 - Train Acc: 0.5976\n",
      "Epoch [96/100] - Loss: 0.5531 - Con2: 0.0046 - MMD: 0.0053 - Cls: 0.5503 - Train Acc: 0.5956\n",
      "📊 Validation Accuracy on Subject 1: 0.1901\n",
      "Epoch [97/100] - Loss: 0.5498 - Con2: 0.0046 - MMD: 0.0051 - Cls: 0.5470 - Train Acc: 0.5970\n",
      "Epoch [98/100] - Loss: 0.5499 - Con2: 0.0048 - MMD: 0.0058 - Cls: 0.5470 - Train Acc: 0.5981\n",
      "Epoch [99/100] - Loss: 0.5479 - Con2: 0.0041 - MMD: 0.0053 - Cls: 0.5453 - Train Acc: 0.5993\n",
      "Epoch [100/100] - Loss: 0.5481 - Con2: 0.0041 - MMD: 0.0055 - Cls: 0.5455 - Train Acc: 0.6001\n",
      "📊 Validation Accuracy on Subject 1: 0.1964\n",
      "🎯 Final Accuracy on Subject 1: 0.1964\n",
      "\n",
      "🚀 Starting Fold 2/32 - Test Subject: s02\n",
      "Epoch [1/100] - Loss: 4.5849 - Con1: 4.5849 - MMD: 0.0031 - Train Acc: 0.0000\n",
      "📊 Validation Accuracy on Subject 2: 0.1976\n",
      "Epoch [2/100] - Loss: 4.1387 - Con1: 4.1387 - MMD: 0.0023 - Train Acc: 0.0000\n",
      "Epoch [3/100] - Loss: 4.1330 - Con1: 4.1329 - MMD: 0.0032 - Train Acc: 0.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 283\u001b[0m\n\u001b[0;32m    280\u001b[0m label_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mE:/FYP/Finalise Fyp/EEg-based-Emotion-Recognition/de_labels.npy\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    282\u001b[0m \u001b[38;5;66;03m# Run training\u001b[39;00m\n\u001b[1;32m--> 283\u001b[0m fold_accuracies, global_true, global_pred \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    284\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    285\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_subjects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    287\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    288\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwarmup_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    289\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\n\u001b[0;32m    290\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[13], line 71\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(feature_path, label_path, num_subjects, num_epochs, warmup_epochs, batch_size)\u001b[0m\n\u001b[0;32m     68\u001b[0m x_s, y_s \u001b[38;5;241m=\u001b[39m x_s\u001b[38;5;241m.\u001b[39mto(device), y_s\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# Get target batch\u001b[39;00m\n\u001b[1;32m---> 71\u001b[0m x_t, y_t \u001b[38;5;241m=\u001b[39m \u001b[43mget_target_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_subject\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     72\u001b[0m x_t, y_t \u001b[38;5;241m=\u001b[39m x_t\u001b[38;5;241m.\u001b[39mto(device), y_t\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# Skip this batch if either source or target is empty\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[12], line 5\u001b[0m, in \u001b[0;36mget_target_batch\u001b[1;34m(dataset, exclude, batch_size)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m subj \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m32\u001b[39m):\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m subj \u001b[38;5;241m!=\u001b[39m exclude:\n\u001b[1;32m----> 5\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_subject_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m batch_size:\n\u001b[0;32m      7\u001b[0m             indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandperm(\u001b[38;5;28mlen\u001b[39m(data))[:batch_size]\n",
      "Cell \u001b[1;32mIn[9], line 55\u001b[0m, in \u001b[0;36mDEAPDataset.get_subject_data\u001b[1;34m(self, subject)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_subject_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, subject):\n\u001b[1;32m---> 55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mint\u001b[39m(y))\n\u001b[0;32m     56\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m x, y, subj \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples \u001b[38;5;28;01mif\u001b[39;00m subj \u001b[38;5;241m==\u001b[39m subject]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train_model(feature_path, label_path, num_subjects=32, num_epochs=100, \n",
    "                warmup_epochs=15, batch_size=64):\n",
    "    \"\"\"Main training function for all folds\"\"\"\n",
    "    \n",
    "    # Hyperparameters\n",
    "    lambda_1 = 1.0  # For contrastive_loss_con1 during warmup\n",
    "    lambda_2 = 0.5  # For contrastive_loss_con2 after warmup\n",
    "    lambda_3_init = 0.1  # Initial value for MMD loss\n",
    "    \n",
    "    # Logging containers\n",
    "    fold_accuracies = []\n",
    "    global_true = []\n",
    "    global_pred = []\n",
    "    \n",
    "    for test_subject in range(num_subjects):\n",
    "        print(f\"\\n🚀 Starting Fold {test_subject+1}/{num_subjects} - Test Subject: s{test_subject+1:02d}\")\n",
    "        \n",
    "        # Create datasets and loaders\n",
    "        train_dataset = DEAPDataset(feature_path, label_path, exclude_subject=test_subject, normalize=True)\n",
    "        test_dataset = DEAPDataset(feature_path, label_path, only_subject=test_subject, normalize=True)\n",
    "        \n",
    "        # Create balanced sampler\n",
    "        all_labels = np.array([label for _, label in train_dataset])\n",
    "        class_sample_counts = np.bincount(all_labels)\n",
    "        class_weights = 1. / class_sample_counts\n",
    "        sample_weights = [class_weights[label] for label in all_labels]\n",
    "        sampler = WeightedRandomSampler(sample_weights, len(sample_weights), replacement=True)\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        # Initialize models\n",
    "        cfe = CommonFeatureExtractor().to(device)\n",
    "        sfe = SubjectSpecificMapper().to(device)\n",
    "        ssc = SubjectSpecificClassifier().to(device)\n",
    "        \n",
    "        # Initialize losses\n",
    "        contrastive_loss_con1 = SupervisedContrastiveLoss(temperature=0.07).to(device)\n",
    "        contrastive_loss_con2 = ContrastiveLossLcon2(tau=0.1).to(device)\n",
    "        mmd_loss = MMDLoss().to(device)\n",
    "        gce_loss = GeneralizedCrossEntropy(q=0.7).to(device)\n",
    "        \n",
    "        # Initialize optimizer and scheduler\n",
    "        all_params = list(cfe.parameters()) + list(sfe.parameters()) + list(ssc.parameters())\n",
    "        optimizer = torch.optim.AdamW(all_params, lr=1e-3, weight_decay=1e-4)\n",
    "        scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=1e-6)\n",
    "        \n",
    "        # Training tracking\n",
    "        best_acc = 0\n",
    "        transition_start = warmup_epochs // 2\n",
    "        \n",
    "        # Track loss components\n",
    "        loss_history = {\n",
    "            'total': [], 'con1': [], 'con2': [], 'mmd': [], 'cls': [],\n",
    "            'train_acc': [], 'val_acc': []\n",
    "        }\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(num_epochs):\n",
    "            cfe.train()\n",
    "            sfe.train()\n",
    "            ssc.train()\n",
    "            \n",
    "            epoch_losses = {'total': 0, 'con1': 0, 'con2': 0, 'mmd': 0, 'cls': 0}\n",
    "            total_correct, total_samples = 0, 0\n",
    "            \n",
    "            for x_s, y_s in train_loader:\n",
    "                x_s, y_s = x_s.to(device), y_s.to(device)\n",
    "                \n",
    "                # Get target batch\n",
    "                x_t, y_t = get_target_batch(train_dataset, exclude=test_subject, batch_size=batch_size)\n",
    "                x_t, y_t = x_t.to(device), y_t.to(device)\n",
    "                \n",
    "                # Skip this batch if either source or target is empty\n",
    "                if x_s.shape[0] == 0 or x_t.shape[0] == 0:\n",
    "                    continue\n",
    "                \n",
    "                # Match batch sizes\n",
    "                min_size = min(x_s.size(0), x_t.size(0))\n",
    "                x_s = x_s[:min_size]\n",
    "                y_s = y_s[:min_size]\n",
    "                x_t = x_t[:min_size]\n",
    "                y_t = y_t[:min_size]\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Extract features\n",
    "                z_s_common = cfe(x_s)\n",
    "                z_t_common = cfe(x_t)\n",
    "                \n",
    "                # Always extract subject-specific features\n",
    "                z_s_subject = sfe(z_s_common)\n",
    "                z_t_subject = sfe(z_t_common)\n",
    "                \n",
    "                # Domain adaptation loss (always applied)\n",
    "                loss_mmd = mmd_loss(z_s_common, z_t_common)\n",
    "                \n",
    "                # Adjusted MMD weight - gradually increase during warmup\n",
    "                lambda_3 = lambda_3_init\n",
    "                if epoch < warmup_epochs:\n",
    "                    lambda_3 = lambda_3_init * (epoch + 1) / warmup_epochs\n",
    "                \n",
    "                # Different training phases\n",
    "                if epoch < transition_start:\n",
    "                    # Pure contrastive phase\n",
    "                    loss_con1 = contrastive_loss_con1(z_s_common, y_s)\n",
    "                    loss = lambda_1 * loss_con1 + lambda_3 * loss_mmd\n",
    "                    \n",
    "                    # Track loss components\n",
    "                    epoch_losses['con1'] += loss_con1.item()\n",
    "                    epoch_losses['mmd'] += loss_mmd.item()\n",
    "                    \n",
    "                elif epoch < warmup_epochs:\n",
    "                    # Transition phase - gradually introduce classification\n",
    "                    transition_factor = (epoch - transition_start) / (warmup_epochs - transition_start)\n",
    "                    \n",
    "                    loss_con1 = contrastive_loss_con1(z_s_common, y_s)\n",
    "                    logits = ssc(z_s_subject)\n",
    "                    loss_cls = gce_loss(logits, y_s)\n",
    "                    \n",
    "                    loss = (1 - transition_factor) * (lambda_1 * loss_con1) + \\\n",
    "                           transition_factor * loss_cls + lambda_3 * loss_mmd\n",
    "                    \n",
    "                    # Track loss components\n",
    "                    epoch_losses['con1'] += loss_con1.item()\n",
    "                    epoch_losses['cls'] += loss_cls.item()\n",
    "                    epoch_losses['mmd'] += loss_mmd.item()\n",
    "                    \n",
    "                    # Calculate accuracy\n",
    "                    preds = torch.argmax(logits, dim=1)\n",
    "                    total_correct += (preds == y_s).sum().item()\n",
    "                    total_samples += y_s.size(0)\n",
    "                    \n",
    "                else:\n",
    "                    # Full classification phase\n",
    "                    logits = ssc(z_s_subject)\n",
    "                    loss_cls = gce_loss(logits, y_s)\n",
    "                    \n",
    "                    # Use pseudo labels for target domain\n",
    "                    with torch.no_grad():\n",
    "                        pseudo_logits = ssc(z_t_subject)\n",
    "                        pseudo_labels = torch.argmax(pseudo_logits, dim=1)\n",
    "                    \n",
    "                    loss_con2 = contrastive_loss_con2(z_t_subject, pseudo_labels)\n",
    "                    loss = loss_cls + lambda_2 * loss_con2 + lambda_3 * loss_mmd\n",
    "                    \n",
    "                    # Track loss components\n",
    "                    epoch_losses['cls'] += loss_cls.item()\n",
    "                    epoch_losses['con2'] += loss_con2.item()\n",
    "                    epoch_losses['mmd'] += loss_mmd.item()\n",
    "                    \n",
    "                    # Calculate accuracy\n",
    "                    preds = torch.argmax(logits, dim=1)\n",
    "                    total_correct += (preds == y_s).sum().item()\n",
    "                    total_samples += y_s.size(0)\n",
    "                \n",
    "                # Update total loss\n",
    "                epoch_losses['total'] += loss.item()\n",
    "                \n",
    "                # Backpropagation with gradient clipping\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(all_params, max_norm=1.0)\n",
    "                optimizer.step()\n",
    "            \n",
    "            # Update scheduler\n",
    "            scheduler.step()\n",
    "            \n",
    "            # Calculate average losses\n",
    "            num_batches = len(train_loader)\n",
    "            avg_total_loss = epoch_losses['total'] / num_batches\n",
    "            avg_train_acc = total_correct / total_samples if total_samples > 0 else 0\n",
    "            \n",
    "            # Update loss history\n",
    "            loss_history['total'].append(avg_total_loss)\n",
    "            loss_history['train_acc'].append(avg_train_acc)\n",
    "            \n",
    "            for key in ['con1', 'con2', 'mmd', 'cls']:\n",
    "                if epoch_losses[key] > 0:\n",
    "                    loss_history[key].append(epoch_losses[key] / num_batches)\n",
    "                else:\n",
    "                    loss_history[key].append(0)\n",
    "            \n",
    "            # Print progress\n",
    "            progress_str = f\"Epoch [{epoch+1}/{num_epochs}] - Loss: {avg_total_loss:.4f}\"\n",
    "            if epoch < warmup_epochs:\n",
    "                progress_str += f\" - Con1: {loss_history['con1'][-1]:.4f}\"\n",
    "            else:\n",
    "                progress_str += f\" - Con2: {loss_history['con2'][-1]:.4f}\"\n",
    "            \n",
    "            progress_str += f\" - MMD: {loss_history['mmd'][-1]:.4f}\"\n",
    "            \n",
    "            if epoch >= transition_start:\n",
    "                progress_str += f\" - Cls: {loss_history['cls'][-1]:.4f}\"\n",
    "                \n",
    "            progress_str += f\" - Train Acc: {avg_train_acc:.4f}\"\n",
    "            print(progress_str)\n",
    "            \n",
    "            # Save checkpoint if accuracy improves\n",
    "            if avg_train_acc > best_acc:\n",
    "                best_acc = avg_train_acc\n",
    "                print(f\"🏆 New best accuracy for fold {test_subject+1}: {best_acc:.4f}\")\n",
    "                torch.save({\n",
    "                    'epoch': epoch + 1,\n",
    "                    'cfe_state_dict': cfe.state_dict(),\n",
    "                    'sfe_state_dict': sfe.state_dict(),\n",
    "                    'ssc_state_dict': ssc.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'scheduler_state_dict': scheduler.state_dict(),\n",
    "                    'loss': avg_total_loss,\n",
    "                    'accuracy': avg_train_acc\n",
    "                }, f\"checkpoints/fold{test_subject+1}_best.pt\")\n",
    "            \n",
    "            # Evaluation after each epoch\n",
    "            if epoch % 5 == 0 or epoch == num_epochs - 1:\n",
    "                cfe.eval()\n",
    "                sfe.eval()\n",
    "                ssc.eval()\n",
    "                \n",
    "                all_preds, all_labels = [], []\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    for x_test, y_test in test_loader:\n",
    "                        x_test = x_test.to(device)\n",
    "                        z_common = cfe(x_test)\n",
    "                        z_subject = sfe(z_common)\n",
    "                        logits = ssc(z_subject)\n",
    "                        preds = torch.argmax(logits, dim=1)\n",
    "                        \n",
    "                        all_preds.extend(preds.cpu().numpy())\n",
    "                        all_labels.extend(y_test.numpy())\n",
    "                \n",
    "                val_acc = accuracy_score(all_labels, all_preds)\n",
    "                loss_history['val_acc'].append(val_acc)\n",
    "                print(f\"📊 Validation Accuracy on Subject {test_subject+1}: {val_acc:.4f}\")\n",
    "        \n",
    "        # Final evaluation\n",
    "        cfe.eval()\n",
    "        sfe.eval()\n",
    "        ssc.eval()\n",
    "        \n",
    "        all_preds, all_labels = [], []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for x_test, y_test in test_loader:\n",
    "                x_test = x_test.to(device)\n",
    "                z_common = cfe(x_test)\n",
    "                z_subject = sfe(z_common)\n",
    "                logits = ssc(z_subject)\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "                \n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(y_test.numpy())\n",
    "        \n",
    "        final_acc = accuracy_score(all_labels, all_preds)\n",
    "        fold_accuracies.append(final_acc)\n",
    "        print(f\"🎯 Final Accuracy on Subject {test_subject+1}: {final_acc:.4f}\")\n",
    "        \n",
    "        # Plot confusion matrix\n",
    "        plot_confusion_matrix(all_labels, all_preds, test_subject)\n",
    "        global_true.extend(all_labels)\n",
    "        global_pred.extend(all_preds)\n",
    "        \n",
    "        # Save loss history for this fold\n",
    "        np.save(f\"logs/fold{test_subject+1}_loss_history.npy\", loss_history)\n",
    "    \n",
    "    # Final report\n",
    "    print(\"\\n✅ Training complete.\")\n",
    "    print(\"Subject-wise accuracies:\", fold_accuracies)\n",
    "    print(\"Average Accuracy:\", np.mean(fold_accuracies))\n",
    "    print(\"Overall Accuracy:\", accuracy_score(global_true, global_pred))\n",
    "    \n",
    "    # Plot overall confusion matrix\n",
    "    plot_confusion_matrix(global_true, global_pred, 32)  # Use index 32 for global\n",
    "    \n",
    "    return fold_accuracies, global_true, global_pred\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set paths to your data\n",
    "    feature_path = 'E:/FYP/Finalise Fyp/EEg-based-Emotion-Recognition/de_features.npy'\n",
    "    label_path = 'E:/FYP/Finalise Fyp/EEg-based-Emotion-Recognition/de_labels.npy'\n",
    "    \n",
    "    # Run training\n",
    "    fold_accuracies, global_true, global_pred = train_model(\n",
    "        feature_path=feature_path,\n",
    "        label_path=label_path,\n",
    "        num_subjects=32,\n",
    "        num_epochs=100,\n",
    "        warmup_epochs=15,\n",
    "        batch_size=64\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196c2ad8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
