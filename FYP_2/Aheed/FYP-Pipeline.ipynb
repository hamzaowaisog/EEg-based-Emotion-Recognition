{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e45ff45",
   "metadata": {},
   "source": [
    "# EEG-Facial Fusion with Contrastive Learning for Subject-Invariant Emotion Recognition\n",
    "# Research-Grade Pipeline with SOTA Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2c544076",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d5e617",
   "metadata": {},
   "source": [
    "1.1 EEG Data Loading and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0309ac78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_eeg_data(data_path, num_subjects=22):\n",
    "    \"\"\"\n",
    "    Load EEG data from DEAP dataset\n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "    all_labels = []\n",
    "    subject_ids = []\n",
    "    \n",
    "    for subject in range(1, num_subjects+1):\n",
    "        # Load data\n",
    "        file_name = f\"{data_path}/s{subject:02d}.dat\"\n",
    "        with open(file_name, 'rb') as f:\n",
    "            data = pickle.load(f, encoding='latin1')\n",
    "        \n",
    "        # Extract EEG channels (first 32 channels)\n",
    "        eeg_data = data['data'][:, :32, :]\n",
    "        \n",
    "        # Extract labels (valence, arousal, dominance, liking)\n",
    "        labels = data['labels']\n",
    "        \n",
    "        # Add to lists\n",
    "        all_data.append(eeg_data)\n",
    "        all_labels.append(labels)\n",
    "        subject_ids.extend([subject] * eeg_data.shape[0])\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    all_data = np.vstack([data for data in all_data])\n",
    "    all_labels = np.vstack([label for label in all_labels])\n",
    "    subject_ids = np.array(subject_ids)\n",
    "    \n",
    "    return all_data, all_labels, subject_ids\n",
    "\n",
    "# Advanced EEG feature extraction\n",
    "def extract_eeg_features(eeg_data):\n",
    "    \"\"\"\n",
    "    Extract advanced EEG features based on SOTA approaches\n",
    "    \n",
    "    References:\n",
    "    [1] Zheng, W. L., & Lu, B. L. (2015). Investigating critical frequency bands and channels \n",
    "        for EEG-based emotion recognition with deep neural networks. IEEE Transactions on \n",
    "        Autonomous Mental Development, 7(3), 162-175.\n",
    "    [2] Li, J., Zhang, Z., & He, H. (2018). Hierarchical convolutional neural networks for \n",
    "        EEG-based emotion recognition. Cognitive Computation, 10(2), 368-380.\n",
    "    \"\"\"\n",
    "    # Extract frequency band power using Welch's method\n",
    "    from scipy import signal\n",
    "    \n",
    "    features = []\n",
    "    \n",
    "    # Define frequency bands\n",
    "    bands = {\n",
    "        'theta': (4, 8),\n",
    "        'alpha': (8, 13),\n",
    "        'beta': (13, 30),\n",
    "        'gamma': (30, 45)\n",
    "    }\n",
    "    \n",
    "    # For each trial\n",
    "    for trial in range(eeg_data.shape[0]):\n",
    "        trial_features = []\n",
    "        \n",
    "        # For each channel\n",
    "        for channel in range(eeg_data.shape[1]):\n",
    "            channel_data = eeg_data[trial, channel, :]\n",
    "            \n",
    "            # Extract band power features\n",
    "            for band_name, (low_freq, high_freq) in bands.items():\n",
    "                # Apply bandpass filter\n",
    "                fs = 128  # Sampling frequency\n",
    "                nyq = 0.5 * fs\n",
    "                low = low_freq / nyq\n",
    "                high = high_freq / nyq\n",
    "                b, a = signal.butter(4, [low, high], btype='band')\n",
    "                filtered_data = signal.filtfilt(b, a, channel_data)\n",
    "                \n",
    "                # Calculate band power (DE feature as in [1])\n",
    "                # Differential Entropy (DE) feature\n",
    "                variance = np.var(filtered_data)\n",
    "                de = 0.5 * np.log(2 * np.pi * np.e * variance)\n",
    "                trial_features.append(de)\n",
    "                \n",
    "                # Add more advanced features\n",
    "                # Hjorth parameters (Activity, Mobility, Complexity)\n",
    "                activity = np.var(filtered_data)\n",
    "                mobility = np.sqrt(np.var(np.diff(filtered_data)) / activity)\n",
    "                complexity = np.sqrt(np.var(np.diff(np.diff(filtered_data))) / np.var(np.diff(filtered_data))) / mobility\n",
    "                trial_features.extend([activity, mobility, complexity])\n",
    "        \n",
    "        features.append(trial_features)\n",
    "    \n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809de318",
   "metadata": {},
   "source": [
    "1.2 Facial Feature Processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0fd89e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_facial_features(features_path):\n",
    "    \"\"\"\n",
    "    Load pre-extracted facial features from ViT\n",
    "    \"\"\"\n",
    "    # Load facial features from npy files\n",
    "    excited_happy = np.load(f\"{features_path}/Excited_Happy_features.npy\")\n",
    "    calm_content = np.load(f\"{features_path}/Calm_Content_features.npy\")\n",
    "    sad_bored = np.load(f\"{features_path}/Sad_Bored_features.npy\")\n",
    "    angry_fearful = np.load(f\"{features_path}/Angry_Fearful_features.npy\")\n",
    "    \n",
    "    # Combine features\n",
    "    all_features = np.vstack([excited_happy, calm_content, sad_bored, angry_fearful])\n",
    "    \n",
    "    # Create labels based on the emotion categories\n",
    "    # 0: Excited/Happy, 1: Calm/Content, 2: Sad/Bored, 3: Angry/Fearful\n",
    "    labels = np.concatenate([\n",
    "        np.zeros(excited_happy.shape[0]),\n",
    "        np.ones(calm_content.shape[0]),\n",
    "        np.ones(sad_bored.shape[0]) * 2,\n",
    "        np.ones(angry_fearful.shape[0]) * 3\n",
    "    ])\n",
    "    \n",
    "    return all_features, labels\n",
    "\n",
    "# Enhance facial features with temporal dynamics\n",
    "def enhance_facial_features(facial_features, window_size=5, stride=1):\n",
    "    \"\"\"\n",
    "    Enhance facial features by incorporating temporal dynamics\n",
    "    \n",
    "    References:\n",
    "    [3] Zhao, Z., Zheng, Z., & Zhang, L. (2021). Temporal relation modeling for \n",
    "        facial expression recognition. Pattern Recognition, 118, 107997.\n",
    "    \"\"\"\n",
    "    enhanced_features = []\n",
    "    \n",
    "    # Group features by emotion category and subject\n",
    "    # Assuming features are ordered by emotion category and then by subject\n",
    "    \n",
    "    # Apply sliding window to capture temporal dynamics\n",
    "    for i in range(0, len(facial_features) - window_size + 1, stride):\n",
    "        window = facial_features[i:i+window_size]\n",
    "        \n",
    "        # Extract statistical features from the window\n",
    "        mean_features = np.mean(window, axis=0)\n",
    "        std_features = np.std(window, axis=0)\n",
    "        max_features = np.max(window, axis=0)\n",
    "        min_features = np.min(window, axis=0)\n",
    "        \n",
    "        # Combine features\n",
    "        combined = np.concatenate([mean_features, std_features, max_features, min_features])\n",
    "        enhanced_features.append(combined)\n",
    "    \n",
    "    return np.array(enhanced_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce5c070",
   "metadata": {},
   "source": [
    "1.3 Custom Dataset for Multimodal Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "71cc3eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DEAPMultimodalDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset for multimodal EEG and facial data with contrastive learning support\n",
    "    \"\"\"\n",
    "    def __init__(self, eeg_features, facial_features, labels, subject_ids, transform=None):\n",
    "        self.eeg_features = eeg_features\n",
    "        self.facial_features = facial_features\n",
    "        self.labels = labels\n",
    "        self.subject_ids = subject_ids\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Normalize features\n",
    "        self.eeg_scaler = StandardScaler()\n",
    "        self.facial_scaler = StandardScaler()\n",
    "        \n",
    "        self.eeg_features = self.eeg_scaler.fit_transform(self.eeg_features)\n",
    "        self.facial_features = self.facial_scaler.fit_transform(self.facial_features)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.eeg_features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        eeg_feat = self.eeg_features[idx]\n",
    "        facial_feat = self.facial_features[idx]\n",
    "        label = self.labels[idx]\n",
    "        subject_id = self.subject_ids[idx]\n",
    "        \n",
    "        # Apply transforms if any\n",
    "        if self.transform:\n",
    "            eeg_feat = self.transform(eeg_feat)\n",
    "            facial_feat = self.transform(facial_feat)\n",
    "        \n",
    "        return {\n",
    "            'eeg': torch.FloatTensor(eeg_feat),\n",
    "            'facial': torch.FloatTensor(facial_feat),\n",
    "            'label': torch.FloatTensor(label),\n",
    "            'subject_id': torch.LongTensor([subject_id])\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a776f44e",
   "metadata": {},
   "source": [
    "2.1 Encoder Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3e6b2c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Advanced EEG encoder with attention mechanism\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim=128, latent_dim=64, dropout=0.5):\n",
    "        super(EEGEncoder, self).__init__()\n",
    "        \n",
    "        # Print dimensions for debugging\n",
    "        print(f\"EEGEncoder: input_dim={input_dim}, hidden_dim={hidden_dim}, latent_dim={latent_dim}\")\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        # Self-attention mechanism\n",
    "        self.query = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.key = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.value = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "        # Output projection\n",
    "        self.projection = nn.Linear(hidden_dim, latent_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Initial encoding\n",
    "        x = self.encoder(x)\n",
    "        \n",
    "        # Self-attention\n",
    "        q = self.query(x).unsqueeze(1)  # [B, 1, H]\n",
    "        k = self.key(x).unsqueeze(1)    # [B, 1, H]\n",
    "        v = self.value(x).unsqueeze(1)  # [B, 1, H]\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / (k.size(-1) ** 0.5)\n",
    "        attention = F.softmax(scores, dim=-1)\n",
    "        context = torch.matmul(attention, v).squeeze(1)\n",
    "        \n",
    "        # Final projection\n",
    "        output = self.projection(context)\n",
    "        \n",
    "        return output\n",
    "\n",
    "class FacialEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Facial feature encoder with residual connections\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim=256, latent_dim=64, dropout=0.3):\n",
    "        super(FacialEncoder, self).__init__()\n",
    "        \n",
    "        # Print dimensions for debugging\n",
    "        print(f\"FacialEncoder: input_dim={input_dim}, hidden_dim={hidden_dim}, latent_dim={latent_dim}\")\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        \n",
    "        # Residual blocks\n",
    "        self.res_block1 = ResidualBlock(hidden_dim, hidden_dim // 2)\n",
    "        self.res_block2 = ResidualBlock(hidden_dim, hidden_dim // 2)\n",
    "        \n",
    "        # Output projection\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, latent_dim),\n",
    "            nn.BatchNorm1d(latent_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.bn1(self.fc1(x)))\n",
    "        \n",
    "        # Apply residual blocks\n",
    "        x = self.res_block1(x)\n",
    "        x = self.res_block2(x)\n",
    "        \n",
    "        # Final projection\n",
    "        output = self.projection(x)\n",
    "        \n",
    "        return output\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Residual block for the facial encoder\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, hidden_features):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        \n",
    "        self.block = nn.Sequential(\n",
    "            nn.Linear(in_features, hidden_features),\n",
    "            nn.BatchNorm1d(hidden_features),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_features, in_features),\n",
    "            nn.BatchNorm1d(in_features)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.block(x)\n",
    "        out += residual\n",
    "        out = F.leaky_relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576d59df",
   "metadata": {},
   "source": [
    "2.2 Multimodal Fusion with Cross-Attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5b0f454c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossModalAttentionFusion(nn.Module):\n",
    "    \"\"\"\n",
    "    Cross-modal attention fusion module for EEG and facial features\n",
    "    \n",
    "    References:\n",
    "    [6] Huang, D., Chen, S., Liu, C., Zheng, W. L., & Lu, B. L. (2020). Multimodal emotion \n",
    "        recognition with cross-modal attention networks. IEEE Transactions on Multimedia.\n",
    "    [7] Nagrani, A., Yang, S., Arnab, A., Jansen, A., Schmid, C., & Sun, C. (2021). \n",
    "        Attention bottlenecks for multimodal fusion. NeurIPS 2021.\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_dim, output_dim):\n",
    "        super(CrossModalAttentionFusion, self).__init__()\n",
    "        \n",
    "        self.eeg_projection = nn.Linear(feature_dim, feature_dim)\n",
    "        self.facial_projection = nn.Linear(feature_dim, feature_dim)\n",
    "        \n",
    "        # Cross-modal attention\n",
    "        self.eeg_to_facial_attn = nn.Linear(feature_dim, feature_dim)\n",
    "        self.facial_to_eeg_attn = nn.Linear(feature_dim, feature_dim)\n",
    "        \n",
    "        # Gating mechanism\n",
    "        self.eeg_gate = nn.Linear(feature_dim * 2, feature_dim)\n",
    "        self.facial_gate = nn.Linear(feature_dim * 2, feature_dim)\n",
    "        \n",
    "        # Output projection\n",
    "        self.output_projection = nn.Linear(feature_dim * 2, output_dim)\n",
    "        \n",
    "    def forward(self, eeg_features, facial_features):\n",
    "        # Project features\n",
    "        eeg_proj = self.eeg_projection(eeg_features)\n",
    "        facial_proj = self.facial_projection(facial_features)\n",
    "        \n",
    "        # Cross-modal attention\n",
    "        eeg_to_facial = torch.sigmoid(self.eeg_to_facial_attn(eeg_proj))\n",
    "        facial_to_eeg = torch.sigmoid(self.facial_to_eeg_attn(facial_proj))\n",
    "        \n",
    "        # Apply attention\n",
    "        eeg_attended = eeg_proj * facial_to_eeg\n",
    "        facial_attended = facial_proj * eeg_to_facial\n",
    "        \n",
    "        # Gating mechanism\n",
    "        eeg_concat = torch.cat([eeg_proj, eeg_attended], dim=1)\n",
    "        facial_concat = torch.cat([facial_proj, facial_attended], dim=1)\n",
    "        \n",
    "        eeg_gated = torch.sigmoid(self.eeg_gate(eeg_concat)) * eeg_attended\n",
    "        facial_gated = torch.sigmoid(self.facial_gate(facial_concat)) * facial_attended\n",
    "        \n",
    "        # Combine features\n",
    "        combined = torch.cat([eeg_gated, facial_gated], dim=1)\n",
    "        \n",
    "        # Final projection\n",
    "        output = self.output_projection(combined)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440ee253",
   "metadata": {},
   "source": [
    "2.3 Complete Multimodal Contrastive Learning Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "50dd5134",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalContrastiveModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete multimodal contrastive learning model with subject-invariant features\n",
    "    \n",
    "    References:\n",
    "    [8] Khosla, P., Teterwak, P., Wang, C., Sarna, A., Tian, Y., Isola, P., ... & \n",
    "        Krishnan, D. (2020). Supervised contrastive learning. NeurIPS 2020.\n",
    "    [9] Saeed, A., Ozcelebi, T., & Lukkien, J. (2019). Multi-task self-supervised \n",
    "        learning for human activity detection. IMWUT, 3(2), 1-30.\n",
    "    \"\"\"\n",
    "    def __init__(self, eeg_dim, facial_dim, hidden_dim=128, latent_dim=64, fusion_dim=128):\n",
    "        super(MultimodalContrastiveModel, self).__init__()\n",
    "        \n",
    "        # Encoders\n",
    "        self.eeg_encoder = EEGEncoder(eeg_dim, hidden_dim, latent_dim)\n",
    "        self.facial_encoder = FacialEncoder(facial_dim, hidden_dim, latent_dim)\n",
    "        \n",
    "        # Fusion module\n",
    "        self.fusion = CrossModalAttentionFusion(latent_dim, fusion_dim)\n",
    "        \n",
    "        # Projection heads for contrastive learning\n",
    "        self.eeg_projector = nn.Sequential(\n",
    "            nn.Linear(latent_dim, latent_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(latent_dim, latent_dim)\n",
    "        )\n",
    "        \n",
    "        self.facial_projector = nn.Sequential(\n",
    "            nn.Linear(latent_dim, latent_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(latent_dim, latent_dim)\n",
    "        )\n",
    "        \n",
    "        self.fusion_projector = nn.Sequential(\n",
    "            nn.Linear(fusion_dim, latent_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(latent_dim, latent_dim)\n",
    "        )\n",
    "        \n",
    "        # Classifier for downstream task\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(fusion_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(hidden_dim, 4)  # 4 emotion classes\n",
    "        )\n",
    "        \n",
    "    def forward(self, eeg, facial, return_features=False):\n",
    "        # Encode features\n",
    "        eeg_features = self.eeg_encoder(eeg)\n",
    "        facial_features = self.facial_encoder(facial)\n",
    "        \n",
    "        # Fuse features\n",
    "        fused_features = self.fusion(eeg_features, facial_features)\n",
    "        \n",
    "        if return_features:\n",
    "            return {\n",
    "                'eeg': eeg_features,\n",
    "                'facial': facial_features,\n",
    "                'fused': fused_features\n",
    "            }\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.classifier(fused_features)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    def get_contrastive_features(self, eeg, facial):\n",
    "        # Encode features\n",
    "        eeg_features = self.eeg_encoder(eeg)\n",
    "        facial_features = self.facial_encoder(facial)\n",
    "        \n",
    "        # Fuse features\n",
    "        fused_features = self.fusion(eeg_features, facial_features)\n",
    "        \n",
    "        # Project features for contrastive learning\n",
    "        eeg_proj = self.eeg_projector(eeg_features)\n",
    "        facial_proj = self.facial_projector(facial_features)\n",
    "        fusion_proj = self.fusion_projector(fused_features)\n",
    "        \n",
    "        return eeg_proj, facial_proj, fusion_proj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a0f236",
   "metadata": {},
   "source": [
    "3.1 Multi-View Supervised Contrastive Loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "314dc59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Multi-view supervised contrastive loss with subject-invariance regularization\n",
    "    \n",
    "    References:\n",
    "    [10] Khosla, P., Teterwak, P., Wang, C., Sarna, A., Tian, Y., Isola, P., ... & \n",
    "         Krishnan, D. (2020). Supervised contrastive learning. NeurIPS 2020.\n",
    "    [11] Li, C., Xie, W., & Xiang, T. (2022). Cross-modal center loss for 3d cross-modal \n",
    "         retrieval. In Proceedings of the IEEE/CVF Conference on Computer Vision and \n",
    "         Pattern Recognition (pp. 3142-3151).\n",
    "    \"\"\"\n",
    "class MultiViewSupConLoss(nn.Module):\n",
    "    def __init__(self, temperature=0.07, subject_weight=0.5, contrast_mode='all'):\n",
    "        super(MultiViewSupConLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "        self.subject_weight = subject_weight\n",
    "        self.contrast_mode = contrast_mode\n",
    "        \n",
    "    def forward(self, features, labels, subject_ids):\n",
    "        device = features['eeg'].device\n",
    "        batch_size = features['eeg'].size(0)\n",
    "        \n",
    "        # Normalize features\n",
    "        for key in features:\n",
    "            features[key] = F.normalize(features[key], dim=1)\n",
    "        \n",
    "        # Combine all features for multi-view contrastive learning\n",
    "        all_features = torch.cat([\n",
    "            features['eeg'],\n",
    "            features['facial'],\n",
    "            features['fused']\n",
    "        ], dim=0)\n",
    "        \n",
    "        # Process labels\n",
    "        if len(labels.shape) > 1:  # One-hot encoded labels\n",
    "            labels_idx = labels.argmax(dim=1)\n",
    "        else:  # Class indices\n",
    "            labels_idx = labels\n",
    "            \n",
    "        # Repeat for each view\n",
    "        labels_idx_repeat = labels_idx.repeat(3)\n",
    "        \n",
    "        # Process subject IDs\n",
    "        subject_ids = subject_ids.squeeze()  # Handle case where subject_ids is [batch_size, 1]\n",
    "        subject_ids_repeat = subject_ids.repeat(3)\n",
    "        \n",
    "        # Compute similarity matrix\n",
    "        sim_matrix = torch.matmul(all_features, all_features.T) / self.temperature\n",
    "        \n",
    "        # Create masks\n",
    "        mask = torch.zeros_like(sim_matrix)\n",
    "        \n",
    "        # Same class mask\n",
    "        for i in range(batch_size * 3):\n",
    "            for j in range(batch_size * 3):\n",
    "                if labels_idx_repeat[i] == labels_idx_repeat[j]:\n",
    "                    mask[i, j] = 1.0\n",
    "        \n",
    "        # Remove self-similarity\n",
    "        mask.fill_diagonal_(0)\n",
    "        \n",
    "        # Subject invariance mask\n",
    "        subject_inv_mask = torch.zeros_like(sim_matrix)\n",
    "        for i in range(batch_size * 3):\n",
    "            for j in range(batch_size * 3):\n",
    "                if labels_idx_repeat[i] == labels_idx_repeat[j] and subject_ids_repeat[i] != subject_ids_repeat[j]:\n",
    "                    subject_inv_mask[i, j] = 1.0\n",
    "        \n",
    "        # Compute log_prob\n",
    "        exp_sim = torch.exp(sim_matrix)\n",
    "        log_prob = sim_matrix - torch.log(exp_sim.sum(dim=1, keepdim=True))\n",
    "        \n",
    "        # Compute mean of log-likelihood over positive pairs\n",
    "        mean_log_prob_pos = (mask * log_prob).sum(1) / mask.sum(1).clamp(min=1e-8)\n",
    "        \n",
    "        # Compute subject-invariance loss\n",
    "        subject_inv_loss = 0.0\n",
    "        if subject_inv_mask.sum() > 0:  # Only compute if we have valid pairs\n",
    "            subject_inv_loss = -torch.mean((subject_inv_mask * log_prob).sum(1) / \n",
    "                                         subject_inv_mask.sum(1).clamp(min=1e-8))\n",
    "        \n",
    "        # Combine losses\n",
    "        loss = -mean_log_prob_pos.mean() + self.subject_weight * subject_inv_loss\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1109a524",
   "metadata": {},
   "source": [
    "3.2 Adversarial Subject-Invariance Loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "47cd2cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubjectDiscriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    Adversarial discriminator for subject-invariant feature learning\n",
    "    \n",
    "    References:\n",
    "    [12] Ganin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette, F., ... & \n",
    "         Lempitsky, V. (2016). Domain-adversarial training of neural networks. \n",
    "         The journal of machine learning research, 17(1), 2096-2030.\n",
    "    [13] Li, Y., Tian, X., Liu, X., & Tao, D. (2018). On better exploring and exploiting \n",
    "         task relationships in multitask learning: Joint model and feature learning. \n",
    "         IEEE transactions on neural networks and learning systems, 29(5), 1975-1985.\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_dim, hidden_dim=64, num_subjects=22):\n",
    "        super(SubjectDiscriminator, self).__init__()\n",
    "        \n",
    "        self.discriminator = nn.Sequential(\n",
    "            nn.Linear(feature_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim, num_subjects)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, alpha=1.0):\n",
    "        # Apply gradient reversal for adversarial training\n",
    "        x = GradientReversalFunction.apply(x, alpha)\n",
    "        return self.discriminator(x)\n",
    "\n",
    "class GradientReversalFunction(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    Gradient Reversal Layer for adversarial training\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, alpha):\n",
    "        ctx.alpha = alpha\n",
    "        return x.view_as(x)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        output = grad_output.neg() * ctx.alpha\n",
    "        return output, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af85321f",
   "metadata": {},
   "source": [
    "4.1 Data Augmentation for Contrastive Learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "268826cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGAugmentation:\n",
    "    \"\"\"\n",
    "    Data augmentation techniques for EEG signals\n",
    "    \n",
    "    References:\n",
    "    [14] Cheng, J. Y., Goh, H., Dogrusoz, K., Tuzel, O., & Azemi, E. (2020). \n",
    "         Subject-aware contrastive learning for biosignals. arXiv preprint arXiv:2007.04871.\n",
    "    [15] Mohsenvand, M. N., Izadi, M. R., & Maes, P. (2020). Contrastive representation \n",
    "         learning for electroencephalogram classification. In Machine Learning for Health \n",
    "         (pp. 238-253). PMLR.\n",
    "    \"\"\"\n",
    "    def __init__(self, noise_level=0.1, mask_prob=0.1, time_shift_samples=10):\n",
    "        self.noise_level = noise_level\n",
    "        self.mask_prob = mask_prob\n",
    "        self.time_shift_samples = time_shift_samples\n",
    "    \n",
    "    def __call__(self, eeg_features):\n",
    "        \"\"\"Apply a random augmentation to EEG features\"\"\"\n",
    "        aug_type = np.random.choice(['noise', 'mask', 'time_shift', 'none'])\n",
    "        \n",
    "        if aug_type == 'noise':\n",
    "            # Add Gaussian noise\n",
    "            noise = torch.randn_like(eeg_features) * self.noise_level\n",
    "            return eeg_features + noise\n",
    "        \n",
    "        elif aug_type == 'mask':\n",
    "            # Random feature masking\n",
    "            mask = torch.rand(eeg_features.shape) > self.mask_prob\n",
    "            return eeg_features * mask\n",
    "        \n",
    "        elif aug_type == 'time_shift':\n",
    "            # Only applicable if features have temporal dimension\n",
    "            # For simplicity, we'll just shuffle some features\n",
    "            idx = torch.randperm(eeg_features.shape[0])\n",
    "            num_to_shift = int(eeg_features.shape[0] * 0.2)\n",
    "            eeg_features[0:num_to_shift] = eeg_features[idx[0:num_to_shift]]\n",
    "            return eeg_features\n",
    "        \n",
    "        else:\n",
    "            # No augmentation\n",
    "            return eeg_features\n",
    "\n",
    "class FacialAugmentation:\n",
    "    \"\"\"\n",
    "    Data augmentation techniques for facial features\n",
    "    \n",
    "    References:\n",
    "    [16] Wang, F., Cheng, J., Liu, W., & Liu, H. (2018). Additive margin softmax for \n",
    "         face verification. IEEE Signal Processing Letters, 25(7), 926-930.\n",
    "    \"\"\"\n",
    "    def __init__(self, noise_level=0.05, dropout_prob=0.1):\n",
    "        self.noise_level = noise_level\n",
    "        self.dropout_prob = dropout_prob\n",
    "    \n",
    "    def __call__(self, facial_features):\n",
    "        \"\"\"Apply a random augmentation to facial features\"\"\"\n",
    "        aug_type = np.random.choice(['noise', 'dropout', 'none'])\n",
    "        \n",
    "        if aug_type == 'noise':\n",
    "            # Add Gaussian noise\n",
    "            noise = torch.randn_like(facial_features) * self.noise_level\n",
    "            return facial_features + noise\n",
    "        \n",
    "        elif aug_type == 'dropout':\n",
    "            # Feature dropout\n",
    "            mask = torch.rand(facial_features.shape) > self.dropout_prob\n",
    "            return facial_features * mask\n",
    "        \n",
    "        else:\n",
    "            # No augmentation\n",
    "            return facial_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d230d4f",
   "metadata": {},
   "source": [
    "4.2 Training Function with Subject-Invariant Learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c4b558f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Training function for one epoch with contrastive and adversarial learning\n",
    "    \n",
    "    References:\n",
    "    [17] Ganin, Y., & Lempitsky, V. (2015). Unsupervised domain adaptation by \n",
    "         backpropagation. In International conference on machine learning (pp. 1180-1189).\n",
    "    [18] Chen, T., Kornblith, S., Norouzi, M., & Hinton, G. (2020). A simple framework for \n",
    "         contrastive learning of visual representations. In International conference on \n",
    "         machine learning (pp. 1597-1607).\n",
    "    \"\"\"\n",
    "def train_epoch(model, discriminator, train_loader, optimizer, disc_optimizer, \n",
    "                contrastive_criterion, ce_criterion, adv_weight=0.1, device='cuda'):\n",
    "    model.train()\n",
    "    discriminator.train()\n",
    "    \n",
    "    total_loss = 0\n",
    "    contrastive_losses = 0\n",
    "    ce_losses = 0\n",
    "    adv_losses = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        eeg = batch['eeg'].to(device)\n",
    "        facial = batch['facial'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        subject_ids = batch['subject_id'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        features = model(eeg, facial, return_features=True)\n",
    "        logits = model.classifier(features['fused'])\n",
    "        \n",
    "        # Get contrastive features\n",
    "        eeg_proj, facial_proj, fusion_proj = model.get_contrastive_features(eeg, facial)\n",
    "        \n",
    "        # Contrastive loss\n",
    "        contrastive_features = {\n",
    "            'eeg': eeg_proj,\n",
    "            'facial': facial_proj,\n",
    "            'fused': fusion_proj\n",
    "        }\n",
    "        \n",
    "        # Ensure we have valid batch size\n",
    "        if eeg.size(0) < 2:\n",
    "            print(f\"Skipping batch {batch_idx} due to small batch size: {eeg.size(0)}\")\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            contrastive_loss = contrastive_criterion(contrastive_features, labels, subject_ids)\n",
    "        except Exception as e:\n",
    "            print(f\"Error in contrastive loss: {e}\")\n",
    "            print(f\"Shapes: eeg={eeg.shape}, facial={facial.shape}, labels={labels.shape}, subject_ids={subject_ids.shape}\")\n",
    "            contrastive_loss = torch.tensor(0.0, device=device)\n",
    "        \n",
    "        # Classification loss\n",
    "        try:\n",
    "            ce_loss = ce_criterion(logits, labels.argmax(dim=1) if len(labels.shape) > 1 else labels)\n",
    "        except Exception as e:\n",
    "            print(f\"Error in CE loss: {e}\")\n",
    "            ce_loss = torch.tensor(0.0, device=device)\n",
    "        \n",
    "        # Adversarial subject-invariance loss\n",
    "        try:\n",
    "            adv_logits = discriminator(features['fused'])\n",
    "            adv_loss = F.cross_entropy(adv_logits, subject_ids.squeeze())\n",
    "        except Exception as e:\n",
    "            print(f\"Error in adversarial loss: {e}\")\n",
    "            adv_loss = torch.tensor(0.0, device=device)\n",
    "        \n",
    "        # Combined loss\n",
    "        loss = ce_loss + contrastive_loss - adv_weight * adv_loss\n",
    "        \n",
    "        # Optimize model\n",
    "        optimizer.zero_grad()\n",
    "        try:\n",
    "            loss.backward()  # Remove retain_graph=True to avoid memory issues\n",
    "            optimizer.step()\n",
    "        except Exception as e:\n",
    "            print(f\"Error in backward pass: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # Optimize discriminator separately\n",
    "        disc_optimizer.zero_grad()\n",
    "        try:\n",
    "            adv_logits = discriminator(features['fused'].detach())\n",
    "            disc_loss = F.cross_entropy(adv_logits, subject_ids.squeeze())\n",
    "            disc_loss.backward()\n",
    "            disc_optimizer.step()\n",
    "        except Exception as e:\n",
    "            print(f\"Error in discriminator update: {e}\")\n",
    "        \n",
    "        # Track metrics\n",
    "        total_loss += loss.item()\n",
    "        contrastive_losses += contrastive_loss.item()\n",
    "        ce_losses += ce_loss.item()\n",
    "        adv_losses += adv_loss.item()\n",
    "        \n",
    "        pred = logits.argmax(dim=1)\n",
    "        if len(labels.shape) > 1:\n",
    "            correct += (pred == labels.argmax(dim=1)).sum().item()\n",
    "        else:\n",
    "            correct += (pred == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    \n",
    "    if len(train_loader) == 0:\n",
    "        return {\n",
    "            'loss': 0,\n",
    "            'contrastive_loss': 0,\n",
    "            'ce_loss': 0,\n",
    "            'adv_loss': 0,\n",
    "            'accuracy': 0\n",
    "        }\n",
    "        \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    avg_contrastive = contrastive_losses / len(train_loader)\n",
    "    avg_ce = ce_losses / len(train_loader)\n",
    "    avg_adv = adv_losses / len(train_loader)\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'loss': avg_loss,\n",
    "        'contrastive_loss': avg_contrastive,\n",
    "        'ce_loss': avg_ce,\n",
    "        'adv_loss': avg_adv,\n",
    "        'accuracy': accuracy\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755b23dd",
   "metadata": {},
   "source": [
    "5. Main Training Loop with Subject-Independent Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "34532bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_subject_independent(model, test_loader, criterion, device='cuda'):\n",
    "    model.eval()\n",
    "    \n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(test_loader):\n",
    "            eeg = batch['eeg'].to(device)\n",
    "            facial = batch['facial'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            features = model(eeg, facial, return_features=True)\n",
    "            logits = model.classifier(features['fused'])\n",
    "            \n",
    "            # Classification loss\n",
    "            try:\n",
    "                if len(labels.shape) > 1:\n",
    "                    loss = criterion(logits, labels.argmax(dim=1))\n",
    "                    true_labels = labels.argmax(dim=1)\n",
    "                else:\n",
    "                    loss = criterion(logits, labels)\n",
    "                    true_labels = labels\n",
    "            except Exception as e:\n",
    "                print(f\"Error in evaluation loss: {e}\")\n",
    "                loss = torch.tensor(0.0, device=device)\n",
    "                true_labels = torch.zeros_like(logits.argmax(dim=1))\n",
    "            \n",
    "            # Track metrics\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            pred = logits.argmax(dim=1)\n",
    "            correct += (pred == true_labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            \n",
    "            all_preds.extend(pred.cpu().numpy())\n",
    "            all_labels.extend(true_labels.cpu().numpy())\n",
    "    \n",
    "    if len(test_loader) == 0:\n",
    "        return {\n",
    "            'loss': 0,\n",
    "            'accuracy': 0,\n",
    "            'f1': 0,\n",
    "            'precision': 0,\n",
    "            'recall': 0,\n",
    "            'confusion_matrix': np.zeros((4, 4))\n",
    "        }\n",
    "        \n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "    \n",
    "    try:\n",
    "        f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "        precision = precision_score(all_labels, all_preds, average='weighted')\n",
    "        recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "        conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating metrics: {e}\")\n",
    "        f1 = precision = recall = 0\n",
    "        conf_matrix = np.zeros((4, 4))\n",
    "    \n",
    "    return {\n",
    "        'loss': avg_loss,\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'confusion_matrix': conf_matrix\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d515239a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EEGEncoder: input_dim=512, hidden_dim=128, latent_dim=64\n",
      "FacialEncoder: input_dim=3072, hidden_dim=128, latent_dim=64\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 130\u001b[0m\n\u001b[0;32m    121\u001b[0m         json\u001b[38;5;241m.\u001b[39mdump({\n\u001b[0;32m    122\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubject_results\u001b[39m\u001b[38;5;124m'\u001b[39m: all_results,\n\u001b[0;32m    123\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(np\u001b[38;5;241m.\u001b[39mmean(accuracies)),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    126\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstd_f1\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(np\u001b[38;5;241m.\u001b[39mstd(f1_scores))\n\u001b[0;32m    127\u001b[0m         }, f, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 130\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[50], line 36\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     33\u001b[0m eeg_dim \u001b[38;5;241m=\u001b[39m eeg_features\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     34\u001b[0m facial_dim \u001b[38;5;241m=\u001b[39m enhanced_facial\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m---> 36\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mMultimodalContrastiveModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43meeg_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfacial_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m discriminator \u001b[38;5;241m=\u001b[39m SubjectDiscriminator(\u001b[38;5;241m128\u001b[39m, num_subjects\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m22\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Define loss functions\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\tahir\\Documents\\EEg-based-Emotion-Recognition\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1340\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1337\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1338\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m-> 1340\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tahir\\Documents\\EEg-based-Emotion-Recognition\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\tahir\\Documents\\EEg-based-Emotion-Recognition\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\tahir\\Documents\\EEg-based-Emotion-Recognition\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\tahir\\Documents\\EEg-based-Emotion-Recognition\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:927\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    923\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    924\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    925\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    926\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 927\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    928\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    930\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\tahir\\Documents\\EEg-based-Emotion-Recognition\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1326\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m   1320\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[0;32m   1321\u001b[0m             device,\n\u001b[0;32m   1322\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1323\u001b[0m             non_blocking,\n\u001b[0;32m   1324\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[0;32m   1325\u001b[0m         )\n\u001b[1;32m-> 1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1330\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1331\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Load data\n",
    "    eeg_data, eeg_labels, subject_ids = load_eeg_data('data_preprocessed_python')\n",
    "    facial_features, _ = load_facial_features('Pytorch_Retinaface-master/output_facial_features')\n",
    "    \n",
    "    # Extract advanced EEG features\n",
    "    eeg_features = extract_eeg_features(eeg_data)\n",
    "    \n",
    "    # Enhance facial features with temporal dynamics\n",
    "    enhanced_facial = enhance_facial_features(facial_features)\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = DEAPMultimodalDataset(eeg_features, enhanced_facial, eeg_labels, subject_ids)\n",
    "    \n",
    "    # Leave-one-subject-out cross-validation\n",
    "    logo = LeaveOneGroupOut()\n",
    "    \n",
    "    # Track results\n",
    "    all_results = []\n",
    "    \n",
    "    for train_idx, test_idx in logo.split(eeg_features, eeg_labels, subject_ids):\n",
    "        # Create data loaders\n",
    "        train_dataset = torch.utils.data.Subset(dataset, train_idx)\n",
    "        test_dataset = torch.utils.data.Subset(dataset, test_idx)\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "        \n",
    "        # Initialize model\n",
    "        eeg_dim = eeg_features.shape[1]\n",
    "        facial_dim = enhanced_facial.shape[1]\n",
    "        \n",
    "        model = MultimodalContrastiveModel(eeg_dim, facial_dim).to(device)\n",
    "        discriminator = SubjectDiscriminator(128, num_subjects=22).to(device)\n",
    "        \n",
    "        # Define loss functions\n",
    "        contrastive_criterion = MultiViewSupConLoss(temperature=0.07, subject_weight=0.5)\n",
    "        ce_criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Define optimizers\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5)\n",
    "        disc_optimizer = torch.optim.Adam(discriminator.parameters(), lr=0.0001)\n",
    "        \n",
    "        # Learning rate scheduler\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
    "        )\n",
    "        \n",
    "        # Training loop\n",
    "        best_val_acc = 0\n",
    "        best_model_state = None\n",
    "        \n",
    "        for epoch in range(150):  # 100 epochs\n",
    "            # Train\n",
    "            train_metrics = train_epoch(\n",
    "                model, discriminator, train_loader, optimizer, disc_optimizer,\n",
    "                contrastive_criterion, ce_criterion, adv_weight=0.1, device=device\n",
    "            )\n",
    "            \n",
    "            # Evaluate\n",
    "            val_metrics = evaluate_subject_independent(\n",
    "                model, test_loader, ce_criterion, device=device\n",
    "            )\n",
    "            \n",
    "            # Update learning rate\n",
    "            scheduler.step(val_metrics['loss'])\n",
    "            \n",
    "            # Save best model\n",
    "            if val_metrics['accuracy'] > best_val_acc:\n",
    "                best_val_acc = val_metrics['accuracy']\n",
    "                best_model_state = model.state_dict().copy()\n",
    "            \n",
    "            # Print metrics\n",
    "            print(f\"Epoch {epoch+1}/150:\")\n",
    "            print(f\"Train Loss: {train_metrics['loss']:.4f}, Train Acc: {train_metrics['accuracy']:.4f}\")\n",
    "            print(f\"Val Loss: {val_metrics['loss']:.4f}, Val Acc: {val_metrics['accuracy']:.4f}\")\n",
    "            print(f\"Val F1: {val_metrics['f1']:.4f}, Val Precision: {val_metrics['precision']:.4f}\")\n",
    "            print(f\"Val Recall: {val_metrics['recall']:.4f}\")\n",
    "            print(\"-\" * 50)\n",
    "        \n",
    "        # Load best model\n",
    "        model.load_state_dict(best_model_state)\n",
    "        \n",
    "        # Final evaluation\n",
    "        final_metrics = evaluate_subject_independent(\n",
    "            model, test_loader, ce_criterion, device=device\n",
    "        )\n",
    "        \n",
    "        # Save results\n",
    "                # Save results\n",
    "        test_subject = subject_ids[test_idx[0]]\n",
    "        all_results.append({\n",
    "            'test_subject': test_subject,\n",
    "            'accuracy': final_metrics['accuracy'],\n",
    "            'f1': final_metrics['f1'],\n",
    "            'precision': final_metrics['precision'],\n",
    "            'recall': final_metrics['recall'],\n",
    "            'confusion_matrix': final_metrics['confusion_matrix']\n",
    "        })\n",
    "        \n",
    "        # Save model for this fold\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'test_subject': test_subject,\n",
    "            'metrics': final_metrics\n",
    "        }, f'models/subject_{test_subject}_model.pt')\n",
    "    \n",
    "    # Analyze cross-validation results\n",
    "    accuracies = [result['accuracy'] for result in all_results]\n",
    "    f1_scores = [result['f1'] for result in all_results]\n",
    "    \n",
    "    print(\"Cross-validation Results:\")\n",
    "    print(f\"Mean Accuracy: {np.mean(accuracies):.4f} ± {np.std(accuracies):.4f}\")\n",
    "    print(f\"Mean F1 Score: {np.mean(f1_scores):.4f} ± {np.std(f1_scores):.4f}\")\n",
    "    \n",
    "    # Save overall results\n",
    "    with open('results/cross_validation_results.json', 'w') as f:\n",
    "        json.dump({\n",
    "            'subject_results': all_results,\n",
    "            'mean_accuracy': float(np.mean(accuracies)),\n",
    "            'std_accuracy': float(np.std(accuracies)),\n",
    "            'mean_f1': float(np.mean(f1_scores)),\n",
    "            'std_f1': float(np.std(f1_scores))\n",
    "        }, f, indent=4)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78dd62d",
   "metadata": {},
   "source": [
    "6. Visualization and Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f835ff74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_features(model, test_loader, device='cuda'):\n",
    "    \"\"\"\n",
    "    Visualize learned features using t-SNE\n",
    "    \n",
    "    References:\n",
    "    [20] Van der Maaten, L., & Hinton, G. (2008). Visualizing data using t-SNE. \n",
    "         Journal of machine learning research, 9(11).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    all_subjects = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            eeg = batch['eeg'].to(device)\n",
    "            facial = batch['facial'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            subjects = batch['subject_id'].to(device)\n",
    "            \n",
    "            # Get features\n",
    "            features = model(eeg, facial, return_features=True)\n",
    "            \n",
    "            # Store features and metadata\n",
    "            all_features.append(features['fused'].cpu().numpy())\n",
    "            all_labels.append(labels.argmax(dim=1).cpu().numpy())\n",
    "            all_subjects.append(subjects.cpu().numpy())\n",
    "    \n",
    "    # Concatenate all batches\n",
    "    all_features = np.concatenate(all_features, axis=0)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "    all_subjects = np.concatenate(all_subjects, axis=0)\n",
    "    \n",
    "    # Apply t-SNE\n",
    "    from sklearn.manifold import TSNE\n",
    "    \n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    features_2d = tsne.fit_transform(all_features)\n",
    "    \n",
    "    # Plot by emotion\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    emotion_names = ['Excited/Happy', 'Calm/Content', 'Sad/Bored', 'Angry/Fearful']\n",
    "    colors = ['#ff7f0e', '#2ca02c', '#1f77b4', '#d62728']\n",
    "    \n",
    "    for i, emotion in enumerate(emotion_names):\n",
    "        idx = all_labels == i\n",
    "        plt.scatter(features_2d[idx, 0], features_2d[idx, 1], c=colors[i], label=emotion, alpha=0.7)\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.title('t-SNE Visualization of Emotion Features')\n",
    "    plt.savefig('results/tsne_emotions.png')\n",
    "    \n",
    "    # Plot by subject\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    unique_subjects = np.unique(all_subjects)\n",
    "    cmap = plt.cm.get_cmap('tab20', len(unique_subjects))\n",
    "    \n",
    "    for i, subject in enumerate(unique_subjects):\n",
    "        idx = all_subjects == subject\n",
    "        plt.scatter(features_2d[idx, 0], features_2d[idx, 1], c=[cmap(i)], label=f'Subject {subject}', alpha=0.7)\n",
    "    \n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.title('t-SNE Visualization of Subject Features')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('results/tsne_subjects.png')\n",
    "\n",
    "def analyze_subject_invariance(all_results):\n",
    "    \"\"\"\n",
    "    Analyze subject invariance by comparing performance across subjects\n",
    "    \n",
    "    References:\n",
    "    [21] Zheng, W. L., & Lu, B. L. (2016). Personalizing EEG-based affective models \n",
    "         with transfer learning. In Proceedings of the Twenty-Fifth International \n",
    "         Joint Conference on Artificial Intelligence (pp. 2732-2738).\n",
    "    \"\"\"\n",
    "    # Extract accuracies by subject\n",
    "    subjects = [result['test_subject'] for result in all_results]\n",
    "    accuracies = [result['accuracy'] for result in all_results]\n",
    "    \n",
    "    # Plot subject-wise performance\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(range(len(subjects)), accuracies, color='skyblue')\n",
    "    plt.axhline(y=np.mean(accuracies), color='r', linestyle='-', label=f'Mean: {np.mean(accuracies):.4f}')\n",
    "    plt.xlabel('Subject ID')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Subject-wise Classification Accuracy')\n",
    "    plt.xticks(range(len(subjects)), subjects)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('results/subject_wise_accuracy.png')\n",
    "    \n",
    "    # Analyze confusion matrices\n",
    "    avg_conf_matrix = np.zeros_like(all_results[0]['confusion_matrix'], dtype=float)\n",
    "    \n",
    "    for result in all_results:\n",
    "        cm = result['confusion_matrix']\n",
    "        # Normalize by row (true labels)\n",
    "        row_sums = cm.sum(axis=1, keepdims=True)\n",
    "        norm_cm = cm / row_sums\n",
    "        avg_conf_matrix += norm_cm\n",
    "    \n",
    "    avg_conf_matrix /= len(all_results)\n",
    "    \n",
    "    # Plot average confusion matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    emotion_names = ['Excited/Happy', 'Calm/Content', 'Sad/Bored', 'Angry/Fearful']\n",
    "    \n",
    "    plt.imshow(avg_conf_matrix, cmap='Blues')\n",
    "    plt.colorbar()\n",
    "    plt.xticks(range(4), emotion_names, rotation=45)\n",
    "    plt.yticks(range(4), emotion_names)\n",
    "    plt.xlabel('Predicted Emotion')\n",
    "    plt.ylabel('True Emotion')\n",
    "    plt.title('Average Confusion Matrix Across Subjects')\n",
    "    \n",
    "    # Add text annotations\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            plt.text(j, i, f'{avg_conf_matrix[i, j]:.2f}', \n",
    "                     ha='center', va='center', \n",
    "                     color='white' if avg_conf_matrix[i, j] > 0.5 else 'black')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('results/average_confusion_matrix.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc8ad36",
   "metadata": {},
   "source": [
    "7. Ablation Studies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a565ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ablation_studies():\n",
    "    \"\"\"\n",
    "    Perform ablation studies to analyze the contribution of different components\n",
    "    \n",
    "    References:\n",
    "    [22] Mehmood, R. M., Du, R., & Lee, H. J. (2017). Optimal feature selection and \n",
    "         deep learning ensembles method for emotion recognition from human brain EEG sensors. \n",
    "         IEEE Access, 5, 14797-14806.\n",
    "    [23] Zheng, W. L., Liu, W., Lu, Y., Lu, B. L., & Cichocki, A. (2018). EmotionMeter: \n",
    "         A multimodal framework for recognizing human emotions. IEEE transactions on \n",
    "         cybernetics, 49(3), 1110-1122.\n",
    "    \"\"\"\n",
    "    # Define configurations for ablation studies\n",
    "    ablation_configs = [\n",
    "        {\n",
    "            'name': 'Full Model',\n",
    "            'use_eeg': True,\n",
    "            'use_facial': True,\n",
    "            'use_contrastive': True,\n",
    "            'use_adversarial': True\n",
    "        },\n",
    "        {\n",
    "            'name': 'EEG Only',\n",
    "            'use_eeg': True,\n",
    "            'use_facial': False,\n",
    "            'use_contrastive': True,\n",
    "            'use_adversarial': True\n",
    "        },\n",
    "        {\n",
    "            'name': 'Facial Only',\n",
    "            'use_eeg': False,\n",
    "            'use_facial': True,\n",
    "            'use_contrastive': True,\n",
    "            'use_adversarial': True\n",
    "        },\n",
    "        {\n",
    "            'name': 'No Contrastive',\n",
    "            'use_eeg': True,\n",
    "            'use_facial': True,\n",
    "            'use_contrastive': False,\n",
    "            'use_adversarial': True\n",
    "        },\n",
    "        {\n",
    "            'name': 'No Adversarial',\n",
    "            'use_eeg': True,\n",
    "            'use_facial': True,\n",
    "            'use_contrastive': True,\n",
    "            'use_adversarial': False\n",
    "        },\n",
    "        {\n",
    "            'name': 'Basic Fusion',\n",
    "            'use_eeg': True,\n",
    "            'use_facial': True,\n",
    "            'use_contrastive': False,\n",
    "            'use_adversarial': False\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Run each configuration\n",
    "    ablation_results = []\n",
    "    \n",
    "    for config in ablation_configs:\n",
    "        print(f\"Running ablation study: {config['name']}\")\n",
    "        \n",
    "        # Modify model and training based on configuration\n",
    "        # (Implementation details omitted for brevity)\n",
    "        \n",
    "        # Run cross-validation\n",
    "        # (Implementation details omitted for brevity)\n",
    "        \n",
    "        # Store results\n",
    "        ablation_results.append({\n",
    "            'config': config,\n",
    "            'accuracy': 0.0,  # Placeholder\n",
    "            'f1': 0.0,        # Placeholder\n",
    "        })\n",
    "    \n",
    "    # Plot comparison\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    names = [config['name'] for config in ablation_configs]\n",
    "    accuracies = [result['accuracy'] for result in ablation_results]\n",
    "    \n",
    "    plt.bar(range(len(names)), accuracies, color='skyblue')\n",
    "    plt.xlabel('Model Configuration')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Ablation Study Results')\n",
    "    plt.xticks(range(len(names)), names, rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('results/ablation_study.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692903c7",
   "metadata": {},
   "source": [
    "8. Hyperparameter Optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5288f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_hyperparameters():\n",
    "    \"\"\"\n",
    "    Optimize hyperparameters using Bayesian optimization\n",
    "    \n",
    "    References:\n",
    "    [24] Snoek, J., Larochelle, H., & Adams, R. P. (2012). Practical bayesian optimization \n",
    "         of machine learning algorithms. Advances in neural information processing systems, 25.\n",
    "    \"\"\"\n",
    "    from skopt import gp_minimize\n",
    "    from skopt.space import Real, Integer, Categorical\n",
    "    \n",
    "    # Define hyperparameter space\n",
    "    space = [\n",
    "        Real(1e-5, 1e-2, name='learning_rate', prior='log-uniform'),\n",
    "        Real(0.0, 0.7, name='dropout_rate'),\n",
    "        Integer(32, 256, name='hidden_dim'),\n",
    "        Integer(16, 128, name='latent_dim'),\n",
    "        Real(0.01, 0.5, name='contrastive_weight'),\n",
    "        Real(0.01, 0.5, name='adversarial_weight'),\n",
    "        Real(0.01, 0.2, name='temperature'),\n",
    "        Categorical(['early', 'mid', 'late'], name='fusion_type')\n",
    "    ]\n",
    "    \n",
    "    # Define objective function\n",
    "    def objective(params):\n",
    "        learning_rate, dropout_rate, hidden_dim, latent_dim, \\\n",
    "        contrastive_weight, adversarial_weight, temperature, fusion_type = params\n",
    "        \n",
    "        # Create and train model with these hyperparameters\n",
    "        # (Implementation details omitted for brevity)\n",
    "        \n",
    "        # Return negative accuracy (to maximize)\n",
    "        return -accuracy\n",
    "    \n",
    "    # Run Bayesian optimization\n",
    "    result = gp_minimize(objective, space, n_calls=50, random_state=42)\n",
    "    \n",
    "    # Print best hyperparameters\n",
    "    print(\"Best hyperparameters:\")\n",
    "    print(f\"Learning Rate: {result.x[0]}\")\n",
    "    print(f\"Dropout Rate: {result.x[1]}\")\n",
    "    print(f\"Hidden Dim: {result.x[2]}\")\n",
    "    print(f\"Latent Dim: {result.x[3]}\")\n",
    "    print(f\"Contrastive Weight: {result.x[4]}\")\n",
    "    print(f\"Adversarial Weight: {result.x[5]}\")\n",
    "    print(f\"Temperature: {result.x[6]}\")\n",
    "    print(f\"Fusion Type: {result.x[7]}\")\n",
    "    print(f\"Best Accuracy: {-result.fun}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
