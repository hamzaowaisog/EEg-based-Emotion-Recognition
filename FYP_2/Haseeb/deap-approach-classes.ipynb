{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10845560,"sourceType":"datasetVersion","datasetId":6735639}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport numpy as np\nimport os\nimport scipy.io\nfrom scipy.signal import butter, lfilter\nimport scipy.signal","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-09T18:07:15.828273Z","iopub.execute_input":"2025-03-09T18:07:15.828610Z","iopub.status.idle":"2025-03-09T18:07:20.032608Z","shell.execute_reply.started":"2025-03-09T18:07:15.828582Z","shell.execute_reply":"2025-03-09T18:07:20.031665Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Load DEAP dataset (Change path accordingly)\ndata_path = \"/kaggle/input/deap-matlab/\"\nsubject_data = []\nsubject_labels = []\n\n# Load all 32 subjects\nfor i in range(1, 33):\n    mat = scipy.io.loadmat(f\"{data_path}s{i:02d}.mat\")\n    \n    # Extract EEG data and labels\n    subject_data.append(mat[\"data\"])     # Shape: (40 trials, 40 channels, 8064 samples)\n    subject_labels.append(mat[\"labels\"]) # Shape: (40 trials, 4 labels)\n\n# Convert lists to NumPy arrays\nsubject_data = np.array(subject_data)     # Expected Shape: (32, 40, 40, 8064)\nsubject_labels = np.array(subject_labels) # Expected Shape: (32, 40, 4)\n\n# Print shapes to confirm\nprint(\"EEG Data Shape:\", subject_data.shape)   # (32, 40, 40, 8064)\nprint(\"Labels Shape:\", subject_labels.shape)   # (32, 40, 4)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T18:07:20.033782Z","iopub.execute_input":"2025-03-09T18:07:20.034229Z","iopub.status.idle":"2025-03-09T18:07:59.470680Z","shell.execute_reply.started":"2025-03-09T18:07:20.034198Z","shell.execute_reply":"2025-03-09T18:07:59.469917Z"}},"outputs":[{"name":"stdout","text":"EEG Data Shape: (32, 40, 40, 8064)\nLabels Shape: (32, 40, 4)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import numpy as np\nimport scipy.signal\n# Frequency band definitions\nfreq_bands = {\n    \"delta\": (1, 4),\n    \"theta\": (4, 8),\n    \"alpha\": (8, 14),\n    \"beta\": (14, 30),\n    \"gamma\": (31, 50),\n}\n\n# Define function to compute Differential Entropy (DE)\ndef compute_de(signal):\n    \"\"\"Compute Differential Entropy (DE) for a given EEG segment\"\"\"\n    variance = np.var(signal, axis=-1, keepdims=True)  # Compute variance\n    de = 0.5 * np.log(2 * np.pi * np.e * variance)  # Apply DE formula\n    return de.squeeze()  # Remove extra dimensions\n\n# Define function to extract DE features\ndef extract_de_features(subject_data, fs=128, window_size=128):\n    \"\"\"\n    Extract DE features from EEG data.\n    - subject_data: EEG data of shape (32, 40, 40, 8064)\n    - fs: Sampling frequency (128 Hz)\n    - window_size: 1 second (128 samples)\n    Returns: DE feature array of shape (32, 40, 40, 5, 63)\n    \"\"\"\n    num_subjects, num_trials, num_channels, num_samples = subject_data.shape\n    num_bands = len(freq_bands)\n    num_windows = num_samples // window_size  # 8064 / 128 = 63 windows\n\n    # Initialize DE feature array\n    de_features = np.zeros((num_subjects, num_trials, num_channels, num_bands, num_windows))\n\n    # Loop through subjects, trials, and channels\n    for subj in range(num_subjects):\n        for trial in range(num_trials):\n            for ch in range(num_channels):\n                # Extract single-channel EEG data for this trial\n                signal = subject_data[subj, trial, ch, :]\n\n                # Apply bandpass filters and compute DE for each frequency band\n                for b_idx, (band, (low, high)) in enumerate(freq_bands.items()):\n                    # Bandpass filter\n                    sos = scipy.signal.butter(4, [low, high], btype=\"bandpass\", fs=fs, output=\"sos\")\n                    filtered_signal = scipy.signal.sosfilt(sos, signal)\n\n                    # Segment into 1-second windows (128 samples each)\n                    segmented = np.array(np.split(filtered_signal, num_windows, axis=-1))\n\n                    # Compute DE for each window\n                    de_features[subj, trial, ch, b_idx, :] = compute_de(segmented)\n\n    return de_features\n\n# Extract DE features\nde_features = extract_de_features(subject_data)\n\n# Print shape to confirm\nprint(\"DE Feature Shape:\", de_features.shape)  # Expected: (32, 40, 40, 5, 63)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T18:07:59.472170Z","iopub.execute_input":"2025-03-09T18:07:59.472418Z","iopub.status.idle":"2025-03-09T18:13:03.280860Z","shell.execute_reply.started":"2025-03-09T18:07:59.472398Z","shell.execute_reply":"2025-03-09T18:13:03.280053Z"}},"outputs":[{"name":"stdout","text":"DE Feature Shape: (32, 40, 40, 5, 63)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"1. Common Feature Extractor (CFE)\nclass CommonFeatureExtractor(nn.Module):\n    def __init__(self, input_dim=200, output_dim=64):\n        super(CommonFeatureExtractor, self).__init__()\n        self.fc1 = nn.Linear(input_dim, 256)\n        self.bn1 = nn.BatchNorm1d(256)\n        self.fc2 = nn.Linear(256, 128)\n        self.bn2 = nn.BatchNorm1d(128)\n        self.fc3 = nn.Linear(128, output_dim)\n        self.activation = nn.LeakyReLU()\n        self.dropout = nn.Dropout(0.3)\n\n    def forward(self, x):\n        x = self.activation(self.bn1(self.fc1(x)))\n        x = self.dropout(x)\n        x = self.activation(self.bn2(self.fc2(x)))\n        x = self.fc3(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T18:13:03.282596Z","iopub.execute_input":"2025-03-09T18:13:03.282884Z","iopub.status.idle":"2025-03-09T18:13:03.289075Z","shell.execute_reply.started":"2025-03-09T18:13:03.282861Z","shell.execute_reply":"2025-03-09T18:13:03.288257Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# 2. Contrastive Loss L_con1\nclass ContrastiveLossLcon1(nn.Module):\n    def __init__(self, tau=0.2):\n        super(ContrastiveLossLcon1, self).__init__()\n        self.tau = tau\n\n    def forward(self, q):\n        batch_size = q.shape[0]\n        q = F.normalize(q, dim=-1)\n        sim_matrix = torch.mm(q, q.T)  \n        mask = torch.eye(batch_size, dtype=torch.bool, device=q.device)\n        sim_matrix.masked_fill_(mask, -5.0)  \n        if labels is None:\n            exp_sim = torch.exp(sim_matrix / self.tau)\n            loss = -torch.log(exp_sim / (exp_sim.sum(dim=-1, keepdim=True) + 1e-9)).mean()\n        # Supervised contrastive loss (labels provided)\n        else:\n            exp_sim = torch.exp(sim_matrix / self.tau)\n            mask_same_class = labels.unsqueeze(1) == labels.unsqueeze(0)\n            exp_sim_same = exp_sim * mask_same_class.float()  \n            loss = -torch.log(exp_sim_same.sum(dim=-1) / (exp_sim.sum(dim=-1) + 1e-9)).mean()\n\n        return loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T18:13:03.289980Z","iopub.execute_input":"2025-03-09T18:13:03.290205Z","iopub.status.idle":"2025-03-09T18:13:03.311295Z","shell.execute_reply.started":"2025-03-09T18:13:03.290187Z","shell.execute_reply":"2025-03-09T18:13:03.310611Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# 3. Subject-Specific Feature Extractor (SFE)\n\nclass SubjectSpecificFeatureExtractor(nn.Module):\n    def __init__(self, input_dim=64, output_dim=32):\n        super(SubjectSpecificFeatureExtractor, self).__init__()\n        self.fc = nn.Linear(input_dim, output_dim)\n        self.activation = nn.LeakyReLU()\n\n    def forward(self, x):\n        return self.activation(self.fc(x))  # Output shape: (batch_size, 32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T18:13:03.312157Z","iopub.execute_input":"2025-03-09T18:13:03.312375Z","iopub.status.idle":"2025-03-09T18:13:03.332832Z","shell.execute_reply.started":"2025-03-09T18:13:03.312356Z","shell.execute_reply":"2025-03-09T18:13:03.332153Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"#4. Maximum Mean Discrepancy (MMD) Loss\ndef mmd_loss(source_features, target_features):\n    source_mean = source_features.mean(dim=0)\n    target_mean = target_features.mean(dim=0)\n    loss = torch.norm(source_mean - target_mean, p=2) ** 2  \n    return loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T18:13:03.333467Z","iopub.execute_input":"2025-03-09T18:13:03.333717Z","iopub.status.idle":"2025-03-09T18:13:03.350170Z","shell.execute_reply.started":"2025-03-09T18:13:03.333674Z","shell.execute_reply":"2025-03-09T18:13:03.349362Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# 5. Contrastive Loss L_con2 with Class Prototypes\nclass ContrastiveLossLcon2(nn.Module):\n    def __init__(self, feature_dim=32, num_classes=4, tau=0.3, gamma=0.5, queue_size=1024):\n        super(ContrastiveLossLcon2, self).__init__()\n        self.tau = tau\n        self.gamma = gamma\n        self.num_classes = num_classes\n        self.queue_size = queue_size\n\n        # Initialize class prototypes (Î¼_c)\n        self.prototypes = nn.Parameter(torch.randn(num_classes, feature_dim))  # Shape: (4, 32)\n\n        # Initialize memory queue for negative samples\n        self.queue = torch.randn(queue_size, feature_dim)  # Shape: (1024, 32)\n        self.queue = F.normalize(self.queue, dim=-1)  # Normalize queue embeddings\n\n    def forward(self, z_t, pseudo_labels):\n        \"\"\"\n        Compute contrastive loss L_con2 for inter-domain alignment.\n        - z_t: Target domain features (batch_size, trials, time_windows, 32)\n        - pseudo_labels: Pseudo-labels for target samples (batch_size, trials, time_windows)\n\n        Returns:\n            Contrastive loss scalar\n        \"\"\"\n        batch_size, trials, time_windows, feature_dim = z_t.shape\n\n        # Flatten input for processing\n        z_t = z_t.view(-1, feature_dim)  # Shape: (batch_size * trials * time_windows, 32)\n        pseudo_labels = pseudo_labels.view(-1)  # Shape: (batch_size * trials * time_windows)\n\n        # Normalize embeddings\n        z_t = F.normalize(z_t, dim=-1)  # Normalize target embeddings\n        self.prototypes.data = F.normalize(self.prototypes.data, dim=-1)  # Normalize prototypes\n\n        # Compute similarity to class prototypes\n        similarity = torch.mm(z_t, self.prototypes.T)  # Shape: (batch_size * trials * time_windows, 4)\n        \n        # Select correct class prototype based on pseudo-labels\n        proto_sim = similarity.gather(1, pseudo_labels.unsqueeze(1))  # Shape: (batch_size * trials * time_windows, 1)\n\n        # Compute softmax denominator (all possible embeddings)\n        queue_sim = torch.mm(z_t, self.queue.T)  # Shape: (batch_size * trials * time_windows, queue_size)\n        exp_sim = torch.cat([proto_sim, queue_sim], dim=1)  # Concatenate prototypes & queue\n        exp_sim = torch.exp(exp_sim / self.tau)  # Apply temperature scaling\n\n        # Contrastive loss computation\n        loss = -torch.log(exp_sim[:, 0] / exp_sim.sum(dim=1))  # Only consider prototype similarity\n        loss = loss.mean()\n\n        # Update prototypes with momentum (Î³)\n        for i in range(self.num_classes):\n            class_mask = (pseudo_labels == i).float().unsqueeze(1)  # Mask for samples of class i\n            class_mean = (class_mask * z_t).sum(dim=0) / (class_mask.sum() + 1e-9)  # Compute class mean\n            self.prototypes.data[i] = F.normalize(self.gamma * self.prototypes.data[i] + (1 - self.gamma) * class_mean, dim=-1)\n\n        # Update memory queue (FIFO replacement)\n        self.queue = torch.cat([self.queue[batch_size:], z_t.detach()], dim=0)  # Remove old, add new\n\n        return loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T18:13:03.352197Z","iopub.execute_input":"2025-03-09T18:13:03.352444Z","iopub.status.idle":"2025-03-09T18:13:03.366637Z","shell.execute_reply.started":"2025-03-09T18:13:03.352415Z","shell.execute_reply":"2025-03-09T18:13:03.365816Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# 6. Subject-Specific Classifier (SSC)\nclass SubjectSpecificClassifier(nn.Module):\n    def __init__(self, input_dim=32, num_classes=4):\n        super(SubjectSpecificClassifier, self).__init__()\n        self.fc = nn.Linear(input_dim, num_classes)\n\n    def forward(self, x):\n        return self.fc(x)  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T18:13:03.367445Z","iopub.execute_input":"2025-03-09T18:13:03.367637Z","iopub.status.idle":"2025-03-09T18:13:03.384639Z","shell.execute_reply.started":"2025-03-09T18:13:03.367620Z","shell.execute_reply":"2025-03-09T18:13:03.383911Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# 7. Generalized Cross-Entropy (GCE) Loss\nclass GCELoss(nn.Module):\n    def __init__(self, q=0.55):\n        super(GCELoss, self).__init__()\n        self.q = q\n\n    def forward(self, logits, targets):\n        probs = F.softmax(logits, dim=-1)\n        true_probs = probs.gather(1, targets.unsqueeze(1)).squeeze()\n        loss = (1 - true_probs ** self.q) / self.q\n        return loss.mean()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T18:13:03.385179Z","iopub.execute_input":"2025-03-09T18:13:03.385402Z","iopub.status.idle":"2025-03-09T18:13:03.399962Z","shell.execute_reply.started":"2025-03-09T18:13:03.385384Z","shell.execute_reply":"2025-03-09T18:13:03.399219Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"de_features = torch.tensor(de_features, dtype=torch.float32)  # (32, 40, 40, 5, 63)\nnum_subjects, num_trials, num_channels, num_bands, num_windows = de_features.shape\nde_features = de_features.view(num_subjects, num_trials, num_windows, num_channels * num_bands)  # (32, 40, 63, 200)\nprint(\"Final Shape:\", de_features.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ncfe = CommonFeatureExtractor().to(device)\nsfe = SubjectSpecificFeatureExtractor().to(device)\nssc = SubjectSpecificClassifier().to(device)\ncontrastive_loss_lcon1 = ContrastiveLossLcon1().to(device)\ncontrastive_loss_lcon2 = ContrastiveLossLcon2().to(device)\ngce_loss_fn = GCELoss().to(device)\n\n# Optimizer\noptimizer = optim.Adam(list(cfe.parameters()) + list(sfe.parameters()) + list(ssc.parameters()), lr=0.01)\n\n# Training settings\nepochs = 50\nbatch_size = 32  # Matches subjects in dataset\n\n\nfor epoch in range(epochs):\n    optimizer.zero_grad()\n\n    # Reshape DE features for CFE input\n    num_subjects, num_trials, num_windows, feature_dim = de_features.shape  # (32, 40, 63, 200)\n\n    # Reshape into (batch_size * trials * time_windows, 200)\n    reshaped_features = de_features.reshape(-1, feature_dim)\n    reshaped_features = reshaped_features.to(device)\n\n    print(\"Final Shape:\", de_features.shape)  # Expected: (32 * 40 * 63, 200)\n\n    # Forward pass through CFE\n    shared_features = cfe(reshaped_features)  # (batch_size, 64)\n    common_loss = contrastive_loss_lcon1(shared_features)\n\n    # Forward pass through SFE\n    subject_features = sfe(shared_features)  # (batch_size, 32)\n\n    # Compute MMD loss (iterations < 420)\n    if epoch < 420:\n        mmd_value = mmd_loss(shared_features, subject_features)\n    else:\n        mmd_value = torch.tensor(0.0)\n\n    # Forward pass through SSC\n    predictions = ssc(subject_features)  # (batch_size, 4)\n\n    # Compute GCE Loss\n    labels = torch.randint(0, 4, (batch_size,))\n    gce_loss = gce_loss_fn(predictions, labels)\n\n    # Compute L_con2\n    pseudo_labels = torch.randint(0, 4, (batch_size,))\n    lcon2_value = contrastive_loss_lcon2(subject_features, pseudo_labels)\n\n    # Final Loss\n    total_loss = gce_loss + mmd_value + common_loss + lcon2_value\n\n    # Backpropagation\n    total_loss.backward()\n    optimizer.step()\n\n    print(f\"Epoch {epoch+1}/{epochs} - Total Loss: {total_loss.item():.4f}\")\n\nprint(\"â MSCL Training Complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T18:21:21.503964Z","iopub.execute_input":"2025-03-09T18:21:21.504270Z","iopub.status.idle":"2025-03-09T18:21:21.556129Z","shell.execute_reply.started":"2025-03-09T18:21:21.504244Z","shell.execute_reply":"2025-03-09T18:21:21.555116Z"}},"outputs":[{"name":"stdout","text":"Final Shape: torch.Size([32, 40, 63, 200])\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m<ipython-input-17-ea563279aa64>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m# Forward pass through CFE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mshared_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcfe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreshaped_features\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (batch_size, 64)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mcommon_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontrastive_loss_lcon1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshared_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;31m# Forward pass through SFE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-5-6a5425e90dfb>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, q)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0msim_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0msim_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked_fill_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m5.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 24.23 GiB. GPU 0 has a total capacity of 14.74 GiB of which 13.89 GiB is free. Process 5515 has 872.00 MiB memory in use. Of the allocated memory 729.07 MiB is allocated by PyTorch, and 16.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 24.23 GiB. GPU 0 has a total capacity of 14.74 GiB of which 13.89 GiB is free. Process 5515 has 872.00 MiB memory in use. Of the allocated memory 729.07 MiB is allocated by PyTorch, and 16.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","output_type":"error"}],"execution_count":17}]}