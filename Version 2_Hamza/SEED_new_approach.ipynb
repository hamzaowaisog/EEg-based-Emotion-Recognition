{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-12-06 21:50:44] INFO (torcheeg/MainThread) üîç | Detected cached processing results, reading cache from E:/FYP/Egg-Based Emotion Recognition/EEg-based-Emotion-Recognition/.torcheeg/datasets_1733174610032_5iJyS.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw EEG data shape: (62, 200)\n"
     ]
    }
   ],
   "source": [
    "from torcheeg.datasets import SEEDDataset\n",
    "from torcheeg import transforms\n",
    "\n",
    "raw_dataset = SEEDDataset(\n",
    "    root_path='./SEED/SEED_EEG/Preprocessed_EEG',\n",
    "    io_path = 'E:/FYP/Egg-Based Emotion Recognition/EEg-based-Emotion-Recognition/.torcheeg/datasets_1733174610032_5iJyS',\n",
    "    online_transform=None,  # Disable transforms\n",
    "    label_transform=None,\n",
    "    num_worker=4\n",
    ")\n",
    "\n",
    "raw_sample = raw_dataset[0]\n",
    "print(f\"Raw EEG data shape: {raw_sample[0].shape}\")  # Should be [62, ...] for SEED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[-19.28210258,  -9.29832458, -25.77900887, ..., -15.55681229,\n",
      "        -17.49396324, -19.90795135],\n",
      "       [ 11.92092896,  20.53380013,  11.41428947, ...,   3.21865082,\n",
      "         -1.16229057,  13.76867294],\n",
      "       [  0.56624413,  18.80526543, -11.08646393, ..., -26.4942646 ,\n",
      "        -23.66304398,   8.31484795],\n",
      "       ...,\n",
      "       [  3.4570694 ,  10.72883606,  -6.7949295 , ...,  12.54677773,\n",
      "         10.46061516,  14.42432404],\n",
      "       [  5.24520874,  11.77191734,  -3.66568565, ...,  15.07997513,\n",
      "         11.41428947,  20.02716064],\n",
      "       [  9.08970833,   6.34789467,   1.16229057, ...,  10.37120819,\n",
      "         12.54677773,  17.94099808]]), {'start_at': 0, 'end_at': 200, 'clip_id': '10_20131130.mat_0', 'subject_id': 10, 'trial_id': 'ww_eeg1', 'emotion': 1, 'date': 20131130, '_record_id': '_record_0'})\n"
     ]
    }
   ],
   "source": [
    "print(raw_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from torcheeg.utils import plot_2d_tensor\n",
    "\n",
    "img = plot_2d_tensor(torch.tensor(raw_dataset[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed EEG data shape: (62, 200)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.signal import butter, lfilter\n",
    "\n",
    "# Define a bandpass filter (4-47 Hz for SEED dataset)\n",
    "def bandpass_filter(data, lowcut=4, highcut=47, fs=200, order=4):\n",
    "    nyquist = 0.5 * fs\n",
    "    low = lowcut / nyquist\n",
    "    high = highcut / nyquist\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return lfilter(b, a, data, axis=-1)\n",
    "\n",
    "# Apply bandpass filtering to the raw EEG data\n",
    "preprocessed_data = bandpass_filter(raw_sample[0])\n",
    "\n",
    "print(f\"Preprocessed EEG data shape: {preprocessed_data.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = plot_2d_tensor(torch.tensor(preprocessed_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualize the preprocessed EEG data\n",
    "plt.imshow(preprocessed_data, aspect='auto', cmap='viridis')\n",
    "plt.colorbar()\n",
    "plt.title('Preprocessed EEG Data')\n",
    "plt.xlabel('Time Points')\n",
    "plt.ylabel('Channels')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized EEG data shape: (62, 200)\n"
     ]
    }
   ],
   "source": [
    "# Perform stratified normalization (z-score)\n",
    "def stratified_normalization(data):\n",
    "    normalized_data = (data - np.mean(data, axis=-1, keepdims=True)) / np.std(data, axis=-1, keepdims=True)\n",
    "    return normalized_data\n",
    "\n",
    "# Normalize the preprocessed EEG data\n",
    "normalized_data = stratified_normalization(preprocessed_data)\n",
    "\n",
    "print(f\"Normalized EEG data shape: {normalized_data.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of segments: 1832760\n",
      "Segment shape: (62, 30) , Subject ID: 10\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def data_sampler(dataset , time_length=30 , step_size=15):\n",
    "    samples = []\n",
    "    for sample in dataset:\n",
    "        data, metadata = sample\n",
    "        subject_id = metadata['subject_id']\n",
    "        trial_length = data.shape[1]\n",
    "        \n",
    "        for start in range(0, trial_length - time_length + 1, step_size):\n",
    "            segment = data[: , start:start+time_length]\n",
    "            samples.append((segment , subject_id))\n",
    "    return samples\n",
    "\n",
    "# Define the time length and step size for segmenting the EEG data\n",
    "time_length = 30  # 30 time points (150 ms) per segment\n",
    "step_size = 15  # 15 time points (75 ms) step size between segments\n",
    "\n",
    "segments = data_sampler(raw_dataset, time_length=time_length, step_size=step_size)\n",
    "print(f\"Number of segments: {len(segments)}\")\n",
    "print(f\"Segment shape: {segments[0][0].shape} , Subject ID: {segments[0][1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample segment shape: torch.Size([1, 62, 30])\n",
      "Encoded output shape: torch.Size([1, 16, 31])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class BaseEncoder(nn.Module):\n",
    "    def __init__(self, num_channels=62, spatial_filters=16 , temporal_filter_size=48 , temporal_filters=16):\n",
    "        super(BaseEncoder, self).__init__()\n",
    "        \n",
    "        # Spatial convolutional layer\n",
    "        self.spatial_conv = nn.Conv1d(in_channels=num_channels,out_channels=spatial_filters,kernel_size=1)\n",
    "        self.temporal_conv = nn.Conv1d(in_channels=spatial_filters,out_channels=temporal_filters,kernel_size=temporal_filter_size,padding=temporal_filter_size//2)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Apply spatial convolution\n",
    "        x = self.spatial_conv(x)\n",
    "        x = self.relu(x)\n",
    "        # Apply temporal convolution\n",
    "        x = self.temporal_conv(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "    \n",
    "#Initialize the encoder\n",
    "base_encoder = BaseEncoder(num_channels=62, spatial_filters=16, temporal_filter_size=48, temporal_filters=16)\n",
    "\n",
    "#Test on the sample segment\n",
    "sample_segment = torch.tensor(segments[0][0], dtype=torch.float32).unsqueeze(0)\n",
    "encoded_output = base_encoder(sample_segment)\n",
    "\n",
    "print(f\"Sample segment shape: {sample_segment.shape}\")\n",
    "print(f\"Encoded output shape: {encoded_output.shape}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded output shape: torch.Size([1, 16, 31])\n",
      "Projected output shape: torch.Size([1, 64, 2])\n"
     ]
    }
   ],
   "source": [
    "# Define the Projector\n",
    "class Projector(nn.Module):\n",
    "    def __init__(self, spatial_filters=16, pooling_kernel=24, temporal_filter_size=4 , c=2):\n",
    "        super(Projector, self).__init__()\n",
    "        #Average Pooling\n",
    "        \n",
    "        self.avg_pool = nn.AvgPool1d(kernel_size=pooling_kernel , stride=pooling_kernel)\n",
    "        #spatial convolution        \n",
    "        self.spatial_conv = nn.Conv1d(in_channels=spatial_filters, out_channels=spatial_filters*c,kernel_size=1)\n",
    "        #temporal convolution\n",
    "        self.temporal_conv = nn.Conv1d(in_channels=spatial_filters*c,out_channels=(spatial_filters*c)*c, kernel_size=temporal_filter_size, padding=temporal_filter_size//2)\n",
    "        #Activation\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #Applying avg pooling\n",
    "        x = self.avg_pool(x)\n",
    "        #Applying spatial convolution\n",
    "        x = self.spatial_conv(x)\n",
    "        x = self.relu(x)\n",
    "        #Applying temporal convolution\n",
    "        x = self.temporal_conv(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "\n",
    "# Initialize the projector\n",
    "projector = Projector(spatial_filters=16, pooling_kernel=24, temporal_filter_size=4,c=2)\n",
    "\n",
    "# Test on the encoded output\n",
    "projected_output = projector(encoded_output)\n",
    "\n",
    "print(f\"Encoded output shape: {encoded_output.shape}\")\n",
    "print(f\"Projected output shape: {projected_output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contrastive loss: 0.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, temperature=0.5):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "        \n",
    "    def forward(self, z_i, z_j):\n",
    "        # Normalize embeddings\n",
    "        z_i = F.normalize(z_i, dim=1)\n",
    "        z_j = F.normalize(z_j, dim=1)\n",
    "        \n",
    "        # Compute similarity matrix\n",
    "        similarities = torch.matmul(z_i, z_j.T) / self.temperature\n",
    "        \n",
    "        # Labels for contrastive loss\n",
    "        batch_size = z_i.size(0)\n",
    "        labels = torch.arange(batch_size).long().to(z_i.device)\n",
    "        \n",
    "        # Cross-entropy loss\n",
    "        loss = F.cross_entropy(similarities, labels)\n",
    "        return loss\n",
    "\n",
    "# Initialize the contrastive loss\n",
    "contrastive_loss = ContrastiveLoss(temperature=0.5)\n",
    "\n",
    "#Generate mock positive pairs( outputs from the projector for paired samples)\n",
    "z_i = projected_output.view(projected_output.size(0), -1)\n",
    "z_j = projected_output.view(projected_output.size(0), -1)\n",
    "\n",
    "#Compute the contrastive loss\n",
    "loss = contrastive_loss(z_i, z_j)\n",
    "print(f\"Contrastive loss: {loss.item()}\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contrastive loss with mock positive pairs: 0.9725548028945923\n"
     ]
    }
   ],
   "source": [
    "# Generate mock embeddings for testing\n",
    "batch_size = 4  # Example batch size\n",
    "feature_dim = 64  # Match the projector's output size\n",
    "\n",
    "# Create positive pairs (slightly perturbed to simulate similarity)\n",
    "z_i = torch.rand(batch_size, feature_dim)\n",
    "z_j = z_i + torch.normal(mean=0, std=0.01, size=z_i.size())  # Add small noise to simulate similarity\n",
    "\n",
    "# Normalize embeddings\n",
    "z_i = F.normalize(z_i, dim=1)\n",
    "z_j = F.normalize(z_j, dim=1)\n",
    "\n",
    "# Compute contrastive loss\n",
    "loss = contrastive_loss(z_i, z_j)\n",
    "print(f\"Contrastive loss with mock positive pairs: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DE features shape: (16,)\n",
      "DE features: [       -inf  2.1926346   2.00243     3.037949   -0.10271225  3.308196\n",
      "  2.112591    1.8291397   1.7693679   1.6866695   2.7851958   2.5211203\n",
      "  2.8297029   2.0737329   3.1563616   1.4511888 ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#function to compute (DE)\n",
    "def compute_de_features(eeg_data):\n",
    "    variance = np.var(eeg_data, axis=-1)\n",
    "    de_features = 0.5 * np.log2(2* np.pi * np.e * variance)\n",
    "    return de_features\n",
    "\n",
    "# Compute DE features for the sample segment\n",
    "aligned_representation = encoded_output.detach().numpy().squeeze()\n",
    "de_features = compute_de_features(aligned_representation)\n",
    "\n",
    "print(f\"DE features shape: {de_features.shape}\")\n",
    "print(f\"DE features: {de_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DE features shape: torch.Size([1, 16])\n",
      "Classifier output shape: torch.Size([1, 3])\n",
      "Predictied emotion probabilities: tensor([[nan, nan, nan]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class MLPCLassifier(nn.Module):\n",
    "    def __init__(self, input_dim=16, hidden_dim=30, output_dim=3):\n",
    "        super(MLPCLassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "    \n",
    "# Initialize the classifier\n",
    "mlp_classifier = MLPCLassifier(input_dim=16, hidden_dim=30, output_dim=3)\n",
    "\n",
    "#Test the classifier\n",
    "de_features_tensor = torch.tensor(de_features, dtype=torch.float32).unsqueeze(0)\n",
    "output = mlp_classifier(de_features_tensor)\n",
    "\n",
    "print(f\"DE features shape: {de_features_tensor.shape}\")\n",
    "print(f\"Classifier output shape: {output.shape}\")\n",
    "print(f\"Predictied emotion probabilities: {output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.1022374289376395\n",
      "Epoch 2, Loss: 1.0968850340162004\n",
      "Epoch 3, Loss: 1.0922445740018571\n",
      "Epoch 4, Loss: 1.0931646142687117\n",
      "Epoch 5, Loss: 1.0903737545013428\n"
     ]
    }
   ],
   "source": [
    "import torch.utils.data as data\n",
    "\n",
    "# Mock Dataset for Training (Replace with actual DE feature dataset)\n",
    "class DEFeatureDataset(data.Dataset):\n",
    "    def __init__(self, num_samples=100):\n",
    "        self.num_samples = num_samples\n",
    "        self.features = torch.randn(num_samples, 16)  # Random DE features\n",
    "        self.labels = torch.randint(0, 3, (num_samples,))  # Random labels (3 emotion categories)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "# Initialize Dataset and DataLoader\n",
    "dataset = DEFeatureDataset(num_samples=100)\n",
    "data_loader = data.DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Define Loss Function and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(mlp_classifier.parameters(), lr=0.001)\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(5):  # Train for 5 epochs\n",
    "    total_loss = 0\n",
    "    for batch_features, batch_labels in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        predictions = mlp_classifier(batch_features)\n",
    "        loss = criterion(predictions, batch_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(data_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 25.00%\n"
     ]
    }
   ],
   "source": [
    "# Evaluation Dataset (Mock Testing Data)\n",
    "test_dataset = DEFeatureDataset(num_samples=20)  # Smaller dataset for testing\n",
    "test_loader = data.DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "# Evaluation Loop\n",
    "mlp_classifier.eval()  # Set the classifier to evaluation mode\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_features, batch_labels in test_loader:\n",
    "        predictions = mlp_classifier(batch_features)\n",
    "        predicted_labels = torch.argmax(predictions, dim=1)\n",
    "        correct += (predicted_labels == batch_labels).sum().item()\n",
    "        total += batch_labels.size(0)\n",
    "\n",
    "accuracy = correct / total * 100\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "APPLICATION TO WHOLE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 152730 EEG samples.\n",
      "First processed sample shape: (62, 200)\n"
     ]
    }
   ],
   "source": [
    "# Function to process the entire dataset\n",
    "def process_dataset(dataset):\n",
    "    processed_segments = []\n",
    "    for sample in dataset:\n",
    "        eeg_data, metadata = sample\n",
    "\n",
    "        # Bandpass filter\n",
    "        filtered_data = bandpass_filter(eeg_data)\n",
    "\n",
    "        # Append metadata for reference\n",
    "        processed_segments.append((filtered_data, metadata))\n",
    "\n",
    "    return processed_segments\n",
    "\n",
    "# Process the raw dataset\n",
    "processed_data = process_dataset(raw_dataset)\n",
    "print(f\"Processed {len(processed_data)} EEG samples.\")\n",
    "print(f\"First processed sample shape: {processed_data[0][0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking for Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualize EEG signal for the first sample and first 5 channels\n",
    "eeg_data, _ = processed_data[0]  # Replace with normalized_data if already normalized\n",
    "for channel in range(5):  # Visualize first 5 channels\n",
    "    plt.plot(eeg_data[channel, :], label=f'Channel {channel+1}')\n",
    "plt.title('EEG Signal (First Sample, First 5 Channels)')\n",
    "plt.xlabel('Time Points')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance across channels: [2.88711124e+02 2.18970653e+02 2.39640774e+02 9.92589084e+01\n",
      " 1.03980378e+02 6.59008687e+01 5.36339461e+01 3.75502305e+01\n",
      " 3.17774269e+01 3.08489418e+01 3.45634075e+01 4.07416981e+01\n",
      " 7.47830643e+01 5.56228761e+01 7.56380017e+01 3.47239044e+01\n",
      " 1.75527078e+01 1.10857114e+01 1.47321980e+01 1.08047042e+01\n",
      " 2.32862554e+01 5.71502909e+01 8.13472500e+01 5.19968752e+01\n",
      " 2.90521157e+01 1.67339063e+01 9.67938366e+00 1.79470810e-01\n",
      " 6.10604941e+00 1.75851697e+01 6.74993834e+01 3.16524470e+02\n",
      " 3.82455975e+01 2.45000311e+01 1.68617385e+01 1.09527791e+01\n",
      " 1.60944414e-01 8.10107852e+00 2.59409116e+01 3.52847480e+01\n",
      " 9.64959093e+01 4.23698233e+01 3.60389108e+01 2.61315133e+01\n",
      " 6.39326734e+01 1.30697148e+01 1.84383763e+01 3.47911670e+01\n",
      " 5.46073495e+01 7.45168250e+01 4.60223185e+01 4.80595469e+01\n",
      " 3.57531413e+01 2.93699280e+01 4.23142032e+01 6.74216867e+01\n",
      " 7.09997871e+01 4.97636229e+01 4.03753037e+01 4.10292845e+01\n",
      " 4.82002698e+01 5.01593071e+01]\n",
      "Channels with artifacts: [ 0  2 31]\n"
     ]
    }
   ],
   "source": [
    "# Compute variance across time points for each channel\n",
    "eeg_data, _ = processed_data[0]\n",
    "variance = np.var(eeg_data, axis=1)\n",
    "print(f'Variance across channels: {variance}')\n",
    "\n",
    "# Flag channels with unusually high variance\n",
    "threshold = np.mean(variance) + 3 * np.std(variance)  # 3-sigma rule\n",
    "artifact_channels = np.where(variance > threshold)[0]\n",
    "print(f'Channels with artifacts: {artifact_channels}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize next 5 channels of the first sample\n",
    "for channel in range(5, 10):  # Channels 6 to 10\n",
    "    plt.plot(eeg_data[channel, :], label=f'Channel {channel+1}')\n",
    "plt.title('EEG Signal (First Sample, Channels 6-10)')\n",
    "plt.xlabel('Time Points')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance across channels: [2.88711124e+02 2.18970653e+02 2.39640774e+02 9.92589084e+01\n",
      " 1.03980378e+02 6.59008687e+01 5.36339461e+01 3.75502305e+01\n",
      " 3.17774269e+01 3.08489418e+01 3.45634075e+01 4.07416981e+01\n",
      " 7.47830643e+01 5.56228761e+01 7.56380017e+01 3.47239044e+01\n",
      " 1.75527078e+01 1.10857114e+01 1.47321980e+01 1.08047042e+01\n",
      " 2.32862554e+01 5.71502909e+01 8.13472500e+01 5.19968752e+01\n",
      " 2.90521157e+01 1.67339063e+01 9.67938366e+00 1.79470810e-01\n",
      " 6.10604941e+00 1.75851697e+01 6.74993834e+01 3.16524470e+02\n",
      " 3.82455975e+01 2.45000311e+01 1.68617385e+01 1.09527791e+01\n",
      " 1.60944414e-01 8.10107852e+00 2.59409116e+01 3.52847480e+01\n",
      " 9.64959093e+01 4.23698233e+01 3.60389108e+01 2.61315133e+01\n",
      " 6.39326734e+01 1.30697148e+01 1.84383763e+01 3.47911670e+01\n",
      " 5.46073495e+01 7.45168250e+01 4.60223185e+01 4.80595469e+01\n",
      " 3.57531413e+01 2.93699280e+01 4.23142032e+01 6.74216867e+01\n",
      " 7.09997871e+01 4.97636229e+01 4.03753037e+01 4.10292845e+01\n",
      " 4.82002698e+01 5.01593071e+01]\n"
     ]
    }
   ],
   "source": [
    "# Compute variance for the first sample\n",
    "variance = np.var(eeg_data, axis=1)\n",
    "print(f'Variance across channels: {variance}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after removing artifact channels: (59, 200)\n"
     ]
    }
   ],
   "source": [
    "# Remove artifact channels\n",
    "artifact_free_data = np.delete(eeg_data, [0, 2, 31], axis=0)  # Remove channels 0, 2, and 31\n",
    "print(f\"Shape after removing artifact channels: {artifact_free_data.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated variance across channels: [2.18970653e+02 9.92589084e+01 1.03980378e+02 6.59008687e+01\n",
      " 5.36339461e+01 3.75502305e+01 3.17774269e+01 3.08489418e+01\n",
      " 3.45634075e+01 4.07416981e+01 7.47830643e+01 5.56228761e+01\n",
      " 7.56380017e+01 3.47239044e+01 1.75527078e+01 1.10857114e+01\n",
      " 1.47321980e+01 1.08047042e+01 2.32862554e+01 5.71502909e+01\n",
      " 8.13472500e+01 5.19968752e+01 2.90521157e+01 1.67339063e+01\n",
      " 9.67938366e+00 1.79470810e-01 6.10604941e+00 1.75851697e+01\n",
      " 6.74993834e+01 3.82455975e+01 2.45000311e+01 1.68617385e+01\n",
      " 1.09527791e+01 1.60944414e-01 8.10107852e+00 2.59409116e+01\n",
      " 3.52847480e+01 9.64959093e+01 4.23698233e+01 3.60389108e+01\n",
      " 2.61315133e+01 6.39326734e+01 1.30697148e+01 1.84383763e+01\n",
      " 3.47911670e+01 5.46073495e+01 7.45168250e+01 4.60223185e+01\n",
      " 4.80595469e+01 3.57531413e+01 2.93699280e+01 4.23142032e+01\n",
      " 6.74216867e+01 7.09997871e+01 4.97636229e+01 4.03753037e+01\n",
      " 4.10292845e+01 4.82002698e+01 5.01593071e+01]\n"
     ]
    }
   ],
   "source": [
    "# Recompute variance\n",
    "updated_variance = np.var(artifact_free_data, axis=1)\n",
    "print(f\"Updated variance across channels: {updated_variance}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving towards normalization\n",
    "The next step is to normalize the data. Normalization is a process of scaling the data to a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized 152730 EEG samples.\n",
      "First normalized sample shape: (62, 200)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def incremental_normalization(process_dataset):\n",
    "    normalized_segments = []\n",
    "    subject_data_stats = {}\n",
    "    \n",
    "    #First pass : computing mean and variance incrementally for each subject\n",
    "    for eeg_data, metadata in process_dataset:\n",
    "        subject_id = metadata['subject_id']\n",
    "        if subject_id not in subject_data_stats:\n",
    "            subject_data_stats[subject_id] = {'sum':0, 'sum_sq':0, \"count\":0}\n",
    "            \n",
    "        stats = subject_data_stats[subject_id]\n",
    "        stats['sum'] += np.sum(eeg_data, axis=-1, keepdims=True)\n",
    "        stats['sum_sq'] += np.sum(eeg_data**2, axis=-1, keepdims=True)\n",
    "        stats['count'] += eeg_data.shape[1]\n",
    "        \n",
    "    #Compute mean and standard deviation for each subject\n",
    "    for subject_id, stats in subject_data_stats.items():\n",
    "        stats['mean'] = stats['sum'] / stats['count']\n",
    "        stats['std'] = np.sqrt(stats['sum_sq'] / stats['count'] - stats['mean']**2)\n",
    "        \n",
    "        \n",
    "    #Second pass : Normalizing each segment using computed stats\n",
    "    for eeg_data, metadata in process_dataset:\n",
    "        subject_id = metadata['subject_id']\n",
    "        stats = subject_data_stats[subject_id]\n",
    "        mean = stats['mean']\n",
    "        std = stats['std']\n",
    "        normalized_data = (eeg_data - mean) / std\n",
    "        normalized_segments.append((normalized_data, metadata))\n",
    "        \n",
    "    return normalized_segments\n",
    "\n",
    "# Normalize the processed dataset\n",
    "normalized_data = incremental_normalization(processed_data)\n",
    "\n",
    "#output the normalized data\n",
    "print(f\"Normalized {len(normalized_data)} EEG samples.\")\n",
    "print(f\"First normalized sample shape: {normalized_data[0][0].shape}\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1832760 EEG segments.\n",
      "First sampled segment shape: (62, 30)\n"
     ]
    }
   ],
   "source": [
    "# Function to sample data into fixed length segments\n",
    "def sample_data(normalized_data, time_length=30, step_size=15):\n",
    "    sampled_segments = []\n",
    "    \n",
    "    for eeg_data, metadata in normalized_data:\n",
    "        trial_length = eeg_data.shape[1]\n",
    "        \n",
    "        for start in range(0 , trial_length - time_length + 1, step_size):\n",
    "            segment = eeg_data[:, start:start+time_length]\n",
    "            new_metadata = metadata.copy()\n",
    "            new_metadata['segment_start'] = start\n",
    "            new_metadata['segment_end'] = start + time_length\n",
    "            sampled_segments.append((segment, new_metadata))\n",
    "    \n",
    "    return sampled_segments\n",
    "\n",
    "# Sample the normalized data\n",
    "time_length = 30\n",
    "step_size = 15\n",
    "sampled_data = sample_data(normalized_data, time_length=time_length, step_size=step_size)\n",
    "            \n",
    "# Output the sampled data\n",
    "print(f\"Generated {len(sampled_data)} EEG segments.\")\n",
    "print(f\"First sampled segment shape: {sampled_data[0][0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed DE features for 1832760 EEG segments.\n",
      "First DE features shape: (62,)\n"
     ]
    }
   ],
   "source": [
    "# Function to compute DE features for each segment\n",
    "def compute_de_features(eeg_segments):\n",
    "    # eeg_segment shape: [channels, timepoints]\n",
    "    variance = np.var(eeg_segments, axis=-1)\n",
    "    de_features = 0.5 * np.log2(2 * np.pi * np.e * variance)\n",
    "    return de_features\n",
    "\n",
    "# Extract DE features for the sampled data\n",
    "de_features_data = []\n",
    "for segment, metadata in sampled_data:\n",
    "    de_features = compute_de_features(segment)\n",
    "    de_features_data.append((de_features, metadata))\n",
    "    \n",
    "# Output the DE features data\n",
    "print(f\"Computed DE features for {len(de_features_data)} EEG segments.\")\n",
    "print(f\"First DE features shape: {de_features_data[0][0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class BaseEncoder(nn.Module):\n",
    "    def __init__(self, input_channels=62, temporal_filter_length=48, spatial_filters=16, temporal_filters=16):\n",
    "        super(BaseEncoder, self).__init__()\n",
    "        # Spatial Convolution\n",
    "        self.spatial_conv = nn.Conv1d(input_channels, spatial_filters, kernel_size=1)\n",
    "        # Temporal Convolution\n",
    "        self.temporal_conv = nn.Conv1d(spatial_filters, temporal_filters, kernel_size=temporal_filter_length, padding=temporal_filter_length // 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, input_channels, time_points]\n",
    "        print(\"Entered BaseEncoder\")\n",
    "        x = self.spatial_conv(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.temporal_conv(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        print(\"Exit BaseEncoder\")\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded output shape: torch.Size([1, 16, 31])\n",
      "Sample encoded output (first 5 channels): tensor([[0.0000, 0.2231, 0.0890, 0.0000, 0.0964],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.1257],\n",
      "        [0.1435, 0.0277, 0.2629, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0158, 0.0000, 0.0228, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0264, 0.0000, 0.0161],\n",
      "        [0.0395, 0.0254, 0.0000, 0.2917, 0.0980],\n",
      "        [0.0000, 0.0069, 0.3017, 0.0564, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0547, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1138, 0.3318, 0.0863, 0.2381],\n",
      "        [0.0000, 0.0575, 0.0237, 0.0000, 0.1973],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3192, 0.0975, 0.3419, 0.0000, 0.0000],\n",
      "        [0.0546, 0.0078, 0.0000, 0.0000, 0.3408],\n",
      "        [0.0296, 0.0000, 0.0000, 0.0000, 0.0595],\n",
      "        [0.0406, 0.0755, 0.2397, 0.3373, 0.2540]], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the base encoder\n",
    "base_encoder = BaseEncoder(input_channels=62, temporal_filter_length=48, spatial_filters=16, temporal_filters=16)\n",
    "\n",
    "# Create a mock EEG sample\n",
    "sample_segment = torch.randn(1, 62, 30)  # Shape: [batch_size, input_channels, time_points]\n",
    "\n",
    "# Pass the sample through the base encoder\n",
    "encoded_output = base_encoder(sample_segment)\n",
    "\n",
    "# Output the shape and sample values\n",
    "print(f\"Encoded output shape: {encoded_output.shape}\")\n",
    "print(f\"Sample encoded output (first 5 channels): {encoded_output[0, :, :5]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entered Projector\n",
      "Exit Projector\n",
      "Encoded output shape: torch.Size([1, 16, 31])\n",
      "Projected output shape: torch.Size([1, 64, 2])\n"
     ]
    }
   ],
   "source": [
    "# Define the Projector\n",
    "class Projector(nn.Module):\n",
    "    def __init__(self, spatial_filters=16, pooling_kernel=24, temporal_filter_size=4 , c=2):\n",
    "        super(Projector, self).__init__()\n",
    "        #Average Pooling\n",
    "        \n",
    "        self.avg_pool = nn.AvgPool1d(kernel_size=pooling_kernel , stride=pooling_kernel)\n",
    "        #spatial convolution        \n",
    "        self.spatial_conv = nn.Conv1d(in_channels=spatial_filters, out_channels=spatial_filters*c,kernel_size=1)\n",
    "        #temporal convolution\n",
    "        self.temporal_conv = nn.Conv1d(in_channels=spatial_filters*c,out_channels=(spatial_filters*c)*c, kernel_size=temporal_filter_size, padding=temporal_filter_size//2)\n",
    "        #Activation\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        print(\"Entered Projector\")\n",
    "        #Applying avg pooling\n",
    "        x = self.avg_pool(x)\n",
    "        #Applying spatial convolution\n",
    "        x = self.spatial_conv(x)\n",
    "        x = self.relu(x)\n",
    "        #Applying temporal convolution\n",
    "        x = self.temporal_conv(x)\n",
    "        x = self.relu(x)\n",
    "        print(\"Exit Projector\")\n",
    "        return x\n",
    "    \n",
    "\n",
    "\n",
    "# Initialize the projector\n",
    "projector = Projector(spatial_filters=16, pooling_kernel=24, temporal_filter_size=4,c=2)\n",
    "\n",
    "# Test on the encoded output\n",
    "projected_output = projector(encoded_output)\n",
    "\n",
    "print(f\"Encoded output shape: {encoded_output.shape}\")\n",
    "print(f\"Projected output shape: {projected_output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, temperature=1.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, z_i, z_j):\n",
    "        print(\"Entered ContrastiveLoss\")\n",
    "        \n",
    "        # Normalize embeddings\n",
    "        z_i = nn.functional.normalize(z_i, dim=1)\n",
    "        z_j = nn.functional.normalize(z_j, dim=1)\n",
    "\n",
    "        # Debugging normalization\n",
    "        print(f\"z_i Mean: {z_i.mean().item()}, Std: {z_i.std().item()}\")\n",
    "        print(f\"z_j Mean: {z_j.mean().item()}, Std: {z_j.std().item()}\")\n",
    "\n",
    "        # Compute similarity matrix\n",
    "        similarities = torch.matmul(z_i, z_j.T) / self.temperature\n",
    "\n",
    "        # Clamp similarities to prevent overflow in exp\n",
    "        similarities = torch.clamp(similarities, min=-5, max=5)\n",
    "        \n",
    "        # Debugging similarities\n",
    "        print(f\"Similarities: {similarities[:5, :5]}\")\n",
    "        print(f\"Similarities Range: [{similarities.min().item()}, {similarities.max().item()}]\")\n",
    "\n",
    "        # Create labels for contrastive loss\n",
    "        # batch_size = z_i.size(0)\n",
    "        batch_size = similarities.size(0).to(similarities.device)\n",
    "        # labels = torch.arange(batch_size).to(z_i.device)\n",
    "        labels = torch.arange(batch_size).to(similarities.device)\n",
    "        print(f\"Labels: {labels[:5]}\")\n",
    "\n",
    "        # Check for NaNs/Infs before computing loss\n",
    "        if torch.isnan(similarities).any() or torch.isinf(similarities).any():\n",
    "            print(\"NaN or Inf detected in similarities\")\n",
    "\n",
    "        # Compute cross-entropy loss\n",
    "        loss = nn.CrossEntropyLoss()(similarities, labels)\n",
    "        \n",
    "        print(f\"Exit ContrastiveLoss with Loss: {loss.item()}\")\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.0008723717764951289, Std: 0.08847048133611679\n",
      "z_j Mean: 0.00018321722745895386, Std: 0.08847460150718689\n",
      "Similarities: tensor([[-1.5923e-02,  1.8144e-02,  3.0793e-02,  5.4898e-02],\n",
      "        [ 2.1284e-01, -5.5849e-02, -2.0748e-02, -2.5636e-02],\n",
      "        [-3.0704e-01, -1.1834e-04,  1.8439e-01, -1.3120e-02],\n",
      "        [-1.9545e-01,  9.5966e-02, -9.7414e-02,  2.1207e-02]])\n",
      "Similarities Range: [-0.3070354461669922, 0.21284393966197968]\n",
      "Labels: tensor([0, 1, 2, 3])\n",
      "Exit ContrastiveLoss with Loss: 1.3526623249053955\n",
      "Contrastive Loss: 1.3526623249053955\n"
     ]
    }
   ],
   "source": [
    "# Sample data\n",
    "batch_size = 4\n",
    "embedding_dim = 128\n",
    "\n",
    "# Simulate output embeddings from the projector for two augmented views\n",
    "z_A = torch.randn(batch_size, embedding_dim)\n",
    "z_B = torch.randn(batch_size, embedding_dim)\n",
    "\n",
    "# Initialize the contrastive loss\n",
    "contrastive_loss = ContrastiveLoss(temperature=0.7)\n",
    "\n",
    "# Compute the loss\n",
    "loss = contrastive_loss(z_A, z_B)\n",
    "\n",
    "print(f\"Contrastive Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Subject IDs: [10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Subject Sample Counts: Counter({10: 122184, 11: 122184, 12: 122184, 13: 122184, 14: 122184, 15: 122184, 1: 122184, 2: 122184, 3: 122184, 4: 122184, 5: 122184, 6: 122184, 7: 122184, 8: 122184, 9: 122184})\n",
      "Updated Subject IDs: [9, 10, 11, 12, 13, 14, 0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "Updated Subject Sample Counts: Counter({9: 122184, 10: 122184, 11: 122184, 12: 122184, 13: 122184, 14: 122184, 0: 122184, 1: 122184, 2: 122184, 3: 122184, 4: 122184, 5: 122184, 6: 122184, 7: 122184, 8: 122184})\n"
     ]
    }
   ],
   "source": [
    "# Check unique subject IDs and their sample counts\n",
    "from collections import Counter\n",
    "\n",
    "subject_ids = [item[1]['subject_id'] for item in sampled_data]\n",
    "subject_distribution = Counter(subject_ids)\n",
    "\n",
    "print(f\"Unique Subject IDs: {list(subject_distribution.keys())}\")\n",
    "print(f\"Subject Sample Counts: {subject_distribution}\")\n",
    "\n",
    "# Create a mapping for subject IDs\n",
    "unique_subject_ids = sorted(set(subject_ids))\n",
    "subject_id_map = {old_id: new_id for new_id, old_id in enumerate(unique_subject_ids)}\n",
    "\n",
    "# Update subject IDs in metadata\n",
    "for _, metadata in sampled_data:\n",
    "    metadata['subject_id'] = subject_id_map[metadata['subject_id']]\n",
    "\n",
    "# Verify remapping\n",
    "updated_subject_ids = [item[1]['subject_id'] for item in sampled_data]\n",
    "updated_subject_distribution = Counter(updated_subject_ids)\n",
    "\n",
    "print(f\"Updated Subject IDs: {list(updated_subject_distribution.keys())}\")\n",
    "print(f\"Updated Subject Sample Counts: {updated_subject_distribution}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 1710576, Testing samples: 122184\n"
     ]
    }
   ],
   "source": [
    "def loso_split(dataset, num_subjects):\n",
    "    \"\"\"\n",
    "    Create training and testing splits for Leave-One-Subject-Out (LOSO) Cross-Validation.\n",
    "    Args:\n",
    "        dataset: list of tuples (EEG_data, metadata)\n",
    "        num_subjects: total number of unique subjects in the dataset\n",
    "    Returns:\n",
    "        List of splits [(train_data, test_data), ...]\n",
    "    \"\"\"\n",
    "    splits = []\n",
    "    for test_subject in range(num_subjects):\n",
    "        # Split dataset into training and testing based on subject ID\n",
    "        train_data = [item for item in dataset if item[1]['subject_id'] != test_subject]\n",
    "        test_data = [item for item in dataset if item[1]['subject_id'] == test_subject]\n",
    "        splits.append((train_data, test_data))\n",
    "    return splits\n",
    "\n",
    "# Determine the number of unique subjects\n",
    "num_subjects = len(set(item[1]['subject_id'] for item in sampled_data))\n",
    "\n",
    "# Perform LOSO split\n",
    "loso_splits = loso_split(sampled_data, num_subjects)\n",
    "\n",
    "# Example: Use the first split\n",
    "train_data, test_data = loso_splits[0]\n",
    "print(f\"Training samples: {len(train_data)}, Testing samples: {len(test_data)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting Training and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Subject IDs: {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14}\n",
      "Testing Subject IDs: {0}\n"
     ]
    }
   ],
   "source": [
    "# Extract unique subject IDs in training and testing data\n",
    "train_subjects = set(item[1]['subject_id'] for item in train_data)\n",
    "test_subjects = set(item[1]['subject_id'] for item in test_data)\n",
    "\n",
    "print(f\"Training Subject IDs: {train_subjects}\")\n",
    "print(f\"Testing Subject IDs: {test_subjects}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Training Sample Shape: (62, 30)\n",
      "Random Training Sample Metadata: {'start_at': 0, 'end_at': 200, 'clip_id': '10_20131130.mat_0', 'subject_id': 9, 'trial_id': 'ww_eeg1', 'emotion': 1, 'date': 20131130, '_record_id': '_record_0', 'segment_start': 0, 'segment_end': 30}\n",
      "Random Test Sample Shape: (62, 30)\n",
      "Random Test Sample Metadata: {'start_at': 0, 'end_at': 200, 'clip_id': '1_20131027.mat_0', 'subject_id': 0, 'trial_id': 'djc_eeg1', 'emotion': 1, 'date': 20131027, '_record_id': '_record_18', 'segment_start': 0, 'segment_end': 30}\n"
     ]
    }
   ],
   "source": [
    "# Inspect a random training sample\n",
    "random_train_sample = train_data[0]\n",
    "print(f\"Random Training Sample Shape: {random_train_sample[0].shape}\")\n",
    "print(f\"Random Training Sample Metadata: {random_train_sample[1]}\")\n",
    "\n",
    "# Inspect a random test sample\n",
    "random_test_sample = test_data[0]\n",
    "print(f\"Random Test Sample Shape: {random_test_sample[0].shape}\")\n",
    "print(f\"Random Test Sample Metadata: {random_test_sample[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Label Distribution: Counter({1: 589680, -1: 564480, 0: 556416})\n",
      "Testing Label Distribution: Counter({1: 42120, -1: 40320, 0: 39744})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Check label distribution\n",
    "train_labels = [item[1]['emotion'] for item in train_data]  # Replace 'emotion' with the actual key for emotion labels\n",
    "test_labels = [item[1]['emotion'] for item in test_data]\n",
    "\n",
    "print(f\"Training Label Distribution: {Counter(train_labels)}\")\n",
    "print(f\"Testing Label Distribution: {Counter(test_labels)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the data for later usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "def save_to_hdf5(data, filename):\n",
    "    \"\"\"\n",
    "    Save processed EEG data to an HDF5 file.\n",
    "    Args:\n",
    "        data: List of tuples (eeg_data, metadata)\n",
    "        filename: Name of the HDF5 file\n",
    "    \"\"\"\n",
    "    with h5py.File(filename, \"w\") as h5file:\n",
    "        for i, (eeg_data, metadata) in enumerate(data):\n",
    "            # Create a group for each sample\n",
    "            group = h5file.create_group(str(i))\n",
    "            # Save EEG data\n",
    "            group.create_dataset(\"eeg_data\", data=eeg_data)\n",
    "            # Save metadata\n",
    "            for key, value in metadata.items():\n",
    "                group.attrs[key] = value\n",
    "\n",
    "# Save training and testing data\n",
    "save_to_hdf5(train_data, \"./Data/Train/train_data_processed.h5\")\n",
    "save_to_hdf5(test_data, \"./Data/Test/test_data_processed.h5\")\n",
    "\n",
    "print(\"Processed data saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "def load_from_hdf5(filename):\n",
    "    \"\"\"\n",
    "    Load processed EEG data from an HDF5 file.\n",
    "    Args:\n",
    "        filename: Name of the HDF5 file\n",
    "    Returns:\n",
    "        Loaded data as a list of tuples (eeg_data, metadata)\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with h5py.File(filename, \"r\") as h5file:\n",
    "        for key in h5file.keys():\n",
    "            # Load EEG data\n",
    "            eeg_data = np.array(h5file[key][\"eeg_data\"])\n",
    "            # Load metadata\n",
    "            metadata = dict(h5file[key].attrs)\n",
    "            data.append((eeg_data, metadata))\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training and testing data from HDF5\n",
    "train_data = load_from_hdf5(\"./Data/Train/train_data_processed.h5\")\n",
    "test_data = load_from_hdf5(\"./Data/Test/test_data_processed.h5\")\n",
    "\n",
    "# Verify loaded data\n",
    "print(f\"Loaded {len(train_data)} training samples and {len(test_data)} testing samples.\")\n",
    "print(f\"First Training Sample Shape: {train_data[0][0].shape}\")\n",
    "print(f\"First Training Sample Metadata: {train_data[0][1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODELLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 1710576\n",
      "Sample 0: EEG Shape = (62, 30), Metadata = {'start_at': 0, 'end_at': 200, 'clip_id': '10_20131130.mat_0', 'subject_id': 9, 'trial_id': 'ww_eeg1', 'emotion': 1, 'date': 20131130, '_record_id': '_record_0', 'segment_start': 0, 'segment_end': 30}\n",
      "Sample 1: EEG Shape = (62, 30), Metadata = {'start_at': 0, 'end_at': 200, 'clip_id': '10_20131130.mat_0', 'subject_id': 9, 'trial_id': 'ww_eeg1', 'emotion': 1, 'date': 20131130, '_record_id': '_record_0', 'segment_start': 15, 'segment_end': 45}\n",
      "Sample 2: EEG Shape = (62, 30), Metadata = {'start_at': 0, 'end_at': 200, 'clip_id': '10_20131130.mat_0', 'subject_id': 9, 'trial_id': 'ww_eeg1', 'emotion': 1, 'date': 20131130, '_record_id': '_record_0', 'segment_start': 30, 'segment_end': 60}\n",
      "Sample 3: EEG Shape = (62, 30), Metadata = {'start_at': 0, 'end_at': 200, 'clip_id': '10_20131130.mat_0', 'subject_id': 9, 'trial_id': 'ww_eeg1', 'emotion': 1, 'date': 20131130, '_record_id': '_record_0', 'segment_start': 45, 'segment_end': 75}\n",
      "Sample 4: EEG Shape = (62, 30), Metadata = {'start_at': 0, 'end_at': 200, 'clip_id': '10_20131130.mat_0', 'subject_id': 9, 'trial_id': 'ww_eeg1', 'emotion': 1, 'date': 20131130, '_record_id': '_record_0', 'segment_start': 60, 'segment_end': 90}\n"
     ]
    }
   ],
   "source": [
    "# Check the first few samples in your dataset\n",
    "print(f\"Total samples: {len(train_data)}\")\n",
    "for i in range(5):  # Check the first 5 samples\n",
    "    eeg_data, metadata = train_data[i]\n",
    "    print(f\"Sample {i}: EEG Shape = {eeg_data.shape}, Metadata = {metadata}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # print(\"Entered EEGDataset\")\n",
    "        eeg_data, metadata = self.data[idx]\n",
    "        if not isinstance(metadata, dict):\n",
    "            raise ValueError(f\"Metadata must be a dictionary, but got {type(metadata)} at index {idx}.\")\n",
    "        eeg_data_tensor = torch.tensor(eeg_data, dtype=torch.float32)\n",
    "        emotion_label = metadata.get('emotion', -1) # Replace 'emotion' with the actual key for emotion labels\n",
    "        # print(\"Exit EEGDataset\")\n",
    "        return eeg_data_tensor, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0: Data Shape = torch.Size([62, 30]), Label = {'start_at': 0, 'end_at': 200, 'clip_id': '10_20131130.mat_0', 'subject_id': 9, 'trial_id': 'ww_eeg1', 'emotion': 1, 'date': 20131130, '_record_id': '_record_0', 'segment_start': 0, 'segment_end': 30}\n",
      "Sample 1: Data Shape = torch.Size([62, 30]), Label = {'start_at': 0, 'end_at': 200, 'clip_id': '10_20131130.mat_0', 'subject_id': 9, 'trial_id': 'ww_eeg1', 'emotion': 1, 'date': 20131130, '_record_id': '_record_0', 'segment_start': 15, 'segment_end': 45}\n",
      "Sample 2: Data Shape = torch.Size([62, 30]), Label = {'start_at': 0, 'end_at': 200, 'clip_id': '10_20131130.mat_0', 'subject_id': 9, 'trial_id': 'ww_eeg1', 'emotion': 1, 'date': 20131130, '_record_id': '_record_0', 'segment_start': 30, 'segment_end': 60}\n",
      "Sample 3: Data Shape = torch.Size([62, 30]), Label = {'start_at': 0, 'end_at': 200, 'clip_id': '10_20131130.mat_0', 'subject_id': 9, 'trial_id': 'ww_eeg1', 'emotion': 1, 'date': 20131130, '_record_id': '_record_0', 'segment_start': 45, 'segment_end': 75}\n",
      "Sample 4: Data Shape = torch.Size([62, 30]), Label = {'start_at': 0, 'end_at': 200, 'clip_id': '10_20131130.mat_0', 'subject_id': 9, 'trial_id': 'ww_eeg1', 'emotion': 1, 'date': 20131130, '_record_id': '_record_0', 'segment_start': 60, 'segment_end': 90}\n"
     ]
    }
   ],
   "source": [
    "train_dataset = EEGDataset(train_data)\n",
    "\n",
    "# Access a few samples\n",
    "for i in range(5):\n",
    "    eeg_data, label = train_dataset[i]\n",
    "    print(f\"Sample {i}: Data Shape = {eeg_data.shape}, Label = {label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = EEGDataset(train_data)\n",
    "test_dataset = EEGDataset(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0: Data Shape = torch.Size([256, 62, 30]), Labels = {'start_at': tensor([36000,  5200,  7800, 23800, 21400, 37000,  1200, 12600, 16200, 37000,\n",
      "        16800,   600, 31600,  9200, 27600, 42400, 26400, 18800, 15200, 42400,\n",
      "        31600, 44200, 18400, 21000, 45800,   200, 17800, 30600,   800, 17600,\n",
      "        15000, 43000, 18600, 46600, 19400, 28800, 12000, 45800, 26000, 37400,\n",
      "        16600, 23400,  7400,  7000, 24400, 45600, 23000, 21800,  6600, 24600,\n",
      "        10400, 46400,  2600, 11000, 31200, 14400,  4600,  3400, 16000, 35800,\n",
      "        23200, 16400, 10200, 23400, 31800, 13800, 28400,  4400, 15200,  4400,\n",
      "        35000, 32000, 31200,  3000, 19400, 32400, 29400, 31200, 13800, 26000,\n",
      "        21800, 21200, 31000, 42600, 11600, 35000, 40600, 28600, 32600, 17000,\n",
      "        10000, 45000, 12000, 14400, 35400,  6200, 43200, 31800, 24600, 22600,\n",
      "        35200, 14400, 34000, 35000, 11400,  7400, 19600, 31200, 17000, 42800,\n",
      "         9200,   200, 34800, 28800, 23000, 40200, 20600,  4600,     0,  2400,\n",
      "        20400, 28600,  3400,  2800, 25400, 14200, 33800,   400, 40000,  4200,\n",
      "        17400, 40600, 41000, 25000, 46400, 42800, 17400,  3200,  9600, 13400,\n",
      "        30600,  6800, 28600, 33200, 34400, 22800,  7200, 37000, 25600, 44000,\n",
      "        10200, 11800, 32000, 11000,  8000,  2200, 20800, 13200, 39000, 35800,\n",
      "        15800, 10800, 12400, 26600, 40000,  4000, 45800, 38200, 24600, 15200,\n",
      "        28600, 27800, 14600, 16800, 32600, 20200, 14600, 18200, 46000, 29200,\n",
      "         2000, 24600, 14800, 24600, 36600, 18000,  6800, 15600, 20400, 12400,\n",
      "        33600,     0, 33400, 14200, 33600, 28600, 25000, 20600, 30800, 44400,\n",
      "        18200, 22800,  6400, 28800, 34400,  8000,  4800, 42000, 16000,  1800,\n",
      "         3600, 34400, 24200, 44800, 38400, 13000, 23000,  5600, 26400, 17000,\n",
      "        16000, 10600, 20200,  6800,  8600, 29400, 28400, 12600, 34200, 13400,\n",
      "         6400, 16200, 13200, 19200, 45600, 36400, 27600, 19400, 22400, 24400,\n",
      "         4400,  5400, 33000, 21800,  7000, 18200, 28400, 42400, 10400,  8200,\n",
      "        32200,  1600, 44000,  4000, 27800,  8000]), 'end_at': tensor([36200,  5400,  8000, 24000, 21600, 37200,  1400, 12800, 16400, 37200,\n",
      "        17000,   800, 31800,  9400, 27800, 42600, 26600, 19000, 15400, 42600,\n",
      "        31800, 44400, 18600, 21200, 46000,   400, 18000, 30800,  1000, 17800,\n",
      "        15200, 43200, 18800, 46800, 19600, 29000, 12200, 46000, 26200, 37600,\n",
      "        16800, 23600,  7600,  7200, 24600, 45800, 23200, 22000,  6800, 24800,\n",
      "        10600, 46600,  2800, 11200, 31400, 14600,  4800,  3600, 16200, 36000,\n",
      "        23400, 16600, 10400, 23600, 32000, 14000, 28600,  4600, 15400,  4600,\n",
      "        35200, 32200, 31400,  3200, 19600, 32600, 29600, 31400, 14000, 26200,\n",
      "        22000, 21400, 31200, 42800, 11800, 35200, 40800, 28800, 32800, 17200,\n",
      "        10200, 45200, 12200, 14600, 35600,  6400, 43400, 32000, 24800, 22800,\n",
      "        35400, 14600, 34200, 35200, 11600,  7600, 19800, 31400, 17200, 43000,\n",
      "         9400,   400, 35000, 29000, 23200, 40400, 20800,  4800,   200,  2600,\n",
      "        20600, 28800,  3600,  3000, 25600, 14400, 34000,   600, 40200,  4400,\n",
      "        17600, 40800, 41200, 25200, 46600, 43000, 17600,  3400,  9800, 13600,\n",
      "        30800,  7000, 28800, 33400, 34600, 23000,  7400, 37200, 25800, 44200,\n",
      "        10400, 12000, 32200, 11200,  8200,  2400, 21000, 13400, 39200, 36000,\n",
      "        16000, 11000, 12600, 26800, 40200,  4200, 46000, 38400, 24800, 15400,\n",
      "        28800, 28000, 14800, 17000, 32800, 20400, 14800, 18400, 46200, 29400,\n",
      "         2200, 24800, 15000, 24800, 36800, 18200,  7000, 15800, 20600, 12600,\n",
      "        33800,   200, 33600, 14400, 33800, 28800, 25200, 20800, 31000, 44600,\n",
      "        18400, 23000,  6600, 29000, 34600,  8200,  5000, 42200, 16200,  2000,\n",
      "         3800, 34600, 24400, 45000, 38600, 13200, 23200,  5800, 26600, 17200,\n",
      "        16200, 10800, 20400,  7000,  8800, 29600, 28600, 12800, 34400, 13600,\n",
      "         6600, 16400, 13400, 19400, 45800, 36600, 27800, 19600, 22600, 24600,\n",
      "         4600,  5600, 33200, 22000,  7200, 18400, 28600, 42600, 10600,  8400,\n",
      "        32400,  1800, 44200,  4200, 28000,  8200]), 'clip_id': ['10_20131211.mat_415', '2_20140413.mat_2273', '12_20131127.mat_2989', '10_20131204.mat_793', '6_20131016.mat_3057', '10_20131204.mat_185', '15_20130709.mat_1751', '2_20140413.mat_737', '7_20131030.mat_1826', '2_20140404.mat_1714', '8_20140514.mat_758', '11_20140630.mat_915', '11_20140630.mat_158', '8_20140511.mat_958', '13_20140527.mat_2385', '5_20140411.mat_1741', '3_20140611.mat_3082', '3_20140603.mat_3282', '10_20131204.mat_1368', '12_20131127.mat_1957', '14_20140601.mat_626', '12_20131127.mat_2703', '9_20140620.mat_92', '3_20140629.mat_2587', '8_20140521.mat_2711', '6_20131016.mat_2483', '4_20140702.mat_89', '12_20131127.mat_1065', '4_20140702.mat_4', '3_20140629.mat_2570', '8_20140514.mat_2085', '15_20131016.mat_1960', '8_20140521.mat_328', '11_20140625.mat_233', '14_20140601.mat_565', '12_20131201.mat_1889', '13_20140610.mat_1805', '11_20140618.mat_2476', '14_20140601.mat_365', '2_20140419.mat_1932', '6_20130712.mat_83', '13_20140527.mat_1409', '8_20140511.mat_272', '15_20130709.mat_947', '8_20140511.mat_796', '3_20140611.mat_228', '12_20131127.mat_3065', '7_20131027.mat_3059', '14_20140615.mat_33', '14_20140615.mat_358', '8_20140514.mat_2767', '9_20140704.mat_1524', '6_20130712.mat_2728', '14_20140615.mat_55', '10_20131204.mat_3344', '7_20131030.mat_746', '3_20140611.mat_1315', '14_20140601.mat_2732', '5_20140506.mat_2562', '7_20131030.mat_3129', '9_20140627.mat_351', '8_20140521.mat_2092', '9_20140627.mat_1343', '15_20131016.mat_1029', '3_20140629.mat_1256', '5_20140418.mat_1814', '12_20131207.mat_1671', '14_20140615.mat_1551', '15_20130709.mat_1173', '2_20140413.mat_3210', '4_20140702.mat_2422', '13_20140603.mat_1452', '12_20131207.mat_2638', '3_20140611.mat_1307', '15_20131105.mat_2579', '14_20140627.mat_1907', '9_20140704.mat_147', '14_20140627.mat_1448', '14_20140627.mat_2316', '11_20140618.mat_3080', '2_20140419.mat_1401', '6_20130712.mat_574', '3_20140603.mat_1900', '15_20131105.mat_213', '2_20140419.mat_2773', '3_20140603.mat_1467', '2_20140419.mat_1495', '9_20140704.mat_2153', '8_20140511.mat_2878', '6_20131016.mat_320', '10_20131130.mat_50', '10_20131204.mat_3175', '15_20131105.mat_972', '8_20140521.mat_2082', '8_20140521.mat_2659', '5_20140411.mat_2278', '7_20131027.mat_451', '9_20140704.mat_394', '13_20140610.mat_123', '8_20140514.mat_3301', '9_20140704.mat_2658', '15_20131016.mat_984', '14_20140601.mat_170', '8_20140514.mat_2657', '8_20140514.mat_1802', '10_20131130.mat_505', '13_20140527.mat_1195', '7_20131030.mat_1685', '8_20140514.mat_1614', '15_20131016.mat_3164', '10_20131211.mat_1575', '5_20140418.mat_3189', '4_20140705.mat_2889', '7_20131106.mat_1673', '14_20140627.mat_2125', '5_20140411.mat_1730', '9_20140627.mat_1200', '8_20140521.mat_935', '14_20140615.mat_1292', '15_20130709.mat_2494', '2_20140413.mat_776', '8_20140514.mat_817', '14_20140601.mat_485', '15_20130709.mat_1111', '10_20131211.mat_2374', '8_20140511.mat_2081', '10_20131130.mat_404', '4_20140705.mat_914', '9_20140627.mat_435', '2_20140413.mat_2971', '9_20140620.mat_2569', '2_20140404.mat_203', '5_20140506.mat_440', '6_20131113.mat_2840', '10_20131204.mat_1524', '4_20140705.mat_2224', '9_20140704.mat_999', '13_20140610.mat_1761', '9_20140620.mat_2530', '2_20140404.mat_67', '7_20131106.mat_1682', '10_20131130.mat_3222', '12_20131201.mat_1672', '13_20140603.mat_1078', '7_20131027.mat_2419', '15_20131016.mat_2596', '10_20131204.mat_3224', '12_20131207.mat_1714', '4_20140621.mat_2138', '12_20131207.mat_894', '9_20140704.mat_519', '10_20131130.mat_1588', '11_20140625.mat_3348', '12_20131127.mat_523', '5_20140411.mat_2050', '15_20131105.mat_2493', '13_20140610.mat_339', '9_20140704.mat_978', '7_20131106.mat_2677', '6_20131113.mat_2426', '12_20131207.mat_314', '5_20140506.mat_3242', '15_20130709.mat_1807', '3_20140611.mat_601', '8_20140521.mat_3150', '2_20140404.mat_1117', '11_20140618.mat_903', '8_20140514.mat_2438', '7_20131106.mat_1415', '11_20140618.mat_988', '8_20140511.mat_1435', '3_20140603.mat_1051', '2_20140413.mat_308', '12_20131201.mat_758', '9_20140627.mat_3351', '10_20131211.mat_2111', '3_20140629.mat_2320', '9_20140704.mat_2806', '6_20130712.mat_1975', '8_20140514.mat_146', '10_20131130.mat_1539', '10_20131204.mat_2605', '10_20131130.mat_1603', '5_20140411.mat_1035', '13_20140527.mat_2430', '10_20131211.mat_2805', '10_20131204.mat_1131', '8_20140514.mat_78', '9_20140620.mat_102', '13_20140610.mat_1807', '12_20131201.mat_3356', '15_20131105.mat_468', '9_20140620.mat_167', '3_20140611.mat_745', '6_20131113.mat_636', '15_20131105.mat_378', '10_20131204.mat_2372', '8_20140514.mat_1015', '12_20131201.mat_1683', '14_20140627.mat_3172', '10_20131204.mat_91', '2_20140404.mat_114', '4_20140621.mat_32', '14_20140601.mat_1241', '13_20140527.mat_1701', '14_20140627.mat_952', '9_20140704.mat_2271', '4_20140705.mat_2220', '6_20130712.mat_315', '10_20131204.mat_2491', '14_20140601.mat_930', '4_20140702.mat_3122', '9_20140620.mat_3309', '9_20140620.mat_2234', '10_20131130.mat_3142', '6_20131113.mat_1162', '6_20130712.mat_1407', '11_20140625.mat_263', '3_20140629.mat_3082', '12_20131201.mat_553', '2_20140419.mat_2562', '9_20140620.mat_53', '8_20140514.mat_2816', '14_20140601.mat_2281', '4_20140702.mat_717', '6_20130712.mat_1892', '13_20140603.mat_2152', '14_20140627.mat_1355', '15_20130709.mat_1463', '12_20131127.mat_2314', '10_20131204.mat_267', '4_20140705.mat_549', '3_20140629.mat_1358', '6_20131113.mat_770', '2_20140419.mat_2710', '2_20140404.mat_1711', '7_20131030.mat_1667', '14_20140627.mat_332', '7_20131030.mat_347', '4_20140621.mat_122', '2_20140419.mat_696', '12_20131201.mat_1124', '14_20140627.mat_3115', '4_20140702.mat_1206', '12_20131127.mat_1564', '14_20140601.mat_2806', '7_20131027.mat_1887', '7_20131030.mat_2459', '11_20140618.mat_52', '9_20140620.mat_953', '10_20131211.mat_835', '5_20140418.mat_3196', '7_20131030.mat_220', '9_20140704.mat_2267', '5_20140418.mat_1668', '10_20131211.mat_2990'], 'subject_id': tensor([ 9,  1, 11,  9,  5,  9, 14,  1,  6,  1,  7, 10, 10,  7, 12,  4,  2,  2,\n",
      "         9, 11, 13, 11,  8,  2,  7,  5,  3, 11,  3,  2,  7, 14,  7, 10, 13, 11,\n",
      "        12, 10, 13,  1,  5, 12,  7, 14,  7,  2, 11,  6, 13, 13,  7,  8,  5, 13,\n",
      "         9,  6,  2, 13,  4,  6,  8,  7,  8, 14,  2,  4, 11, 13, 14,  1,  3, 12,\n",
      "        11,  2, 14, 13,  8, 13, 13, 10,  1,  5,  2, 14,  1,  2,  1,  8,  7,  5,\n",
      "         9,  9, 14,  7,  7,  4,  6,  8, 12,  7,  8, 14, 13,  7,  7,  9, 12,  6,\n",
      "         7, 14,  9,  4,  3,  6, 13,  4,  8,  7, 13, 14,  1,  7, 13, 14,  9,  7,\n",
      "         9,  3,  8,  1,  8,  1,  4,  5,  9,  3,  8, 12,  8,  1,  6,  9, 11, 12,\n",
      "         6, 14,  9, 11,  3, 11,  8,  9, 10, 11,  4, 14, 12,  8,  6,  5, 11,  4,\n",
      "        14,  2,  7,  1, 10,  7,  6, 10,  7,  2,  1, 11,  8,  9,  2,  8,  5,  7,\n",
      "         9,  9,  9,  4, 12,  9,  9,  7,  8, 12, 11, 14,  8,  2,  5, 14,  9,  7,\n",
      "        11, 13,  9,  1,  3, 13, 12, 13,  8,  3,  5,  9, 13,  3,  8,  8,  9,  5,\n",
      "         5, 10,  2, 11,  1,  8,  7, 13,  3,  5, 12, 13, 14, 11,  9,  3,  2,  5,\n",
      "         1,  1,  6, 13,  6,  3,  1, 11, 13,  3, 11, 13,  6,  6, 10,  8,  9,  4,\n",
      "         6,  8,  4,  9]), 'trial_id': ['ww_eeg2', 'jl_eeg11', 'wyw_eeg14', 'ww_eeg4', 'mhw_eeg14', 'ww_eeg1', 'zjy_eeg9', 'jl_eeg4', 'phl_eeg9', 'jl_eeg8', 'sxy_eeg4', 'wsf_eeg5', 'wsf_eeg1', 'sxy_eeg5', 'xyl_eeg11', 'ly_eeg8', 'jj_eeg14', 'jj_eeg15', 'ww_eeg7', 'wyw_eeg9', 'ys_eeg3', 'wyw_eeg12', 'wk_eeg1', 'jj_eeg12', 'sxy_eeg12', 'mhw_eeg12', 'lqj_eeg1', 'wyw_eeg5', 'lqj_eeg1', 'jj_eeg12', 'sxy_eeg10', 'zjy_eeg9', 'sxy_eeg2', 'wsf_eeg1', 'ys_eeg3', 'wyw_eeg9', 'xyl_eeg9', 'wsf_eeg11', 'ys_eeg2', 'jl_eeg9', 'mhw_eeg1', 'xyl_eeg7', 'sxy_eeg2', 'zjy_eeg5', 'sxy_eeg4', 'jj_eeg1', 'wyw_eeg14', 'phl_eeg14', 'ys_eeg1', 'ys_eeg2', 'sxy_eeg13', 'wk_eeg7', 'mhw_eeg13', 'ys_eeg1', 'ww_eeg15', 'phl_eeg4', 'jj_eeg7', 'ys_eeg13', 'ly_eeg12', 'phl_eeg14', 'wk_eeg2', 'sxy_eeg10', 'wk_eeg7', 'zjy_eeg5', 'jj_eeg6', 'ly_eeg9', 'wyw_eeg8', 'ys_eeg8', 'zjy_eeg6', 'jl_eeg15', 'lqj_eeg11', 'xyl_eeg7', 'wyw_eeg12', 'jj_eeg7', 'zjy_eeg12', 'ys_eeg9', 'wk_eeg1', 'ys_eeg7', 'ys_eeg11', 'wsf_eeg14', 'jl_eeg7', 'mhw_eeg3', 'jj_eeg9', 'zjy_eeg1', 'jl_eeg13', 'jj_eeg7', 'jl_eeg7', 'wk_eeg10', 'sxy_eeg13', 'mhw_eeg2', 'ww_eeg1', 'ww_eeg14', 'zjy_eeg5', 'sxy_eeg10', 'sxy_eeg12', 'ly_eeg11', 'phl_eeg2', 'wk_eeg2', 'xyl_eeg1', 'sxy_eeg15', 'wk_eeg12', 'zjy_eeg5', 'ys_eeg1', 'sxy_eeg12', 'sxy_eeg9', 'ww_eeg3', 'xyl_eeg6', 'phl_eeg8', 'sxy_eeg8', 'zjy_eeg14', 'ww_eeg8', 'ly_eeg15', 'lqj_eeg13', 'phl_eeg8', 'ys_eeg10', 'ly_eeg8', 'wk_eeg6', 'sxy_eeg5', 'ys_eeg7', 'zjy_eeg12', 'jl_eeg4', 'sxy_eeg4', 'ys_eeg3', 'zjy_eeg6', 'ww_eeg11', 'sxy_eeg10', 'ww_eeg2', 'lqj_eeg5', 'wk_eeg2', 'jl_eeg14', 'wk_eeg12', 'jl_eeg1', 'ly_eeg2', 'mhw_eeg13', 'ww_eeg7', 'lqj_eeg10', 'wk_eeg5', 'xyl_eeg9', 'wk_eeg12', 'jl_eeg1', 'phl_eeg8', 'ww_eeg15', 'wyw_eeg8', 'xyl_eeg5', 'phl_eeg11', 'zjy_eeg12', 'ww_eeg15', 'wyw_eeg8', 'lqj_eeg10', 'wyw_eeg4', 'wk_eeg3', 'ww_eeg8', 'wsf_eeg15', 'wyw_eeg3', 'ly_eeg10', 'zjy_eeg12', 'xyl_eeg2', 'wk_eeg5', 'phl_eeg12', 'mhw_eeg11', 'wyw_eeg2', 'ly_eeg15', 'zjy_eeg9', 'jj_eeg3', 'sxy_eeg14', 'jl_eeg6', 'wsf_eeg4', 'sxy_eeg11', 'phl_eeg7', 'wsf_eeg5', 'sxy_eeg7', 'jj_eeg5', 'jl_eeg2', 'wyw_eeg4', 'wk_eeg15', 'ww_eeg10', 'jj_eeg11', 'wk_eeg13', 'mhw_eeg9', 'sxy_eeg1', 'ww_eeg8', 'ww_eeg12', 'ww_eeg8', 'ly_eeg5', 'xyl_eeg11', 'ww_eeg13', 'ww_eeg6', 'sxy_eeg1', 'wk_eeg1', 'xyl_eeg9', 'wyw_eeg15', 'zjy_eeg3', 'wk_eeg1', 'jj_eeg4', 'mhw_eeg3', 'zjy_eeg2', 'ww_eeg11', 'sxy_eeg5', 'wyw_eeg8', 'ys_eeg14', 'ww_eeg1', 'jl_eeg1', 'lqj_eeg1', 'ys_eeg6', 'xyl_eeg8', 'ys_eeg5', 'wk_eeg11', 'lqj_eeg10', 'mhw_eeg2', 'ww_eeg12', 'ys_eeg5', 'lqj_eeg14', 'wk_eeg15', 'wk_eeg10', 'ww_eeg14', 'mhw_eeg6', 'mhw_eeg7', 'wsf_eeg2', 'jj_eeg14', 'wyw_eeg3', 'jl_eeg12', 'wk_eeg1', 'sxy_eeg13', 'ys_eeg11', 'lqj_eeg4', 'mhw_eeg9', 'xyl_eeg10', 'ys_eeg7', 'zjy_eeg7', 'wyw_eeg11', 'ww_eeg2', 'lqj_eeg3', 'jj_eeg7', 'mhw_eeg4', 'jl_eeg12', 'jl_eeg8', 'phl_eeg8', 'ys_eeg2', 'phl_eeg2', 'lqj_eeg1', 'jl_eeg4', 'wyw_eeg6', 'ys_eeg14', 'lqj_eeg6', 'wyw_eeg8', 'ys_eeg13', 'phl_eeg9', 'phl_eeg11', 'wsf_eeg1', 'wk_eeg5', 'ww_eeg4', 'ly_eeg15', 'phl_eeg1', 'wk_eeg11', 'ly_eeg8', 'ww_eeg14'], 'emotion': tensor([ 0,  0,  1, -1,  1,  1,  1, -1,  1,  0, -1,  0,  1,  0,  0,  0,  1, -1,\n",
      "        -1,  1, -1, -1,  1, -1, -1, -1,  1,  0,  1, -1,  1,  1,  0,  1, -1,  1,\n",
      "         1,  0,  0,  1,  1, -1,  0,  0, -1,  1,  1,  1,  1,  0,  0, -1,  0,  1,\n",
      "        -1, -1, -1,  0, -1,  1,  0,  1, -1,  0,  1,  1,  0,  0,  1, -1,  0, -1,\n",
      "        -1, -1, -1,  1,  1, -1,  0,  1, -1, -1,  1,  1,  0, -1, -1,  1,  0,  0,\n",
      "         1,  1,  0,  1, -1,  0,  0,  0,  1, -1, -1,  0,  1, -1,  1, -1,  1,  0,\n",
      "         0,  1,  0, -1,  0,  0,  1,  0,  1,  0, -1, -1, -1, -1, -1,  1,  0,  1,\n",
      "         0,  0,  0,  1, -1,  1,  0,  0, -1,  1,  0,  1, -1,  1,  0, -1,  0,  0,\n",
      "         0, -1, -1,  0,  1, -1, -1,  0, -1, -1,  1, -1,  0,  0, -1,  0,  0, -1,\n",
      "         1, -1,  1,  1, -1,  0, -1,  0, -1,  0,  0, -1, -1,  1,  0,  0,  1,  1,\n",
      "         0, -1,  0,  0,  0,  0,  1,  1,  1,  1, -1, -1,  1, -1, -1,  0,  0,  0,\n",
      "         0,  1,  1,  1,  1,  1,  0,  0,  0,  1,  0, -1,  0,  1, -1,  1,  1,  1,\n",
      "        -1,  0,  1, -1, -1,  1,  0,  0, -1,  1,  1, -1, -1,  0,  0, -1, -1, -1,\n",
      "        -1,  0,  0,  0,  0,  1, -1,  1,  1,  1,  0,  0,  1,  0,  1,  0, -1, -1,\n",
      "         1,  0,  0,  1]), 'date': tensor([20131211, 20140413, 20131127, 20131204, 20131016, 20131204, 20130709,\n",
      "        20140413, 20131030, 20140404, 20140514, 20140630, 20140630, 20140511,\n",
      "        20140527, 20140411, 20140611, 20140603, 20131204, 20131127, 20140601,\n",
      "        20131127, 20140620, 20140629, 20140521, 20131016, 20140702, 20131127,\n",
      "        20140702, 20140629, 20140514, 20131016, 20140521, 20140625, 20140601,\n",
      "        20131201, 20140610, 20140618, 20140601, 20140419, 20130712, 20140527,\n",
      "        20140511, 20130709, 20140511, 20140611, 20131127, 20131027, 20140615,\n",
      "        20140615, 20140514, 20140704, 20130712, 20140615, 20131204, 20131030,\n",
      "        20140611, 20140601, 20140506, 20131030, 20140627, 20140521, 20140627,\n",
      "        20131016, 20140629, 20140418, 20131207, 20140615, 20130709, 20140413,\n",
      "        20140702, 20140603, 20131207, 20140611, 20131105, 20140627, 20140704,\n",
      "        20140627, 20140627, 20140618, 20140419, 20130712, 20140603, 20131105,\n",
      "        20140419, 20140603, 20140419, 20140704, 20140511, 20131016, 20131130,\n",
      "        20131204, 20131105, 20140521, 20140521, 20140411, 20131027, 20140704,\n",
      "        20140610, 20140514, 20140704, 20131016, 20140601, 20140514, 20140514,\n",
      "        20131130, 20140527, 20131030, 20140514, 20131016, 20131211, 20140418,\n",
      "        20140705, 20131106, 20140627, 20140411, 20140627, 20140521, 20140615,\n",
      "        20130709, 20140413, 20140514, 20140601, 20130709, 20131211, 20140511,\n",
      "        20131130, 20140705, 20140627, 20140413, 20140620, 20140404, 20140506,\n",
      "        20131113, 20131204, 20140705, 20140704, 20140610, 20140620, 20140404,\n",
      "        20131106, 20131130, 20131201, 20140603, 20131027, 20131016, 20131204,\n",
      "        20131207, 20140621, 20131207, 20140704, 20131130, 20140625, 20131127,\n",
      "        20140411, 20131105, 20140610, 20140704, 20131106, 20131113, 20131207,\n",
      "        20140506, 20130709, 20140611, 20140521, 20140404, 20140618, 20140514,\n",
      "        20131106, 20140618, 20140511, 20140603, 20140413, 20131201, 20140627,\n",
      "        20131211, 20140629, 20140704, 20130712, 20140514, 20131130, 20131204,\n",
      "        20131130, 20140411, 20140527, 20131211, 20131204, 20140514, 20140620,\n",
      "        20140610, 20131201, 20131105, 20140620, 20140611, 20131113, 20131105,\n",
      "        20131204, 20140514, 20131201, 20140627, 20131204, 20140404, 20140621,\n",
      "        20140601, 20140527, 20140627, 20140704, 20140705, 20130712, 20131204,\n",
      "        20140601, 20140702, 20140620, 20140620, 20131130, 20131113, 20130712,\n",
      "        20140625, 20140629, 20131201, 20140419, 20140620, 20140514, 20140601,\n",
      "        20140702, 20130712, 20140603, 20140627, 20130709, 20131127, 20131204,\n",
      "        20140705, 20140629, 20131113, 20140419, 20140404, 20131030, 20140627,\n",
      "        20131030, 20140621, 20140419, 20131201, 20140627, 20140702, 20131127,\n",
      "        20140601, 20131027, 20131030, 20140618, 20140620, 20131211, 20140418,\n",
      "        20131030, 20140704, 20140418, 20131211]), '_record_id': ['_record_2', '_record_22', '_record_6', '_record_1', '_record_34', '_record_1', '_record_15', '_record_22', '_record_37', '_record_21', '_record_40', '_record_5', '_record_5', '_record_39', '_record_9', '_record_30', '_record_25', '_record_24', '_record_1', '_record_6', '_record_12', '_record_6', '_record_42', '_record_26', '_record_41', '_record_34', '_record_28', '_record_6', '_record_28', '_record_26', '_record_40', '_record_16', '_record_41', '_record_4', '_record_12', '_record_7', '_record_11', '_record_3', '_record_12', '_record_23', '_record_33', '_record_9', '_record_39', '_record_15', '_record_39', '_record_25', '_record_6', '_record_36', '_record_13', '_record_13', '_record_40', '_record_44', '_record_33', '_record_13', '_record_1', '_record_37', '_record_25', '_record_12', '_record_32', '_record_37', '_record_43', '_record_41', '_record_43', '_record_16', '_record_26', '_record_31', '_record_8', '_record_13', '_record_15', '_record_22', '_record_28', '_record_10', '_record_8', '_record_25', '_record_17', '_record_14', '_record_44', '_record_14', '_record_14', '_record_3', '_record_23', '_record_33', '_record_24', '_record_17', '_record_23', '_record_24', '_record_23', '_record_44', '_record_39', '_record_34', '_record_0', '_record_1', '_record_17', '_record_41', '_record_41', '_record_30', '_record_36', '_record_44', '_record_11', '_record_40', '_record_44', '_record_16', '_record_12', '_record_40', '_record_40', '_record_0', '_record_9', '_record_37', '_record_40', '_record_16', '_record_2', '_record_31', '_record_29', '_record_38', '_record_14', '_record_30', '_record_43', '_record_41', '_record_13', '_record_15', '_record_22', '_record_40', '_record_12', '_record_15', '_record_2', '_record_39', '_record_0', '_record_29', '_record_43', '_record_22', '_record_42', '_record_21', '_record_32', '_record_35', '_record_1', '_record_29', '_record_44', '_record_11', '_record_42', '_record_21', '_record_38', '_record_0', '_record_7', '_record_10', '_record_36', '_record_16', '_record_1', '_record_8', '_record_27', '_record_8', '_record_44', '_record_0', '_record_4', '_record_6', '_record_30', '_record_17', '_record_11', '_record_44', '_record_38', '_record_35', '_record_8', '_record_32', '_record_15', '_record_25', '_record_41', '_record_21', '_record_3', '_record_40', '_record_38', '_record_3', '_record_39', '_record_24', '_record_22', '_record_7', '_record_43', '_record_2', '_record_26', '_record_44', '_record_33', '_record_40', '_record_0', '_record_1', '_record_0', '_record_30', '_record_9', '_record_2', '_record_1', '_record_40', '_record_42', '_record_11', '_record_7', '_record_17', '_record_42', '_record_25', '_record_35', '_record_17', '_record_1', '_record_40', '_record_7', '_record_14', '_record_1', '_record_21', '_record_27', '_record_12', '_record_9', '_record_14', '_record_44', '_record_29', '_record_33', '_record_1', '_record_12', '_record_28', '_record_42', '_record_42', '_record_0', '_record_35', '_record_33', '_record_4', '_record_26', '_record_7', '_record_23', '_record_42', '_record_40', '_record_12', '_record_28', '_record_33', '_record_10', '_record_14', '_record_15', '_record_6', '_record_1', '_record_29', '_record_26', '_record_35', '_record_23', '_record_21', '_record_37', '_record_14', '_record_37', '_record_27', '_record_23', '_record_7', '_record_14', '_record_28', '_record_6', '_record_12', '_record_36', '_record_37', '_record_3', '_record_42', '_record_2', '_record_31', '_record_37', '_record_44', '_record_31', '_record_2'], 'segment_start': tensor([ 30,  90,  60, 165, 120,  30,  90, 165, 150,   0,  60, 150,  90,  75,\n",
      "         60,  60, 165,  60,  90,  60, 105,  90, 120, 105,  90,  75,  15,  60,\n",
      "        150, 105, 105, 150, 120,  15, 165,   0,  15,  60,  45,  45,   0,  45,\n",
      "        105, 135,  15, 120, 165,  30,  90,  45,  75,  15,  15, 120,  45,  30,\n",
      "        105,  60,  45, 120,  15,  15, 135, 105, 150,  15, 105, 135,  15,  60,\n",
      "         60,  30, 135,  60, 120,  60, 165,  15,  30,  75, 105, 150,  75,  45,\n",
      "        120, 150, 150, 135,  60, 105,   0,  60, 165,  60,  15,  75,  75,  75,\n",
      "        135, 105,  30,  15,   0, 105,  15,  15,  75, 120, 165,  60, 120, 105,\n",
      "        135,  90,   0,  60,  15,  75, 135,  45, 135,  45,  30,  45, 135, 120,\n",
      "         90, 165,  30,  75,   0,  60, 105,   0,  15, 150,  90,   0, 105, 150,\n",
      "         15,  60, 165, 120,  15,  60,   0,  45, 105, 135, 150,  60, 120,  75,\n",
      "          0, 120,  45,   0, 135,  45,  90, 165,  30, 165,   0, 165, 120,  30,\n",
      "         30,  15, 165, 165, 105, 105, 150, 165,  90, 150,  30, 135,  90,  60,\n",
      "        150,   0, 120,  45,  30,  60, 105,  60,  90, 120,  15,   0,  45,  30,\n",
      "        165,  15,  75, 150, 135,   0,  15,  30,   0,  30,  45,  30, 150, 150,\n",
      "        150,  30, 135,  60,  75, 105,   0,  75,  45,  45,   0,  60,   0, 135,\n",
      "        165,  60, 120, 135, 165, 165, 165,  60, 135, 135,  15, 165,  75,  75,\n",
      "        120,  15,  60, 120, 120,  75,  75,   0,  90, 105, 150, 135, 135,  30,\n",
      "          0,  45,  75,  30]), 'segment_end': tensor([ 60, 120,  90, 195, 150,  60, 120, 195, 180,  30,  90, 180, 120, 105,\n",
      "         90,  90, 195,  90, 120,  90, 135, 120, 150, 135, 120, 105,  45,  90,\n",
      "        180, 135, 135, 180, 150,  45, 195,  30,  45,  90,  75,  75,  30,  75,\n",
      "        135, 165,  45, 150, 195,  60, 120,  75, 105,  45,  45, 150,  75,  60,\n",
      "        135,  90,  75, 150,  45,  45, 165, 135, 180,  45, 135, 165,  45,  90,\n",
      "         90,  60, 165,  90, 150,  90, 195,  45,  60, 105, 135, 180, 105,  75,\n",
      "        150, 180, 180, 165,  90, 135,  30,  90, 195,  90,  45, 105, 105, 105,\n",
      "        165, 135,  60,  45,  30, 135,  45,  45, 105, 150, 195,  90, 150, 135,\n",
      "        165, 120,  30,  90,  45, 105, 165,  75, 165,  75,  60,  75, 165, 150,\n",
      "        120, 195,  60, 105,  30,  90, 135,  30,  45, 180, 120,  30, 135, 180,\n",
      "         45,  90, 195, 150,  45,  90,  30,  75, 135, 165, 180,  90, 150, 105,\n",
      "         30, 150,  75,  30, 165,  75, 120, 195,  60, 195,  30, 195, 150,  60,\n",
      "         60,  45, 195, 195, 135, 135, 180, 195, 120, 180,  60, 165, 120,  90,\n",
      "        180,  30, 150,  75,  60,  90, 135,  90, 120, 150,  45,  30,  75,  60,\n",
      "        195,  45, 105, 180, 165,  30,  45,  60,  30,  60,  75,  60, 180, 180,\n",
      "        180,  60, 165,  90, 105, 135,  30, 105,  75,  75,  30,  90,  30, 165,\n",
      "        195,  90, 150, 165, 195, 195, 195,  90, 165, 165,  45, 195, 105, 105,\n",
      "        150,  45,  90, 150, 150, 105, 105,  30, 120, 135, 180, 165, 165,  60,\n",
      "         30,  75, 105,  60])}\n",
      "Batch 1: Data Shape = torch.Size([256, 62, 30]), Labels = {'start_at': tensor([ 8200, 18400, 26800, 34000, 35600, 31800, 40600, 18200, 28000, 22600,\n",
      "        44600, 47000, 12600, 32000, 43400, 23200, 10400, 14000,  1600, 12400,\n",
      "         2400, 38400,  9800, 36400, 46600, 14400, 13000,  9000,   400, 35800,\n",
      "        45800,  5800,  6400,  8600,  7800, 40600,  2200, 29200, 10000, 13000,\n",
      "        20000, 29600, 24200, 29800, 28000,  9800, 17400, 35800, 27000, 15800,\n",
      "        33000, 14000, 42600,  8600, 37600, 43800, 32000, 47200, 42400, 19200,\n",
      "        35800, 38000, 27800, 33400, 19600, 12400, 25000, 35400,  2000,  1800,\n",
      "        34600,   800, 40200, 44800, 24200,  2000,  1800, 37600, 26200, 19200,\n",
      "         9000,  9000, 22400,  5200, 26000, 51600, 32800, 15000, 39000,   400,\n",
      "        14600, 29800, 40400,  8000, 34200, 22600,  4800,  1400, 21800,  9600,\n",
      "        39600, 14200, 14800, 20000, 40800, 25400, 26600, 32600, 20800, 21600,\n",
      "         7200,  3800, 27200, 28800, 18400, 16200,  8400,  3800, 37400,  4400,\n",
      "        23400,  1000, 20400,   800, 15000, 31400,  8200, 33600, 24600, 43800,\n",
      "        39400, 30000, 19600,  1600, 19200, 31200,  2800, 10600, 31200, 30600,\n",
      "        21000, 16200, 13400, 16400, 20000, 24000, 27600, 25400, 26400, 25800,\n",
      "        46000,  7200, 21800, 30800, 43000,  2600, 29400,  4400,  8600, 46600,\n",
      "        18200, 17400, 12400, 20400, 25200, 31800,  2600, 38800, 33800, 17800,\n",
      "        10800, 27800, 24000,  3200, 14800,  3000,  4200, 16400, 42800, 15000,\n",
      "        14800, 35400, 30000, 17000, 29600, 35400, 22200, 23000, 19200, 30200,\n",
      "        28200, 13600, 24000,  9200, 42600, 21600, 39000,  5000, 40600,  7000,\n",
      "        45600, 41400, 10600, 39800, 23400,   800, 31800,  5200,  2800, 33400,\n",
      "        41800, 22200, 23000, 35600,  8800,  5600, 17200, 15800, 35200, 41800,\n",
      "        24800,   800, 15800, 31200, 34000, 18800, 21800,  1000, 33800, 14400,\n",
      "        50600, 37800, 48600,  1200, 31200,  8400, 17800, 40400, 12200, 28800,\n",
      "        10000, 33800, 47000, 29600, 26400,  7400,  8800, 35800, 22400, 12600,\n",
      "        13400, 32400,   200, 12600,  4000,   600]), 'end_at': tensor([ 8400, 18600, 27000, 34200, 35800, 32000, 40800, 18400, 28200, 22800,\n",
      "        44800, 47200, 12800, 32200, 43600, 23400, 10600, 14200,  1800, 12600,\n",
      "         2600, 38600, 10000, 36600, 46800, 14600, 13200,  9200,   600, 36000,\n",
      "        46000,  6000,  6600,  8800,  8000, 40800,  2400, 29400, 10200, 13200,\n",
      "        20200, 29800, 24400, 30000, 28200, 10000, 17600, 36000, 27200, 16000,\n",
      "        33200, 14200, 42800,  8800, 37800, 44000, 32200, 47400, 42600, 19400,\n",
      "        36000, 38200, 28000, 33600, 19800, 12600, 25200, 35600,  2200,  2000,\n",
      "        34800,  1000, 40400, 45000, 24400,  2200,  2000, 37800, 26400, 19400,\n",
      "         9200,  9200, 22600,  5400, 26200, 51800, 33000, 15200, 39200,   600,\n",
      "        14800, 30000, 40600,  8200, 34400, 22800,  5000,  1600, 22000,  9800,\n",
      "        39800, 14400, 15000, 20200, 41000, 25600, 26800, 32800, 21000, 21800,\n",
      "         7400,  4000, 27400, 29000, 18600, 16400,  8600,  4000, 37600,  4600,\n",
      "        23600,  1200, 20600,  1000, 15200, 31600,  8400, 33800, 24800, 44000,\n",
      "        39600, 30200, 19800,  1800, 19400, 31400,  3000, 10800, 31400, 30800,\n",
      "        21200, 16400, 13600, 16600, 20200, 24200, 27800, 25600, 26600, 26000,\n",
      "        46200,  7400, 22000, 31000, 43200,  2800, 29600,  4600,  8800, 46800,\n",
      "        18400, 17600, 12600, 20600, 25400, 32000,  2800, 39000, 34000, 18000,\n",
      "        11000, 28000, 24200,  3400, 15000,  3200,  4400, 16600, 43000, 15200,\n",
      "        15000, 35600, 30200, 17200, 29800, 35600, 22400, 23200, 19400, 30400,\n",
      "        28400, 13800, 24200,  9400, 42800, 21800, 39200,  5200, 40800,  7200,\n",
      "        45800, 41600, 10800, 40000, 23600,  1000, 32000,  5400,  3000, 33600,\n",
      "        42000, 22400, 23200, 35800,  9000,  5800, 17400, 16000, 35400, 42000,\n",
      "        25000,  1000, 16000, 31400, 34200, 19000, 22000,  1200, 34000, 14600,\n",
      "        50800, 38000, 48800,  1400, 31400,  8600, 18000, 40600, 12400, 29000,\n",
      "        10200, 34000, 47200, 29800, 26600,  7600,  9000, 36000, 22600, 12800,\n",
      "        13600, 32600,   400, 12800,  4200,   800]), 'clip_id': ['11_20140630.mat_1786', '7_20131106.mat_1384', '2_20140413.mat_1426', '13_20140603.mat_3120', '2_20140404.mat_646', '6_20130712.mat_2641', '14_20140615.mat_3391', '10_20131130.mat_2806', '11_20140630.mat_3328', '14_20140601.mat_1025', '10_20131130.mat_2470', '10_20131211.mat_3185', '11_20140625.mat_1355', '5_20140411.mat_2875', '6_20130712.mat_1509', '8_20140521.mat_1645', '2_20140419.mat_287', '6_20130712.mat_305', '14_20140615.mat_2958', '12_20131127.mat_3250', '8_20140514.mat_12', '6_20131016.mat_192', '7_20131106.mat_1341', '7_20131027.mat_856', '7_20131030.mat_1525', '3_20140629.mat_1601', '7_20131027.mat_1357', '6_20130712.mat_719', '5_20140411.mat_237', '12_20131201.mat_414', '3_20140603.mat_903', '5_20140418.mat_1558', '15_20131105.mat_32', '13_20140610.mat_955', '4_20140705.mat_951', '7_20131106.mat_877', '4_20140702.mat_685', '7_20131027.mat_1891', '4_20140705.mat_962', '10_20131204.mat_1162', '10_20131211.mat_3050', '9_20140627.mat_2863', '2_20140413.mat_795', '10_20131130.mat_1246', '14_20140601.mat_1052', '4_20140621.mat_2531', '11_20140625.mat_3275', '10_20131204.mat_2189', '7_20131106.mat_370', '8_20140521.mat_3029', '3_20140629.mat_2880', '3_20140629.mat_70', '15_20131105.mat_2223', '8_20140514.mat_717', '12_20131127.mat_2903', '15_20130709.mat_2934', '13_20140603.mat_1452', '5_20140411.mat_2246', '6_20130712.mat_447', '4_20140702.mat_564', '9_20140704.mat_1276', '4_20140702.mat_1719', '11_20140630.mat_813', '2_20140419.mat_2882', '2_20140419.mat_98', '7_20131106.mat_1591', '10_20131130.mat_593', '8_20140521.mat_2659', '12_20131201.mat_2257', '3_20140603.mat_2959', '9_20140620.mat_2183', '9_20140620.mat_1533', '13_20140603.mat_3389', '7_20131030.mat_2471', '8_20140514.mat_1218', '4_20140702.mat_245', '9_20140704.mat_683', '13_20140527.mat_2903', '5_20140418.mat_3081', '14_20140601.mat_1193', '13_20140527.mat_1337', '15_20131105.mat_3233', '14_20140627.mat_2122', '10_20131130.mat_1318', '7_20131106.mat_2140', '3_20140629.mat_2003', '10_20131211.mat_1076', '15_20131016.mat_3263', '4_20140621.mat_1487', '3_20140603.mat_914', '10_20131130.mat_308', '2_20140419.mat_1894', '8_20140511.mat_3390', '13_20140610.mat_2522', '4_20140705.mat_2181', '15_20131016.mat_3301', '6_20131113.mat_3212', '4_20140705.mat_7', '13_20140603.mat_1021', '14_20140627.mat_1145', '15_20130709.mat_2913', '3_20140603.mat_71', '6_20130712.mat_74', '13_20140610.mat_3288', '9_20140627.mat_2686', '2_20140419.mat_1656', '4_20140705.mat_3083', '2_20140419.mat_631', '4_20140621.mat_2351', '11_20140625.mat_782', '5_20140506.mat_2986', '6_20131113.mat_1311', '12_20131207.mat_1233', '4_20140621.mat_3332', '4_20140705.mat_92', '6_20130712.mat_316', '10_20131211.mat_2757', '9_20140627.mat_1311', '13_20140603.mat_2197', '8_20140514.mat_2269', '13_20140603.mat_2599', '11_20140618.mat_5', '11_20140618.mat_3052', '12_20131201.mat_2486', '9_20140620.mat_75', '11_20140630.mat_2404', '8_20140521.mat_2288', '13_20140603.mat_403', '10_20131204.mat_3073', '7_20131027.mat_2466', '13_20140527.mat_1726', '3_20140629.mat_3100', '5_20140411.mat_1627', '6_20131113.mat_2723', '10_20131211.mat_2343', '15_20130709.mat_624', '8_20140511.mat_2496', '6_20131113.mat_965', '11_20140618.mat_3344', '8_20140514.mat_2400', '2_20140413.mat_2820', '12_20131201.mat_3031', '15_20130709.mat_3255', '7_20131027.mat_994', '2_20140413.mat_1845', '5_20140506.mat_1865', '9_20140620.mat_2620', '11_20140630.mat_127', '13_20140603.mat_1877', '6_20130712.mat_1658', '11_20140618.mat_2240', '11_20140618.mat_2751', '10_20131130.mat_1854', '12_20131207.mat_389', '7_20131106.mat_1507', '13_20140610.mat_481', '8_20140514.mat_1676', '4_20140705.mat_1119', '5_20140506.mat_2993', '4_20140705.mat_2243', '9_20140704.mat_765', '4_20140621.mat_3275', '4_20140621.mat_974', '3_20140603.mat_2817', '9_20140704.mat_1223', '13_20140610.mat_3347', '12_20131207.mat_481', '13_20140603.mat_2441', '2_20140419.mat_1266', '11_20140630.mat_324', '11_20140630.mat_2301', '10_20131211.mat_2386', '5_20140411.mat_794', '15_20131016.mat_1761', '15_20131016.mat_74', '15_20131016.mat_15', '15_20130709.mat_2971', '5_20140506.mat_550', '5_20140411.mat_3164', '12_20131201.mat_3025', '8_20140511.mat_1366', '4_20140621.mat_2659', '11_20140630.mat_618', '5_20140411.mat_1830', '12_20131127.mat_383', '2_20140413.mat_177', '5_20140418.mat_1023', '2_20140413.mat_2125', '4_20140705.mat_1841', '13_20140527.mat_2866', '14_20140615.mat_1433', '5_20140411.mat_2078', '7_20131027.mat_3070', '7_20131106.mat_514', '4_20140621.mat_2928', '14_20140627.mat_1205', '2_20140413.mat_3383', '6_20131113.mat_1770', '12_20131201.mat_2685', '15_20131105.mat_35', '8_20140511.mat_2475', '2_20140419.mat_2454', '11_20140630.mat_288', '12_20131201.mat_434', '2_20140419.mat_2127', '10_20131211.mat_1101', '2_20140419.mat_1451', '9_20140620.mat_26', '4_20140621.mat_249', '5_20140411.mat_1079', '5_20140411.mat_1501', '3_20140629.mat_1208', '6_20130712.mat_2597', '15_20131016.mat_3128', '2_20140419.mat_2759', '9_20140704.mat_263', '15_20131105.mat_998', '5_20140506.mat_79', '8_20140511.mat_1921', '6_20131113.mat_1954', '11_20140630.mat_1416', '13_20140603.mat_678', '6_20130712.mat_1176', '15_20131105.mat_1685', '13_20140527.mat_2652', '14_20140627.mat_1191', '7_20131030.mat_2824', '8_20140521.mat_2015', '3_20140611.mat_1461', '2_20140419.mat_1169', '6_20131113.mat_1998', '6_20130712.mat_424', '7_20131027.mat_1988', '15_20131105.mat_1298', '11_20140630.mat_391', '9_20140620.mat_277', '4_20140702.mat_763', '5_20140418.mat_1731', '6_20131016.mat_1806', '12_20131201.mat_612', '5_20140418.mat_2532', '13_20140603.mat_637', '13_20140610.mat_1527', '15_20130709.mat_1245', '2_20140404.mat_1661', '9_20140704.mat_2987', '15_20130709.mat_2291', '5_20140506.mat_1471', '6_20130712.mat_1857', '2_20140404.mat_1355', '11_20140630.mat_3255', '2_20140413.mat_397', '4_20140705.mat_2716', '2_20140404.mat_737', '15_20131016.mat_3208', '11_20140625.mat_471'], 'subject_id': tensor([10,  6,  1, 12,  1,  5, 13,  9, 10, 13,  9,  9, 10,  4,  5,  7,  1,  5,\n",
      "        13, 11,  7,  5,  6,  6,  6,  2,  6,  5,  4, 11,  2,  4, 14, 12,  3,  6,\n",
      "         3,  6,  3,  9,  9,  8,  1,  9, 13,  3, 10,  9,  6,  7,  2,  2, 14,  7,\n",
      "        11, 14, 12,  4,  5,  3,  8,  3, 10,  1,  1,  6,  9,  7, 11,  2,  8,  8,\n",
      "        12,  6,  7,  3,  8, 12,  4, 13, 12, 14, 13,  9,  6,  2,  9, 14,  3,  2,\n",
      "         9,  1,  7, 12,  3, 14,  5,  3, 12, 13, 14,  2,  5, 12,  8,  1,  3,  1,\n",
      "         3, 10,  4,  5, 11,  3,  3,  5,  9,  8, 12,  7, 12, 10, 10, 11,  8, 10,\n",
      "         7, 12,  9,  6, 12,  2,  4,  5,  9, 14,  7,  5, 10,  7,  1, 11, 14,  6,\n",
      "         1,  4,  8, 10, 12,  5, 10, 10,  9, 11,  6, 12,  7,  3,  4,  3,  8,  3,\n",
      "         3,  2,  8, 12, 11, 12,  1, 10, 10,  9,  4, 14, 14, 14, 14,  4,  4, 11,\n",
      "         7,  3, 10,  4, 11,  1,  4,  1,  3, 12, 13,  4,  6,  6,  3, 13,  1,  5,\n",
      "        11, 14,  7,  1, 10, 11,  1,  9,  1,  8,  3,  4,  4,  2,  5, 14,  1,  8,\n",
      "        14,  4,  7,  5, 10, 12,  5, 14, 12, 13,  6,  7,  2,  1,  5,  5,  6, 14,\n",
      "        10,  8,  3,  4,  5, 11,  4, 12, 12, 14,  1,  8, 14,  4,  5,  1, 10,  1,\n",
      "         3,  1, 14, 10]), 'trial_id': ['wsf_eeg9', 'phl_eeg7', 'jl_eeg7', 'xyl_eeg14', 'jl_eeg3', 'mhw_eeg12', 'ys_eeg15', 'ww_eeg13', 'wsf_eeg15', 'ys_eeg5', 'ww_eeg11', 'ww_eeg14', 'wsf_eeg7', 'ly_eeg13', 'mhw_eeg7', 'sxy_eeg8', 'jl_eeg2', 'mhw_eeg2', 'ys_eeg14', 'wyw_eeg15', 'sxy_eeg1', 'mhw_eeg1', 'phl_eeg7', 'phl_eeg4', 'phl_eeg7', 'jj_eeg8', 'phl_eeg7', 'mhw_eeg4', 'ly_eeg2', 'wyw_eeg2', 'jj_eeg4', 'ly_eeg8', 'zjy_eeg1', 'xyl_eeg5', 'lqj_eeg5', 'phl_eeg4', 'lqj_eeg4', 'phl_eeg9', 'lqj_eeg5', 'ww_eeg6', 'ww_eeg14', 'wk_eeg13', 'jl_eeg4', 'ww_eeg6', 'ys_eeg5', 'lqj_eeg12', 'wsf_eeg15', 'ww_eeg10', 'phl_eeg2', 'sxy_eeg14', 'jj_eeg13', 'jj_eeg1', 'zjy_eeg10', 'sxy_eeg4', 'wyw_eeg13', 'zjy_eeg13', 'xyl_eeg7', 'ly_eeg10', 'mhw_eeg2', 'lqj_eeg3', 'wk_eeg6', 'lqj_eeg8', 'wsf_eeg4', 'jl_eeg13', 'jl_eeg1', 'phl_eeg8', 'ww_eeg3', 'sxy_eeg12', 'wyw_eeg11', 'jj_eeg14', 'wk_eeg10', 'wk_eeg8', 'xyl_eeg15', 'phl_eeg11', 'sxy_eeg6', 'lqj_eeg2', 'wk_eeg4', 'xyl_eeg13', 'ly_eeg14', 'ys_eeg6', 'xyl_eeg7', 'zjy_eeg15', 'ys_eeg10', 'ww_eeg7', 'phl_eeg10', 'jj_eeg9', 'ww_eeg5', 'zjy_eeg15', 'lqj_eeg7', 'jj_eeg5', 'ww_eeg2', 'jl_eeg9', 'sxy_eeg15', 'xyl_eeg12', 'lqj_eeg10', 'zjy_eeg15', 'mhw_eeg15', 'lqj_eeg1', 'xyl_eeg5', 'ys_eeg6', 'zjy_eeg13', 'jj_eeg1', 'mhw_eeg1', 'xyl_eeg15', 'wk_eeg12', 'jl_eeg8', 'lqj_eeg14', 'jl_eeg3', 'lqj_eeg11', 'wsf_eeg4', 'ly_eeg14', 'mhw_eeg7', 'wyw_eeg6', 'lqj_eeg15', 'lqj_eeg1', 'mhw_eeg2', 'ww_eeg13', 'wk_eeg7', 'xyl_eeg10', 'sxy_eeg11', 'xyl_eeg12', 'wsf_eeg1', 'wsf_eeg14', 'wyw_eeg12', 'wk_eeg1', 'wsf_eeg11', 'sxy_eeg11', 'xyl_eeg2', 'ww_eeg14', 'phl_eeg11', 'xyl_eeg8', 'jj_eeg14', 'ly_eeg8', 'mhw_eeg13', 'ww_eeg11', 'zjy_eeg3', 'sxy_eeg12', 'mhw_eeg5', 'wsf_eeg15', 'sxy_eeg11', 'jl_eeg13', 'wyw_eeg14', 'zjy_eeg15', 'phl_eeg5', 'jl_eeg9', 'ly_eeg9', 'wk_eeg12', 'wsf_eeg1', 'xyl_eeg9', 'mhw_eeg8', 'wsf_eeg10', 'wsf_eeg13', 'ww_eeg9', 'wyw_eeg2', 'phl_eeg7', 'xyl_eeg3', 'sxy_eeg8', 'lqj_eeg6', 'ly_eeg14', 'lqj_eeg10', 'wk_eeg4', 'lqj_eeg15', 'lqj_eeg5', 'jj_eeg13', 'wk_eeg6', 'xyl_eeg15', 'wyw_eeg3', 'xyl_eeg11', 'jl_eeg6', 'wsf_eeg2', 'wsf_eeg11', 'ww_eeg11', 'ly_eeg4', 'zjy_eeg9', 'zjy_eeg1', 'zjy_eeg1', 'zjy_eeg14', 'ly_eeg3', 'ly_eeg14', 'wyw_eeg14', 'sxy_eeg7', 'lqj_eeg12', 'wsf_eeg3', 'ly_eeg9', 'wyw_eeg2', 'jl_eeg1', 'ly_eeg5', 'jl_eeg10', 'lqj_eeg9', 'xyl_eeg13', 'ys_eeg7', 'ly_eeg10', 'phl_eeg14', 'phl_eeg3', 'lqj_eeg13', 'ys_eeg6', 'jl_eeg15', 'mhw_eeg9', 'wyw_eeg12', 'zjy_eeg1', 'sxy_eeg11', 'jl_eeg11', 'wsf_eeg2', 'wyw_eeg2', 'jl_eeg10', 'ww_eeg6', 'jl_eeg7', 'wk_eeg1', 'lqj_eeg2', 'ly_eeg5', 'ly_eeg7', 'jj_eeg6', 'mhw_eeg12', 'zjy_eeg14', 'jl_eeg13', 'wk_eeg2', 'zjy_eeg5', 'ly_eeg1', 'sxy_eeg9', 'mhw_eeg9', 'wsf_eeg7', 'xyl_eeg4', 'mhw_eeg6', 'zjy_eeg8', 'xyl_eeg12', 'ys_eeg6', 'phl_eeg13', 'sxy_eeg10', 'jj_eeg7', 'jl_eeg6', 'mhw_eeg9', 'mhw_eeg2', 'phl_eeg9', 'zjy_eeg7', 'wsf_eeg2', 'wk_eeg2', 'lqj_eeg4', 'ly_eeg8', 'mhw_eeg9', 'wyw_eeg3', 'ly_eeg12', 'xyl_eeg3', 'xyl_eeg7', 'zjy_eeg6', 'jl_eeg8', 'wk_eeg14', 'zjy_eeg11', 'ly_eeg7', 'mhw_eeg9', 'jl_eeg7', 'wsf_eeg15', 'jl_eeg2', 'lqj_eeg13', 'jl_eeg4', 'zjy_eeg15', 'wsf_eeg3'], 'emotion': tensor([ 1, -1, -1,  1, -1, -1, -1,  0, -1,  0,  0,  1, -1,  0, -1,  0,  0,  0,\n",
      "         1, -1,  1,  1, -1, -1, -1,  0, -1, -1,  0,  0, -1,  0,  1,  0,  0, -1,\n",
      "        -1,  1,  0,  1,  1,  0, -1,  1,  0, -1, -1,  1,  0,  1,  0,  1,  1, -1,\n",
      "         0,  0, -1,  1,  0, -1,  1,  0, -1,  0,  1,  0, -1, -1,  0,  1,  1,  0,\n",
      "        -1,  0,  1,  0, -1,  0,  1,  1, -1, -1,  1, -1,  1,  1,  0, -1, -1,  0,\n",
      "         0,  1, -1, -1,  1, -1, -1,  1,  0,  1,  0,  1,  1, -1, -1,  0,  1, -1,\n",
      "         0, -1,  1, -1,  1, -1,  1,  0,  0, -1,  1,  0, -1,  1,  1, -1,  1,  0,\n",
      "         0,  0,  1,  0,  0,  1,  0,  0,  0, -1, -1,  0, -1,  0,  0,  1, -1,  0,\n",
      "         1,  1, -1,  1,  1,  0,  1,  0,  1,  0, -1, -1,  0,  1,  1,  1, -1, -1,\n",
      "         0,  0,  1, -1, -1,  0,  1,  0,  0,  0, -1,  1,  1,  1,  1, -1,  1,  1,\n",
      "        -1, -1, -1,  1,  0,  1,  0,  1,  1,  0, -1,  1,  1, -1,  0,  1, -1,  1,\n",
      "        -1,  1,  0,  0,  0,  0,  1,  1, -1,  1,  0,  0, -1,  1, -1,  1,  0,  0,\n",
      "         0,  1,  1,  1, -1, -1,  1,  0, -1,  1,  0,  1, -1,  1,  1,  0,  1, -1,\n",
      "         0,  0, -1,  0,  1, -1, -1, -1, -1,  1,  0,  1,  0, -1,  1, -1, -1,  0,\n",
      "         0, -1, -1, -1]), 'date': tensor([20140630, 20131106, 20140413, 20140603, 20140404, 20130712, 20140615,\n",
      "        20131130, 20140630, 20140601, 20131130, 20131211, 20140625, 20140411,\n",
      "        20130712, 20140521, 20140419, 20130712, 20140615, 20131127, 20140514,\n",
      "        20131016, 20131106, 20131027, 20131030, 20140629, 20131027, 20130712,\n",
      "        20140411, 20131201, 20140603, 20140418, 20131105, 20140610, 20140705,\n",
      "        20131106, 20140702, 20131027, 20140705, 20131204, 20131211, 20140627,\n",
      "        20140413, 20131130, 20140601, 20140621, 20140625, 20131204, 20131106,\n",
      "        20140521, 20140629, 20140629, 20131105, 20140514, 20131127, 20130709,\n",
      "        20140603, 20140411, 20130712, 20140702, 20140704, 20140702, 20140630,\n",
      "        20140419, 20140419, 20131106, 20131130, 20140521, 20131201, 20140603,\n",
      "        20140620, 20140620, 20140603, 20131030, 20140514, 20140702, 20140704,\n",
      "        20140527, 20140418, 20140601, 20140527, 20131105, 20140627, 20131130,\n",
      "        20131106, 20140629, 20131211, 20131016, 20140621, 20140603, 20131130,\n",
      "        20140419, 20140511, 20140610, 20140705, 20131016, 20131113, 20140705,\n",
      "        20140603, 20140627, 20130709, 20140603, 20130712, 20140610, 20140627,\n",
      "        20140419, 20140705, 20140419, 20140621, 20140625, 20140506, 20131113,\n",
      "        20131207, 20140621, 20140705, 20130712, 20131211, 20140627, 20140603,\n",
      "        20140514, 20140603, 20140618, 20140618, 20131201, 20140620, 20140630,\n",
      "        20140521, 20140603, 20131204, 20131027, 20140527, 20140629, 20140411,\n",
      "        20131113, 20131211, 20130709, 20140511, 20131113, 20140618, 20140514,\n",
      "        20140413, 20131201, 20130709, 20131027, 20140413, 20140506, 20140620,\n",
      "        20140630, 20140603, 20130712, 20140618, 20140618, 20131130, 20131207,\n",
      "        20131106, 20140610, 20140514, 20140705, 20140506, 20140705, 20140704,\n",
      "        20140621, 20140621, 20140603, 20140704, 20140610, 20131207, 20140603,\n",
      "        20140419, 20140630, 20140630, 20131211, 20140411, 20131016, 20131016,\n",
      "        20131016, 20130709, 20140506, 20140411, 20131201, 20140511, 20140621,\n",
      "        20140630, 20140411, 20131127, 20140413, 20140418, 20140413, 20140705,\n",
      "        20140527, 20140615, 20140411, 20131027, 20131106, 20140621, 20140627,\n",
      "        20140413, 20131113, 20131201, 20131105, 20140511, 20140419, 20140630,\n",
      "        20131201, 20140419, 20131211, 20140419, 20140620, 20140621, 20140411,\n",
      "        20140411, 20140629, 20130712, 20131016, 20140419, 20140704, 20131105,\n",
      "        20140506, 20140511, 20131113, 20140630, 20140603, 20130712, 20131105,\n",
      "        20140527, 20140627, 20131030, 20140521, 20140611, 20140419, 20131113,\n",
      "        20130712, 20131027, 20131105, 20140630, 20140620, 20140702, 20140418,\n",
      "        20131016, 20131201, 20140418, 20140603, 20140610, 20130709, 20140404,\n",
      "        20140704, 20130709, 20140506, 20130712, 20140404, 20140630, 20140413,\n",
      "        20140705, 20140404, 20131016, 20140625]), '_record_id': ['_record_5', '_record_38', '_record_22', '_record_10', '_record_21', '_record_33', '_record_13', '_record_0', '_record_5', '_record_12', '_record_0', '_record_2', '_record_4', '_record_30', '_record_33', '_record_41', '_record_23', '_record_33', '_record_13', '_record_6', '_record_40', '_record_34', '_record_38', '_record_36', '_record_37', '_record_26', '_record_36', '_record_33', '_record_30', '_record_7', '_record_24', '_record_31', '_record_17', '_record_11', '_record_29', '_record_38', '_record_28', '_record_36', '_record_29', '_record_1', '_record_2', '_record_43', '_record_22', '_record_0', '_record_12', '_record_27', '_record_4', '_record_1', '_record_38', '_record_41', '_record_26', '_record_26', '_record_17', '_record_40', '_record_6', '_record_15', '_record_10', '_record_30', '_record_33', '_record_28', '_record_44', '_record_28', '_record_5', '_record_23', '_record_23', '_record_38', '_record_0', '_record_41', '_record_7', '_record_24', '_record_42', '_record_42', '_record_10', '_record_37', '_record_40', '_record_28', '_record_44', '_record_9', '_record_31', '_record_12', '_record_9', '_record_17', '_record_14', '_record_0', '_record_38', '_record_26', '_record_2', '_record_16', '_record_27', '_record_24', '_record_0', '_record_23', '_record_39', '_record_11', '_record_29', '_record_16', '_record_35', '_record_29', '_record_10', '_record_14', '_record_15', '_record_24', '_record_33', '_record_11', '_record_43', '_record_23', '_record_29', '_record_23', '_record_27', '_record_4', '_record_32', '_record_35', '_record_8', '_record_27', '_record_29', '_record_33', '_record_2', '_record_43', '_record_10', '_record_40', '_record_10', '_record_3', '_record_3', '_record_7', '_record_42', '_record_5', '_record_41', '_record_10', '_record_1', '_record_36', '_record_9', '_record_26', '_record_30', '_record_35', '_record_2', '_record_15', '_record_39', '_record_35', '_record_3', '_record_40', '_record_22', '_record_7', '_record_15', '_record_36', '_record_22', '_record_32', '_record_42', '_record_5', '_record_10', '_record_33', '_record_3', '_record_3', '_record_0', '_record_8', '_record_38', '_record_11', '_record_40', '_record_29', '_record_32', '_record_29', '_record_44', '_record_27', '_record_27', '_record_24', '_record_44', '_record_11', '_record_8', '_record_10', '_record_23', '_record_5', '_record_5', '_record_2', '_record_30', '_record_16', '_record_16', '_record_16', '_record_15', '_record_32', '_record_30', '_record_7', '_record_39', '_record_27', '_record_5', '_record_30', '_record_6', '_record_22', '_record_31', '_record_22', '_record_29', '_record_9', '_record_13', '_record_30', '_record_36', '_record_38', '_record_27', '_record_14', '_record_22', '_record_35', '_record_7', '_record_17', '_record_39', '_record_23', '_record_5', '_record_7', '_record_23', '_record_2', '_record_23', '_record_42', '_record_27', '_record_30', '_record_30', '_record_26', '_record_33', '_record_16', '_record_23', '_record_44', '_record_17', '_record_32', '_record_39', '_record_35', '_record_5', '_record_10', '_record_33', '_record_17', '_record_9', '_record_14', '_record_37', '_record_41', '_record_25', '_record_23', '_record_35', '_record_33', '_record_36', '_record_17', '_record_5', '_record_42', '_record_28', '_record_31', '_record_34', '_record_7', '_record_31', '_record_10', '_record_11', '_record_15', '_record_21', '_record_44', '_record_15', '_record_32', '_record_33', '_record_21', '_record_5', '_record_22', '_record_29', '_record_21', '_record_16', '_record_4'], 'segment_start': tensor([ 60, 120,  30,  15,   0,  45, 105,   0,  45,  45, 150, 135, 120, 135,\n",
      "        105,  90,  45, 135, 120,  60,  45,  30,  90,  75, 150, 150,  45, 165,\n",
      "         90, 135,  30,  45,  90, 150,  15,   0, 120, 120, 165,  60,  30,  60,\n",
      "         75,   0,  45,  30, 135,  60,  30,  15, 135,  30, 135, 165,  60, 120,\n",
      "         15, 120,  60, 150, 150, 135,  60,  60,  60,  30,  75, 120, 165,  15,\n",
      "        105, 150,  75, 165,  30,  45, 105,  60, 165,  90,  75,  60,  60,  15,\n",
      "          0, 120, 150,  60, 150,  90,  90, 120, 135, 165,  45,  15, 135,  75,\n",
      "        105, 135,  60,  60,  15, 150,   0,  75, 135,   0,   0, 150,  75, 105,\n",
      "        135, 135,  90, 135,  60, 165, 105,  90,  45, 105,  75, 105,  90,  60,\n",
      "         60,   0,  60, 150,   0,  60,  75,  90,  60, 150,  15, 135, 165, 165,\n",
      "         90, 135,  60,  75,  90, 165, 165, 105, 150,  75,  90, 150,  45, 165,\n",
      "         45, 165,  60,   0, 105, 135,  30,  45,   0, 105,  75,  60,  60,  45,\n",
      "        135, 105, 105, 105,  75,   0, 165,   0,  90,  75, 150, 150, 120, 120,\n",
      "        150,  30, 120, 120,  75, 105, 135, 120,   0, 150,  15,   0,  60,  15,\n",
      "        105,  75,  60,  90,  75,  75,   0, 135,  75,  75,  15,  90,  45, 105,\n",
      "         90,  90, 135,   0,  75, 150,  75, 105, 120,   0,  60,  45,  45,  45,\n",
      "        165,   0,  30, 120, 150,  30,  60,  30,  60,  15, 165,  60,  90, 165,\n",
      "         30, 105, 135, 105,  60,   0,  15, 135, 150,  60, 105, 165,   0,  75,\n",
      "        120, 165, 135, 105]), 'segment_end': tensor([ 90, 150,  60,  45,  30,  75, 135,  30,  75,  75, 180, 165, 150, 165,\n",
      "        135, 120,  75, 165, 150,  90,  75,  60, 120, 105, 180, 180,  75, 195,\n",
      "        120, 165,  60,  75, 120, 180,  45,  30, 150, 150, 195,  90,  60,  90,\n",
      "        105,  30,  75,  60, 165,  90,  60,  45, 165,  60, 165, 195,  90, 150,\n",
      "         45, 150,  90, 180, 180, 165,  90,  90,  90,  60, 105, 150, 195,  45,\n",
      "        135, 180, 105, 195,  60,  75, 135,  90, 195, 120, 105,  90,  90,  45,\n",
      "         30, 150, 180,  90, 180, 120, 120, 150, 165, 195,  75,  45, 165, 105,\n",
      "        135, 165,  90,  90,  45, 180,  30, 105, 165,  30,  30, 180, 105, 135,\n",
      "        165, 165, 120, 165,  90, 195, 135, 120,  75, 135, 105, 135, 120,  90,\n",
      "         90,  30,  90, 180,  30,  90, 105, 120,  90, 180,  45, 165, 195, 195,\n",
      "        120, 165,  90, 105, 120, 195, 195, 135, 180, 105, 120, 180,  75, 195,\n",
      "         75, 195,  90,  30, 135, 165,  60,  75,  30, 135, 105,  90,  90,  75,\n",
      "        165, 135, 135, 135, 105,  30, 195,  30, 120, 105, 180, 180, 150, 150,\n",
      "        180,  60, 150, 150, 105, 135, 165, 150,  30, 180,  45,  30,  90,  45,\n",
      "        135, 105,  90, 120, 105, 105,  30, 165, 105, 105,  45, 120,  75, 135,\n",
      "        120, 120, 165,  30, 105, 180, 105, 135, 150,  30,  90,  75,  75,  75,\n",
      "        195,  30,  60, 150, 180,  60,  90,  60,  90,  45, 195,  90, 120, 195,\n",
      "         60, 135, 165, 135,  90,  30,  45, 165, 180,  90, 135, 195,  30, 105,\n",
      "        150, 195, 165, 135])}\n",
      "Batch 2: Data Shape = torch.Size([256, 62, 30]), Labels = {'start_at': tensor([12200,  2200, 45600, 39800, 26400,   200,  9000, 33800, 40400, 46400,\n",
      "        20200, 14400, 11800, 44800, 32600,  7000, 20600,  4200, 36800, 38400,\n",
      "        45200,  8200,  8600, 24200, 34000, 18400, 38600,  5200, 22200,  4200,\n",
      "        45000, 28000, 11800, 15600,  7600,  8400, 44600,  1400, 38400, 21200,\n",
      "        27400, 19800,  7600, 11600, 16800, 23000, 21600, 41400, 28600, 32800,\n",
      "        26200, 44800, 36200, 11200, 22600,  8000, 15400,   400, 36400,  7000,\n",
      "         6800, 29400, 15200, 16000, 10400, 14000, 11400, 14000, 46800, 18400,\n",
      "         1000,  8600, 25400, 24200, 15600, 25000, 19200, 26800, 40600,  8200,\n",
      "        18600,  8200, 27400, 42400,  5200, 12400,  2200, 18800, 21800, 15400,\n",
      "         5800,  5800,  7000, 22400, 13200,  6600, 14800, 33200, 28400, 33600,\n",
      "        15000, 16800,  7600,  8800, 22600, 30800, 29400, 24000, 21400, 10400,\n",
      "        21400, 49800, 21600,  5400, 42800, 22600, 42400,  8400, 12400, 37000,\n",
      "          400, 30200, 37000, 30400, 31800, 15400, 31800, 15000, 12400, 31000,\n",
      "        14000, 44400, 35400, 34200,  5200, 12000,  5400, 17200, 17200,  9600,\n",
      "         9600, 14400, 23400, 51400, 21200, 15800, 11800, 23200, 31000, 22400,\n",
      "        12200, 32000, 12200, 40000,  6400, 23600, 49200, 18000,  7800, 28400,\n",
      "        38000,   200,  4400, 18400, 29400,  1600, 38000, 38200,  2600,  2600,\n",
      "         9000,   800, 21400, 11200,   800, 18200, 23000, 12200,  7800, 12400,\n",
      "         5200, 25200, 21600, 52400, 29200, 39200, 15000, 15800,  5000, 23000,\n",
      "         7000, 22600, 38200, 20200, 24400, 11600, 20600, 28400, 38200, 25600,\n",
      "        17200, 14400, 19000, 12000,  6200, 26000, 21800, 18200,  7400, 35600,\n",
      "        15200, 29200, 22200, 42600, 36200, 37000, 16200, 45400, 50400, 23000,\n",
      "        45600, 23000, 17000, 45600, 39800, 32200, 35200, 37000, 45800, 12200,\n",
      "        32000,  1800, 16800, 33200, 24800,  7200, 43000, 40600, 20000, 27000,\n",
      "        16800, 42400, 39400, 18400, 40000, 31800, 38200, 11000, 23600, 32200,\n",
      "        28000, 42600,  9200, 18000, 34000, 25000]), 'end_at': tensor([12400,  2400, 45800, 40000, 26600,   400,  9200, 34000, 40600, 46600,\n",
      "        20400, 14600, 12000, 45000, 32800,  7200, 20800,  4400, 37000, 38600,\n",
      "        45400,  8400,  8800, 24400, 34200, 18600, 38800,  5400, 22400,  4400,\n",
      "        45200, 28200, 12000, 15800,  7800,  8600, 44800,  1600, 38600, 21400,\n",
      "        27600, 20000,  7800, 11800, 17000, 23200, 21800, 41600, 28800, 33000,\n",
      "        26400, 45000, 36400, 11400, 22800,  8200, 15600,   600, 36600,  7200,\n",
      "         7000, 29600, 15400, 16200, 10600, 14200, 11600, 14200, 47000, 18600,\n",
      "         1200,  8800, 25600, 24400, 15800, 25200, 19400, 27000, 40800,  8400,\n",
      "        18800,  8400, 27600, 42600,  5400, 12600,  2400, 19000, 22000, 15600,\n",
      "         6000,  6000,  7200, 22600, 13400,  6800, 15000, 33400, 28600, 33800,\n",
      "        15200, 17000,  7800,  9000, 22800, 31000, 29600, 24200, 21600, 10600,\n",
      "        21600, 50000, 21800,  5600, 43000, 22800, 42600,  8600, 12600, 37200,\n",
      "          600, 30400, 37200, 30600, 32000, 15600, 32000, 15200, 12600, 31200,\n",
      "        14200, 44600, 35600, 34400,  5400, 12200,  5600, 17400, 17400,  9800,\n",
      "         9800, 14600, 23600, 51600, 21400, 16000, 12000, 23400, 31200, 22600,\n",
      "        12400, 32200, 12400, 40200,  6600, 23800, 49400, 18200,  8000, 28600,\n",
      "        38200,   400,  4600, 18600, 29600,  1800, 38200, 38400,  2800,  2800,\n",
      "         9200,  1000, 21600, 11400,  1000, 18400, 23200, 12400,  8000, 12600,\n",
      "         5400, 25400, 21800, 52600, 29400, 39400, 15200, 16000,  5200, 23200,\n",
      "         7200, 22800, 38400, 20400, 24600, 11800, 20800, 28600, 38400, 25800,\n",
      "        17400, 14600, 19200, 12200,  6400, 26200, 22000, 18400,  7600, 35800,\n",
      "        15400, 29400, 22400, 42800, 36400, 37200, 16400, 45600, 50600, 23200,\n",
      "        45800, 23200, 17200, 45800, 40000, 32400, 35400, 37200, 46000, 12400,\n",
      "        32200,  2000, 17000, 33400, 25000,  7400, 43200, 40800, 20200, 27200,\n",
      "        17000, 42600, 39600, 18600, 40200, 32000, 38400, 11200, 23800, 32400,\n",
      "        28200, 42800,  9400, 18200, 34200, 25200]), 'clip_id': ['5_20140418.mat_61', '3_20140611.mat_2726', '14_20140615.mat_1520', '4_20140702.mat_2209', '12_20131207.mat_1229', '4_20140702.mat_469', '6_20131113.mat_1337', '8_20140521.mat_637', '6_20130712.mat_437', '4_20140702.mat_1524', '15_20131105.mat_2111', '13_20140610.mat_307', '14_20140601.mat_3247', '5_20140506.mat_2471', '7_20131027.mat_837', '15_20130709.mat_1132', '2_20140419.mat_2818', '5_20140506.mat_2268', '13_20140527.mat_2666', '6_20130712.mat_660', '2_20140404.mat_1971', '15_20131016.mat_276', '6_20131016.mat_278', '9_20140704.mat_2603', '3_20140611.mat_405', '2_20140419.mat_1837', '9_20140620.mat_193', '7_20131106.mat_2508', '3_20140603.mat_2826', '13_20140603.mat_1313', '10_20131211.mat_2235', '9_20140627.mat_2150', '2_20140404.mat_2541', '3_20140603.mat_313', '2_20140404.mat_38', '11_20140630.mat_42', '9_20140627.mat_2470', '6_20130712.mat_1536', '15_20130709.mat_1484', '13_20140527.mat_3056', '15_20130709.mat_1882', '9_20140620.mat_773', '11_20140618.mat_1567', '9_20140627.mat_2068', '11_20140618.mat_758', '2_20140413.mat_115', '7_20131106.mat_1020', '8_20140521.mat_442', '13_20140603.mat_2153', '11_20140618.mat_838', '3_20140629.mat_366', '8_20140521.mat_2939', '2_20140404.mat_1710', '14_20140615.mat_291', '8_20140511.mat_787', '14_20140601.mat_275', '11_20140630.mat_545', '5_20140418.mat_1747', '4_20140621.mat_2192', '5_20140418.mat_947', '8_20140521.mat_2984', '2_20140419.mat_2394', '15_20131105.mat_2791', '14_20140615.mat_754', '9_20140620.mat_964', '10_20131130.mat_982', '4_20140621.mat_969', '5_20140418.mat_1167', '12_20131207.mat_2244', '12_20131207.mat_1837', '2_20140419.mat_2487', '7_20131027.mat_2758', '12_20131201.mat_1872', '5_20140506.mat_2368', '7_20131030.mat_990', '14_20140615.mat_2135', '2_20140419.mat_3284', '4_20140702.mat_2144', '2_20140413.mat_438', '9_20140620.mat_1138', '3_20140603.mat_1622', '7_20131027.mat_509', '4_20140702.mat_2384', '8_20140514.mat_2694', '10_20131211.mat_2273', '13_20140527.mat_297', '5_20140411.mat_1540', '15_20130709.mat_1006', '2_20140419.mat_2119', '5_20140411.mat_1369', '8_20140511.mat_2979', '12_20131207.mat_2511', '2_20140404.mat_2750', '2_20140419.mat_1024', '7_20131027.mat_1163', '10_20131204.mat_33', '14_20140627.mat_1603', '13_20140610.mat_1458', '2_20140419.mat_1887', '15_20131016.mat_168', '12_20131201.mat_2322', '2_20140413.mat_552', '9_20140704.mat_712', '8_20140514.mat_3232', '15_20131016.mat_1210', '8_20140514.mat_1899', '2_20140404.mat_147', '12_20131207.mat_1865', '2_20140413.mat_1204', '3_20140603.mat_726', '2_20140419.mat_2354', '12_20131127.mat_1994', '4_20140702.mat_3058', '11_20140618.mat_939', '4_20140621.mat_449', '11_20140630.mat_348', '12_20131207.mat_2927', '3_20140629.mat_2289', '10_20131204.mat_1591', '3_20140611.mat_1714', '10_20131211.mat_2', '9_20140627.mat_2398', '10_20131204.mat_2195', '15_20131016.mat_1444', '6_20131016.mat_627', '12_20131127.mat_2792', '10_20131211.mat_2406', '6_20131016.mat_2085', '11_20140618.mat_2777', '8_20140514.mat_1900', '2_20140413.mat_538', '2_20140419.mat_457', '5_20140411.mat_2659', '3_20140629.mat_845', '15_20130709.mat_1123', '11_20140630.mat_2775', '8_20140514.mat_1319', '9_20140704.mat_1378', '13_20140610.mat_2333', '10_20131211.mat_3236', '13_20140610.mat_2998', '4_20140705.mat_72', '11_20140618.mat_2599', '5_20140418.mat_2002', '13_20140603.mat_574', '7_20131030.mat_1608', '9_20140620.mat_1351', '5_20140411.mat_2363', '3_20140629.mat_3105', '13_20140527.mat_2359', '4_20140705.mat_3011', '4_20140621.mat_3110', '8_20140511.mat_1590', '5_20140411.mat_2915', '15_20130709.mat_3220', '3_20140603.mat_3306', '11_20140618.mat_1991', '2_20140413.mat_1619', '4_20140702.mat_2049', '5_20140506.mat_610', '6_20131016.mat_658', '5_20140411.mat_1098', '11_20140618.mat_490', '12_20131207.mat_1189', '11_20140618.mat_2862', '4_20140705.mat_1105', '9_20140627.mat_2905', '9_20140620.mat_2201', '11_20140625.mat_1110', '11_20140630.mat_1110', '6_20130712.mat_957', '9_20140704.mat_1749', '2_20140419.mat_575', '3_20140629.mat_1348', '7_20131027.mat_1296', '6_20131113.mat_3279', '3_20140611.mat_1027', '8_20140514.mat_1806', '2_20140419.mat_2049', '13_20140610.mat_2544', '2_20140404.mat_494', '9_20140704.mat_1038', '14_20140601.mat_1400', '11_20140630.mat_2007', '9_20140627.mat_820', '9_20140620.mat_1941', '11_20140625.mat_1604', '13_20140603.mat_1608', '13_20140527.mat_1317', '5_20140411.mat_3065', '12_20131127.mat_2282', '2_20140404.mat_787', '2_20140413.mat_2201', '11_20140618.mat_1013', '13_20140527.mat_357', '10_20131211.mat_2068', '2_20140404.mat_1395', '11_20140630.mat_2624', '12_20131127.mat_2201', '11_20140618.mat_1873', '11_20140618.mat_2333', '13_20140610.mat_2787', '7_20131030.mat_3045', '13_20140527.mat_1589', '7_20131030.mat_943', '7_20131027.mat_130', '9_20140704.mat_783', '9_20140704.mat_326', '12_20131201.mat_272', '8_20140511.mat_1470', '12_20131127.mat_3026', '12_20131207.mat_2393', '8_20140511.mat_2593', '15_20131105.mat_1505', '5_20140411.mat_3369', '9_20140627.mat_185', '6_20131016.mat_2328', '9_20140704.mat_227', '6_20130712.mat_1997', '2_20140404.mat_1407', '2_20140413.mat_2710', '15_20130709.mat_3303', '10_20131211.mat_3035', '14_20140627.mat_3178', '14_20140627.mat_1728', '11_20140618.mat_161', '15_20130709.mat_1468', '12_20131201.mat_2667', '12_20131201.mat_464', '2_20140419.mat_1158', '12_20131207.mat_2875', '15_20131105.mat_1301', '2_20140404.mat_3034', '9_20140704.mat_634', '11_20140625.mat_2371', '5_20140411.mat_948', '15_20131016.mat_450', '12_20131207.mat_671', '11_20140618.mat_1392', '10_20131130.mat_2617', '2_20140413.mat_2094', '9_20140620.mat_447', '6_20131113.mat_871', '2_20140419.mat_1837', '8_20140521.mat_1945', '8_20140514.mat_159', '11_20140625.mat_191', '10_20131130.mat_2302', '3_20140603.mat_2600', '7_20131027.mat_1690', '3_20140611.mat_2855', '13_20140603.mat_448', '4_20140705.mat_2056', '12_20131201.mat_1187', '11_20140625.mat_170', '10_20131130.mat_3075'], 'subject_id': tensor([ 4,  2, 13,  3, 11,  3,  5,  7,  5,  3, 14, 12, 13,  4,  6, 14,  1,  4,\n",
      "        12,  5,  1, 14,  5,  8,  2,  1,  8,  6,  2, 12,  9,  8,  1,  2,  1, 10,\n",
      "         8,  5, 14, 12, 14,  8, 10,  8, 10,  1,  6,  7, 12, 10,  2,  7,  1, 13,\n",
      "         7, 13, 10,  4,  3,  4,  7,  1, 14, 13,  8,  9,  3,  4, 11, 11,  1,  6,\n",
      "        11,  4,  6, 13,  1,  3,  1,  8,  2,  6,  3,  7,  9, 12,  4, 14,  1,  4,\n",
      "         7, 11,  1,  1,  6,  9, 13, 12,  1, 14, 11,  1,  8,  7, 14,  7,  1, 11,\n",
      "         1,  2,  1, 11,  3, 10,  3, 10, 11,  2,  9,  2,  9,  8,  9, 14,  5, 11,\n",
      "         9,  5, 10,  7,  1,  1,  4,  2, 14, 10,  7,  8, 12,  9, 12,  3, 10,  4,\n",
      "        12,  6,  8,  4,  2, 12,  3,  3,  7,  4, 14,  2, 10,  1,  3,  4,  5,  4,\n",
      "        10, 11, 10,  3,  8,  8, 10, 10,  5,  8,  1,  2,  6,  5,  2,  7,  1, 12,\n",
      "         1,  8, 13, 10,  8,  8, 10, 12, 12,  4, 11,  1,  1, 10, 12,  9,  1, 10,\n",
      "        11, 10, 10, 12,  6, 12,  6,  6,  8,  8, 11,  7, 11, 11,  7, 14,  4,  8,\n",
      "         5,  8,  5,  1,  1, 14,  9, 13, 13, 10, 14, 11, 11,  1, 11, 14,  1,  8,\n",
      "        10,  4, 14, 11, 10,  9,  1,  8,  5,  1,  7,  7, 10,  9,  2,  6,  2, 12,\n",
      "         3, 11, 10,  9]), 'trial_id': ['ly_eeg1', 'jj_eeg13', 'ys_eeg7', 'lqj_eeg10', 'wyw_eeg6', 'lqj_eeg3', 'mhw_eeg7', 'sxy_eeg3', 'mhw_eeg2', 'lqj_eeg7', 'zjy_eeg10', 'xyl_eeg2', 'ys_eeg15', 'ly_eeg11', 'phl_eeg4', 'zjy_eeg6', 'jl_eeg13', 'ly_eeg11', 'xyl_eeg12', 'mhw_eeg3', 'jl_eeg9', 'zjy_eeg2', 'mhw_eeg2', 'wk_eeg12', 'jj_eeg2', 'jl_eeg9', 'wk_eeg1', 'phl_eeg12', 'jj_eeg13', 'xyl_eeg7', 'ww_eeg10', 'wk_eeg10', 'jl_eeg12', 'jj_eeg2', 'jl_eeg1', 'wsf_eeg1', 'wk_eeg11', 'mhw_eeg8', 'zjy_eeg7', 'xyl_eeg14', 'zjy_eeg9', 'wk_eeg4', 'wsf_eeg8', 'wk_eeg10', 'wsf_eeg4', 'jl_eeg1', 'phl_eeg5', 'sxy_eeg2', 'xyl_eeg10', 'wsf_eeg4', 'jj_eeg2', 'sxy_eeg13', 'jl_eeg8', 'ys_eeg2', 'sxy_eeg4', 'ys_eeg2', 'wsf_eeg3', 'ly_eeg9', 'lqj_eeg10', 'ly_eeg5', 'sxy_eeg14', 'jl_eeg11', 'zjy_eeg13', 'ys_eeg4', 'wk_eeg5', 'ww_eeg5', 'lqj_eeg5', 'ly_eeg6', 'wyw_eeg10', 'wyw_eeg9', 'jl_eeg12', 'phl_eeg13', 'wyw_eeg9', 'ly_eeg11', 'phl_eeg5', 'ys_eeg10', 'jl_eeg15', 'lqj_eeg10', 'jl_eeg2', 'wk_eeg6', 'jj_eeg8', 'phl_eeg3', 'lqj_eeg11', 'sxy_eeg12', 'ww_eeg11', 'xyl_eeg2', 'ly_eeg8', 'zjy_eeg5', 'jl_eeg10', 'ly_eeg7', 'sxy_eeg14', 'wyw_eeg12', 'jl_eeg13', 'jl_eeg5', 'phl_eeg6', 'ww_eeg1', 'ys_eeg8', 'xyl_eeg7', 'jl_eeg9', 'zjy_eeg1', 'wyw_eeg11', 'jl_eeg3', 'wk_eeg4', 'sxy_eeg15', 'zjy_eeg6', 'sxy_eeg9', 'jl_eeg1', 'wyw_eeg9', 'jl_eeg6', 'jj_eeg4', 'jl_eeg11', 'wyw_eeg9', 'lqj_eeg14', 'wsf_eeg5', 'lqj_eeg2', 'wsf_eeg2', 'wyw_eeg13', 'jj_eeg11', 'ww_eeg8', 'jj_eeg8', 'ww_eeg1', 'wk_eeg11', 'ww_eeg10', 'zjy_eeg7', 'mhw_eeg3', 'wyw_eeg13', 'ww_eeg11', 'mhw_eeg10', 'wsf_eeg13', 'sxy_eeg9', 'jl_eeg3', 'jl_eeg2', 'ly_eeg12', 'jj_eeg4', 'zjy_eeg6', 'wsf_eeg13', 'sxy_eeg7', 'wk_eeg7', 'xyl_eeg11', 'ww_eeg15', 'xyl_eeg14', 'lqj_eeg1', 'wsf_eeg12', 'ly_eeg9', 'xyl_eeg3', 'phl_eeg8', 'wk_eeg7', 'ly_eeg11', 'jj_eeg14', 'xyl_eeg11', 'lqj_eeg14', 'lqj_eeg14', 'sxy_eeg8', 'ly_eeg13', 'zjy_eeg15', 'jj_eeg15', 'wsf_eeg9', 'jl_eeg8', 'lqj_eeg10', 'ly_eeg3', 'mhw_eeg3', 'ly_eeg6', 'wsf_eeg3', 'wyw_eeg6', 'wsf_eeg13', 'lqj_eeg6', 'wk_eeg13', 'wk_eeg10', 'wsf_eeg6', 'wsf_eeg6', 'mhw_eeg5', 'wk_eeg9', 'jl_eeg3', 'jj_eeg7', 'phl_eeg7', 'mhw_eeg15', 'jj_eeg5', 'sxy_eeg9', 'jl_eeg10', 'xyl_eeg12', 'jl_eeg3', 'wk_eeg5', 'ys_eeg7', 'wsf_eeg9', 'wk_eeg4', 'wk_eeg9', 'wsf_eeg8', 'xyl_eeg8', 'xyl_eeg7', 'ly_eeg14', 'wyw_eeg11', 'jl_eeg4', 'jl_eeg10', 'wsf_eeg5', 'xyl_eeg2', 'ww_eeg10', 'jl_eeg7', 'wsf_eeg12', 'wyw_eeg10', 'wsf_eeg9', 'wsf_eeg11', 'xyl_eeg13', 'phl_eeg14', 'xyl_eeg8', 'phl_eeg5', 'phl_eeg1', 'wk_eeg4', 'wk_eeg2', 'wyw_eeg2', 'sxy_eeg7', 'wyw_eeg14', 'wyw_eeg11', 'sxy_eeg12', 'zjy_eeg7', 'ly_eeg15', 'wk_eeg1', 'mhw_eeg11', 'wk_eeg1', 'mhw_eeg9', 'jl_eeg7', 'jl_eeg12', 'zjy_eeg15', 'ww_eeg14', 'ys_eeg14', 'ys_eeg8', 'wsf_eeg1', 'zjy_eeg7', 'wyw_eeg12', 'wyw_eeg2', 'jl_eeg6', 'wyw_eeg13', 'zjy_eeg7', 'jl_eeg14', 'wk_eeg3', 'wsf_eeg11', 'ly_eeg5', 'zjy_eeg2', 'wyw_eeg3', 'wsf_eeg7', 'ww_eeg12', 'jl_eeg10', 'wk_eeg2', 'mhw_eeg4', 'jl_eeg9', 'sxy_eeg9', 'sxy_eeg1', 'wsf_eeg1', 'ww_eeg11', 'jj_eeg12', 'phl_eeg8', 'jj_eeg13', 'xyl_eeg2', 'lqj_eeg10', 'wyw_eeg6', 'wsf_eeg1', 'ww_eeg14'], 'emotion': tensor([ 1,  0, -1,  1,  1, -1, -1, -1,  0, -1,  1,  0, -1,  0, -1,  1,  0,  0,\n",
      "        -1, -1,  1,  0,  0, -1,  0,  1,  1, -1,  0, -1,  1,  1, -1,  0,  1,  1,\n",
      "         0,  0, -1,  1,  1, -1,  0,  1, -1,  1,  0,  0,  1, -1,  0,  0,  0,  0,\n",
      "        -1,  0, -1,  1,  1,  0,  1,  0,  0, -1,  0,  0,  0,  1,  1,  1, -1,  0,\n",
      "         1,  0,  0,  1, -1,  1,  0,  1,  0, -1,  0, -1,  0,  0,  0,  0,  1, -1,\n",
      "         1, -1,  0,  0,  1,  1,  0, -1,  1,  1,  0, -1, -1, -1,  1,  1,  1,  1,\n",
      "         1, -1,  0,  1,  1,  0,  0,  0,  0,  0,  0,  0,  1,  0,  1, -1, -1,  0,\n",
      "         0,  1,  0,  1, -1,  0, -1, -1,  1,  0, -1, -1,  0, -1,  1,  1, -1,  1,\n",
      "        -1,  0, -1,  0,  1,  0,  1,  1,  0,  0, -1, -1,  1,  0,  1, -1, -1,  1,\n",
      "        -1,  1,  0,  1,  0,  1,  1,  1,  0,  1, -1, -1, -1, -1,  0,  1,  1, -1,\n",
      "        -1,  0, -1,  1, -1,  1,  0,  0, -1,  1,  0, -1,  1,  0,  0,  1, -1, -1,\n",
      "         1,  1,  0,  0,  1,  0,  0,  1, -1,  0,  0, -1,  1,  0, -1, -1, -1,  1,\n",
      "         0,  1,  1, -1, -1, -1,  1,  1,  0,  1, -1, -1,  0,  1,  0, -1,  1, -1,\n",
      "         0,  0,  0, -1, -1, -1,  1,  0, -1,  1,  1,  1,  1,  0, -1,  0,  0,  0,\n",
      "         1,  1,  1,  1]), 'date': tensor([20140418, 20140611, 20140615, 20140702, 20131207, 20140702, 20131113,\n",
      "        20140521, 20130712, 20140702, 20131105, 20140610, 20140601, 20140506,\n",
      "        20131027, 20130709, 20140419, 20140506, 20140527, 20130712, 20140404,\n",
      "        20131016, 20131016, 20140704, 20140611, 20140419, 20140620, 20131106,\n",
      "        20140603, 20140603, 20131211, 20140627, 20140404, 20140603, 20140404,\n",
      "        20140630, 20140627, 20130712, 20130709, 20140527, 20130709, 20140620,\n",
      "        20140618, 20140627, 20140618, 20140413, 20131106, 20140521, 20140603,\n",
      "        20140618, 20140629, 20140521, 20140404, 20140615, 20140511, 20140601,\n",
      "        20140630, 20140418, 20140621, 20140418, 20140521, 20140419, 20131105,\n",
      "        20140615, 20140620, 20131130, 20140621, 20140418, 20131207, 20131207,\n",
      "        20140419, 20131027, 20131201, 20140506, 20131030, 20140615, 20140419,\n",
      "        20140702, 20140413, 20140620, 20140603, 20131027, 20140702, 20140514,\n",
      "        20131211, 20140527, 20140411, 20130709, 20140419, 20140411, 20140511,\n",
      "        20131207, 20140404, 20140419, 20131027, 20131204, 20140627, 20140610,\n",
      "        20140419, 20131016, 20131201, 20140413, 20140704, 20140514, 20131016,\n",
      "        20140514, 20140404, 20131207, 20140413, 20140603, 20140419, 20131127,\n",
      "        20140702, 20140618, 20140621, 20140630, 20131207, 20140629, 20131204,\n",
      "        20140611, 20131211, 20140627, 20131204, 20131016, 20131016, 20131127,\n",
      "        20131211, 20131016, 20140618, 20140514, 20140413, 20140419, 20140411,\n",
      "        20140629, 20130709, 20140630, 20140514, 20140704, 20140610, 20131211,\n",
      "        20140610, 20140705, 20140618, 20140418, 20140603, 20131030, 20140620,\n",
      "        20140411, 20140629, 20140527, 20140705, 20140621, 20140511, 20140411,\n",
      "        20130709, 20140603, 20140618, 20140413, 20140702, 20140506, 20131016,\n",
      "        20140411, 20140618, 20131207, 20140618, 20140705, 20140627, 20140620,\n",
      "        20140625, 20140630, 20130712, 20140704, 20140419, 20140629, 20131027,\n",
      "        20131113, 20140611, 20140514, 20140419, 20140610, 20140404, 20140704,\n",
      "        20140601, 20140630, 20140627, 20140620, 20140625, 20140603, 20140527,\n",
      "        20140411, 20131127, 20140404, 20140413, 20140618, 20140527, 20131211,\n",
      "        20140404, 20140630, 20131127, 20140618, 20140618, 20140610, 20131030,\n",
      "        20140527, 20131030, 20131027, 20140704, 20140704, 20131201, 20140511,\n",
      "        20131127, 20131207, 20140511, 20131105, 20140411, 20140627, 20131016,\n",
      "        20140704, 20130712, 20140404, 20140413, 20130709, 20131211, 20140627,\n",
      "        20140627, 20140618, 20130709, 20131201, 20131201, 20140419, 20131207,\n",
      "        20131105, 20140404, 20140704, 20140625, 20140411, 20131016, 20131207,\n",
      "        20140618, 20131130, 20140413, 20140620, 20131113, 20140419, 20140521,\n",
      "        20140514, 20140625, 20131130, 20140603, 20131027, 20140611, 20140603,\n",
      "        20140705, 20131201, 20140625, 20131130]), '_record_id': ['_record_31', '_record_25', '_record_13', '_record_28', '_record_8', '_record_28', '_record_35', '_record_41', '_record_33', '_record_28', '_record_17', '_record_11', '_record_12', '_record_32', '_record_36', '_record_15', '_record_23', '_record_32', '_record_9', '_record_33', '_record_21', '_record_16', '_record_34', '_record_44', '_record_25', '_record_23', '_record_42', '_record_38', '_record_24', '_record_10', '_record_2', '_record_43', '_record_21', '_record_24', '_record_21', '_record_5', '_record_43', '_record_33', '_record_15', '_record_9', '_record_15', '_record_42', '_record_3', '_record_43', '_record_3', '_record_22', '_record_38', '_record_41', '_record_10', '_record_3', '_record_26', '_record_41', '_record_21', '_record_13', '_record_39', '_record_12', '_record_5', '_record_31', '_record_27', '_record_31', '_record_41', '_record_23', '_record_17', '_record_13', '_record_42', '_record_0', '_record_27', '_record_31', '_record_8', '_record_8', '_record_23', '_record_36', '_record_7', '_record_32', '_record_37', '_record_13', '_record_23', '_record_28', '_record_22', '_record_42', '_record_24', '_record_36', '_record_28', '_record_40', '_record_2', '_record_9', '_record_30', '_record_15', '_record_23', '_record_30', '_record_39', '_record_8', '_record_21', '_record_23', '_record_36', '_record_1', '_record_14', '_record_11', '_record_23', '_record_16', '_record_7', '_record_22', '_record_44', '_record_40', '_record_16', '_record_40', '_record_21', '_record_8', '_record_22', '_record_24', '_record_23', '_record_6', '_record_28', '_record_3', '_record_27', '_record_5', '_record_8', '_record_26', '_record_1', '_record_25', '_record_2', '_record_43', '_record_1', '_record_16', '_record_34', '_record_6', '_record_2', '_record_34', '_record_3', '_record_40', '_record_22', '_record_23', '_record_30', '_record_26', '_record_15', '_record_5', '_record_40', '_record_44', '_record_11', '_record_2', '_record_11', '_record_29', '_record_3', '_record_31', '_record_10', '_record_37', '_record_42', '_record_30', '_record_26', '_record_9', '_record_29', '_record_27', '_record_39', '_record_30', '_record_15', '_record_24', '_record_3', '_record_22', '_record_28', '_record_32', '_record_34', '_record_30', '_record_3', '_record_8', '_record_3', '_record_29', '_record_43', '_record_42', '_record_4', '_record_5', '_record_33', '_record_44', '_record_23', '_record_26', '_record_36', '_record_35', '_record_25', '_record_40', '_record_23', '_record_11', '_record_21', '_record_44', '_record_12', '_record_5', '_record_43', '_record_42', '_record_4', '_record_10', '_record_9', '_record_30', '_record_6', '_record_21', '_record_22', '_record_3', '_record_9', '_record_2', '_record_21', '_record_5', '_record_6', '_record_3', '_record_3', '_record_11', '_record_37', '_record_9', '_record_37', '_record_36', '_record_44', '_record_44', '_record_7', '_record_39', '_record_6', '_record_8', '_record_39', '_record_17', '_record_30', '_record_43', '_record_34', '_record_44', '_record_33', '_record_21', '_record_22', '_record_15', '_record_2', '_record_14', '_record_14', '_record_3', '_record_15', '_record_7', '_record_7', '_record_23', '_record_8', '_record_17', '_record_21', '_record_44', '_record_4', '_record_30', '_record_16', '_record_8', '_record_3', '_record_0', '_record_22', '_record_42', '_record_35', '_record_23', '_record_41', '_record_40', '_record_4', '_record_0', '_record_24', '_record_36', '_record_25', '_record_10', '_record_29', '_record_7', '_record_4', '_record_0'], 'segment_start': tensor([ 75, 120,   0,   0,  45,  30, 120,  75,  30,   0, 150, 135,  15,   0,\n",
      "        120,  60, 165, 135, 120,  75,  90,  30,  30,  90,  45,  75,  75, 120,\n",
      "         45,  15,  15, 120,  90, 165,   0,  45,  15, 150, 150, 135, 165,  15,\n",
      "         60,  75, 150,  75,  60, 105,  60,  75,  60, 150, 120, 105,   0,  30,\n",
      "         30,  15, 135,  90,  75,  75,   0, 135,  15, 135, 105,   0,  90,  75,\n",
      "        150, 150,  90,  60,  45,  90, 120, 150,  90, 135,  30, 150, 165, 165,\n",
      "         15,  45,  45, 105,  30,   0, 105,  45, 135,  15, 165,   0,  30,  90,\n",
      "          0,   0, 120,  45,  90,  60,  75,  60, 165,  75, 165,  75, 165, 120,\n",
      "          0, 120, 105, 135,  90,  60,  30, 105, 150, 165, 105,  45, 120, 135,\n",
      "        135,  90,  15,  90,  60, 120,  30, 150,  60, 135, 165, 135,  60, 120,\n",
      "        165, 135, 120,  30,  60,  30,  15,  45, 150,   0,   0,  60,  45, 135,\n",
      "        165,  30,  30,  30,  15,  75, 165,  45,  15,  15,  30,  60, 150, 135,\n",
      "         90, 150,  90,  60,  90,  75, 150,   0,  30,  75,  90,  15,  60,   0,\n",
      "         75, 105,   0,  75,  30, 150,  15,  60,  75, 120,  15, 135,  90,  60,\n",
      "         90,  15,  75,  75,  75,  30,  75, 165,  45, 120, 165,  45, 135, 105,\n",
      "         60,  45,  15, 105,  15,   0,  75,  30, 120,  45, 120,  45, 120, 150,\n",
      "         15, 105,  90, 105,  45,   0, 150,  75,  90, 135, 150,  30,  15,  30,\n",
      "         45,  45,  75,  15,  90, 135,  75, 105, 150, 165,  45,  45,   0, 135,\n",
      "        150, 105,   0,  90]), 'segment_end': tensor([105, 150,  30,  30,  75,  60, 150, 105,  60,  30, 180, 165,  45,  30,\n",
      "        150,  90, 195, 165, 150, 105, 120,  60,  60, 120,  75, 105, 105, 150,\n",
      "         75,  45,  45, 150, 120, 195,  30,  75,  45, 180, 180, 165, 195,  45,\n",
      "         90, 105, 180, 105,  90, 135,  90, 105,  90, 180, 150, 135,  30,  60,\n",
      "         60,  45, 165, 120, 105, 105,  30, 165,  45, 165, 135,  30, 120, 105,\n",
      "        180, 180, 120,  90,  75, 120, 150, 180, 120, 165,  60, 180, 195, 195,\n",
      "         45,  75,  75, 135,  60,  30, 135,  75, 165,  45, 195,  30,  60, 120,\n",
      "         30,  30, 150,  75, 120,  90, 105,  90, 195, 105, 195, 105, 195, 150,\n",
      "         30, 150, 135, 165, 120,  90,  60, 135, 180, 195, 135,  75, 150, 165,\n",
      "        165, 120,  45, 120,  90, 150,  60, 180,  90, 165, 195, 165,  90, 150,\n",
      "        195, 165, 150,  60,  90,  60,  45,  75, 180,  30,  30,  90,  75, 165,\n",
      "        195,  60,  60,  60,  45, 105, 195,  75,  45,  45,  60,  90, 180, 165,\n",
      "        120, 180, 120,  90, 120, 105, 180,  30,  60, 105, 120,  45,  90,  30,\n",
      "        105, 135,  30, 105,  60, 180,  45,  90, 105, 150,  45, 165, 120,  90,\n",
      "        120,  45, 105, 105, 105,  60, 105, 195,  75, 150, 195,  75, 165, 135,\n",
      "         90,  75,  45, 135,  45,  30, 105,  60, 150,  75, 150,  75, 150, 180,\n",
      "         45, 135, 120, 135,  75,  30, 180, 105, 120, 165, 180,  60,  45,  60,\n",
      "         75,  75, 105,  45, 120, 165, 105, 135, 180, 195,  75,  75,  30, 165,\n",
      "        180, 135,  30, 120])}\n",
      "Batch 3: Data Shape = torch.Size([256, 62, 30]), Labels = {'start_at': tensor([39200, 18800, 17400,  2400, 36200, 28200, 41600, 27400,  8200,   600,\n",
      "        46600, 14400, 10600, 32600, 35600, 13000, 46000, 18600,  3600, 23600,\n",
      "        24200, 11200, 36400, 20800, 42800, 13200, 30800, 33800, 18000, 10600,\n",
      "        36600, 33000, 11200, 29200, 22000,  6200, 38600, 30800, 25000, 38200,\n",
      "        43000, 39800, 46000, 32800, 42400, 17600, 43200, 28600,   800, 13800,\n",
      "        36000, 25400,  4800, 23600, 34400, 38000, 28200,   800, 11600, 18800,\n",
      "        12600, 38200, 47400, 33800, 15200, 37600,  6400, 34200, 42000,  1600,\n",
      "        15000,  9600, 28400, 19800, 40800,  1800, 36600, 27200, 32400, 10400,\n",
      "        26600, 19000, 23400, 44000, 37000,  7600, 28600,  3400, 10200, 42400,\n",
      "        28600, 25000,  6400, 16600, 16200, 43600, 45600, 34400, 37600,   600,\n",
      "        46400, 25000, 40800,  8000, 14600, 34400, 30600,  4200, 28200,  9200,\n",
      "         4000, 21600, 41000, 15600, 16800, 30400,  6800, 25400, 13800, 11000,\n",
      "        31600, 20200, 16800, 34400,  5200, 46800,  4800, 46600, 28600, 30800,\n",
      "        20400, 45800, 21600, 26800,   800,  1000, 32600, 29800,  1600,  6200,\n",
      "        23400, 28800, 46600, 31000,  7800, 16200,  5600, 10000,  2800,  4000,\n",
      "        13000, 30600, 38400, 15800, 25600,  8800, 13600, 33600,  6200, 14200,\n",
      "         9800, 29000, 10000,  1000,  6800,  1400, 43600, 17800, 23200, 47200,\n",
      "        34000,  9400, 44400,  9400, 40200, 22000, 16200, 25800, 26800, 13600,\n",
      "        27000, 13400, 32200, 40200, 19200, 32600, 35000, 28600, 30800, 24200,\n",
      "        12000,   800, 14000, 19200, 26600,  5000, 38800, 28600, 22600, 39400,\n",
      "        23000, 29000, 10200, 35400,  5000,  6400, 31600, 45600,  4000, 24800,\n",
      "         2000, 45600,  5000, 37600, 19800, 17800, 16800,  5800,  5800, 38000,\n",
      "        19000, 23200,   200, 34400, 33800,  5800, 17800, 45200, 39400, 10000,\n",
      "        28000, 42400, 37200, 39800, 30200, 24600, 12400,  9800, 17400,  3800,\n",
      "        12200,  6600, 31000, 41000, 25800, 17600, 35200, 43200, 38200, 28400,\n",
      "        36600, 29200, 14000, 32200, 42600, 27400]), 'end_at': tensor([39400, 19000, 17600,  2600, 36400, 28400, 41800, 27600,  8400,   800,\n",
      "        46800, 14600, 10800, 32800, 35800, 13200, 46200, 18800,  3800, 23800,\n",
      "        24400, 11400, 36600, 21000, 43000, 13400, 31000, 34000, 18200, 10800,\n",
      "        36800, 33200, 11400, 29400, 22200,  6400, 38800, 31000, 25200, 38400,\n",
      "        43200, 40000, 46200, 33000, 42600, 17800, 43400, 28800,  1000, 14000,\n",
      "        36200, 25600,  5000, 23800, 34600, 38200, 28400,  1000, 11800, 19000,\n",
      "        12800, 38400, 47600, 34000, 15400, 37800,  6600, 34400, 42200,  1800,\n",
      "        15200,  9800, 28600, 20000, 41000,  2000, 36800, 27400, 32600, 10600,\n",
      "        26800, 19200, 23600, 44200, 37200,  7800, 28800,  3600, 10400, 42600,\n",
      "        28800, 25200,  6600, 16800, 16400, 43800, 45800, 34600, 37800,   800,\n",
      "        46600, 25200, 41000,  8200, 14800, 34600, 30800,  4400, 28400,  9400,\n",
      "         4200, 21800, 41200, 15800, 17000, 30600,  7000, 25600, 14000, 11200,\n",
      "        31800, 20400, 17000, 34600,  5400, 47000,  5000, 46800, 28800, 31000,\n",
      "        20600, 46000, 21800, 27000,  1000,  1200, 32800, 30000,  1800,  6400,\n",
      "        23600, 29000, 46800, 31200,  8000, 16400,  5800, 10200,  3000,  4200,\n",
      "        13200, 30800, 38600, 16000, 25800,  9000, 13800, 33800,  6400, 14400,\n",
      "        10000, 29200, 10200,  1200,  7000,  1600, 43800, 18000, 23400, 47400,\n",
      "        34200,  9600, 44600,  9600, 40400, 22200, 16400, 26000, 27000, 13800,\n",
      "        27200, 13600, 32400, 40400, 19400, 32800, 35200, 28800, 31000, 24400,\n",
      "        12200,  1000, 14200, 19400, 26800,  5200, 39000, 28800, 22800, 39600,\n",
      "        23200, 29200, 10400, 35600,  5200,  6600, 31800, 45800,  4200, 25000,\n",
      "         2200, 45800,  5200, 37800, 20000, 18000, 17000,  6000,  6000, 38200,\n",
      "        19200, 23400,   400, 34600, 34000,  6000, 18000, 45400, 39600, 10200,\n",
      "        28200, 42600, 37400, 40000, 30400, 24800, 12600, 10000, 17600,  4000,\n",
      "        12400,  6800, 31200, 41200, 26000, 17800, 35400, 43400, 38400, 28600,\n",
      "        36800, 29400, 14200, 32400, 42800, 27600]), 'clip_id': ['5_20140411.mat_431', '10_20131130.mat_2809', '10_20131211.mat_555', '15_20130709.mat_2962', '6_20130712.mat_1926', '4_20140705.mat_609', '2_20140404.mat_1953', '15_20130709.mat_2384', '12_20131207.mat_715', '8_20140521.mat_3191', '13_20140527.mat_1978', '2_20140404.mat_1364', '12_20131127.mat_727', '13_20140527.mat_3113', '6_20131016.mat_2188', '10_20131204.mat_65', '7_20131027.mat_1522', '7_20131106.mat_2340', '11_20140630.mat_3206', '6_20130712.mat_1215', '13_20140603.mat_121', '8_20140511.mat_2771', '13_20140527.mat_1927', '5_20140506.mat_1396', '6_20130712.mat_449', '3_20140629.mat_1595', '4_20140705.mat_1066', '4_20140702.mat_3119', '4_20140705.mat_3278', '4_20140705.mat_727', '7_20131030.mat_2430', '8_20140511.mat_400', '13_20140603.mat_291', '13_20140610.mat_1891', '7_20131027.mat_3298', '13_20140603.mat_2513', '10_20131130.mat_867', '3_20140629.mat_389', '8_20140521.mat_1037', '11_20140625.mat_1483', '10_20131211.mat_1744', '4_20140621.mat_1728', '8_20140521.mat_465', '13_20140610.mat_2879', '3_20140611.mat_1957', '8_20140521.mat_762', '7_20131030.mat_1508', '13_20140610.mat_3093', '9_20140620.mat_1101', '13_20140610.mat_743', '7_20131106.mat_2895', '14_20140615.mat_1872', '6_20131016.mat_1121', '7_20131030.mat_1647', '4_20140705.mat_640', '11_20140625.mat_3140', '4_20140702.mat_1238', '15_20131016.mat_2954', '8_20140521.mat_3008', '8_20140514.mat_2809', '8_20140514.mat_1160', '14_20140601.mat_2201', '4_20140705.mat_1982', '12_20131201.mat_1266', '12_20131201.mat_544', '14_20140601.mat_2903', '4_20140621.mat_2042', '9_20140704.mat_406', '6_20130712.mat_1502', '7_20131027.mat_2958', '11_20140618.mat_749', '12_20131201.mat_1577', '12_20131207.mat_1054', '8_20140521.mat_3049', '2_20140404.mat_204', '15_20130709.mat_2959', '9_20140627.mat_651', '12_20131201.mat_2146', '4_20140705.mat_630', '8_20140511.mat_2062', '7_20131106.mat_2848', '5_20140411.mat_1387', '14_20140627.mat_352', '4_20140621.mat_3170', '15_20131105.mat_1477', '2_20140413.mat_1567', '9_20140627.mat_378', '4_20140621.mat_2967', '3_20140603.mat_3239', '3_20140603.mat_3162', '10_20131204.mat_1055', '15_20131105.mat_1654', '6_20131113.mat_1777', '12_20131207.mat_2093', '5_20140411.mat_993', '9_20140704.mat_2700', '8_20140521.mat_2238', '14_20140615.mat_1701', '7_20131106.mat_2903', '8_20140514.mat_2953', '4_20140702.mat_3182', '3_20140611.mat_1654', '9_20140704.mat_204', '3_20140611.mat_1785', '8_20140514.mat_541', '7_20131030.mat_1464', '2_20140413.mat_621', '11_20140618.mat_1313', '15_20131016.mat_609', '9_20140627.mat_1338', '7_20131030.mat_20', '12_20131127.mat_1020', '11_20140618.mat_879', '12_20131201.mat_2560', '9_20140620.mat_3272', '9_20140627.mat_1444', '10_20131204.mat_1131', '5_20140418.mat_2374', '10_20131130.mat_69', '10_20131130.mat_2770', '8_20140514.mat_2168', '8_20140514.mat_2816', '2_20140413.mat_3034', '2_20140404.mat_1917', '11_20140618.mat_2976', '15_20130709.mat_2244', '12_20131127.mat_1121', '6_20130712.mat_2480', '12_20131201.mat_378', '8_20140514.mat_3342', '10_20131204.mat_776', '8_20140521.mat_903', '7_20131106.mat_2355', '6_20131113.mat_1231', '11_20140618.mat_1296', '11_20140630.mat_1297', '3_20140611.mat_3113', '2_20140404.mat_617', '3_20140629.mat_2018', '13_20140610.mat_499', '11_20140618.mat_2599', '7_20131030.mat_144', '4_20140705.mat_2480', '12_20131207.mat_1447', '2_20140413.mat_507', '6_20131016.mat_1373', '5_20140411.mat_28', '2_20140404.mat_50', '12_20131207.mat_1543', '4_20140702.mat_20', '9_20140627.mat_1594', '12_20131201.mat_1445', '3_20140629.mat_1484', '12_20131207.mat_3029', '11_20140618.mat_1225', '15_20131105.mat_2054', '3_20140603.mat_536', '8_20140521.mat_2650', '10_20131130.mat_943', '6_20130712.mat_745', '14_20140615.mat_1341', '2_20140413.mat_2627', '10_20131211.mat_724', '10_20131130.mat_2720', '11_20140625.mat_34', '9_20140627.mat_681', '11_20140630.mat_1963', '3_20140611.mat_2571', '4_20140705.mat_1861', '7_20131027.mat_2246', '14_20140601.mat_2180', '6_20131016.mat_1792', '2_20140413.mat_2232', '13_20140603.mat_47', '9_20140627.mat_1493', '7_20131030.mat_1402', '3_20140603.mat_993', '14_20140627.mat_129', '13_20140610.mat_2616', '7_20131106.mat_68', '12_20131127.mat_809', '4_20140705.mat_2782', '12_20131207.mat_3349', '9_20140704.mat_1493', '3_20140611.mat_2343', '15_20130709.mat_3351', '10_20131204.mat_2657', '12_20131127.mat_2858', '13_20140610.mat_2401', '11_20140630.mat_2131', '11_20140618.mat_2307', '13_20140603.mat_239', '11_20140630.mat_1362', '12_20131201.mat_1841', '4_20140621.mat_2848', '9_20140627.mat_25', '15_20131105.mat_1723', '2_20140419.mat_611', '6_20131113.mat_2828', '11_20140618.mat_3147', '7_20131027.mat_1407', '9_20140620.mat_1437', '13_20140610.mat_725', '6_20131016.mat_851', '13_20140527.mat_1122', '6_20131113.mat_2279', '2_20140413.mat_1687', '2_20140419.mat_1973', '8_20140511.mat_1765', '11_20140618.mat_2606', '13_20140603.mat_1539', '6_20131016.mat_228', '13_20140527.mat_2975', '6_20131016.mat_188', '5_20140411.mat_2346', '9_20140620.mat_3277', '13_20140610.mat_2799', '13_20140610.mat_2511', '4_20140702.mat_29', '9_20140627.mat_3378', '5_20140506.mat_2342', '3_20140629.mat_2598', '11_20140618.mat_675', '14_20140627.mat_1701', '15_20131105.mat_2884', '6_20131113.mat_1774', '7_20131027.mat_1618', '2_20140413.mat_1518', '2_20140419.mat_2912', '6_20131113.mat_285', '11_20140630.mat_375', '11_20140618.mat_3162', '5_20140411.mat_1478', '13_20140527.mat_199', '5_20140411.mat_3101', '4_20140705.mat_358', '3_20140629.mat_1159', '8_20140521.mat_517', '6_20131016.mat_2334', '2_20140404.mat_1764', '5_20140411.mat_3011', '13_20140610.mat_501', '12_20131201.mat_390', '11_20140630.mat_440', '4_20140705.mat_1658', '14_20140627.mat_1617', '5_20140418.mat_1468', '11_20140618.mat_3166', '14_20140627.mat_2673', '10_20131211.mat_2624', '9_20140620.mat_857', '13_20140610.mat_2156', '7_20131106.mat_1599', '6_20130712.mat_396', '7_20131027.mat_1742', '9_20140620.mat_372'], 'subject_id': tensor([ 4,  9,  9, 14,  5,  3,  1, 14, 11,  7, 12,  1, 11, 12,  5,  9,  6,  6,\n",
      "        10,  5, 12,  7, 12,  4,  5,  2,  3,  3,  3,  3,  6,  7, 12, 12,  6, 12,\n",
      "         9,  2,  7, 10,  9,  3,  7, 12,  2,  7,  6, 12,  8, 12,  6, 13,  5,  6,\n",
      "         3, 10,  3, 14,  7,  7,  7, 13,  3, 11, 11, 13,  3,  8,  5,  6, 10, 11,\n",
      "        11,  7,  1, 14,  8, 11,  3,  7,  6,  4, 13,  3, 14,  1,  8,  3,  2,  2,\n",
      "         9, 14,  5, 11,  4,  8,  7, 13,  6,  7,  3,  2,  8,  2,  7,  6,  1, 10,\n",
      "        14,  8,  6, 11, 10, 11,  8,  8,  9,  4,  9,  9,  7,  7,  1,  1, 10, 14,\n",
      "        11,  5, 11,  7,  9,  7,  6,  5, 10, 10,  2,  1,  2, 12, 10,  6,  3, 11,\n",
      "         1,  5,  4,  1, 11,  3,  8, 11,  2, 11, 10, 14,  2,  7,  9,  5, 13,  1,\n",
      "         9,  9, 10,  8, 10,  2,  3,  6, 13,  5,  1, 12,  8,  6,  2, 13, 12,  6,\n",
      "        11,  3, 11,  8,  2, 14,  9, 11, 12, 10, 10, 12, 10, 11,  3,  8, 14,  1,\n",
      "         5, 10,  6,  8, 12,  5, 12,  5,  1,  1,  7, 10, 12,  5, 12,  5,  4,  8,\n",
      "        12, 12,  3,  8,  4,  2, 10, 13, 14,  5,  6,  1,  1,  5, 10, 10,  4, 12,\n",
      "         4,  3,  2,  7,  5,  1,  4, 12, 11, 10,  3, 13,  4, 10, 13,  9,  8, 12,\n",
      "         6,  5,  6,  8]), 'trial_id': ['ly_eeg2', 'ww_eeg13', 'ww_eeg3', 'zjy_eeg14', 'mhw_eeg9', 'lqj_eeg3', 'jl_eeg9', 'zjy_eeg11', 'wyw_eeg4', 'sxy_eeg15', 'xyl_eeg9', 'jl_eeg7', 'wyw_eeg4', 'xyl_eeg14', 'mhw_eeg10', 'ww_eeg1', 'phl_eeg7', 'phl_eeg11', 'wsf_eeg15', 'mhw_eeg6', 'xyl_eeg1', 'sxy_eeg13', 'xyl_eeg9', 'ly_eeg7', 'mhw_eeg2', 'jj_eeg8', 'lqj_eeg5', 'lqj_eeg14', 'lqj_eeg15', 'lqj_eeg4', 'phl_eeg11', 'sxy_eeg2', 'xyl_eeg2', 'xyl_eeg9', 'phl_eeg15', 'xyl_eeg12', 'ww_eeg4', 'jj_eeg2', 'sxy_eeg5', 'wsf_eeg7', 'ww_eeg8', 'lqj_eeg8', 'sxy_eeg2', 'xyl_eeg13', 'jj_eeg9', 'sxy_eeg4', 'phl_eeg7', 'xyl_eeg14', 'wk_eeg6', 'xyl_eeg4', 'phl_eeg13', 'ys_eeg9', 'mhw_eeg6', 'phl_eeg8', 'lqj_eeg3', 'wsf_eeg14', 'lqj_eeg6', 'zjy_eeg14', 'sxy_eeg14', 'sxy_eeg13', 'sxy_eeg6', 'ys_eeg10', 'lqj_eeg9', 'wyw_eeg6', 'wyw_eeg3', 'ys_eeg13', 'lqj_eeg10', 'wk_eeg2', 'mhw_eeg7', 'phl_eeg14', 'wsf_eeg4', 'wyw_eeg8', 'wyw_eeg5', 'sxy_eeg14', 'jl_eeg1', 'zjy_eeg14', 'wk_eeg3', 'wyw_eeg10', 'lqj_eeg3', 'sxy_eeg10', 'phl_eeg13', 'ly_eeg7', 'ys_eeg2', 'lqj_eeg14', 'zjy_eeg7', 'jl_eeg8', 'wk_eeg2', 'lqj_eeg14', 'jj_eeg15', 'jj_eeg14', 'ww_eeg5', 'zjy_eeg8', 'mhw_eeg9', 'wyw_eeg10', 'ly_eeg5', 'wk_eeg12', 'sxy_eeg10', 'ys_eeg8', 'phl_eeg13', 'sxy_eeg14', 'lqj_eeg14', 'jj_eeg8', 'wk_eeg1', 'jj_eeg9', 'sxy_eeg3', 'phl_eeg7', 'jl_eeg3', 'wsf_eeg7', 'zjy_eeg3', 'wk_eeg7', 'phl_eeg1', 'wyw_eeg5', 'wsf_eeg4', 'wyw_eeg12', 'wk_eeg15', 'wk_eeg7', 'ww_eeg6', 'ly_eeg11', 'ww_eeg1', 'ww_eeg13', 'sxy_eeg10', 'sxy_eeg13', 'jl_eeg14', 'jl_eeg9', 'wsf_eeg14', 'zjy_eeg10', 'wyw_eeg6', 'mhw_eeg11', 'wyw_eeg2', 'sxy_eeg15', 'ww_eeg4', 'sxy_eeg4', 'phl_eeg11', 'mhw_eeg6', 'wsf_eeg7', 'wsf_eeg7', 'jj_eeg14', 'jl_eeg3', 'jj_eeg10', 'xyl_eeg3', 'wsf_eeg12', 'phl_eeg1', 'lqj_eeg11', 'wyw_eeg7', 'jl_eeg3', 'mhw_eeg7', 'ly_eeg1', 'jl_eeg1', 'wyw_eeg8', 'lqj_eeg1', 'wk_eeg8', 'wyw_eeg7', 'jj_eeg7', 'wyw_eeg14', 'wsf_eeg6', 'zjy_eeg10', 'jj_eeg3', 'sxy_eeg12', 'ww_eeg5', 'mhw_eeg4', 'ys_eeg7', 'jl_eeg12', 'ww_eeg4', 'ww_eeg13', 'wsf_eeg1', 'wk_eeg4', 'wsf_eeg9', 'jj_eeg12', 'lqj_eeg9', 'phl_eeg10', 'ys_eeg10', 'mhw_eeg9', 'jl_eeg10', 'xyl_eeg1', 'wk_eeg7', 'phl_eeg7', 'jj_eeg5', 'ys_eeg1', 'xyl_eeg12', 'phl_eeg1', 'wyw_eeg4', 'lqj_eeg13', 'wyw_eeg15', 'wk_eeg7', 'jj_eeg11', 'zjy_eeg15', 'ww_eeg12', 'wyw_eeg13', 'xyl_eeg11', 'wsf_eeg10', 'wsf_eeg11', 'xyl_eeg2', 'wsf_eeg7', 'wyw_eeg9', 'lqj_eeg13', 'wk_eeg1', 'zjy_eeg8', 'jl_eeg3', 'mhw_eeg13', 'wsf_eeg14', 'phl_eeg7', 'wk_eeg7', 'xyl_eeg4', 'mhw_eeg4', 'xyl_eeg6', 'mhw_eeg11', 'jl_eeg8', 'jl_eeg9', 'sxy_eeg9', 'wsf_eeg12', 'xyl_eeg8', 'mhw_eeg1', 'xyl_eeg14', 'mhw_eeg1', 'ly_eeg11', 'wk_eeg15', 'xyl_eeg13', 'xyl_eeg12', 'lqj_eeg1', 'wk_eeg15', 'ly_eeg11', 'jj_eeg12', 'wsf_eeg4', 'ys_eeg8', 'zjy_eeg13', 'mhw_eeg9', 'phl_eeg8', 'jl_eeg7', 'jl_eeg13', 'mhw_eeg2', 'wsf_eeg2', 'wsf_eeg14', 'ly_eeg7', 'xyl_eeg1', 'ly_eeg14', 'lqj_eeg2', 'jj_eeg6', 'sxy_eeg3', 'mhw_eeg11', 'jl_eeg9', 'ly_eeg14', 'xyl_eeg3', 'wyw_eeg2', 'wsf_eeg2', 'lqj_eeg8', 'ys_eeg8', 'ly_eeg7', 'wsf_eeg14', 'ys_eeg12', 'ww_eeg12', 'wk_eeg4', 'xyl_eeg10', 'phl_eeg8', 'mhw_eeg2', 'phl_eeg8', 'wk_eeg2'], 'emotion': tensor([ 0,  0, -1,  1,  1, -1,  1,  0, -1, -1,  1, -1, -1,  1,  1,  1, -1,  0,\n",
      "        -1,  1,  1,  0,  1, -1,  0,  0,  0,  1, -1, -1,  0,  0,  0,  1, -1, -1,\n",
      "        -1,  0,  0, -1,  0,  0,  0,  0,  1, -1, -1,  1,  1, -1,  0,  1,  1,  0,\n",
      "        -1,  1,  1,  1,  1,  0,  1,  1,  1,  1, -1,  0,  1,  0, -1,  1, -1,  0,\n",
      "         0,  1,  1,  1, -1,  1, -1,  1,  0, -1,  0,  1, -1,  0,  0,  1, -1,  1,\n",
      "         0,  0,  1,  1,  0, -1,  1,  0,  0,  1,  1,  0,  1,  1, -1, -1, -1, -1,\n",
      "        -1, -1,  1,  0, -1, -1, -1, -1,  1,  0,  1,  0,  1,  0,  1,  1,  1,  1,\n",
      "         1,  0,  0, -1, -1, -1,  0,  1, -1, -1,  1, -1,  1, -1, -1,  1,  0, -1,\n",
      "        -1, -1,  1,  1,  0,  1,  0, -1, -1,  1,  1,  1, -1, -1,  0, -1, -1, -1,\n",
      "        -1,  0,  1, -1,  1, -1,  1,  1,  1,  1,  1,  1, -1, -1,  0,  1, -1,  1,\n",
      "        -1,  0, -1, -1,  0, -1, -1,  0,  0,  1,  0,  0, -1,  1,  0,  1,  0, -1,\n",
      "         0,  1, -1, -1, -1, -1,  1,  0,  0,  1,  1, -1,  0,  1,  1,  1,  0, -1,\n",
      "         0, -1,  1, -1,  0, -1, -1,  0,  0,  1,  0, -1,  0,  0,  0,  1, -1,  1,\n",
      "         1,  0,  1, -1,  0,  1,  1, -1,  0,  0,  0,  0, -1,  1, -1, -1, -1,  1,\n",
      "         0,  0,  0,  0]), 'date': tensor([20140411, 20131130, 20131211, 20130709, 20130712, 20140705, 20140404,\n",
      "        20130709, 20131207, 20140521, 20140527, 20140404, 20131127, 20140527,\n",
      "        20131016, 20131204, 20131027, 20131106, 20140630, 20130712, 20140603,\n",
      "        20140511, 20140527, 20140506, 20130712, 20140629, 20140705, 20140702,\n",
      "        20140705, 20140705, 20131030, 20140511, 20140603, 20140610, 20131027,\n",
      "        20140603, 20131130, 20140629, 20140521, 20140625, 20131211, 20140621,\n",
      "        20140521, 20140610, 20140611, 20140521, 20131030, 20140610, 20140620,\n",
      "        20140610, 20131106, 20140615, 20131016, 20131030, 20140705, 20140625,\n",
      "        20140702, 20131016, 20140521, 20140514, 20140514, 20140601, 20140705,\n",
      "        20131201, 20131201, 20140601, 20140621, 20140704, 20130712, 20131027,\n",
      "        20140618, 20131201, 20131207, 20140521, 20140404, 20130709, 20140627,\n",
      "        20131201, 20140705, 20140511, 20131106, 20140411, 20140627, 20140621,\n",
      "        20131105, 20140413, 20140627, 20140621, 20140603, 20140603, 20131204,\n",
      "        20131105, 20131113, 20131207, 20140411, 20140704, 20140521, 20140615,\n",
      "        20131106, 20140514, 20140702, 20140611, 20140704, 20140611, 20140514,\n",
      "        20131030, 20140413, 20140618, 20131016, 20140627, 20131030, 20131127,\n",
      "        20140618, 20131201, 20140620, 20140627, 20131204, 20140418, 20131130,\n",
      "        20131130, 20140514, 20140514, 20140413, 20140404, 20140618, 20130709,\n",
      "        20131127, 20130712, 20131201, 20140514, 20131204, 20140521, 20131106,\n",
      "        20131113, 20140618, 20140630, 20140611, 20140404, 20140629, 20140610,\n",
      "        20140618, 20131030, 20140705, 20131207, 20140413, 20131016, 20140411,\n",
      "        20140404, 20131207, 20140702, 20140627, 20131201, 20140629, 20131207,\n",
      "        20140618, 20131105, 20140603, 20140521, 20131130, 20130712, 20140615,\n",
      "        20140413, 20131211, 20131130, 20140625, 20140627, 20140630, 20140611,\n",
      "        20140705, 20131027, 20140601, 20131016, 20140413, 20140603, 20140627,\n",
      "        20131030, 20140603, 20140627, 20140610, 20131106, 20131127, 20140705,\n",
      "        20131207, 20140704, 20140611, 20130709, 20131204, 20131127, 20140610,\n",
      "        20140630, 20140618, 20140603, 20140630, 20131201, 20140621, 20140627,\n",
      "        20131105, 20140419, 20131113, 20140618, 20131027, 20140620, 20140610,\n",
      "        20131016, 20140527, 20131113, 20140413, 20140419, 20140511, 20140618,\n",
      "        20140603, 20131016, 20140527, 20131016, 20140411, 20140620, 20140610,\n",
      "        20140610, 20140702, 20140627, 20140506, 20140629, 20140618, 20140627,\n",
      "        20131105, 20131113, 20131027, 20140413, 20140419, 20131113, 20140630,\n",
      "        20140618, 20140411, 20140527, 20140411, 20140705, 20140629, 20140521,\n",
      "        20131016, 20140404, 20140411, 20140610, 20131201, 20140630, 20140705,\n",
      "        20140627, 20140418, 20140618, 20140627, 20131211, 20140620, 20140610,\n",
      "        20131106, 20130712, 20131027, 20140620]), '_record_id': ['_record_30', '_record_0', '_record_2', '_record_15', '_record_33', '_record_29', '_record_21', '_record_15', '_record_8', '_record_41', '_record_9', '_record_21', '_record_6', '_record_9', '_record_34', '_record_1', '_record_36', '_record_38', '_record_5', '_record_33', '_record_10', '_record_39', '_record_9', '_record_32', '_record_33', '_record_26', '_record_29', '_record_28', '_record_29', '_record_29', '_record_37', '_record_39', '_record_10', '_record_11', '_record_36', '_record_10', '_record_0', '_record_26', '_record_41', '_record_4', '_record_2', '_record_27', '_record_41', '_record_11', '_record_25', '_record_41', '_record_37', '_record_11', '_record_42', '_record_11', '_record_38', '_record_13', '_record_34', '_record_37', '_record_29', '_record_4', '_record_28', '_record_16', '_record_41', '_record_40', '_record_40', '_record_12', '_record_29', '_record_7', '_record_7', '_record_12', '_record_27', '_record_44', '_record_33', '_record_36', '_record_3', '_record_7', '_record_8', '_record_41', '_record_21', '_record_15', '_record_43', '_record_7', '_record_29', '_record_39', '_record_38', '_record_30', '_record_14', '_record_27', '_record_17', '_record_22', '_record_43', '_record_27', '_record_24', '_record_24', '_record_1', '_record_17', '_record_35', '_record_8', '_record_30', '_record_44', '_record_41', '_record_13', '_record_38', '_record_40', '_record_28', '_record_25', '_record_44', '_record_25', '_record_40', '_record_37', '_record_22', '_record_3', '_record_16', '_record_43', '_record_37', '_record_6', '_record_3', '_record_7', '_record_42', '_record_43', '_record_1', '_record_31', '_record_0', '_record_0', '_record_40', '_record_40', '_record_22', '_record_21', '_record_3', '_record_15', '_record_6', '_record_33', '_record_7', '_record_40', '_record_1', '_record_41', '_record_38', '_record_35', '_record_3', '_record_5', '_record_25', '_record_21', '_record_26', '_record_11', '_record_3', '_record_37', '_record_29', '_record_8', '_record_22', '_record_34', '_record_30', '_record_21', '_record_8', '_record_28', '_record_43', '_record_7', '_record_26', '_record_8', '_record_3', '_record_17', '_record_24', '_record_41', '_record_0', '_record_33', '_record_13', '_record_22', '_record_2', '_record_0', '_record_4', '_record_43', '_record_5', '_record_25', '_record_29', '_record_36', '_record_12', '_record_34', '_record_22', '_record_10', '_record_43', '_record_37', '_record_24', '_record_14', '_record_11', '_record_38', '_record_6', '_record_29', '_record_8', '_record_44', '_record_25', '_record_15', '_record_1', '_record_6', '_record_11', '_record_5', '_record_3', '_record_10', '_record_5', '_record_7', '_record_27', '_record_43', '_record_17', '_record_23', '_record_35', '_record_3', '_record_36', '_record_42', '_record_11', '_record_34', '_record_9', '_record_35', '_record_22', '_record_23', '_record_39', '_record_3', '_record_10', '_record_34', '_record_9', '_record_34', '_record_30', '_record_42', '_record_11', '_record_11', '_record_28', '_record_43', '_record_32', '_record_26', '_record_3', '_record_14', '_record_17', '_record_35', '_record_36', '_record_22', '_record_23', '_record_35', '_record_5', '_record_3', '_record_30', '_record_9', '_record_30', '_record_29', '_record_26', '_record_41', '_record_34', '_record_21', '_record_30', '_record_11', '_record_7', '_record_5', '_record_29', '_record_14', '_record_31', '_record_3', '_record_14', '_record_2', '_record_42', '_record_11', '_record_38', '_record_33', '_record_36', '_record_42'], 'segment_start': tensor([120,  30, 165, 135,  45,  75, 120,  60,  90,  45, 120, 120,   0,  15,\n",
      "        135,  30, 105, 165,   0,  90,  90, 150, 135, 165,  90,  30,  60,  60,\n",
      "         90,  45, 150,  60,  60, 105, 120,  90,  45,  30, 135, 150, 135, 165,\n",
      "         45,  30,  60, 120, 105, 165, 150,  90,  60, 150,  60, 105, 105,  60,\n",
      "         90, 120, 135, 165,  45, 150, 105,  90,  15,  90, 120, 165,  75, 150,\n",
      "        165, 165, 165,  45, 150,  30,  45,  60, 135, 105, 105, 165, 165, 120,\n",
      "         60,  60,  60, 135,  45,   0,  15, 135,  15, 150,  30, 120, 105,  15,\n",
      "         15, 165, 135, 120, 105,  15,  90, 135, 120, 165, 150,   0,  60, 165,\n",
      "        150,  15,  75,  30, 165,   0,  90,   0,  75, 165,   0, 165, 165,  15,\n",
      "         45,  45,   0,  15, 165,  75, 105,   0,  90,  15, 120,  45, 105,  15,\n",
      "        105,   0,  75, 165,  15,  75, 120,  45,  15, 120, 105,  45, 120,  15,\n",
      "         60, 135,  90,  90, 120,  60, 120, 150, 135,  45,  15,  45, 150,   0,\n",
      "         45, 165,  45, 105,  15, 120, 150,  75,  30,   0,  30,   0,  75, 150,\n",
      "         45,  30,  45, 105,  60, 135, 135, 120,   0,  75,  60,  15,   0,  75,\n",
      "        105,  75, 120,  45,  90,  30, 105, 165, 135,  75,  45, 120,   0,  15,\n",
      "        105,  45,  30, 150,  90,  75,   0,  15,   0,  90, 150, 120,   0,  75,\n",
      "        135, 165, 150,  30,  15,   0,  15, 135, 165, 105, 105,  60,  60, 135,\n",
      "        135,  75, 165,  30,   0, 165,  75,  15, 120, 135,  45, 150, 120, 150,\n",
      "         75, 135,   0,   0]), 'segment_end': tensor([150,  60, 195, 165,  75, 105, 150,  90, 120,  75, 150, 150,  30,  45,\n",
      "        165,  60, 135, 195,  30, 120, 120, 180, 165, 195, 120,  60,  90,  90,\n",
      "        120,  75, 180,  90,  90, 135, 150, 120,  75,  60, 165, 180, 165, 195,\n",
      "         75,  60,  90, 150, 135, 195, 180, 120,  90, 180,  90, 135, 135,  90,\n",
      "        120, 150, 165, 195,  75, 180, 135, 120,  45, 120, 150, 195, 105, 180,\n",
      "        195, 195, 195,  75, 180,  60,  75,  90, 165, 135, 135, 195, 195, 150,\n",
      "         90,  90,  90, 165,  75,  30,  45, 165,  45, 180,  60, 150, 135,  45,\n",
      "         45, 195, 165, 150, 135,  45, 120, 165, 150, 195, 180,  30,  90, 195,\n",
      "        180,  45, 105,  60, 195,  30, 120,  30, 105, 195,  30, 195, 195,  45,\n",
      "         75,  75,  30,  45, 195, 105, 135,  30, 120,  45, 150,  75, 135,  45,\n",
      "        135,  30, 105, 195,  45, 105, 150,  75,  45, 150, 135,  75, 150,  45,\n",
      "         90, 165, 120, 120, 150,  90, 150, 180, 165,  75,  45,  75, 180,  30,\n",
      "         75, 195,  75, 135,  45, 150, 180, 105,  60,  30,  60,  30, 105, 180,\n",
      "         75,  60,  75, 135,  90, 165, 165, 150,  30, 105,  90,  45,  30, 105,\n",
      "        135, 105, 150,  75, 120,  60, 135, 195, 165, 105,  75, 150,  30,  45,\n",
      "        135,  75,  60, 180, 120, 105,  30,  45,  30, 120, 180, 150,  30, 105,\n",
      "        165, 195, 180,  60,  45,  30,  45, 165, 195, 135, 135,  90,  90, 165,\n",
      "        165, 105, 195,  60,  30, 195, 105,  45, 150, 165,  75, 180, 150, 180,\n",
      "        105, 165,  30,  30])}\n",
      "Batch 4: Data Shape = torch.Size([256, 62, 30]), Labels = {'start_at': tensor([39000, 22000, 27800, 19000, 36000, 31400,   200,     0, 22000, 22200,\n",
      "        17000, 22400, 31400, 27600, 21800, 29000,   200, 23400, 47600, 38200,\n",
      "        29400, 17800, 17800, 42600, 17200, 34800, 25000, 30800,  9600,   800,\n",
      "        45800, 25600, 21000, 33200, 35200, 16800, 19200,  2200, 43600,  6600,\n",
      "        20600, 16600,  7200,  9800, 17600,  2000, 39000, 37800, 30600,  9000,\n",
      "        41400, 45400, 17800,  8000, 20600, 26600, 24800, 46800,  9800,  5800,\n",
      "        13600, 27200,  3800, 19800, 20000, 33400,  3800, 40000,  3800,  6400,\n",
      "        33800, 39400, 13400, 29600, 41200, 24000,  9800, 44400, 40000, 41400,\n",
      "        10000,  2600,  3600, 30600, 34600,  7000, 42400, 29200, 13200, 24400,\n",
      "        14800, 16800, 22800,  5600, 12800, 30400, 27000,  9800, 10400, 27200,\n",
      "        33600, 15600, 16800, 22200,   800, 34000, 37600, 47600, 11600, 39600,\n",
      "        32200, 26600, 34200, 20400, 13400, 20800, 18000, 35600, 29800, 30400,\n",
      "        35400, 25800, 32600,  3800, 15400, 30600, 15200, 37000,  7000, 22600,\n",
      "        30200, 33200, 19200, 13400,  4200, 26600, 28400, 40200, 43800, 13400,\n",
      "        23600, 43200, 22000,  7600, 24400, 25000, 33400, 24600, 11800, 28400,\n",
      "        47200, 34400, 46200, 11400, 46000, 37400, 27600, 18200, 44600, 33200,\n",
      "        24600, 16200, 13200, 35000, 33800,  4800, 18000,  7000, 29400, 15600,\n",
      "         6400,  5000,   800, 30000,  2600,  6600, 24400, 19000,  1800, 37200,\n",
      "         9200, 41200, 11400, 13400,  3000, 34400,  9400, 31800, 14400,  1800,\n",
      "         6600,  3600, 39600, 26400,   600,  5000, 33600, 42000, 42000,   800,\n",
      "        18800, 13000, 28400, 14200, 15600, 29600, 34000,   400, 38200, 24600,\n",
      "        11000,  2400, 26000, 40000, 13600, 18400, 11000, 23400, 22200, 36800,\n",
      "        33000, 19800, 10000, 18600,   200, 40600,  9800, 30600, 26600, 23600,\n",
      "        49600, 20600, 27400, 32400, 12000, 16800, 37800, 32600, 12800, 26600,\n",
      "        39600,  4200, 31400,  7400, 33400,   800, 15600, 34400, 19000,  4400,\n",
      "         1400, 43600, 37000, 14400, 37800, 20000]), 'end_at': tensor([39200, 22200, 28000, 19200, 36200, 31600,   400,   200, 22200, 22400,\n",
      "        17200, 22600, 31600, 27800, 22000, 29200,   400, 23600, 47800, 38400,\n",
      "        29600, 18000, 18000, 42800, 17400, 35000, 25200, 31000,  9800,  1000,\n",
      "        46000, 25800, 21200, 33400, 35400, 17000, 19400,  2400, 43800,  6800,\n",
      "        20800, 16800,  7400, 10000, 17800,  2200, 39200, 38000, 30800,  9200,\n",
      "        41600, 45600, 18000,  8200, 20800, 26800, 25000, 47000, 10000,  6000,\n",
      "        13800, 27400,  4000, 20000, 20200, 33600,  4000, 40200,  4000,  6600,\n",
      "        34000, 39600, 13600, 29800, 41400, 24200, 10000, 44600, 40200, 41600,\n",
      "        10200,  2800,  3800, 30800, 34800,  7200, 42600, 29400, 13400, 24600,\n",
      "        15000, 17000, 23000,  5800, 13000, 30600, 27200, 10000, 10600, 27400,\n",
      "        33800, 15800, 17000, 22400,  1000, 34200, 37800, 47800, 11800, 39800,\n",
      "        32400, 26800, 34400, 20600, 13600, 21000, 18200, 35800, 30000, 30600,\n",
      "        35600, 26000, 32800,  4000, 15600, 30800, 15400, 37200,  7200, 22800,\n",
      "        30400, 33400, 19400, 13600,  4400, 26800, 28600, 40400, 44000, 13600,\n",
      "        23800, 43400, 22200,  7800, 24600, 25200, 33600, 24800, 12000, 28600,\n",
      "        47400, 34600, 46400, 11600, 46200, 37600, 27800, 18400, 44800, 33400,\n",
      "        24800, 16400, 13400, 35200, 34000,  5000, 18200,  7200, 29600, 15800,\n",
      "         6600,  5200,  1000, 30200,  2800,  6800, 24600, 19200,  2000, 37400,\n",
      "         9400, 41400, 11600, 13600,  3200, 34600,  9600, 32000, 14600,  2000,\n",
      "         6800,  3800, 39800, 26600,   800,  5200, 33800, 42200, 42200,  1000,\n",
      "        19000, 13200, 28600, 14400, 15800, 29800, 34200,   600, 38400, 24800,\n",
      "        11200,  2600, 26200, 40200, 13800, 18600, 11200, 23600, 22400, 37000,\n",
      "        33200, 20000, 10200, 18800,   400, 40800, 10000, 30800, 26800, 23800,\n",
      "        49800, 20800, 27600, 32600, 12200, 17000, 38000, 32800, 13000, 26800,\n",
      "        39800,  4400, 31600,  7600, 33600,  1000, 15800, 34600, 19200,  4600,\n",
      "         1600, 43800, 37200, 14600, 38000, 20200]), 'clip_id': ['9_20140620.mat_869', '3_20140603.mat_110', '15_20130709.mat_374', '6_20131113.mat_330', '12_20131127.mat_2895', '2_20140419.mat_1254', '13_20140527.mat_2011', '6_20131016.mat_1292', '13_20140610.mat_784', '4_20140621.mat_1023', '11_20140630.mat_2800', '15_20130709.mat_3062', '12_20131201.mat_1686', '14_20140601.mat_1235', '13_20140603.mat_3059', '14_20140615.mat_2627', '8_20140511.mat_3189', '2_20140413.mat_2127', '14_20140601.mat_1983', '14_20140615.mat_2438', '5_20140418.mat_2157', '11_20140625.mat_557', '2_20140413.mat_1618', '8_20140514.mat_1742', '3_20140611.mat_86', '15_20131105.mat_2889', '6_20131113.mat_360', '4_20140702.mat_1683', '3_20140629.mat_2058', '11_20140630.mat_1749', '12_20131207.mat_2476', '13_20140603.mat_1873', '15_20131105.mat_2352', '3_20140611.mat_1078', '6_20130712.mat_1468', '3_20140611.mat_552', '3_20140603.mat_1841', '3_20140611.mat_1756', '4_20140621.mat_218', '15_20131016.mat_2280', '4_20140705.mat_1200', '2_20140404.mat_3033', '7_20131027.mat_36', '2_20140419.mat_2764', '7_20131027.mat_1617', '14_20140615.mat_922', '7_20131106.mat_2205', '4_20140702.mat_863', '11_20140618.mat_827', '14_20140615.mat_1337', '7_20131030.mat_2454', '6_20131016.mat_2942', '8_20140514.mat_1186', '2_20140413.mat_1137', '2_20140413.mat_2585', '12_20131207.mat_3321', '3_20140611.mat_1653', '14_20140627.mat_234', '2_20140419.mat_2296', '14_20140627.mat_2276', '11_20140625.mat_1165', '8_20140521.mat_3086', '9_20140620.mat_2501', '14_20140601.mat_2346', '9_20140620.mat_1845', '7_20131027.mat_1912', '12_20131207.mat_1548', '13_20140610.mat_2447', '5_20140506.mat_1116', '11_20140630.mat_1777', '12_20131127.mat_1698', '2_20140404.mat_665', '14_20140627.mat_535', '4_20140621.mat_822', '15_20130709.mat_1498', '9_20140620.mat_588', '13_20140610.mat_284', '12_20131127.mat_896', '14_20140627.mat_3388', '13_20140610.mat_1499', '6_20131016.mat_962', '4_20140621.mat_1305', '11_20140618.mat_1310', '8_20140514.mat_1065', '5_20140506.mat_1918', '12_20131127.mat_1564', '13_20140527.mat_2694', '9_20140704.mat_2393', '15_20131016.mat_3016', '3_20140611.mat_357', '5_20140418.mat_542', '2_20140413.mat_2331', '14_20140615.mat_2361', '11_20140618.mat_3216', '8_20140511.mat_1809', '6_20131016.mat_2399', '2_20140404.mat_2145', '12_20131201.mat_517', '3_20140603.mat_520', '4_20140702.mat_2146', '11_20140630.mat_1697', '6_20131016.mat_1175', '2_20140419.mat_2094', '15_20131016.mat_2358', '4_20140705.mat_3192', '9_20140627.mat_1462', '9_20140620.mat_423', '2_20140419.mat_1983', '8_20140511.mat_293', '6_20130712.mat_666', '10_20131204.mat_3349', '12_20131207.mat_1230', '2_20140419.mat_1700', '4_20140702.mat_2584', '2_20140419.mat_2549', '7_20131027.mat_3292', '13_20140527.mat_1619', '9_20140627.mat_2893', '10_20131204.mat_1678', '11_20140630.mat_2162', '11_20140625.mat_2187', '15_20131016.mat_1226', '6_20131016.mat_631', '5_20140411.mat_1764', '14_20140601.mat_1174', '7_20131030.mat_3341', '14_20140615.mat_311', '10_20131204.mat_1930', '9_20140704.mat_2282', '10_20131211.mat_3301', '15_20131105.mat_2161', '10_20131130.mat_1911', '11_20140618.mat_1841', '7_20131106.mat_979', '11_20140625.mat_2503', '2_20140413.mat_2848', '3_20140629.mat_2389', '3_20140611.mat_3151', '3_20140629.mat_893', '5_20140411.mat_1596', '13_20140603.mat_1863', '12_20131201.mat_216', '2_20140419.mat_2120', '13_20140527.mat_2285', '7_20131030.mat_2837', '10_20131204.mat_1654', '11_20140630.mat_635', '7_20131030.mat_1868', '4_20140705.mat_971', '2_20140419.mat_3330', '3_20140629.mat_2246', '14_20140615.mat_1701', '10_20131204.mat_2946', '7_20131030.mat_731', '7_20131030.mat_2477', '6_20131016.mat_1284', '8_20140511.mat_606', '2_20140419.mat_91', '9_20140627.mat_1968', '9_20140627.mat_1695', '15_20131105.mat_1220', '7_20131027.mat_1178', '14_20140615.mat_3254', '13_20140603.mat_1272', '10_20131211.mat_1081', '13_20140603.mat_1316', '14_20140627.mat_90', '12_20131127.mat_3223', '8_20140514.mat_3335', '10_20131211.mat_752', '14_20140615.mat_32', '3_20140611.mat_2035', '8_20140521.mat_1533', '2_20140413.mat_1062', '7_20131027.mat_925', '9_20140620.mat_2515', '10_20131211.mat_3310', '4_20140705.mat_2577', '13_20140603.mat_477', '8_20140511.mat_1715', '11_20140630.mat_2528', '8_20140521.mat_3156', '6_20131113.mat_2067', '12_20131127.mat_3255', '12_20131201.mat_483', '6_20131113.mat_2654', '6_20131016.mat_1576', '11_20140630.mat_2406', '10_20131130.mat_2787', '13_20140527.mat_3197', '2_20140419.mat_1130', '8_20140521.mat_1310', '14_20140615.mat_3386', '12_20131207.mat_3320', '4_20140705.mat_3', '10_20131130.mat_3213', '11_20140625.mat_403', '11_20140625.mat_2692', '2_20140413.mat_210', '7_20131030.mat_678', '11_20140625.mat_3044', '8_20140511.mat_300', '5_20140418.mat_1054', '2_20140413.mat_1168', '4_20140621.mat_752', '5_20140411.mat_3336', '5_20140418.mat_2180', '11_20140625.mat_676', '4_20140621.mat_2673', '12_20131201.mat_1868', '4_20140705.mat_729', '3_20140629.mat_2962', '3_20140629.mat_2612', '11_20140630.mat_200', '11_20140618.mat_2078', '5_20140506.mat_1621', '3_20140611.mat_2302', '11_20140630.mat_1214', '14_20140627.mat_1403', '9_20140627.mat_2899', '6_20131113.mat_2412', '9_20140704.mat_773', '10_20131204.mat_2060', '9_20140627.mat_1385', '10_20131130.mat_2716', '8_20140514.mat_2450', '4_20140702.mat_284', '14_20140615.mat_1065', '14_20140601.mat_368', '9_20140627.mat_1647', '4_20140621.mat_1993', '14_20140615.mat_777', '15_20131105.mat_2147', '15_20130709.mat_630', '9_20140627.mat_734', '9_20140620.mat_319', '4_20140705.mat_1286', '9_20140627.mat_1075', '12_20131201.mat_2546', '11_20140625.mat_1878', '14_20140627.mat_666', '7_20131030.mat_2031', '6_20131016.mat_1254', '5_20140411.mat_2284', '10_20131211.mat_1912', '15_20131016.mat_1101', '5_20140506.mat_3028', '10_20131130.mat_2887', '5_20140411.mat_1387', '2_20140404.mat_1119', '3_20140629.mat_2489', '3_20140603.mat_1510', '2_20140419.mat_2667', '12_20131201.mat_307', '15_20130709.mat_1934', '4_20140702.mat_2815'], 'subject_id': tensor([ 8,  2, 14,  5, 11,  1, 12,  5, 12,  3, 10, 14, 11, 13, 12, 13,  7,  1,\n",
      "        13, 13,  4, 10,  1,  7,  2, 14,  5,  3,  2, 10, 11, 12, 14,  2,  5,  2,\n",
      "         2,  2,  3, 14,  3,  1,  6,  1,  6, 13,  6,  3, 10, 13,  6,  5,  7,  1,\n",
      "         1, 11,  2, 13,  1, 13, 10,  7,  8, 13,  8,  6, 11, 12,  4, 10, 11,  1,\n",
      "        13,  3, 14,  8, 12, 11, 13, 12,  5,  3, 10,  7,  4, 11, 12,  8, 14,  2,\n",
      "         4,  1, 13, 10,  7,  5,  1, 11,  2,  3, 10,  5,  1, 14,  3,  8,  8,  1,\n",
      "         7,  5,  9, 11,  1,  3,  1,  6, 12,  8,  9, 10, 10, 14,  5,  4, 13,  6,\n",
      "        13,  9,  8,  9, 14,  9, 10,  6, 10,  1,  2,  2,  2,  4, 12, 11,  1, 12,\n",
      "         6,  9, 10,  6,  3,  1,  2, 13,  9,  6,  6,  5,  7,  1,  8,  8, 14,  6,\n",
      "        13, 12,  9, 12, 13, 11,  7,  9, 13,  2,  7,  1,  6,  8,  9,  3, 12,  7,\n",
      "        10,  7,  5, 11, 11,  5,  5, 10,  9, 12,  1,  7, 13, 11,  3,  9, 10, 10,\n",
      "         1,  6, 10,  7,  4,  1,  3,  4,  4, 10,  3, 11,  3,  2,  2, 10, 10,  4,\n",
      "         2, 10, 13,  8,  5,  8,  9,  8,  9,  7,  3, 13, 13,  8,  3, 13, 14, 14,\n",
      "         8,  8,  3,  8, 11, 10, 13,  6,  5,  4,  9, 14,  4,  9,  4,  1,  2,  2,\n",
      "         1, 11, 14,  3]), 'trial_id': ['wk_eeg4', 'jj_eeg1', 'zjy_eeg2', 'mhw_eeg2', 'wyw_eeg13', 'jl_eeg6', 'xyl_eeg10', 'mhw_eeg7', 'xyl_eeg4', 'lqj_eeg5', 'wsf_eeg13', 'zjy_eeg14', 'wyw_eeg8', 'ys_eeg6', 'xyl_eeg14', 'ys_eeg12', 'sxy_eeg15', 'jl_eeg10', 'ys_eeg9', 'ys_eeg11', 'ly_eeg10', 'wsf_eeg3', 'jl_eeg8', 'sxy_eeg8', 'jj_eeg1', 'zjy_eeg13', 'mhw_eeg2', 'lqj_eeg8', 'jj_eeg10', 'wsf_eeg9', 'wyw_eeg11', 'xyl_eeg9', 'zjy_eeg11', 'jj_eeg5', 'mhw_eeg7', 'jj_eeg3', 'jj_eeg9', 'jj_eeg9', 'lqj_eeg1', 'zjy_eeg11', 'lqj_eeg6', 'jl_eeg14', 'phl_eeg1', 'jl_eeg13', 'phl_eeg8', 'ys_eeg5', 'phl_eeg10', 'lqj_eeg4', 'wsf_eeg4', 'ys_eeg7', 'phl_eeg11', 'mhw_eeg13', 'sxy_eeg6', 'jl_eeg6', 'jl_eeg12', 'wyw_eeg15', 'jj_eeg8', 'ys_eeg1', 'jl_eeg11', 'ys_eeg11', 'wsf_eeg6', 'sxy_eeg14', 'wk_eeg12', 'ys_eeg11', 'wk_eeg9', 'phl_eeg9', 'wyw_eeg8', 'xyl_eeg11', 'ly_eeg6', 'wsf_eeg9', 'wyw_eeg8', 'jl_eeg3', 'ys_eeg3', 'lqj_eeg4', 'zjy_eeg7', 'wk_eeg3', 'xyl_eeg2', 'wyw_eeg4', 'ys_eeg15', 'xyl_eeg7', 'mhw_eeg5', 'lqj_eeg7', 'wsf_eeg7', 'sxy_eeg5', 'ly_eeg9', 'wyw_eeg8', 'xyl_eeg12', 'wk_eeg11', 'zjy_eeg14', 'jj_eeg2', 'ly_eeg3', 'jl_eeg11', 'ys_eeg11', 'wsf_eeg15', 'sxy_eeg9', 'mhw_eeg11', 'jl_eeg10', 'wyw_eeg3', 'jj_eeg3', 'lqj_eeg10', 'wsf_eeg8', 'mhw_eeg6', 'jl_eeg10', 'zjy_eeg11', 'lqj_eeg15', 'wk_eeg7', 'wk_eeg2', 'jl_eeg9', 'sxy_eeg2', 'mhw_eeg3', 'ww_eeg15', 'wyw_eeg6', 'jl_eeg8', 'lqj_eeg12', 'jl_eeg12', 'phl_eeg15', 'xyl_eeg8', 'wk_eeg13', 'ww_eeg8', 'wsf_eeg10', 'wsf_eeg10', 'zjy_eeg6', 'mhw_eeg3', 'ly_eeg9', 'ys_eeg6', 'phl_eeg15', 'ys_eeg2', 'ww_eeg9', 'wk_eeg11', 'ww_eeg15', 'zjy_eeg10', 'ww_eeg9', 'wsf_eeg9', 'phl_eeg5', 'wsf_eeg12', 'jl_eeg13', 'jj_eeg11', 'jj_eeg14', 'jj_eeg4', 'ly_eeg8', 'xyl_eeg9', 'wyw_eeg1', 'jl_eeg10', 'xyl_eeg11', 'phl_eeg13', 'ww_eeg8', 'wsf_eeg3', 'phl_eeg9', 'lqj_eeg5', 'jl_eeg15', 'jj_eeg10', 'ys_eeg8', 'ww_eeg13', 'phl_eeg4', 'phl_eeg11', 'mhw_eeg6', 'sxy_eeg3', 'jl_eeg1', 'wk_eeg9', 'wk_eeg8', 'zjy_eeg6', 'phl_eeg6', 'ys_eeg15', 'xyl_eeg6', 'ww_eeg5', 'xyl_eeg7', 'ys_eeg1', 'wyw_eeg15', 'sxy_eeg15', 'ww_eeg4', 'ys_eeg1', 'jj_eeg10', 'sxy_eeg8', 'jl_eeg5', 'phl_eeg5', 'wk_eeg12', 'ww_eeg15', 'lqj_eeg12', 'xyl_eeg3', 'sxy_eeg8', 'wsf_eeg12', 'sxy_eeg14', 'mhw_eeg10', 'wyw_eeg15', 'wyw_eeg3', 'mhw_eeg12', 'mhw_eeg8', 'wsf_eeg11', 'ww_eeg13', 'xyl_eeg15', 'jl_eeg6', 'sxy_eeg7', 'ys_eeg15', 'wyw_eeg15', 'lqj_eeg1', 'ww_eeg15', 'wsf_eeg2', 'wsf_eeg12', 'jl_eeg1', 'phl_eeg4', 'wsf_eeg14', 'sxy_eeg2', 'ly_eeg5', 'jl_eeg6', 'lqj_eeg4', 'ly_eeg15', 'ly_eeg10', 'wsf_eeg4', 'lqj_eeg12', 'wyw_eeg9', 'lqj_eeg4', 'jj_eeg14', 'jj_eeg12', 'wsf_eeg1', 'wsf_eeg10', 'ly_eeg8', 'jj_eeg11', 'wsf_eeg6', 'ys_eeg7', 'wk_eeg13', 'mhw_eeg11', 'wk_eeg4', 'ww_eeg10', 'wk_eeg7', 'ww_eeg13', 'sxy_eeg11', 'lqj_eeg2', 'ys_eeg5', 'ys_eeg2', 'wk_eeg8', 'lqj_eeg9', 'ys_eeg4', 'zjy_eeg10', 'zjy_eeg3', 'wk_eeg4', 'wk_eeg2', 'lqj_eeg6', 'wk_eeg5', 'wyw_eeg12', 'wsf_eeg9', 'ys_eeg3', 'phl_eeg10', 'mhw_eeg6', 'ly_eeg11', 'ww_eeg9', 'zjy_eeg6', 'ly_eeg14', 'ww_eeg13', 'ly_eeg7', 'jl_eeg6', 'jj_eeg12', 'jj_eeg7', 'jl_eeg12', 'wyw_eeg2', 'zjy_eeg9', 'lqj_eeg13'], 'emotion': tensor([-1,  1,  0,  0,  0,  1,  1, -1, -1,  0,  0,  1,  0,  1,  1, -1, -1,  1,\n",
      "         1,  0,  1, -1,  0,  0,  1,  0,  0,  0,  1,  1,  0,  1,  0,  0, -1, -1,\n",
      "         1,  1,  1,  0,  1,  1,  1,  0,  0,  0,  1, -1, -1, -1,  0,  0,  1,  1,\n",
      "        -1, -1,  0,  1,  0,  0,  1,  1, -1,  0,  1,  1,  0,  0,  1,  1,  0, -1,\n",
      "        -1, -1, -1, -1,  0, -1, -1, -1,  0, -1, -1,  0,  1,  0, -1,  0,  1,  0,\n",
      "        -1,  0,  0, -1,  1,  0,  1, -1, -1,  1,  0,  1,  1,  0, -1, -1,  0,  1,\n",
      "         0, -1, -1,  1,  0, -1, -1, -1,  0,  0,  0,  1,  1,  1, -1,  1,  1, -1,\n",
      "         0,  1,  0, -1,  1,  1,  1,  0, -1,  0,  0,  1, -1,  0,  1,  1,  1,  0,\n",
      "         0,  0, -1,  1,  0, -1,  1,  0,  0, -1,  0,  1, -1,  1,  1,  0,  1,  1,\n",
      "        -1,  1,  0, -1,  1, -1, -1, -1,  1,  1,  0,  0,  0, -1, -1, -1, -1,  0,\n",
      "        -1,  1,  1, -1, -1, -1,  0,  0,  0, -1,  1, -1, -1, -1,  1, -1,  0, -1,\n",
      "         1, -1,  1,  0,  0,  1, -1, -1,  1, -1, -1,  1, -1,  1, -1,  1,  1,  0,\n",
      "         0,  1, -1,  0,  0, -1,  1, -1,  0,  0,  0,  0,  0,  0,  1, -1,  1, -1,\n",
      "        -1,  0,  1,  0, -1,  1, -1,  1,  1,  0,  1,  1,  1,  0, -1,  1, -1, -1,\n",
      "        -1,  0,  1,  0]), 'date': tensor([20140620, 20140603, 20130709, 20131113, 20131127, 20140419, 20140527,\n",
      "        20131016, 20140610, 20140621, 20140630, 20130709, 20131201, 20140601,\n",
      "        20140603, 20140615, 20140511, 20140413, 20140601, 20140615, 20140418,\n",
      "        20140625, 20140413, 20140514, 20140611, 20131105, 20131113, 20140702,\n",
      "        20140629, 20140630, 20131207, 20140603, 20131105, 20140611, 20130712,\n",
      "        20140611, 20140603, 20140611, 20140621, 20131016, 20140705, 20140404,\n",
      "        20131027, 20140419, 20131027, 20140615, 20131106, 20140702, 20140618,\n",
      "        20140615, 20131030, 20131016, 20140514, 20140413, 20140413, 20131207,\n",
      "        20140611, 20140627, 20140419, 20140627, 20140625, 20140521, 20140620,\n",
      "        20140601, 20140620, 20131027, 20131207, 20140610, 20140506, 20140630,\n",
      "        20131127, 20140404, 20140627, 20140621, 20130709, 20140620, 20140610,\n",
      "        20131127, 20140627, 20140610, 20131016, 20140621, 20140618, 20140514,\n",
      "        20140506, 20131127, 20140527, 20140704, 20131016, 20140611, 20140418,\n",
      "        20140413, 20140615, 20140618, 20140511, 20131016, 20140404, 20131201,\n",
      "        20140603, 20140702, 20140630, 20131016, 20140419, 20131016, 20140705,\n",
      "        20140627, 20140620, 20140419, 20140511, 20130712, 20131204, 20131207,\n",
      "        20140419, 20140702, 20140419, 20131027, 20140527, 20140627, 20131204,\n",
      "        20140630, 20140625, 20131016, 20131016, 20140411, 20140601, 20131030,\n",
      "        20140615, 20131204, 20140704, 20131211, 20131105, 20131130, 20140618,\n",
      "        20131106, 20140625, 20140413, 20140629, 20140611, 20140629, 20140411,\n",
      "        20140603, 20131201, 20140419, 20140527, 20131030, 20131204, 20140630,\n",
      "        20131030, 20140705, 20140419, 20140629, 20140615, 20131204, 20131030,\n",
      "        20131030, 20131016, 20140511, 20140419, 20140627, 20140627, 20131105,\n",
      "        20131027, 20140615, 20140603, 20131211, 20140603, 20140627, 20131127,\n",
      "        20140514, 20131211, 20140615, 20140611, 20140521, 20140413, 20131027,\n",
      "        20140620, 20131211, 20140705, 20140603, 20140511, 20140630, 20140521,\n",
      "        20131113, 20131127, 20131201, 20131113, 20131016, 20140630, 20131130,\n",
      "        20140527, 20140419, 20140521, 20140615, 20131207, 20140705, 20131130,\n",
      "        20140625, 20140625, 20140413, 20131030, 20140625, 20140511, 20140418,\n",
      "        20140413, 20140621, 20140411, 20140418, 20140625, 20140621, 20131201,\n",
      "        20140705, 20140629, 20140629, 20140630, 20140618, 20140506, 20140611,\n",
      "        20140630, 20140627, 20140627, 20131113, 20140704, 20131204, 20140627,\n",
      "        20131130, 20140514, 20140702, 20140615, 20140601, 20140627, 20140621,\n",
      "        20140615, 20131105, 20130709, 20140627, 20140620, 20140705, 20140627,\n",
      "        20131201, 20140625, 20140627, 20131030, 20131016, 20140411, 20131211,\n",
      "        20131016, 20140506, 20131130, 20140411, 20140404, 20140629, 20140603,\n",
      "        20140419, 20131201, 20130709, 20140702]), '_record_id': ['_record_42', '_record_24', '_record_15', '_record_35', '_record_6', '_record_23', '_record_9', '_record_34', '_record_11', '_record_27', '_record_5', '_record_15', '_record_7', '_record_12', '_record_10', '_record_13', '_record_39', '_record_22', '_record_12', '_record_13', '_record_31', '_record_4', '_record_22', '_record_40', '_record_25', '_record_17', '_record_35', '_record_28', '_record_26', '_record_5', '_record_8', '_record_10', '_record_17', '_record_25', '_record_33', '_record_25', '_record_24', '_record_25', '_record_27', '_record_16', '_record_29', '_record_21', '_record_36', '_record_23', '_record_36', '_record_13', '_record_38', '_record_28', '_record_3', '_record_13', '_record_37', '_record_34', '_record_40', '_record_22', '_record_22', '_record_8', '_record_25', '_record_14', '_record_23', '_record_14', '_record_4', '_record_41', '_record_42', '_record_12', '_record_42', '_record_36', '_record_8', '_record_11', '_record_32', '_record_5', '_record_6', '_record_21', '_record_14', '_record_27', '_record_15', '_record_42', '_record_11', '_record_6', '_record_14', '_record_11', '_record_34', '_record_27', '_record_3', '_record_40', '_record_32', '_record_6', '_record_9', '_record_44', '_record_16', '_record_25', '_record_31', '_record_22', '_record_13', '_record_3', '_record_39', '_record_34', '_record_21', '_record_7', '_record_24', '_record_28', '_record_5', '_record_34', '_record_23', '_record_16', '_record_29', '_record_43', '_record_42', '_record_23', '_record_39', '_record_33', '_record_1', '_record_8', '_record_23', '_record_28', '_record_23', '_record_36', '_record_9', '_record_43', '_record_1', '_record_5', '_record_4', '_record_16', '_record_34', '_record_30', '_record_12', '_record_37', '_record_13', '_record_1', '_record_44', '_record_2', '_record_17', '_record_0', '_record_3', '_record_38', '_record_4', '_record_22', '_record_26', '_record_25', '_record_26', '_record_30', '_record_10', '_record_7', '_record_23', '_record_9', '_record_37', '_record_1', '_record_5', '_record_37', '_record_29', '_record_23', '_record_26', '_record_13', '_record_1', '_record_37', '_record_37', '_record_34', '_record_39', '_record_23', '_record_43', '_record_43', '_record_17', '_record_36', '_record_13', '_record_10', '_record_2', '_record_10', '_record_14', '_record_6', '_record_40', '_record_2', '_record_13', '_record_25', '_record_41', '_record_22', '_record_36', '_record_42', '_record_2', '_record_29', '_record_10', '_record_39', '_record_5', '_record_41', '_record_35', '_record_6', '_record_7', '_record_35', '_record_34', '_record_5', '_record_0', '_record_9', '_record_23', '_record_41', '_record_13', '_record_8', '_record_29', '_record_0', '_record_4', '_record_4', '_record_22', '_record_37', '_record_4', '_record_39', '_record_31', '_record_22', '_record_27', '_record_30', '_record_31', '_record_4', '_record_27', '_record_7', '_record_29', '_record_26', '_record_26', '_record_5', '_record_3', '_record_32', '_record_25', '_record_5', '_record_14', '_record_43', '_record_35', '_record_44', '_record_1', '_record_43', '_record_0', '_record_40', '_record_28', '_record_13', '_record_12', '_record_43', '_record_27', '_record_13', '_record_17', '_record_15', '_record_43', '_record_42', '_record_29', '_record_43', '_record_7', '_record_4', '_record_14', '_record_37', '_record_34', '_record_30', '_record_2', '_record_16', '_record_32', '_record_0', '_record_30', '_record_21', '_record_26', '_record_24', '_record_23', '_record_7', '_record_15', '_record_28'], 'segment_start': tensor([ 15, 105, 165, 120, 135, 105, 135,   0,  45,  90, 135, 150, 165, 135,\n",
      "        165,  30, 165,   0,  90, 165,  30,  45, 120,  60,  45,  75, 150,  15,\n",
      "        150, 105,   0,  90, 120,  45,  45,  45,  90,   0, 150, 165, 135, 135,\n",
      "        150,  45,  90,  30,  45, 165, 105,  90,  75,  30, 165,  60,  60,  60,\n",
      "         45,  30, 150,  75,  30, 105,   0, 105,  90, 135, 120,   0, 120,  75,\n",
      "         45, 150,  30, 120,  30,   0, 165,  15, 105,  60,  75,  15, 150,  90,\n",
      "        105,  45,  30, 135,  30,  75,  60, 150,  30, 165,  90,  45,  30,  30,\n",
      "         45, 150, 105,  60,  90, 165,  60,  45, 120,  90, 150,  90, 165, 165,\n",
      "         90, 165, 120,  90, 165,   0,  75,   0,  90, 135, 120,  75, 165, 105,\n",
      "         60,  30,  15, 135, 150, 150, 135, 135,  45,  30,  45,   0,  45,   0,\n",
      "         75, 150,  90, 165,  90,  30, 105, 165,  30,  30, 135,  60,  15,  15,\n",
      "        165,  75, 135,  60, 135,  75, 165, 150, 165, 135,  30,  90,  45,  60,\n",
      "        150,  15,  60,  90, 150,  15, 165, 150,   0,  45, 165,  45, 120, 105,\n",
      "         45, 150,  45,  30,  30,  60, 120,  45,   0, 120,  15, 150,  75,  15,\n",
      "         75, 120,  15,  30, 135,  90,  75,  30,  30, 105,  75, 165, 105,  75,\n",
      "        120, 165, 150, 105, 105, 135,  75,  30,   0, 105,  15, 150,  15, 150,\n",
      "         15,  60, 105,  30,   0, 165,  60,  90, 120, 165,  75,  45, 150, 120,\n",
      "         30, 165, 105, 150,  60, 120, 120, 135,  15, 105, 120,  30, 120,  30,\n",
      "        120, 120,  15,  30]), 'segment_end': tensor([ 45, 135, 195, 150, 165, 135, 165,  30,  75, 120, 165, 180, 195, 165,\n",
      "        195,  60, 195,  30, 120, 195,  60,  75, 150,  90,  75, 105, 180,  45,\n",
      "        180, 135,  30, 120, 150,  75,  75,  75, 120,  30, 180, 195, 165, 165,\n",
      "        180,  75, 120,  60,  75, 195, 135, 120, 105,  60, 195,  90,  90,  90,\n",
      "         75,  60, 180, 105,  60, 135,  30, 135, 120, 165, 150,  30, 150, 105,\n",
      "         75, 180,  60, 150,  60,  30, 195,  45, 135,  90, 105,  45, 180, 120,\n",
      "        135,  75,  60, 165,  60, 105,  90, 180,  60, 195, 120,  75,  60,  60,\n",
      "         75, 180, 135,  90, 120, 195,  90,  75, 150, 120, 180, 120, 195, 195,\n",
      "        120, 195, 150, 120, 195,  30, 105,  30, 120, 165, 150, 105, 195, 135,\n",
      "         90,  60,  45, 165, 180, 180, 165, 165,  75,  60,  75,  30,  75,  30,\n",
      "        105, 180, 120, 195, 120,  60, 135, 195,  60,  60, 165,  90,  45,  45,\n",
      "        195, 105, 165,  90, 165, 105, 195, 180, 195, 165,  60, 120,  75,  90,\n",
      "        180,  45,  90, 120, 180,  45, 195, 180,  30,  75, 195,  75, 150, 135,\n",
      "         75, 180,  75,  60,  60,  90, 150,  75,  30, 150,  45, 180, 105,  45,\n",
      "        105, 150,  45,  60, 165, 120, 105,  60,  60, 135, 105, 195, 135, 105,\n",
      "        150, 195, 180, 135, 135, 165, 105,  60,  30, 135,  45, 180,  45, 180,\n",
      "         45,  90, 135,  60,  30, 195,  90, 120, 150, 195, 105,  75, 180, 150,\n",
      "         60, 195, 135, 180,  90, 150, 150, 165,  45, 135, 150,  60, 150,  60,\n",
      "        150, 150,  45,  60])}\n",
      "Batch 5: Data Shape = torch.Size([256, 62, 30]), Labels = {'start_at': tensor([ 3800,  4600, 36600, 42800, 18400, 40600,  4000, 21200, 47000, 16000,\n",
      "         7200, 26000, 13000, 28200, 20800, 27600,  5200, 25400, 37400, 18000,\n",
      "        30600, 28800, 13000, 23200, 36400, 43400,  1200, 40200,  4000, 15400,\n",
      "         6000,  5800, 21200, 40400,   800, 29600, 21800, 37600, 16000, 12800,\n",
      "        12600,   600, 13000, 15400, 13000, 19000, 27400, 23600, 22800, 38200,\n",
      "        10600, 41000,  8800, 21600, 11200,  5400, 34200, 18600,  6000, 43600,\n",
      "         7400, 29000, 39400, 19200, 34400, 15800,  5200, 25600,  7800,  8000,\n",
      "        36000,  5000, 26000, 36400, 28400, 34400, 17800, 41600,  2000, 38200,\n",
      "        37200, 31400, 37600, 14000,  1400, 26400,  3000, 11400, 38600, 36600,\n",
      "        31400, 14200, 23400, 27800, 12200, 23800, 39200,  2600, 32400, 41800,\n",
      "        35800, 22200, 36400,  1000, 21400, 10800, 33400, 44800, 30000, 18400,\n",
      "        46200, 22000, 16200, 13400, 32600, 25000, 11400, 37000, 40800,  7600,\n",
      "        23000, 20400, 16400, 31400, 24200, 32400, 20000,  1800, 13800,  3200,\n",
      "        20600, 37600,  7000, 22000, 29800, 42200, 17000, 42400, 45400, 39600,\n",
      "         2800, 32800, 27600, 11200, 11200, 25400, 41000, 42200, 11400,  2400,\n",
      "         8400, 21000, 16000,  9000,   600,  8600, 28600, 38000, 10800,  4000,\n",
      "         4800, 34200,  4600, 10000, 31400,  7400, 29800, 33400, 34400,  6600,\n",
      "         5400, 14600,   400, 12400, 16200, 28600, 23200, 18000,  2000, 25000,\n",
      "        32400, 32600,  8200, 14800, 46800,  6800, 16600, 43600,  2400, 33400,\n",
      "        37600, 15400, 10000,  5000, 16400, 36800,  1800, 35200, 15200, 16400,\n",
      "        31400, 28400, 12400, 15400,  3800, 44400, 32600, 43200, 19600, 20200,\n",
      "        34000,  3200, 19400, 21600, 46200,  9800,  1800,   400, 41000,  5200,\n",
      "         5400, 34000, 35600, 28800, 31800, 46800, 38400, 10600,   600, 17800,\n",
      "        10800, 46000, 16400, 29200,  6800,  9600, 21800, 45400, 10800, 21000,\n",
      "        11800, 22200,  9200, 44800, 30600, 35200,  9400,  8000, 36200,  4600,\n",
      "        45600, 39600, 29200, 17400, 40800,  7000]), 'end_at': tensor([ 4000,  4800, 36800, 43000, 18600, 40800,  4200, 21400, 47200, 16200,\n",
      "         7400, 26200, 13200, 28400, 21000, 27800,  5400, 25600, 37600, 18200,\n",
      "        30800, 29000, 13200, 23400, 36600, 43600,  1400, 40400,  4200, 15600,\n",
      "         6200,  6000, 21400, 40600,  1000, 29800, 22000, 37800, 16200, 13000,\n",
      "        12800,   800, 13200, 15600, 13200, 19200, 27600, 23800, 23000, 38400,\n",
      "        10800, 41200,  9000, 21800, 11400,  5600, 34400, 18800,  6200, 43800,\n",
      "         7600, 29200, 39600, 19400, 34600, 16000,  5400, 25800,  8000,  8200,\n",
      "        36200,  5200, 26200, 36600, 28600, 34600, 18000, 41800,  2200, 38400,\n",
      "        37400, 31600, 37800, 14200,  1600, 26600,  3200, 11600, 38800, 36800,\n",
      "        31600, 14400, 23600, 28000, 12400, 24000, 39400,  2800, 32600, 42000,\n",
      "        36000, 22400, 36600,  1200, 21600, 11000, 33600, 45000, 30200, 18600,\n",
      "        46400, 22200, 16400, 13600, 32800, 25200, 11600, 37200, 41000,  7800,\n",
      "        23200, 20600, 16600, 31600, 24400, 32600, 20200,  2000, 14000,  3400,\n",
      "        20800, 37800,  7200, 22200, 30000, 42400, 17200, 42600, 45600, 39800,\n",
      "         3000, 33000, 27800, 11400, 11400, 25600, 41200, 42400, 11600,  2600,\n",
      "         8600, 21200, 16200,  9200,   800,  8800, 28800, 38200, 11000,  4200,\n",
      "         5000, 34400,  4800, 10200, 31600,  7600, 30000, 33600, 34600,  6800,\n",
      "         5600, 14800,   600, 12600, 16400, 28800, 23400, 18200,  2200, 25200,\n",
      "        32600, 32800,  8400, 15000, 47000,  7000, 16800, 43800,  2600, 33600,\n",
      "        37800, 15600, 10200,  5200, 16600, 37000,  2000, 35400, 15400, 16600,\n",
      "        31600, 28600, 12600, 15600,  4000, 44600, 32800, 43400, 19800, 20400,\n",
      "        34200,  3400, 19600, 21800, 46400, 10000,  2000,   600, 41200,  5400,\n",
      "         5600, 34200, 35800, 29000, 32000, 47000, 38600, 10800,   800, 18000,\n",
      "        11000, 46200, 16600, 29400,  7000,  9800, 22000, 45600, 11000, 21200,\n",
      "        12000, 22400,  9400, 45000, 30800, 35400,  9600,  8200, 36400,  4800,\n",
      "        45800, 39800, 29400, 17600, 41000,  7200]), 'clip_id': ['11_20140630.mat_2266', '2_20140404.mat_23', '7_20131030.mat_183', '13_20140527.mat_3164', '13_20140610.mat_560', '2_20140404.mat_1732', '6_20130712.mat_255', '13_20140603.mat_2116', '9_20140620.mat_3185', '11_20140625.mat_2327', '4_20140702.mat_3224', '13_20140603.mat_3080', '15_20131105.mat_2075', '3_20140611.mat_1670', '13_20140527.mat_1016', '14_20140601.mat_812', '6_20131113.mat_26', '8_20140521.mat_595', '15_20130709.mat_861', '4_20140702.mat_1002', '4_20140702.mat_827', '12_20131207.mat_612', '8_20140511.mat_533', '6_20131113.mat_2831', '5_20140411.mat_182', '8_20140511.mat_3167', '10_20131130.mat_474', '2_20140404.mat_1946', '7_20131027.mat_2267', '6_20131016.mat_77', '15_20131016.mat_2512', '11_20140630.mat_1774', '11_20140618.mat_2821', '3_20140611.mat_1731', '8_20140521.mat_916', '2_20140419.mat_2395', '15_20131016.mat_2591', '8_20140511.mat_423', '12_20131127.mat_2795', '12_20131201.mat_3014', '10_20131130.mat_1355', '15_20131105.mat_1100', '2_20140413.mat_2075', '9_20140704.mat_2324', '2_20140413.mat_1162', '4_20140702.mat_2810', '11_20140630.mat_1666', '9_20140620.mat_3306', '14_20140627.mat_3302', '13_20140527.mat_191', '10_20131211.mat_1345', '2_20140404.mat_3393', '6_20130712.mat_956', '7_20131027.mat_1853', '8_20140514.mat_524', '15_20130709.mat_701', '4_20140705.mat_1700', '6_20131113.mat_767', '8_20140514.mat_942', '14_20140615.mat_892', '8_20140521.mat_1134', '9_20140627.mat_3095', '5_20140411.mat_3385', '9_20140704.mat_96', '10_20131130.mat_2654', '3_20140629.mat_2561', '15_20131105.mat_938', '8_20140511.mat_596', '2_20140413.mat_2754', '15_20131016.mat_1332', '11_20140618.mat_2190', '5_20140418.mat_937', '8_20140521.mat_3080', '12_20131127.mat_3132', '10_20131204.mat_142', '3_20140611.mat_2419', '5_20140418.mat_763', '11_20140618.mat_1737', '5_20140506.mat_1539', '15_20130709.mat_2438', '9_20140704.mat_421', '5_20140506.mat_392', '6_20131113.mat_1480', '8_20140521.mat_982', '11_20140630.mat_242', '11_20140630.mat_1877', '5_20140506.mat_927', '10_20131130.mat_292', '3_20140611.mat_1938', '15_20131105.mat_1928', '11_20140630.mat_1686', '14_20140627.mat_2318', '8_20140514.mat_585', '13_20140527.mat_1668', '14_20140615.mat_2308', '11_20140625.mat_119', '10_20131204.mat_2678', '13_20140610.mat_481', '9_20140704.mat_2409', '7_20131027.mat_1738', '3_20140603.mat_2661', '8_20140511.mat_1640', '9_20140704.mat_3370', '6_20130712.mat_240', '15_20131016.mat_342', '11_20140618.mat_2064', '6_20131113.mat_2882', '2_20140419.mat_2706', '8_20140514.mat_1442', '14_20140615.mat_2807', '9_20140704.mat_905', '7_20131027.mat_2825', '5_20140506.mat_993', '14_20140615.mat_302', '8_20140521.mat_1075', '5_20140411.mat_1417', '5_20140411.mat_525', '13_20140527.mat_1714', '4_20140705.mat_672', '8_20140514.mat_506', '6_20130712.mat_115', '9_20140704.mat_2112', '8_20140511.mat_1179', '6_20130712.mat_625', '7_20131106.mat_1218', '6_20130712.mat_2877', '2_20140404.mat_3288', '4_20140702.mat_2019', '2_20140404.mat_743', '6_20131113.mat_2026', '5_20140506.mat_1015', '6_20131113.mat_423', '13_20140603.mat_2985', '9_20140627.mat_2120', '2_20140404.mat_384', '15_20131016.mat_1503', '13_20140603.mat_2567', '6_20131113.mat_2222', '8_20140521.mat_3177', '10_20131211.mat_1943', '3_20140603.mat_14', '3_20140611.mat_1076', '3_20140629.mat_1883', '13_20140610.mat_1801', '7_20131030.mat_2066', '6_20131113.mat_1656', '2_20140419.mat_440', '2_20140404.mat_2458', '7_20131027.mat_3245', '6_20131113.mat_2022', '13_20140610.mat_716', '13_20140610.mat_779', '12_20131201.mat_1609', '6_20131113.mat_513', '4_20140705.mat_677', '13_20140527.mat_717', '5_20140411.mat_2153', '11_20140630.mat_864', '6_20130712.mat_1346', '12_20131127.mat_1549', '13_20140603.mat_2034', '5_20140418.mat_171', '7_20131027.mat_491', '10_20131211.mat_2297', '4_20140702.mat_157', '7_20131106.mat_949', '15_20131016.mat_1678', '8_20140521.mat_2177', '2_20140419.mat_1269', '8_20140521.mat_2983', '4_20140702.mat_2977', '3_20140629.mat_1170', '8_20140514.mat_676', '12_20131127.mat_2309', '3_20140629.mat_549', '13_20140610.mat_1055', '2_20140404.mat_2126', '14_20140627.mat_325', '11_20140630.mat_2960', '4_20140702.mat_3075', '15_20130709.mat_2409', '12_20131207.mat_2645', '8_20140511.mat_509', '11_20140618.mat_2321', '5_20140418.mat_2949', '8_20140514.mat_1131', '5_20140411.mat_318', '8_20140521.mat_1510', '3_20140629.mat_3200', '14_20140601.mat_3355', '6_20130712.mat_188', '2_20140413.mat_1606', '2_20140413.mat_1795', '12_20131127.mat_1770', '10_20131211.mat_2329', '15_20131016.mat_1281', '3_20140629.mat_1301', '4_20140702.mat_411', '4_20140705.mat_2558', '6_20131016.mat_3270', '7_20131106.mat_1686', '15_20130709.mat_3330', '14_20140615.mat_3012', '10_20131204.mat_2324', '3_20140603.mat_1548', '4_20140705.mat_2937', '13_20140527.mat_3351', '15_20131016.mat_1508', '12_20131207.mat_566', '10_20131130.mat_2111', '3_20140603.mat_1462', '8_20140514.mat_690', '14_20140601.mat_1626', '13_20140603.mat_3296', '11_20140625.mat_466', '6_20131016.mat_2999', '14_20140627.mat_9', '3_20140629.mat_470', '13_20140603.mat_440', '9_20140704.mat_261', '6_20130712.mat_1124', '5_20140418.mat_638', '12_20131127.mat_646', '10_20131204.mat_2626', '15_20130709.mat_2874', '15_20131016.mat_2481', '2_20140419.mat_427', '14_20140615.mat_3241', '15_20130709.mat_2485', '2_20140404.mat_3277', '13_20140527.mat_2301', '14_20140627.mat_465', '15_20131016.mat_1374', '10_20131211.mat_2861', '13_20140610.mat_1779', '11_20140618.mat_1145', '6_20130712.mat_2119', '10_20131130.mat_227', '13_20140610.mat_1583', '4_20140702.mat_2587', '11_20140625.mat_971', '7_20131030.mat_346', '15_20130709.mat_2761', '8_20140511.mat_224', '10_20131130.mat_1250', '10_20131130.mat_1921', '13_20140610.mat_959', '11_20140630.mat_714', '7_20131030.mat_1278', '9_20140620.mat_2973', '4_20140705.mat_2710', '6_20131016.mat_2208', '2_20140404.mat_1438', '12_20131201.mat_999', '10_20131130.mat_1496', '6_20131016.mat_1132'], 'subject_id': tensor([10,  1,  6, 12, 12,  1,  5, 12,  8, 10,  3, 12, 14,  2, 12, 13,  5,  7,\n",
      "        14,  3,  3, 11,  7,  5,  4,  7,  9,  1,  6,  5, 14, 10, 10,  2,  7,  1,\n",
      "        14,  7, 11, 11,  9, 14,  1,  8,  1,  3, 10,  8, 13, 12,  9,  1,  5,  6,\n",
      "         7, 14,  3,  5,  7, 13,  7,  8,  4,  8,  9,  2, 14,  7,  1, 14, 10,  4,\n",
      "         7, 11,  9,  2,  4, 10,  4, 14,  8,  4,  5,  7, 10, 10,  4,  9,  2, 14,\n",
      "        10, 13,  7, 12, 13, 10,  9, 12,  8,  6,  2,  7,  8,  5, 14, 10,  5,  1,\n",
      "         7, 13,  8,  6,  4, 13,  7,  4,  4, 12,  3,  7,  5,  8,  7,  5,  6,  5,\n",
      "         1,  3,  1,  5,  4,  5, 12,  8,  1, 14, 12,  5,  7,  9,  2,  2,  2, 12,\n",
      "         6,  5,  1,  1,  6,  5, 12, 12, 11,  5,  3, 12,  4, 10,  5, 11, 12,  4,\n",
      "         6,  9,  3,  6, 14,  7,  1,  7,  3,  2,  7, 11,  2, 12,  1, 13, 10,  3,\n",
      "        14, 11,  7, 10,  4,  7,  4,  7,  2, 13,  5,  1,  1, 11,  9, 14,  2,  3,\n",
      "         3,  5,  6, 14, 13,  9,  2,  3, 12, 14, 11,  9,  2,  7, 13, 12, 10,  5,\n",
      "        13,  2, 12,  8,  5,  4, 11,  9, 14, 14,  1, 13, 14,  1, 12, 13, 14,  9,\n",
      "        12, 10,  5,  9, 12,  3, 10,  6, 14,  7,  9,  9, 12, 10,  6,  8,  3,  5,\n",
      "         1, 11,  9,  5]), 'trial_id': ['wsf_eeg11', 'jl_eeg1', 'phl_eeg1', 'xyl_eeg14', 'xyl_eeg3', 'jl_eeg8', 'mhw_eeg2', 'xyl_eeg10', 'wk_eeg14', 'wsf_eeg11', 'lqj_eeg15', 'xyl_eeg14', 'zjy_eeg10', 'jj_eeg8', 'xyl_eeg5', 'ys_eeg4', 'mhw_eeg1', 'sxy_eeg3', 'zjy_eeg4', 'lqj_eeg5', 'lqj_eeg4', 'wyw_eeg3', 'sxy_eeg3', 'mhw_eeg13', 'ly_eeg1', 'sxy_eeg14', 'ww_eeg3', 'jl_eeg9', 'phl_eeg11', 'mhw_eeg1', 'zjy_eeg12', 'wsf_eeg9', 'wsf_eeg13', 'jj_eeg8', 'sxy_eeg5', 'jl_eeg11', 'zjy_eeg12', 'sxy_eeg2', 'wyw_eeg13', 'wyw_eeg14', 'ww_eeg7', 'zjy_eeg6', 'jl_eeg10', 'wk_eeg11', 'jl_eeg6', 'lqj_eeg13', 'wsf_eeg8', 'wk_eeg15', 'ys_eeg15', 'xyl_eeg1', 'ww_eeg7', 'jl_eeg15', 'mhw_eeg5', 'phl_eeg9', 'sxy_eeg3', 'zjy_eeg4', 'lqj_eeg8', 'mhw_eeg4', 'sxy_eeg5', 'ys_eeg4', 'sxy_eeg6', 'wk_eeg14', 'ly_eeg15', 'wk_eeg1', 'ww_eeg12', 'jj_eeg12', 'zjy_eeg5', 'sxy_eeg3', 'jl_eeg13', 'zjy_eeg7', 'wsf_eeg10', 'ly_eeg5', 'sxy_eeg14', 'wyw_eeg14', 'ww_eeg1', 'jj_eeg11', 'ly_eeg4', 'wsf_eeg8', 'ly_eeg8', 'zjy_eeg11', 'wk_eeg2', 'ly_eeg2', 'mhw_eeg7', 'sxy_eeg5', 'wsf_eeg2', 'wsf_eeg9', 'ly_eeg5', 'ww_eeg2', 'jj_eeg9', 'zjy_eeg9', 'wsf_eeg8', 'ys_eeg11', 'sxy_eeg3', 'xyl_eeg8', 'ys_eeg11', 'wsf_eeg1', 'ww_eeg12', 'xyl_eeg3', 'wk_eeg11', 'phl_eeg8', 'jj_eeg12', 'sxy_eeg8', 'wk_eeg15', 'mhw_eeg2', 'zjy_eeg2', 'wsf_eeg10', 'mhw_eeg13', 'jl_eeg12', 'sxy_eeg7', 'ys_eeg13', 'wk_eeg4', 'phl_eeg13', 'ly_eeg5', 'ys_eeg2', 'sxy_eeg5', 'ly_eeg7', 'ly_eeg3', 'xyl_eeg8', 'lqj_eeg3', 'sxy_eeg3', 'mhw_eeg1', 'wk_eeg10', 'sxy_eeg6', 'mhw_eeg3', 'phl_eeg6', 'mhw_eeg13', 'jl_eeg15', 'lqj_eeg10', 'jl_eeg4', 'mhw_eeg10', 'ly_eeg5', 'mhw_eeg2', 'xyl_eeg14', 'wk_eeg10', 'jl_eeg2', 'zjy_eeg7', 'xyl_eeg12', 'mhw_eeg10', 'sxy_eeg14', 'ww_eeg9', 'jj_eeg1', 'jj_eeg5', 'jj_eeg9', 'xyl_eeg9', 'phl_eeg10', 'mhw_eeg8', 'jl_eeg2', 'jl_eeg11', 'phl_eeg15', 'mhw_eeg10', 'xyl_eeg4', 'xyl_eeg4', 'wyw_eeg8', 'mhw_eeg3', 'lqj_eeg4', 'xyl_eeg4', 'ly_eeg10', 'wsf_eeg4', 'mhw_eeg7', 'wyw_eeg8', 'xyl_eeg10', 'ly_eeg1', 'phl_eeg3', 'ww_eeg11', 'lqj_eeg1', 'phl_eeg5', 'zjy_eeg8', 'sxy_eeg10', 'jl_eeg6', 'sxy_eeg14', 'lqj_eeg14', 'jj_eeg6', 'sxy_eeg4', 'wyw_eeg11', 'jj_eeg3', 'xyl_eeg5', 'jl_eeg10', 'ys_eeg2', 'wsf_eeg14', 'lqj_eeg14', 'zjy_eeg11', 'wyw_eeg12', 'sxy_eeg3', 'wsf_eeg11', 'ly_eeg13', 'sxy_eeg6', 'ly_eeg2', 'sxy_eeg7', 'jj_eeg15', 'ys_eeg15', 'mhw_eeg1', 'jl_eeg8', 'jl_eeg9', 'wyw_eeg9', 'ww_eeg11', 'zjy_eeg6', 'jj_eeg7', 'lqj_eeg2', 'lqj_eeg12', 'mhw_eeg15', 'phl_eeg8', 'zjy_eeg15', 'ys_eeg14', 'ww_eeg11', 'jj_eeg8', 'lqj_eeg13', 'xyl_eeg15', 'zjy_eeg7', 'wyw_eeg3', 'ww_eeg10', 'jj_eeg7', 'sxy_eeg4', 'ys_eeg8', 'xyl_eeg15', 'wsf_eeg2', 'mhw_eeg14', 'ys_eeg1', 'jj_eeg3', 'xyl_eeg2', 'wk_eeg2', 'mhw_eeg6', 'ly_eeg3', 'wyw_eeg3', 'ww_eeg12', 'zjy_eeg13', 'zjy_eeg11', 'jl_eeg2', 'ys_eeg15', 'zjy_eeg12', 'jl_eeg15', 'xyl_eeg11', 'ys_eeg2', 'zjy_eeg7', 'ww_eeg13', 'xyl_eeg9', 'wsf_eeg6', 'mhw_eeg10', 'ww_eeg1', 'xyl_eeg8', 'lqj_eeg12', 'wsf_eeg5', 'phl_eeg2', 'zjy_eeg13', 'sxy_eeg1', 'ww_eeg6', 'ww_eeg9', 'xyl_eeg5', 'wsf_eeg4', 'phl_eeg6', 'wk_eeg14', 'lqj_eeg12', 'mhw_eeg10', 'jl_eeg7', 'wyw_eeg5', 'ww_eeg7', 'mhw_eeg6'], 'emotion': tensor([ 0,  1,  1,  1, -1,  0,  0,  1,  1,  0, -1,  1,  1,  0,  0, -1,  1, -1,\n",
      "        -1,  0, -1, -1, -1,  0,  1,  1, -1,  1,  0,  1, -1,  1,  0,  0,  0,  0,\n",
      "        -1,  0,  0,  1, -1,  1,  1,  0,  1,  0,  0, -1, -1,  1, -1, -1,  0,  1,\n",
      "        -1, -1,  0, -1,  0, -1,  1,  1, -1,  1, -1, -1,  0, -1,  0, -1,  1,  0,\n",
      "         1,  1,  1,  0, -1,  0,  0,  0,  0,  0, -1,  0,  0,  1,  0,  0,  1,  1,\n",
      "         0,  0, -1,  0,  0,  1, -1, -1,  0,  0, -1,  0, -1,  0,  0,  1,  0, -1,\n",
      "        -1,  0, -1,  0,  0,  0,  0, -1, -1,  0, -1, -1,  1,  1,  1, -1,  1,  0,\n",
      "        -1,  1, -1,  1,  0,  0,  1,  1,  0, -1, -1,  1,  1,  1,  1,  0,  1,  1,\n",
      "         1,  0,  0,  0, -1,  1, -1, -1,  0, -1, -1, -1,  1, -1, -1,  0,  1,  1,\n",
      "        -1,  0,  1,  0,  0,  1,  1,  1,  1,  1, -1,  0, -1,  0,  1,  0,  1,  1,\n",
      "         0, -1, -1,  0,  0,  1,  0, -1, -1, -1,  1,  0,  1,  1,  0,  1, -1,  0,\n",
      "        -1, -1,  0, -1,  1,  0,  0,  0, -1, -1, -1,  1, -1, -1,  0, -1,  0,  1,\n",
      "         1, -1,  0,  0,  1, -1, -1, -1,  0,  0,  0, -1, -1, -1,  0,  0, -1,  0,\n",
      "         1,  1,  1,  1,  0, -1,  0,  0,  0,  1,  1,  1,  0, -1,  1,  1, -1,  1,\n",
      "        -1,  0, -1,  1]), 'date': tensor([20140630, 20140404, 20131030, 20140527, 20140610, 20140404, 20130712,\n",
      "        20140603, 20140620, 20140625, 20140702, 20140603, 20131105, 20140611,\n",
      "        20140527, 20140601, 20131113, 20140521, 20130709, 20140702, 20140702,\n",
      "        20131207, 20140511, 20131113, 20140411, 20140511, 20131130, 20140404,\n",
      "        20131027, 20131016, 20131016, 20140630, 20140618, 20140611, 20140521,\n",
      "        20140419, 20131016, 20140511, 20131127, 20131201, 20131130, 20131105,\n",
      "        20140413, 20140704, 20140413, 20140702, 20140630, 20140620, 20140627,\n",
      "        20140527, 20131211, 20140404, 20130712, 20131027, 20140514, 20130709,\n",
      "        20140705, 20131113, 20140514, 20140615, 20140521, 20140627, 20140411,\n",
      "        20140704, 20131130, 20140629, 20131105, 20140511, 20140413, 20131016,\n",
      "        20140618, 20140418, 20140521, 20131127, 20131204, 20140611, 20140418,\n",
      "        20140618, 20140506, 20130709, 20140704, 20140506, 20131113, 20140521,\n",
      "        20140630, 20140630, 20140506, 20131130, 20140611, 20131105, 20140630,\n",
      "        20140627, 20140514, 20140527, 20140615, 20140625, 20131204, 20140610,\n",
      "        20140704, 20131027, 20140603, 20140511, 20140704, 20130712, 20131016,\n",
      "        20140618, 20131113, 20140419, 20140514, 20140615, 20140704, 20131027,\n",
      "        20140506, 20140615, 20140521, 20140411, 20140411, 20140527, 20140705,\n",
      "        20140514, 20130712, 20140704, 20140511, 20130712, 20131106, 20130712,\n",
      "        20140404, 20140702, 20140404, 20131113, 20140506, 20131113, 20140603,\n",
      "        20140627, 20140404, 20131016, 20140603, 20131113, 20140521, 20131211,\n",
      "        20140603, 20140611, 20140629, 20140610, 20131030, 20131113, 20140419,\n",
      "        20140404, 20131027, 20131113, 20140610, 20140610, 20131201, 20131113,\n",
      "        20140705, 20140527, 20140411, 20140630, 20130712, 20131127, 20140603,\n",
      "        20140418, 20131027, 20131211, 20140702, 20131106, 20131016, 20140521,\n",
      "        20140419, 20140521, 20140702, 20140629, 20140514, 20131127, 20140629,\n",
      "        20140610, 20140404, 20140627, 20140630, 20140702, 20130709, 20131207,\n",
      "        20140511, 20140618, 20140418, 20140514, 20140411, 20140521, 20140629,\n",
      "        20140601, 20130712, 20140413, 20140413, 20131127, 20131211, 20131016,\n",
      "        20140629, 20140702, 20140705, 20131016, 20131106, 20130709, 20140615,\n",
      "        20131204, 20140603, 20140705, 20140527, 20131016, 20131207, 20131130,\n",
      "        20140603, 20140514, 20140601, 20140603, 20140625, 20131016, 20140627,\n",
      "        20140629, 20140603, 20140704, 20130712, 20140418, 20131127, 20131204,\n",
      "        20130709, 20131016, 20140419, 20140615, 20130709, 20140404, 20140527,\n",
      "        20140627, 20131016, 20131211, 20140610, 20140618, 20130712, 20131130,\n",
      "        20140610, 20140702, 20140625, 20131030, 20130709, 20140511, 20131130,\n",
      "        20131130, 20140610, 20140630, 20131030, 20140620, 20140705, 20131016,\n",
      "        20140404, 20131201, 20131130, 20131016]), '_record_id': ['_record_5', '_record_21', '_record_37', '_record_9', '_record_11', '_record_21', '_record_33', '_record_10', '_record_42', '_record_4', '_record_28', '_record_10', '_record_17', '_record_25', '_record_9', '_record_12', '_record_35', '_record_41', '_record_15', '_record_28', '_record_28', '_record_8', '_record_39', '_record_35', '_record_30', '_record_39', '_record_0', '_record_21', '_record_36', '_record_34', '_record_16', '_record_5', '_record_3', '_record_25', '_record_41', '_record_23', '_record_16', '_record_39', '_record_6', '_record_7', '_record_0', '_record_17', '_record_22', '_record_44', '_record_22', '_record_28', '_record_5', '_record_42', '_record_14', '_record_9', '_record_2', '_record_21', '_record_33', '_record_36', '_record_40', '_record_15', '_record_29', '_record_35', '_record_40', '_record_13', '_record_41', '_record_43', '_record_30', '_record_44', '_record_0', '_record_26', '_record_17', '_record_39', '_record_22', '_record_16', '_record_3', '_record_31', '_record_41', '_record_6', '_record_1', '_record_25', '_record_31', '_record_3', '_record_32', '_record_15', '_record_44', '_record_32', '_record_35', '_record_41', '_record_5', '_record_5', '_record_32', '_record_0', '_record_25', '_record_17', '_record_5', '_record_14', '_record_40', '_record_9', '_record_13', '_record_4', '_record_1', '_record_11', '_record_44', '_record_36', '_record_24', '_record_39', '_record_44', '_record_33', '_record_16', '_record_3', '_record_35', '_record_23', '_record_40', '_record_13', '_record_44', '_record_36', '_record_32', '_record_13', '_record_41', '_record_30', '_record_30', '_record_9', '_record_29', '_record_40', '_record_33', '_record_44', '_record_39', '_record_33', '_record_38', '_record_33', '_record_21', '_record_28', '_record_21', '_record_35', '_record_32', '_record_35', '_record_10', '_record_43', '_record_21', '_record_16', '_record_10', '_record_35', '_record_41', '_record_2', '_record_24', '_record_25', '_record_26', '_record_11', '_record_37', '_record_35', '_record_23', '_record_21', '_record_36', '_record_35', '_record_11', '_record_11', '_record_7', '_record_35', '_record_29', '_record_9', '_record_30', '_record_5', '_record_33', '_record_6', '_record_10', '_record_31', '_record_36', '_record_2', '_record_28', '_record_38', '_record_16', '_record_41', '_record_23', '_record_41', '_record_28', '_record_26', '_record_40', '_record_6', '_record_26', '_record_11', '_record_21', '_record_14', '_record_5', '_record_28', '_record_15', '_record_8', '_record_39', '_record_3', '_record_31', '_record_40', '_record_30', '_record_41', '_record_26', '_record_12', '_record_33', '_record_22', '_record_22', '_record_6', '_record_2', '_record_16', '_record_26', '_record_28', '_record_29', '_record_34', '_record_38', '_record_15', '_record_13', '_record_1', '_record_24', '_record_29', '_record_9', '_record_16', '_record_8', '_record_0', '_record_24', '_record_40', '_record_12', '_record_10', '_record_4', '_record_34', '_record_14', '_record_26', '_record_10', '_record_44', '_record_33', '_record_31', '_record_6', '_record_1', '_record_15', '_record_16', '_record_23', '_record_13', '_record_15', '_record_21', '_record_9', '_record_14', '_record_16', '_record_2', '_record_11', '_record_3', '_record_33', '_record_0', '_record_11', '_record_28', '_record_4', '_record_37', '_record_15', '_record_39', '_record_0', '_record_0', '_record_11', '_record_5', '_record_37', '_record_42', '_record_29', '_record_34', '_record_21', '_record_7', '_record_0', '_record_34'], 'segment_start': tensor([ 30, 135,  60,  60, 105,  75,  15, 120,  90, 165,   0,  60, 150,  30,\n",
      "        135,  60, 105,  15,  90, 135,  30, 120,  30,   0,  90,  45,  75,  45,\n",
      "         15,  45, 120, 135,  45,  75, 165,   0,  45,  30,  75,  60, 105,  30,\n",
      "         75,   0,   0,  15,  60,  90, 150,  15,  30, 150,  75, 150, 120,  15,\n",
      "         30,   0,  75,  45,  75,  45, 120, 165,  15, 120,  45, 135,   0,  60,\n",
      "         45,  15,  90,  30,  30, 150,  30,  60,  45,  15, 135, 120, 165, 105,\n",
      "         60,  30,  60,  15, 165,  75, 165,  75,   0,  90,  90,  15,  60, 105,\n",
      "         75, 105, 120,   0,  60,  45,  45, 120,  45,  75,   0,  75, 105, 150,\n",
      "         75,  45, 150, 105,  15, 120,  60,  60, 120,  45, 120, 120, 105,  45,\n",
      "        135,  60,  90, 105, 120, 150,  90,  15, 165,   0,  75, 120, 150, 165,\n",
      "        150,  15, 120,  60,  45,  75,   0,  45, 105,  75,  90,  45,  90,  30,\n",
      "         75,  90,   0,   0,   0, 165,  30,  15,   0, 135, 120,  90,  45, 120,\n",
      "         15, 105,  30,  15, 165,  30,  60, 105,  15,  15,  15, 135, 165, 105,\n",
      "         30,  90, 120,  60, 165, 150,  15, 165,  15, 105,  75, 135,  15, 150,\n",
      "        120,  30, 165,   0,  15,  45, 150,  15,   0,  90,  30, 165, 135,  90,\n",
      "        150, 120,  75,  30, 135, 120,  30, 150, 165,  45,  60, 150,   0,  60,\n",
      "        165, 135, 120, 135, 105,  15, 120,  90,   0, 105, 135,  75, 105,  90,\n",
      "         45,   0, 165,   0, 165, 105,  45,  30, 105,   0, 135, 150,  75,  60,\n",
      "        105,  15,  45,  60]), 'segment_end': tensor([ 60, 165,  90,  90, 135, 105,  45, 150, 120, 195,  30,  90, 180,  60,\n",
      "        165,  90, 135,  45, 120, 165,  60, 150,  60,  30, 120,  75, 105,  75,\n",
      "         45,  75, 150, 165,  75, 105, 195,  30,  75,  60, 105,  90, 135,  60,\n",
      "        105,  30,  30,  45,  90, 120, 180,  45,  60, 180, 105, 180, 150,  45,\n",
      "         60,  30, 105,  75, 105,  75, 150, 195,  45, 150,  75, 165,  30,  90,\n",
      "         75,  45, 120,  60,  60, 180,  60,  90,  75,  45, 165, 150, 195, 135,\n",
      "         90,  60,  90,  45, 195, 105, 195, 105,  30, 120, 120,  45,  90, 135,\n",
      "        105, 135, 150,  30,  90,  75,  75, 150,  75, 105,  30, 105, 135, 180,\n",
      "        105,  75, 180, 135,  45, 150,  90,  90, 150,  75, 150, 150, 135,  75,\n",
      "        165,  90, 120, 135, 150, 180, 120,  45, 195,  30, 105, 150, 180, 195,\n",
      "        180,  45, 150,  90,  75, 105,  30,  75, 135, 105, 120,  75, 120,  60,\n",
      "        105, 120,  30,  30,  30, 195,  60,  45,  30, 165, 150, 120,  75, 150,\n",
      "         45, 135,  60,  45, 195,  60,  90, 135,  45,  45,  45, 165, 195, 135,\n",
      "         60, 120, 150,  90, 195, 180,  45, 195,  45, 135, 105, 165,  45, 180,\n",
      "        150,  60, 195,  30,  45,  75, 180,  45,  30, 120,  60, 195, 165, 120,\n",
      "        180, 150, 105,  60, 165, 150,  60, 180, 195,  75,  90, 180,  30,  90,\n",
      "        195, 165, 150, 165, 135,  45, 150, 120,  30, 135, 165, 105, 135, 120,\n",
      "         75,  30, 195,  30, 195, 135,  75,  60, 135,  30, 165, 180, 105,  90,\n",
      "        135,  45,  75,  90])}\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, num_workers=0)\n",
    "\n",
    "for batch_idx, (batch_data, batch_labels) in enumerate(train_loader):\n",
    "    print(f\"Batch {batch_idx}: Data Shape = {batch_data.shape}, Labels = {batch_labels}\")\n",
    "    if batch_idx == 5:  # Limit to the first 5 batches\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NEED TO RUN THESE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataloader:\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x000002308676D730>\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample shape: torch.Size([62, 30]), Label: 1\n"
     ]
    }
   ],
   "source": [
    "eeg_sample, label = train_dataset[0]\n",
    "print(f\"Sample shape: {eeg_sample.shape}, Label: {label}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Already ran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_normalization_minibatch(x, metadata):\n",
    "    \"\"\"\n",
    "    Perform stratified normalization of the minibatch based on subject and emotion information.\n",
    "    x: Tensor of shape [batch_size, channels, time_points].\n",
    "    metadata: Dictionary of metadata with keys like 'subject_id', 'emotion', etc.\n",
    "    \"\"\"\n",
    "    print(\"Entered stratified_normalization_minibatch\")\n",
    "    print(f\"x.shape: {x.shape}\")\n",
    "    print(f\"Metadata keys: {list(metadata.keys())}\")\n",
    "\n",
    "    # Extract subject_id and emotion as tensors\n",
    "    subject_ids = metadata['subject_id']\n",
    "    emotion_labels = metadata['emotion']\n",
    "\n",
    "    # Initialize normalized tensor\n",
    "    normalized_x = torch.zeros_like(x)\n",
    "\n",
    "    # Group data by (subject_id, emotion)\n",
    "    group_to_indices = {}\n",
    "    for idx, (subject, emotion) in enumerate(zip(subject_ids, emotion_labels)):\n",
    "        group = (subject.item(), emotion.item())\n",
    "        if group not in group_to_indices:\n",
    "            group_to_indices[group] = []\n",
    "        group_to_indices[group].append(idx)\n",
    "\n",
    "    for group, indices in group_to_indices.items():\n",
    "        group_data = x[indices]  # Extract data for the group\n",
    "        mean = group_data.mean(dim=-1, keepdim=True)  # Mean over time points\n",
    "        std = group_data.std(dim=-1, keepdim=True)    # Std over time points\n",
    "        normalized_x[indices] = (group_data - mean) / (std + 1e-8)  # Normalize\n",
    "\n",
    "    print(\"Exit stratified_normalization_minibatch\")\n",
    "    return normalized_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Batch Shape: torch.Size([256, 62, 30]), Metadata: {'start_at': tensor([14800, 16600, 33800,  6800, 16200, 35000, 42400, 35800, 35200, 24200,\n",
      "        38200, 50400, 30200,  9200,  8600, 43000, 29000, 40200, 15800,  4200,\n",
      "        19400, 17400, 29000, 39200, 27000, 37400, 45200, 43800, 41200, 28400,\n",
      "        26200,  1600, 25800, 30600, 46200, 10800, 12800, 40000,  3200, 34200,\n",
      "        23600, 46400, 20000,  2400, 27200,  3000, 34800, 15600, 27800, 19000,\n",
      "        41400, 30400, 19200, 36200, 18600,  2600, 10800,  6000,     0, 28600,\n",
      "        11800, 17600, 19400, 42000,  1600, 38400,  8200, 44800,  8800, 23200,\n",
      "        34200,  6800, 16000, 10800, 30400, 20200,  2800, 29000, 39000,  9400,\n",
      "         3800, 31800, 10400, 46600, 29800, 39200, 13200, 21200, 14400, 13600,\n",
      "        37600, 12000, 24000, 25800,   200,  7200, 16800, 28200, 20200, 39400,\n",
      "        12800, 34000, 42400, 22200, 33200, 23800,  4400, 38400, 27200, 32200,\n",
      "        17600, 33800, 33800, 43600, 31400, 39000,  4200, 42400, 22800, 17200,\n",
      "        29200, 29200,  2200, 32000, 32400, 27200, 28200, 19600,  5600,  3400,\n",
      "        19600, 37200, 10200, 36400, 19000,  2800, 47000, 10200, 15600, 44800,\n",
      "        12200,  7200, 32000,  3800, 18000, 38200, 19200,  2000, 23200, 24400,\n",
      "         4800, 32600,  5000,  9200, 24600, 10000, 13600, 25800, 44400, 18400,\n",
      "         3400, 41000, 43800, 38200, 35400, 46000,  5400,  6200, 13800, 16000,\n",
      "         6000, 26000, 22400, 26600, 42600,  7600, 18200,   600,  9600, 18400,\n",
      "        39000, 25800,  7600,  7000, 21200, 46000,  9200,  7800, 17600, 30200,\n",
      "        16400, 37800, 28400, 21400,  9000, 18600, 26800, 14600, 41400,  5600,\n",
      "        27200, 30600, 39600, 14600,  7800,  2800, 17200, 22800, 11200, 33400,\n",
      "         3200, 30400,  7000, 27400,  3600, 20600,  4000, 31000, 31400,  8800,\n",
      "         3400, 23800, 26600, 30000, 37600, 15200, 39000,  5800, 19000, 29000,\n",
      "        35600, 22200, 45400, 22200,  5800, 39200, 33400,  4000, 29800,  8200,\n",
      "        16400,  6200, 39600,  6200, 13000, 12200, 17600,  9000, 38800, 17600,\n",
      "        35400, 21600, 13200, 39400, 28800, 26800]), 'end_at': tensor([15000, 16800, 34000,  7000, 16400, 35200, 42600, 36000, 35400, 24400,\n",
      "        38400, 50600, 30400,  9400,  8800, 43200, 29200, 40400, 16000,  4400,\n",
      "        19600, 17600, 29200, 39400, 27200, 37600, 45400, 44000, 41400, 28600,\n",
      "        26400,  1800, 26000, 30800, 46400, 11000, 13000, 40200,  3400, 34400,\n",
      "        23800, 46600, 20200,  2600, 27400,  3200, 35000, 15800, 28000, 19200,\n",
      "        41600, 30600, 19400, 36400, 18800,  2800, 11000,  6200,   200, 28800,\n",
      "        12000, 17800, 19600, 42200,  1800, 38600,  8400, 45000,  9000, 23400,\n",
      "        34400,  7000, 16200, 11000, 30600, 20400,  3000, 29200, 39200,  9600,\n",
      "         4000, 32000, 10600, 46800, 30000, 39400, 13400, 21400, 14600, 13800,\n",
      "        37800, 12200, 24200, 26000,   400,  7400, 17000, 28400, 20400, 39600,\n",
      "        13000, 34200, 42600, 22400, 33400, 24000,  4600, 38600, 27400, 32400,\n",
      "        17800, 34000, 34000, 43800, 31600, 39200,  4400, 42600, 23000, 17400,\n",
      "        29400, 29400,  2400, 32200, 32600, 27400, 28400, 19800,  5800,  3600,\n",
      "        19800, 37400, 10400, 36600, 19200,  3000, 47200, 10400, 15800, 45000,\n",
      "        12400,  7400, 32200,  4000, 18200, 38400, 19400,  2200, 23400, 24600,\n",
      "         5000, 32800,  5200,  9400, 24800, 10200, 13800, 26000, 44600, 18600,\n",
      "         3600, 41200, 44000, 38400, 35600, 46200,  5600,  6400, 14000, 16200,\n",
      "         6200, 26200, 22600, 26800, 42800,  7800, 18400,   800,  9800, 18600,\n",
      "        39200, 26000,  7800,  7200, 21400, 46200,  9400,  8000, 17800, 30400,\n",
      "        16600, 38000, 28600, 21600,  9200, 18800, 27000, 14800, 41600,  5800,\n",
      "        27400, 30800, 39800, 14800,  8000,  3000, 17400, 23000, 11400, 33600,\n",
      "         3400, 30600,  7200, 27600,  3800, 20800,  4200, 31200, 31600,  9000,\n",
      "         3600, 24000, 26800, 30200, 37800, 15400, 39200,  6000, 19200, 29200,\n",
      "        35800, 22400, 45600, 22400,  6000, 39400, 33600,  4200, 30000,  8400,\n",
      "        16600,  6400, 39800,  6400, 13200, 12400, 17800,  9200, 39000, 17800,\n",
      "        35600, 21800, 13400, 39600, 29000, 27000]), 'clip_id': ['15_20130709.mat_1366', '10_20131211.mat_1180', '4_20140621.mat_637', '3_20140629.mat_1779', '11_20140618.mat_81', '9_20140704.mat_1087', '11_20140618.mat_2927', '7_20131106.mat_3367', '9_20140627.mat_1921', '8_20140511.mat_3071', '2_20140419.mat_865', '10_20131204.mat_1997', '4_20140705.mat_2633', '9_20140704.mat_1791', '15_20131016.mat_2993', '10_20131204.mat_2462', '6_20130712.mat_1057', '3_20140603.mat_669', '3_20140603.mat_2089', '12_20131201.mat_1313', '12_20131201.mat_2107', '8_20140511.mat_87', '15_20130709.mat_1242', '3_20140603.mat_3384', '12_20131201.mat_3085', '7_20131106.mat_1932', '6_20131016.mat_2708', '7_20131030.mat_454', '10_20131130.mat_3156', '3_20140603.mat_377', '10_20131204.mat_1228', '13_20140527.mat_682', '11_20140618.mat_2844', '11_20140618.mat_2635', '12_20131201.mat_1976', '4_20140705.mat_728', '5_20140411.mat_3014', '15_20131105.mat_435', '4_20140705.mat_16', '15_20131016.mat_845', '14_20140615.mat_2600', '14_20140601.mat_1977', '12_20131207.mat_774', '13_20140610.mat_480', '14_20140615.mat_1233', '10_20131211.mat_2497', '10_20131211.mat_1086', '7_20131027.mat_752', '3_20140603.mat_813', '13_20140527.mat_769', '3_20140611.mat_1952', '13_20140527.mat_620', '10_20131204.mat_1193', '15_20131105.mat_1473', '6_20130712.mat_1622', '12_20131127.mat_1110', '4_20140702.mat_522', '2_20140404.mat_498', '8_20140521.mat_912', '4_20140702.mat_143', '8_20140514.mat_59', '10_20131211.mat_1833', '4_20140621.mat_1842', '9_20140620.mat_3160', '12_20131127.mat_1537', '10_20131211.mat_192', '15_20131016.mat_1570', '14_20140601.mat_224', '8_20140511.mat_2526', '14_20140627.mat_2598', '9_20140620.mat_2653', '14_20140627.mat_3222', '3_20140603.mat_754', '3_20140603.mat_289', '9_20140620.mat_2399', '14_20140601.mat_569', '8_20140511.mat_926', '3_20140629.mat_2155', '15_20131105.mat_2205', '9_20140627.mat_3235', '7_20131030.mat_487', '7_20131027.mat_3109', '14_20140627.mat_964', '13_20140527.mat_233', '14_20140627.mat_1678', '3_20140603.mat_1941', '11_20140630.mat_534', '6_20130712.mat_341', '11_20140618.mat_984', '11_20140630.mat_2783', '6_20131016.mat_862', '2_20140419.mat_972', '15_20131016.mat_355', '7_20131027.mat_3317', '2_20140413.mat_1746', '10_20131204.mat_2283', '6_20131113.mat_84', '9_20140620.mat_1886', '10_20131130.mat_2816', '15_20130709.mat_871', '4_20140621.mat_3014', '3_20140629.mat_3358', '8_20140514.mat_2694', '7_20131027.mat_785', '6_20131113.mat_166', '14_20140601.mat_587', '12_20131127.mat_1119', '2_20140413.mat_1289', '8_20140521.mat_604', '2_20140404.mat_2408', '3_20140629.mat_1833', '8_20140521.mat_169', '7_20131027.mat_1461', '8_20140511.mat_2228', '4_20140621.mat_3107', '4_20140621.mat_663', '14_20140615.mat_1550', '3_20140611.mat_2694', '9_20140627.mat_2596', '4_20140705.mat_321', '9_20140627.mat_1243', '8_20140521.mat_614', '2_20140419.mat_11', '12_20131207.mat_2170', '4_20140702.mat_3350', '10_20131204.mat_371', '9_20140627.mat_3329', '12_20131127.mat_1390', '12_20131201.mat_263', '7_20131027.mat_2967', '9_20140620.mat_2108', '9_20140620.mat_3136', '13_20140603.mat_2298', '15_20131016.mat_1474', '14_20140615.mat_2577', '8_20140511.mat_1111', '6_20130712.mat_1527', '14_20140627.mat_725', '10_20131204.mat_546', '4_20140705.mat_3174', '10_20131130.mat_529', '9_20140704.mat_1565', '2_20140404.mat_1072', '11_20140618.mat_2969', '15_20131016.mat_764', '8_20140521.mat_1288', '15_20131016.mat_3046', '12_20131127.mat_2725', '4_20140621.mat_1028', '13_20140610.mat_2132', '2_20140404.mat_2271', '15_20131016.mat_3351', '4_20140705.mat_1770', '15_20130709.mat_1338', '4_20140702.mat_2370', '3_20140629.mat_724', '5_20140506.mat_742', '2_20140419.mat_803', '13_20140527.mat_1514', '14_20140627.mat_2574', '8_20140521.mat_1309', '15_20131105.mat_440', '9_20140620.mat_893', '14_20140615.mat_1720', '12_20131201.mat_645', '12_20131207.mat_1522', '12_20131201.mat_1124', '13_20140603.mat_31', '4_20140621.mat_2551', '13_20140603.mat_3268', '8_20140514.mat_3218', '3_20140611.mat_2612', '7_20131027.mat_1404', '15_20131016.mat_1662', '7_20131030.mat_1505', '5_20140506.mat_712', '2_20140419.mat_1836', '15_20130709.mat_2485', '6_20131113.mat_2763', '15_20131016.mat_1621', '5_20140506.mat_2442', '7_20131106.mat_3079', '9_20140620.mat_2753', '5_20140411.mat_1780', '8_20140514.mat_106', '9_20140704.mat_904', '4_20140621.mat_958', '15_20131105.mat_2754', '5_20140411.mat_2335', '5_20140506.mat_2398', '15_20130709.mat_550', '14_20140615.mat_2671', '11_20140630.mat_1239', '10_20131204.mat_342', '4_20140621.mat_1574', '14_20140615.mat_328', '10_20131204.mat_1231', '5_20140418.mat_3023', '6_20131113.mat_2689', '14_20140615.mat_1557', '5_20140506.mat_2851', '12_20131127.mat_2163', '14_20140627.mat_2680', '13_20140610.mat_1602', '11_20140618.mat_2049', '12_20131127.mat_482', '12_20131127.mat_2333', '2_20140404.mat_2829', '10_20131204.mat_1348', '6_20130712.mat_1912', '9_20140627.mat_251', '13_20140603.mat_2162', '15_20131016.mat_947', '4_20140621.mat_1429', '11_20140630.mat_1547', '15_20131016.mat_2818', '12_20131207.mat_2970', '3_20140629.mat_1684', '5_20140506.mat_1069', '12_20131127.mat_2994', '5_20140418.mat_252', '3_20140603.mat_1648', '5_20140506.mat_2380', '7_20131106.mat_2397', '8_20140514.mat_188', '12_20131201.mat_2323', '11_20140630.mat_663', '9_20140620.mat_2276', '2_20140419.mat_2810', '2_20140419.mat_613', '12_20131127.mat_646', '11_20140618.mat_2121', '10_20131211.mat_2942', '2_20140413.mat_785', '15_20131016.mat_2744', '8_20140514.mat_2911', '10_20131211.mat_1912', '5_20140506.mat_932', '15_20131105.mat_2631', '9_20140620.mat_41', '4_20140621.mat_550', '5_20140418.mat_2513', '2_20140404.mat_2680', '15_20131016.mat_2513', '5_20140411.mat_739', '14_20140627.mat_61', '9_20140627.mat_323', '7_20131030.mat_2760', '4_20140705.mat_868', '4_20140705.mat_2098', '7_20131027.mat_1469', '13_20140527.mat_1400', '2_20140419.mat_2548', '14_20140627.mat_432', '14_20140615.mat_3332', '11_20140630.mat_369'], 'subject_id': tensor([14,  9,  3,  2, 10,  8, 10,  6,  8,  7,  1,  9,  3,  8, 14,  9,  5,  2,\n",
      "         2, 11, 11,  7, 14,  2, 11,  6,  5,  6,  9,  2,  9, 12, 10, 10, 11,  3,\n",
      "         4, 14,  3, 14, 13, 13, 11, 12, 13,  9,  9,  6,  2, 12,  2, 12,  9, 14,\n",
      "         5, 11,  3,  1,  7,  3,  7,  9,  3,  8, 11,  9, 14, 13,  7, 13,  8, 13,\n",
      "         2,  2,  8, 13,  7,  2, 14,  8,  6,  6, 13, 12, 13,  2, 10,  5, 10, 10,\n",
      "         5,  1, 14,  6,  1,  9,  5,  8,  9, 14,  3,  2,  7,  6,  5, 13, 11,  1,\n",
      "         7,  1,  2,  7,  6,  7,  3,  3, 13,  2,  8,  3,  8,  7,  1, 11,  3,  9,\n",
      "         8, 11, 11,  6,  8,  8, 12, 14, 13,  7,  5, 13,  9,  3,  9,  8,  1, 10,\n",
      "        14,  7, 14, 11,  3, 12,  1, 14,  3, 14,  3,  2,  4,  1, 12, 13,  7, 14,\n",
      "         8, 13, 11, 11, 11, 12,  3, 12,  7,  2,  6, 14,  6,  4,  1, 14,  5, 14,\n",
      "         4,  6,  8,  4,  7,  8,  3, 14,  4,  4, 14, 13, 10,  9,  3, 13,  9,  4,\n",
      "         5, 13,  4, 11, 13, 12, 10, 11, 11,  1,  9,  5,  8, 12, 14,  3, 10, 14,\n",
      "        11,  2,  4, 11,  4,  2,  4,  6,  7, 11, 10,  8,  1,  1, 11, 10,  9,  1,\n",
      "        14,  7,  9,  4, 14,  8,  3,  4,  1, 14,  4, 13,  8,  6,  3,  3,  6, 12,\n",
      "         1, 13, 13, 10]), 'trial_id': ['zjy_eeg7', 'ww_eeg6', 'lqj_eeg3', 'jj_eeg9', 'wsf_eeg1', 'wk_eeg5', 'wsf_eeg13', 'phl_eeg15', 'wk_eeg9', 'sxy_eeg14', 'jl_eeg4', 'ww_eeg9', 'lqj_eeg12', 'wk_eeg9', 'zjy_eeg14', 'ww_eeg11', 'mhw_eeg5', 'jj_eeg3', 'jj_eeg10', 'wyw_eeg7', 'wyw_eeg10', 'sxy_eeg1', 'zjy_eeg6', 'jj_eeg15', 'wyw_eeg14', 'phl_eeg9', 'mhw_eeg12', 'phl_eeg2', 'ww_eeg14', 'jj_eeg2', 'ww_eeg6', 'xyl_eeg4', 'wsf_eeg13', 'wsf_eeg12', 'wyw_eeg9', 'lqj_eeg4', 'ly_eeg14', 'zjy_eeg2', 'lqj_eeg1', 'zjy_eeg4', 'ys_eeg12', 'ys_eeg9', 'wyw_eeg4', 'xyl_eeg3', 'ys_eeg6', 'ww_eeg12', 'ww_eeg5', 'phl_eeg4', 'jj_eeg4', 'xyl_eeg4', 'jj_eeg9', 'xyl_eeg3', 'ww_eeg6', 'zjy_eeg7', 'mhw_eeg8', 'wyw_eeg6', 'lqj_eeg3', 'jl_eeg3', 'sxy_eeg5', 'lqj_eeg1', 'sxy_eeg1', 'ww_eeg9', 'lqj_eeg9', 'wk_eeg14', 'wyw_eeg8', 'ww_eeg1', 'zjy_eeg8', 'ys_eeg1', 'sxy_eeg12', 'ys_eeg12', 'wk_eeg12', 'ys_eeg15', 'jj_eeg4', 'jj_eeg2', 'wk_eeg11', 'ys_eeg3', 'sxy_eeg5', 'jj_eeg10', 'zjy_eeg10', 'wk_eeg15', 'phl_eeg3', 'phl_eeg14', 'ys_eeg5', 'xyl_eeg1', 'ys_eeg8', 'jj_eeg9', 'wsf_eeg3', 'mhw_eeg2', 'wsf_eeg5', 'wsf_eeg13', 'mhw_eeg4', 'jl_eeg5', 'zjy_eeg2', 'phl_eeg15', 'jl_eeg9', 'ww_eeg11', 'mhw_eeg1', 'wk_eeg9', 'ww_eeg13', 'zjy_eeg4', 'lqj_eeg14', 'jj_eeg15', 'sxy_eeg12', 'phl_eeg4', 'mhw_eeg1', 'ys_eeg3', 'wyw_eeg6', 'jl_eeg6', 'sxy_eeg3', 'jl_eeg11', 'jj_eeg9', 'sxy_eeg1', 'phl_eeg7', 'sxy_eeg10', 'lqj_eeg14', 'lqj_eeg3', 'ys_eeg8', 'jj_eeg12', 'wk_eeg12', 'lqj_eeg2', 'wk_eeg6', 'sxy_eeg3', 'jl_eeg1', 'wyw_eeg10', 'lqj_eeg15', 'ww_eeg2', 'wk_eeg15', 'wyw_eeg7', 'wyw_eeg2', 'phl_eeg14', 'wk_eeg10', 'wk_eeg14', 'xyl_eeg11', 'zjy_eeg7', 'ys_eeg12', 'sxy_eeg6', 'mhw_eeg7', 'ys_eeg4', 'ww_eeg3', 'lqj_eeg14', 'ww_eeg3', 'wk_eeg8', 'jl_eeg5', 'wsf_eeg14', 'zjy_eeg4', 'sxy_eeg6', 'zjy_eeg14', 'wyw_eeg13', 'lqj_eeg5', 'xyl_eeg10', 'jl_eeg11', 'zjy_eeg15', 'lqj_eeg9', 'zjy_eeg7', 'lqj_eeg11', 'jj_eeg4', 'ly_eeg4', 'jl_eeg4', 'xyl_eeg7', 'ys_eeg12', 'sxy_eeg7', 'zjy_eeg2', 'wk_eeg4', 'ys_eeg8', 'wyw_eeg3', 'wyw_eeg7', 'wyw_eeg6', 'xyl_eeg1', 'lqj_eeg12', 'xyl_eeg15', 'sxy_eeg15', 'jj_eeg12', 'phl_eeg7', 'zjy_eeg8', 'phl_eeg7', 'ly_eeg4', 'jl_eeg9', 'zjy_eeg12', 'mhw_eeg13', 'zjy_eeg8', 'ly_eeg11', 'phl_eeg14', 'wk_eeg13', 'ly_eeg9', 'sxy_eeg1', 'wk_eeg4', 'lqj_eeg5', 'zjy_eeg13', 'ly_eeg11', 'ly_eeg11', 'zjy_eeg3', 'ys_eeg12', 'wsf_eeg6', 'ww_eeg2', 'lqj_eeg8', 'ys_eeg2', 'ww_eeg6', 'ly_eeg14', 'mhw_eeg12', 'ys_eeg8', 'ly_eeg13', 'wyw_eeg10', 'ys_eeg12', 'xyl_eeg8', 'wsf_eeg10', 'wyw_eeg3', 'wyw_eeg11', 'jl_eeg13', 'ww_eeg7', 'mhw_eeg9', 'wk_eeg2', 'xyl_eeg10', 'zjy_eeg5', 'lqj_eeg7', 'wsf_eeg8', 'zjy_eeg13', 'wyw_eeg14', 'jj_eeg8', 'ly_eeg5', 'wyw_eeg14', 'ly_eeg2', 'jj_eeg8', 'ly_eeg11', 'phl_eeg11', 'sxy_eeg1', 'wyw_eeg11', 'wsf_eeg3', 'wk_eeg11', 'jl_eeg13', 'jl_eeg3', 'wyw_eeg3', 'wsf_eeg10', 'ww_eeg13', 'jl_eeg4', 'zjy_eeg13', 'sxy_eeg13', 'ww_eeg9', 'ly_eeg5', 'zjy_eeg12', 'wk_eeg1', 'lqj_eeg3', 'ly_eeg12', 'jl_eeg12', 'zjy_eeg12', 'ly_eeg4', 'ys_eeg1', 'wk_eeg2', 'phl_eeg13', 'lqj_eeg4', 'lqj_eeg10', 'phl_eeg7', 'xyl_eeg7', 'jl_eeg12', 'ys_eeg2', 'ys_eeg15', 'wsf_eeg2'], 'emotion': tensor([-1,  1, -1,  1,  1,  0,  0, -1,  1,  1, -1,  1, -1,  1,  1,  0,  0, -1,\n",
      "         1, -1,  1,  1,  1, -1,  1,  1, -1,  0,  1,  0,  1, -1,  0, -1,  1, -1,\n",
      "         1,  0,  1, -1, -1,  1, -1, -1,  1, -1,  0, -1, -1, -1,  1, -1,  1, -1,\n",
      "         0,  1, -1, -1,  0,  1,  1,  1,  1,  1,  0,  1,  0,  1, -1, -1, -1, -1,\n",
      "        -1,  0,  0, -1,  0,  1,  1, -1, -1,  1,  0,  1,  0,  1, -1,  0,  0,  0,\n",
      "        -1,  0,  0, -1,  1,  0,  1,  1,  0, -1,  1, -1, -1, -1,  1, -1,  1,  1,\n",
      "        -1,  0,  1,  1, -1,  1,  1, -1,  0, -1, -1,  0,  1, -1,  1,  1, -1,  0,\n",
      "        -1, -1,  0,  1,  1,  1,  0, -1, -1,  1, -1, -1, -1,  1, -1,  0,  0,  1,\n",
      "        -1,  1,  1,  0,  0,  1,  0, -1,  1, -1,  0, -1, -1, -1, -1, -1, -1,  0,\n",
      "        -1,  0, -1, -1,  1,  1, -1, -1, -1, -1, -1,  0, -1, -1,  1, -1,  0,  0,\n",
      "         0,  1,  0,  1,  1, -1,  0,  0,  0,  0, -1, -1,  1,  0,  0,  0,  1,  1,\n",
      "        -1,  0,  0,  1, -1,  0,  1, -1,  0,  0, -1,  1,  0,  1,  0, -1,  0,  0,\n",
      "         1,  0,  0,  1,  0,  0,  0,  0,  1,  0, -1,  0,  0, -1, -1,  1,  0, -1,\n",
      "         0,  0,  1,  0, -1,  1, -1, -1, -1, -1, -1,  1,  0,  0, -1,  1, -1, -1,\n",
      "        -1,  0, -1,  0]), 'date': tensor([20130709, 20131211, 20140621, 20140629, 20140618, 20140704, 20140618,\n",
      "        20131106, 20140627, 20140511, 20140419, 20131204, 20140705, 20140704,\n",
      "        20131016, 20131204, 20130712, 20140603, 20140603, 20131201, 20131201,\n",
      "        20140511, 20130709, 20140603, 20131201, 20131106, 20131016, 20131030,\n",
      "        20131130, 20140603, 20131204, 20140527, 20140618, 20140618, 20131201,\n",
      "        20140705, 20140411, 20131105, 20140705, 20131016, 20140615, 20140601,\n",
      "        20131207, 20140610, 20140615, 20131211, 20131211, 20131027, 20140603,\n",
      "        20140527, 20140611, 20140527, 20131204, 20131105, 20130712, 20131127,\n",
      "        20140702, 20140404, 20140521, 20140702, 20140514, 20131211, 20140621,\n",
      "        20140620, 20131127, 20131211, 20131016, 20140601, 20140511, 20140627,\n",
      "        20140620, 20140627, 20140603, 20140603, 20140620, 20140601, 20140511,\n",
      "        20140629, 20131105, 20140627, 20131030, 20131027, 20140627, 20140527,\n",
      "        20140627, 20140603, 20140630, 20130712, 20140618, 20140630, 20131016,\n",
      "        20140419, 20131016, 20131027, 20140413, 20131204, 20131113, 20140620,\n",
      "        20131130, 20130709, 20140621, 20140629, 20140514, 20131027, 20131113,\n",
      "        20140601, 20131127, 20140413, 20140521, 20140404, 20140629, 20140521,\n",
      "        20131027, 20140511, 20140621, 20140621, 20140615, 20140611, 20140627,\n",
      "        20140705, 20140627, 20140521, 20140419, 20131207, 20140702, 20131204,\n",
      "        20140627, 20131127, 20131201, 20131027, 20140620, 20140620, 20140603,\n",
      "        20131016, 20140615, 20140511, 20130712, 20140627, 20131204, 20140705,\n",
      "        20131130, 20140704, 20140404, 20140618, 20131016, 20140521, 20131016,\n",
      "        20131127, 20140621, 20140610, 20140404, 20131016, 20140705, 20130709,\n",
      "        20140702, 20140629, 20140506, 20140419, 20140527, 20140627, 20140521,\n",
      "        20131105, 20140620, 20140615, 20131201, 20131207, 20131201, 20140603,\n",
      "        20140621, 20140603, 20140514, 20140611, 20131027, 20131016, 20131030,\n",
      "        20140506, 20140419, 20130709, 20131113, 20131016, 20140506, 20131106,\n",
      "        20140620, 20140411, 20140514, 20140704, 20140621, 20131105, 20140411,\n",
      "        20140506, 20130709, 20140615, 20140630, 20131204, 20140621, 20140615,\n",
      "        20131204, 20140418, 20131113, 20140615, 20140506, 20131127, 20140627,\n",
      "        20140610, 20140618, 20131127, 20131127, 20140404, 20131204, 20130712,\n",
      "        20140627, 20140603, 20131016, 20140621, 20140630, 20131016, 20131207,\n",
      "        20140629, 20140506, 20131127, 20140418, 20140603, 20140506, 20131106,\n",
      "        20140514, 20131201, 20140630, 20140620, 20140419, 20140419, 20131127,\n",
      "        20140618, 20131211, 20140413, 20131016, 20140514, 20131211, 20140506,\n",
      "        20131105, 20140620, 20140621, 20140418, 20140404, 20131016, 20140411,\n",
      "        20140627, 20140627, 20131030, 20140705, 20140705, 20131027, 20140527,\n",
      "        20140419, 20140627, 20140615, 20140630]), '_record_id': ['_record_15', '_record_2', '_record_27', '_record_26', '_record_3', '_record_44', '_record_3', '_record_38', '_record_43', '_record_39', '_record_23', '_record_1', '_record_29', '_record_44', '_record_16', '_record_1', '_record_33', '_record_24', '_record_24', '_record_7', '_record_7', '_record_39', '_record_15', '_record_24', '_record_7', '_record_38', '_record_34', '_record_37', '_record_0', '_record_24', '_record_1', '_record_9', '_record_3', '_record_3', '_record_7', '_record_29', '_record_30', '_record_17', '_record_29', '_record_16', '_record_13', '_record_12', '_record_8', '_record_11', '_record_13', '_record_2', '_record_2', '_record_36', '_record_24', '_record_9', '_record_25', '_record_9', '_record_1', '_record_17', '_record_33', '_record_6', '_record_28', '_record_21', '_record_41', '_record_28', '_record_40', '_record_2', '_record_27', '_record_42', '_record_6', '_record_2', '_record_16', '_record_12', '_record_39', '_record_14', '_record_42', '_record_14', '_record_24', '_record_24', '_record_42', '_record_12', '_record_39', '_record_26', '_record_17', '_record_43', '_record_37', '_record_36', '_record_14', '_record_9', '_record_14', '_record_24', '_record_5', '_record_33', '_record_3', '_record_5', '_record_34', '_record_23', '_record_16', '_record_36', '_record_22', '_record_1', '_record_35', '_record_42', '_record_0', '_record_15', '_record_27', '_record_26', '_record_40', '_record_36', '_record_35', '_record_12', '_record_6', '_record_22', '_record_41', '_record_21', '_record_26', '_record_41', '_record_36', '_record_39', '_record_27', '_record_27', '_record_13', '_record_25', '_record_43', '_record_29', '_record_43', '_record_41', '_record_23', '_record_8', '_record_28', '_record_1', '_record_43', '_record_6', '_record_7', '_record_36', '_record_42', '_record_42', '_record_10', '_record_16', '_record_13', '_record_39', '_record_33', '_record_14', '_record_1', '_record_29', '_record_0', '_record_44', '_record_21', '_record_3', '_record_16', '_record_41', '_record_16', '_record_6', '_record_27', '_record_11', '_record_21', '_record_16', '_record_29', '_record_15', '_record_28', '_record_26', '_record_32', '_record_23', '_record_9', '_record_14', '_record_41', '_record_17', '_record_42', '_record_13', '_record_7', '_record_8', '_record_7', '_record_10', '_record_27', '_record_10', '_record_40', '_record_25', '_record_36', '_record_16', '_record_37', '_record_32', '_record_23', '_record_15', '_record_35', '_record_16', '_record_32', '_record_38', '_record_42', '_record_30', '_record_40', '_record_44', '_record_27', '_record_17', '_record_30', '_record_32', '_record_15', '_record_13', '_record_5', '_record_1', '_record_27', '_record_13', '_record_1', '_record_31', '_record_35', '_record_13', '_record_32', '_record_6', '_record_14', '_record_11', '_record_3', '_record_6', '_record_6', '_record_21', '_record_1', '_record_33', '_record_43', '_record_10', '_record_16', '_record_27', '_record_5', '_record_16', '_record_8', '_record_26', '_record_32', '_record_6', '_record_31', '_record_24', '_record_32', '_record_38', '_record_40', '_record_7', '_record_5', '_record_42', '_record_23', '_record_23', '_record_6', '_record_3', '_record_2', '_record_22', '_record_16', '_record_40', '_record_2', '_record_32', '_record_17', '_record_42', '_record_27', '_record_31', '_record_21', '_record_16', '_record_30', '_record_14', '_record_43', '_record_37', '_record_29', '_record_29', '_record_36', '_record_9', '_record_23', '_record_14', '_record_13', '_record_5'], 'segment_start': tensor([ 15,   0,  75, 105, 165, 135,  75,  30,  45,  15,  90, 120,   0,  30,\n",
      "          0, 120,  45, 150,  15,  75,  15, 165,  90,  15,  45, 135, 120, 165,\n",
      "         75, 165, 165, 165, 165,  45,  90,  75,   0, 135,  90,  45, 120,   0,\n",
      "         45,  45, 165, 105, 120,   0,  45, 105,  30,  30,  30, 120, 150, 150,\n",
      "        150,  30, 165,   0,   0, 105,  45,  30,   0,  45,  30, 150, 150, 165,\n",
      "         30, 105, 150,  15,  90,  75, 120,  60,  90, 120,  60, 150, 165, 135,\n",
      "         90,  45,  60,  15, 105, 105,  75,  75,  75, 105,  60,   0,  45,   0,\n",
      "         60,  75, 165, 150, 165, 120,  60,  90, 105,  45,  15, 150,  90,  15,\n",
      "        105, 105, 135, 120,  30,  15,  90, 165,  90,  60,  75, 120, 150,  90,\n",
      "         45,  30, 165, 150, 150, 105,   0, 105,  45,  15,  30, 135,   0,  90,\n",
      "        120, 120, 165,  60, 120,  60, 165,  75,  90,  45,  30, 165,  30, 165,\n",
      "         60, 150,  75,  60,  90,  45,  75,  60,  30,  45, 150,  90,  30,  75,\n",
      "        105, 165,  15,  30, 120,  15,  60, 135,  45, 120,  45, 105,   0,  60,\n",
      "        120,  30, 120, 165,  90, 150,  15, 150,  15, 150,   0,   0,   0,   0,\n",
      "         30,  90,  75,  45,  90, 165,  30,  15,  45, 150,   0, 120,   0,  60,\n",
      "        150, 150, 105, 120, 105,  30,  60, 165,  60, 150,  90,  75,  90,  15,\n",
      "        120,  15, 120, 165,  75, 105, 135,  60,  30,  45, 150,  15,  45,  90,\n",
      "         15, 150,  90,  30,  30,  15,  45,  60,   0,  15, 165,  60, 165, 120,\n",
      "        150,  45,  45,  75]), 'segment_end': tensor([ 45,  30, 105, 135, 195, 165, 105,  60,  75,  45, 120, 150,  30,  60,\n",
      "         30, 150,  75, 180,  45, 105,  45, 195, 120,  45,  75, 165, 150, 195,\n",
      "        105, 195, 195, 195, 195,  75, 120, 105,  30, 165, 120,  75, 150,  30,\n",
      "         75,  75, 195, 135, 150,  30,  75, 135,  60,  60,  60, 150, 180, 180,\n",
      "        180,  60, 195,  30,  30, 135,  75,  60,  30,  75,  60, 180, 180, 195,\n",
      "         60, 135, 180,  45, 120, 105, 150,  90, 120, 150,  90, 180, 195, 165,\n",
      "        120,  75,  90,  45, 135, 135, 105, 105, 105, 135,  90,  30,  75,  30,\n",
      "         90, 105, 195, 180, 195, 150,  90, 120, 135,  75,  45, 180, 120,  45,\n",
      "        135, 135, 165, 150,  60,  45, 120, 195, 120,  90, 105, 150, 180, 120,\n",
      "         75,  60, 195, 180, 180, 135,  30, 135,  75,  45,  60, 165,  30, 120,\n",
      "        150, 150, 195,  90, 150,  90, 195, 105, 120,  75,  60, 195,  60, 195,\n",
      "         90, 180, 105,  90, 120,  75, 105,  90,  60,  75, 180, 120,  60, 105,\n",
      "        135, 195,  45,  60, 150,  45,  90, 165,  75, 150,  75, 135,  30,  90,\n",
      "        150,  60, 150, 195, 120, 180,  45, 180,  45, 180,  30,  30,  30,  30,\n",
      "         60, 120, 105,  75, 120, 195,  60,  45,  75, 180,  30, 150,  30,  90,\n",
      "        180, 180, 135, 150, 135,  60,  90, 195,  90, 180, 120, 105, 120,  45,\n",
      "        150,  45, 150, 195, 105, 135, 165,  90,  60,  75, 180,  45,  75, 120,\n",
      "         45, 180, 120,  60,  60,  45,  75,  90,  30,  45, 195,  90, 195, 150,\n",
      "        180,  75,  75, 105])}\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([256, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Batch Shape: torch.Size([256, 62, 30])\n"
     ]
    }
   ],
   "source": [
    "for batch_data, batch_labels in train_loader:\n",
    "    print(f\"Original Batch Shape: {batch_data.shape}, Metadata: {batch_labels}\")\n",
    "\n",
    "    normalized_data = stratified_normalization_minibatch(batch_data, batch_labels)\n",
    "    print(f\"Normalized Batch Shape: {normalized_data.shape}\")\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Batch Shape: torch.Size([256, 62, 30])\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([256, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Batch Shape: torch.Size([256, 62, 30])\n"
     ]
    }
   ],
   "source": [
    "for batch_data, batch_labels in train_loader:\n",
    "    print(f\"Original Batch Shape: {batch_data.shape}\")\n",
    "    normalized_data = stratified_normalization_minibatch(batch_data, batch_labels)\n",
    "    print(f\"Normalized Batch Shape: {normalized_data.shape}\")\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "def save_checkpoint(epoch, model, optimizer, loss, path=\"./Checkpoints/\"):\n",
    "    # Ensure the directory exists\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    \n",
    "    save_path = os.path.join(path, f\"checkpoint_epoch_{epoch}.pth\")\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "    }, save_path)\n",
    "    print(f\"Checkpoint saved at: {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(filename, model, optimizer):\n",
    "    \"\"\"\n",
    "    Load a training checkpoint.\n",
    "    Args:\n",
    "        filename: Name of the checkpoint file.\n",
    "        model: The model to load the state into.\n",
    "        optimizer: The optimizer to load the state into.\n",
    "    Returns:\n",
    "        epoch: The epoch at which the checkpoint was saved.\n",
    "        loss: The loss value at the time of saving.\n",
    "    \"\"\"\n",
    "    checkpoint = torch.load(filename)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    loss = checkpoint['loss']\n",
    "    print(f\"Checkpoint loaded: {filename} (epoch {epoch})\")\n",
    "    return epoch, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    eeg_data_batch = torch.stack([item[0] for item in batch])  # Combine data tensors\n",
    "    metadata_batch = [item[1] for item in batch]  # Collect metadata dictionaries\n",
    "    return eeg_data_batch, metadata_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train_model_with_stratified_normalization(train_data, base_encoder, projector, contrastive_loss, epochs=100, batch_size=256, lr=0.0007, patience=30, checkpoint_path=\"./Checkpoints/\"):\n",
    "   \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    writer = SummaryWriter(log_dir=\"E:/FYP/Egg-Based Emotion Recognition/EEg-based-Emotion-Recognition/runs/ContrastiveLearning\")\n",
    "    \n",
    "    checkpoint_dir = \"./Checkpoints/\"\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    \n",
    "    train_dataset = EEGDataset(train_data)\n",
    "    print(f\"Total samples in Dataset: {len(train_dataset)}\")\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, timeout=0)\n",
    "    print(f\"Total batches in DataLoader: {len(train_loader)}\")\n",
    "\n",
    "    # Optimizer and scheduler\n",
    "    model_params = list(base_encoder.parameters()) + list(projector.parameters())\n",
    "    optimizer = optim.Adam(model_params, lr=lr, weight_decay=0.015)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
    "\n",
    "    base_encoder.train()\n",
    "    projector.train()\n",
    "\n",
    "    best_loss = float(\"inf\")\n",
    "    epochs_no_improve = 0\n",
    "    \n",
    "    start_epoch = 0\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        start_epoch, best_loss = load_checkpoint(checkpoint_path, base_encoder, optimizer)\n",
    "        \n",
    "\n",
    "    for epoch in range(start_epoch,epochs):\n",
    "        print(f\"Entered Epoch {epoch + 1}\")\n",
    "        total_loss = 0.0\n",
    "        print(f\"Starting Training Loop with {len(train_loader)} batches.\")\n",
    "\n",
    "\n",
    "        for batch_idx, (x, metadata) in enumerate(train_loader):\n",
    "            print(f\"Processing Batch {batch_idx + 1}/{len(train_loader)}\")\n",
    "            print(f\"Batch {batch_idx + 1} loaded: x.shape={x.shape}, metadata keys = {list(metadata.keys())}\")\n",
    "           \n",
    "            print(\"Entering try block\")\n",
    "            try:\n",
    "                print(\"Inside try block\")\n",
    "                x = x.float().to(device)  # Send input to device\n",
    "              \n",
    "                # Apply stratified normalization on the minibatch\n",
    "                x = stratified_normalization_minibatch(x, metadata)\n",
    "                print(f\"Normalized Input Shape: {x.shape}\")\n",
    "\n",
    "                # Forward pass through Base Encoder and Projector\n",
    "                encoded = base_encoder(x)\n",
    "                print(f\"Encoded Output Shape: {encoded.shape}\")\n",
    "                \n",
    "                z_i = projector(encoded)\n",
    "                print(f\"Projected z_i Shape: {z_i.shape}\")\n",
    "                \n",
    "                z_j = projector(encoded + torch.normal(mean=0, std=0.7, size=encoded.shape).to(device))  \n",
    "                print(f\"Projected z_j Shape: {z_j.shape}\")\n",
    "                \n",
    "                z_i = z_i.view(z_i.size(0), -1)\n",
    "                z_j = z_j.view(z_j.size(0), -1)\n",
    "\n",
    "                # Compute Contrastive Loss\n",
    "                loss = contrastive_loss(z_i,z_j)  # Replace second projected with positive pairs if applicable\n",
    "                print(f\"Batch {batch_idx + 1}, Loss: {loss.item()}\")\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                writer.add_scalar(\"Batch Loss\", loss.item(), epoch * len(train_loader) + batch_idx)\n",
    "\n",
    "                # Backpropagation and optimization step\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model_params, max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in Batch {batch_idx + 1}: {e}\")\n",
    "                break\n",
    "                      \n",
    "\n",
    "        # Average loss over all batches\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        writer.add_scalar(\"Training Loss\", avg_loss, epoch)\n",
    "        \n",
    "        if epoch % 5 == 0 or avg_loss < best_loss:\n",
    "            save_checkpoint_path = os.path.join(checkpoint_dir, f\"checkpoint_epoch_{epoch}.pth\")\n",
    "            # save_checkpoint(epoch, base_encoder, optimizer, avg_loss, save_checkpoint_path)\n",
    "\n",
    "        \n",
    "            \n",
    "        # Early stopping logic\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "        # Step the scheduler\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    writer.close()\n",
    "    print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples in Dataset: 10000\n",
      "Total batches in DataLoader: 157\n",
      "Entered Epoch 1\n",
      "Starting Training Loop with 157 batches.\n",
      "Processing Batch 1/157\n",
      "Batch 1 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.05768028646707535, Std: 0.0669778436422348\n",
      "z_j Mean: 0.05666159465909004, Std: 0.06784191727638245\n",
      "Similarities: tensor([[1.3802, 1.4024, 1.4034, 1.4123, 1.3909],\n",
      "        [1.3719, 1.4019, 1.4074, 1.4170, 1.4049],\n",
      "        [1.3642, 1.3959, 1.4120, 1.4157, 1.4026],\n",
      "        [1.3712, 1.3991, 1.4078, 1.4157, 1.3968],\n",
      "        [1.3637, 1.3993, 1.4114, 1.4212, 1.4109]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [1.3374793529510498, 1.4212467670440674]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 4.156895160675049\n",
      "Batch 1, Loss: 4.156895160675049\n",
      "Processing Batch 2/157\n",
      "Batch 2 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.05748990178108215, Std: 0.06714135408401489\n",
      "z_j Mean: 0.05621696263551712, Std: 0.06821085512638092\n",
      "Similarities: tensor([[1.4021, 1.4100, 1.3835, 1.3927, 1.3505],\n",
      "        [1.4062, 1.4088, 1.3786, 1.3903, 1.3413],\n",
      "        [1.3995, 1.4061, 1.3779, 1.3941, 1.3282],\n",
      "        [1.3993, 1.4084, 1.3870, 1.3973, 1.3426],\n",
      "        [1.3995, 1.4064, 1.3859, 1.3956, 1.3585]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [1.3247603178024292, 1.420728325843811]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 4.155828475952148\n",
      "Batch 2, Loss: 4.155828475952148\n",
      "Processing Batch 3/157\n",
      "Batch 3 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.05738762021064758, Std: 0.06722880899906158\n",
      "z_j Mean: 0.056480709463357925, Std: 0.06799260526895523\n",
      "Similarities: tensor([[1.3753, 1.3937, 1.4058, 1.3546, 1.3802],\n",
      "        [1.3779, 1.3803, 1.4023, 1.3697, 1.3789],\n",
      "        [1.3570, 1.3885, 1.4131, 1.3825, 1.3816],\n",
      "        [1.3656, 1.3756, 1.3986, 1.3822, 1.3787],\n",
      "        [1.3624, 1.3877, 1.4128, 1.3912, 1.3877]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [1.3289227485656738, 1.4202241897583008]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 4.15421199798584\n",
      "Batch 3, Loss: 4.15421199798584\n",
      "Processing Batch 4/157\n",
      "Batch 4 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.05714225396513939, Std: 0.06743751466274261\n",
      "z_j Mean: 0.056429363787174225, Std: 0.06803522258996964\n",
      "Similarities: tensor([[1.3855, 1.3691, 1.4078, 1.3770, 1.3777],\n",
      "        [1.3361, 1.3943, 1.3952, 1.3495, 1.3705],\n",
      "        [1.3605, 1.3767, 1.4105, 1.3938, 1.3985],\n",
      "        [1.3884, 1.3626, 1.4079, 1.3741, 1.3731],\n",
      "        [1.3612, 1.3873, 1.4124, 1.3619, 1.3830]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [1.3020110130310059, 1.4203252792358398]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 4.15210485458374\n",
      "Batch 4, Loss: 4.15210485458374\n",
      "Processing Batch 5/157\n",
      "Batch 5 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.05691274255514145, Std: 0.06763134151697159\n",
      "z_j Mean: 0.056272976100444794, Std: 0.0681646466255188\n",
      "Similarities: tensor([[1.4028, 1.4103, 1.3942, 1.3599, 1.3829],\n",
      "        [1.3751, 1.4039, 1.3634, 1.3993, 1.3957],\n",
      "        [1.3869, 1.4158, 1.3756, 1.3800, 1.3818],\n",
      "        [1.3380, 1.3752, 1.3235, 1.4132, 1.3897],\n",
      "        [1.3809, 1.4120, 1.3661, 1.3818, 1.3959]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [1.2633802890777588, 1.421179175376892]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 4.148014068603516\n",
      "Batch 5, Loss: 4.148014068603516\n",
      "Processing Batch 6/157\n",
      "Batch 6 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.05682756379246712, Std: 0.06770293414592743\n",
      "z_j Mean: 0.05605459213256836, Std: 0.0683443620800972\n",
      "Similarities: tensor([[1.3884, 1.3113, 1.4042, 1.4100, 1.3843],\n",
      "        [1.3076, 1.3924, 1.3680, 1.3173, 1.2904],\n",
      "        [1.3883, 1.3087, 1.4040, 1.4118, 1.3815],\n",
      "        [1.3853, 1.3100, 1.4036, 1.4145, 1.3826],\n",
      "        [1.3832, 1.3103, 1.4041, 1.4074, 1.3908]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [1.2634530067443848, 1.4249082803726196]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 4.143957614898682\n",
      "Batch 6, Loss: 4.143957614898682\n",
      "Processing Batch 7/157\n",
      "Batch 7 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.056529700756073, Std: 0.06795186549425125\n",
      "z_j Mean: 0.0556778609752655, Std: 0.06865166127681732\n",
      "Similarities: tensor([[1.3833, 1.4076, 1.3917, 1.3892, 1.3998],\n",
      "        [1.3869, 1.4141, 1.3911, 1.3983, 1.3766],\n",
      "        [1.4017, 1.3871, 1.4148, 1.3766, 1.4027],\n",
      "        [1.3833, 1.4022, 1.3919, 1.3927, 1.3960],\n",
      "        [1.3822, 1.3764, 1.4051, 1.3649, 1.4088]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [1.1986812353134155, 1.4230639934539795]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 4.138479709625244\n",
      "Batch 7, Loss: 4.138479709625244\n",
      "Processing Batch 8/157\n",
      "Batch 8 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.0564684197306633, Std: 0.06800281256437302\n",
      "z_j Mean: 0.05572853609919548, Std: 0.0686105340719223\n",
      "Similarities: tensor([[1.4159, 1.3659, 1.3684, 1.3781, 1.3459],\n",
      "        [1.3761, 1.4101, 1.3128, 1.3686, 1.3595],\n",
      "        [1.4011, 1.3123, 1.3877, 1.3506, 1.3030],\n",
      "        [1.4100, 1.4026, 1.3478, 1.3910, 1.3675],\n",
      "        [1.4017, 1.4028, 1.3460, 1.3900, 1.3634]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [1.2078263759613037, 1.422312617301941]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 4.1363420486450195\n",
      "Batch 8, Loss: 4.1363420486450195\n",
      "Processing Batch 9/157\n",
      "Batch 9 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.05593321844935417, Std: 0.06844374537467957\n",
      "z_j Mean: 0.0555008128285408, Std: 0.06879489868879318\n",
      "Similarities: tensor([[1.4230, 1.3476, 1.3712, 1.3806, 1.3463],\n",
      "        [1.3875, 1.3945, 1.3999, 1.3704, 1.3618],\n",
      "        [1.3691, 1.3914, 1.4061, 1.3570, 1.3558],\n",
      "        [1.3569, 1.3861, 1.4067, 1.3830, 1.3796],\n",
      "        [1.3152, 1.3691, 1.3976, 1.3349, 1.3663]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [1.0893865823745728, 1.4240189790725708]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 4.127338409423828\n",
      "Batch 9, Loss: 4.127338409423828\n",
      "Processing Batch 10/157\n",
      "Batch 10 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.05561113357543945, Std: 0.0687057301402092\n",
      "z_j Mean: 0.05535321310162544, Std: 0.06891372799873352\n",
      "Similarities: tensor([[1.4106, 1.3906, 1.3807, 1.3339, 1.2469],\n",
      "        [1.3998, 1.4025, 1.4015, 1.3723, 1.3087],\n",
      "        [1.4092, 1.3983, 1.3891, 1.3471, 1.2681],\n",
      "        [1.3637, 1.3671, 1.4104, 1.3938, 1.3729],\n",
      "        [1.3019, 1.3133, 1.3720, 1.3935, 1.4062]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [1.0135642290115356, 1.422153353691101]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 4.098474502563477\n",
      "Batch 10, Loss: 4.098474502563477\n",
      "Processing Batch 11/157\n",
      "Batch 11 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.05566249042749405, Std: 0.06866413354873657\n",
      "z_j Mean: 0.054754890501499176, Std: 0.06939013302326202\n",
      "Similarities: tensor([[1.3853, 1.2221, 1.3913, 1.3410, 1.1651],\n",
      "        [1.0565, 1.4121, 1.2200, 0.9941, 1.3986],\n",
      "        [1.3372, 1.3010, 1.3863, 1.3079, 1.2457],\n",
      "        [1.4031, 1.1165, 1.3600, 1.3911, 1.0531],\n",
      "        [1.0344, 1.4068, 1.2031, 0.9656, 1.3939]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.8887454867362976, 1.4213649034500122]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 4.083422660827637\n",
      "Batch 11, Loss: 4.083422660827637\n",
      "Processing Batch 12/157\n",
      "Batch 12 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.05477141588926315, Std: 0.06937707960605621\n",
      "z_j Mean: 0.05428525432944298, Std: 0.06975819170475006\n",
      "Similarities: tensor([[1.3885, 1.3902, 1.2398, 1.3661, 1.3775],\n",
      "        [1.4108, 1.4052, 1.2998, 1.4021, 1.3988],\n",
      "        [1.2606, 1.3181, 1.3866, 1.3363, 1.2267],\n",
      "        [1.3867, 1.3988, 1.3640, 1.4110, 1.3589],\n",
      "        [1.4081, 1.3833, 1.2872, 1.3859, 1.4079]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.9028135538101196, 1.4241379499435425]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 4.070731163024902\n",
      "Batch 12, Loss: 4.070731163024902\n",
      "Processing Batch 13/157\n",
      "Batch 13 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.05427800118923187, Std: 0.06976383924484253\n",
      "z_j Mean: 0.054133910685777664, Std: 0.06987571716308594\n",
      "Similarities: tensor([[1.3883, 1.3726, 1.0480, 1.3521, 1.0114],\n",
      "        [1.3685, 1.3605, 1.1913, 1.3983, 1.1782],\n",
      "        [1.0465, 1.0847, 1.4042, 1.1824, 1.4131],\n",
      "        [1.3403, 1.3336, 1.2443, 1.4033, 1.2358],\n",
      "        [1.0880, 1.1292, 1.4048, 1.2190, 1.4126]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.7932473421096802, 1.4236339330673218]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 4.0392842292785645\n",
      "Batch 13, Loss: 4.0392842292785645\n",
      "Processing Batch 14/157\n",
      "Batch 14 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.05388125032186508, Std: 0.07007075101137161\n",
      "z_j Mean: 0.05370243638753891, Std: 0.07020790129899979\n",
      "Similarities: tensor([[1.3985, 1.3064, 1.3166, 1.2441, 1.3804],\n",
      "        [1.2857, 1.4055, 1.0195, 0.9024, 1.1730],\n",
      "        [1.2949, 1.0682, 1.4105, 1.3781, 1.3391],\n",
      "        [1.2079, 0.9162, 1.3670, 1.4001, 1.2557],\n",
      "        [1.3645, 1.2083, 1.3751, 1.3252, 1.3956]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.7549613118171692, 1.4234308004379272]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 4.015561103820801\n",
      "Batch 14, Loss: 4.015561103820801\n",
      "Processing Batch 15/157\n",
      "Batch 15 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.05383343994617462, Std: 0.07010749727487564\n",
      "z_j Mean: 0.05348718911409378, Std: 0.07037204504013062\n",
      "Similarities: tensor([[1.4140, 1.3732, 1.2890, 1.1732, 0.8743],\n",
      "        [1.3774, 1.3910, 1.3683, 1.2651, 0.9858],\n",
      "        [1.2883, 1.2913, 1.3902, 1.3829, 1.1654],\n",
      "        [1.1014, 1.1211, 1.3310, 1.3924, 1.3241],\n",
      "        [0.8841, 0.8717, 1.1560, 1.2894, 1.4217]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.7232151627540588, 1.4247705936431885]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.9952335357666016\n",
      "Batch 15, Loss: 3.9952335357666016\n",
      "Processing Batch 16/157\n",
      "Batch 16 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.05400525778532028, Std: 0.06997521221637726\n",
      "z_j Mean: 0.053595829755067825, Std: 0.07028933614492416\n",
      "Similarities: tensor([[1.2932, 1.2560, 1.2648, 1.3597, 1.3611],\n",
      "        [1.3211, 1.3876, 0.8936, 1.2775, 1.1807],\n",
      "        [1.0347, 0.9610, 1.4128, 1.1376, 1.3169],\n",
      "        [1.2980, 1.3576, 1.0673, 1.3727, 1.2929],\n",
      "        [1.2716, 1.2198, 1.3218, 1.2959, 1.4050]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.6337717771530151, 1.4266990423202515]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.9365224838256836\n",
      "Batch 16, Loss: 3.9365224838256836\n",
      "Processing Batch 17/157\n",
      "Batch 17 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.05568043142557144, Std: 0.06864957511425018\n",
      "z_j Mean: 0.055161312222480774, Std: 0.06906744837760925\n",
      "Similarities: tensor([[1.4135, 1.2430, 1.3627, 0.9979, 0.6505],\n",
      "        [1.3487, 1.3533, 1.3327, 1.1443, 0.8160],\n",
      "        [1.3886, 1.2906, 1.3855, 1.0759, 0.7230],\n",
      "        [0.9771, 1.1781, 1.0777, 1.3933, 1.3098],\n",
      "        [0.6755, 0.9853, 0.7352, 1.2892, 1.4171]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.5563557744026184, 1.4253044128417969]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.9651153087615967\n",
      "Batch 17, Loss: 3.9651153087615967\n",
      "Processing Batch 18/157\n",
      "Batch 18 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.05428728833794594, Std: 0.06975661218166351\n",
      "z_j Mean: 0.054069072008132935, Std: 0.06992590427398682\n",
      "Similarities: tensor([[1.4202, 0.9541, 0.5709, 0.5771, 1.3931],\n",
      "        [1.0068, 1.4129, 1.2576, 1.2336, 1.0890],\n",
      "        [0.5769, 1.2422, 1.4106, 1.4150, 0.6817],\n",
      "        [0.5795, 1.2398, 1.4148, 1.4238, 0.6721],\n",
      "        [1.3853, 1.0839, 0.6979, 0.7016, 1.4012]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.5094754099845886, 1.4251216650009155]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.906498432159424\n",
      "Batch 18, Loss: 3.906498432159424\n",
      "Processing Batch 19/157\n",
      "Batch 19 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.054065704345703125, Std: 0.06992851197719574\n",
      "z_j Mean: 0.05357462167739868, Std: 0.07030549645423889\n",
      "Similarities: tensor([[1.4131, 1.4088, 1.3378, 1.4178, 1.1451],\n",
      "        [1.4145, 1.4123, 1.3273, 1.4123, 1.1153],\n",
      "        [1.3380, 1.3110, 1.4076, 1.3303, 1.3171],\n",
      "        [1.4096, 1.4050, 1.3471, 1.4184, 1.1584],\n",
      "        [1.1544, 1.1100, 1.3167, 1.1260, 1.3657]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.46581581234931946, 1.426589846611023]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.8794591426849365\n",
      "Batch 19, Loss: 3.8794591426849365\n",
      "Processing Batch 20/157\n",
      "Batch 20 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.05189834162592888, Std: 0.07155198603868484\n",
      "z_j Mean: 0.05162952095270157, Std: 0.07174622267484665\n",
      "Similarities: tensor([[1.4198, 1.3913, 1.4014, 1.4179, 0.5515],\n",
      "        [1.3900, 1.4244, 1.3894, 1.3957, 0.4386],\n",
      "        [1.4188, 1.4111, 1.4164, 1.4098, 0.5136],\n",
      "        [1.4181, 1.4086, 1.4146, 1.4142, 0.5148],\n",
      "        [0.5351, 0.4368, 0.4995, 0.5083, 1.4011]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.40637966990470886, 1.4262694120407104]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.8407959938049316\n",
      "Batch 20, Loss: 3.8407959938049316\n",
      "Processing Batch 21/157\n",
      "Batch 21 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.05314095690846443, Std: 0.0706338956952095\n",
      "z_j Mean: 0.052918657660484314, Std: 0.07080061733722687\n",
      "Similarities: tensor([[1.4058, 1.4191, 1.2477, 1.4128, 1.2694],\n",
      "        [1.4139, 1.4175, 1.1950, 1.4068, 1.2659],\n",
      "        [1.2076, 1.2606, 1.3940, 1.2827, 1.3757],\n",
      "        [1.3894, 1.4113, 1.2231, 1.4158, 1.2957],\n",
      "        [1.3315, 1.3736, 1.2964, 1.3918, 1.3792]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.3482350707054138, 1.4249093532562256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.8212287425994873\n",
      "Batch 21, Loss: 3.8212287425994873\n",
      "Processing Batch 22/157\n",
      "Batch 22 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.0513739213347435, Std: 0.07192948460578918\n",
      "z_j Mean: 0.05150173604488373, Std: 0.07183802127838135\n",
      "Similarities: tensor([[1.3747, 0.4728, 0.4732, 0.4655, 0.8275],\n",
      "        [0.6227, 1.4125, 1.4150, 1.4010, 1.2233],\n",
      "        [0.6091, 1.4125, 1.4191, 1.4094, 1.2234],\n",
      "        [0.6113, 1.3968, 1.4198, 1.4234, 1.1805],\n",
      "        [1.1296, 1.1288, 1.0805, 1.0403, 1.3470]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.2726556360721588, 1.4269747734069824]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.798672676086426\n",
      "Batch 22, Loss: 3.798672676086426\n",
      "Processing Batch 23/157\n",
      "Batch 23 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.05047221854329109, Std: 0.07256513088941574\n",
      "z_j Mean: 0.050323933362960815, Std: 0.07266805320978165\n",
      "Similarities: tensor([[1.4249, 0.3073, 0.2851, 0.2921, 0.5199],\n",
      "        [0.2987, 1.4196, 1.3744, 1.3917, 1.3196],\n",
      "        [0.2961, 1.4154, 1.4035, 1.4117, 1.3414],\n",
      "        [0.2894, 1.3935, 1.4055, 1.4197, 1.3504],\n",
      "        [0.6890, 1.2058, 1.1755, 1.2488, 1.3743]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.22223715484142303, 1.4271843433380127]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.756082057952881\n",
      "Batch 23, Loss: 3.756082057952881\n",
      "Processing Batch 24/157\n",
      "Batch 24 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.051418937742710114, Std: 0.07189732044935226\n",
      "z_j Mean: 0.05126907676458359, Std: 0.0720042735338211\n",
      "Similarities: tensor([[1.4274, 1.3862, 1.3763, 1.1223, 0.8318],\n",
      "        [1.3980, 1.4148, 1.3466, 1.2113, 0.8905],\n",
      "        [1.3802, 1.3606, 1.4211, 1.2329, 0.9649],\n",
      "        [1.0257, 1.1257, 1.0838, 1.3769, 1.2573],\n",
      "        [0.7649, 0.8594, 0.8746, 1.2278, 1.3719]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.20225359499454498, 1.4274265766143799]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.770207643508911\n",
      "Batch 24, Loss: 3.770207643508911\n",
      "Processing Batch 25/157\n",
      "Batch 25 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.05162346363067627, Std: 0.07175058871507645\n",
      "z_j Mean: 0.05128570646047592, Std: 0.071992427110672\n",
      "Similarities: tensor([[1.4206, 0.2199, 0.4451, 0.2880, 0.1986],\n",
      "        [0.2267, 1.4245, 1.2792, 1.3574, 1.4224],\n",
      "        [0.4362, 1.3084, 1.4136, 1.3782, 1.2916],\n",
      "        [0.3059, 1.3599, 1.3779, 1.4184, 1.3596],\n",
      "        [0.2152, 1.4188, 1.2703, 1.3594, 1.4271]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.1692289113998413, 1.427587628364563]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.75270414352417\n",
      "Batch 25, Loss: 3.75270414352417\n",
      "Processing Batch 26/157\n",
      "Batch 26 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.05045616626739502, Std: 0.07257629185914993\n",
      "z_j Mean: 0.05050981417298317, Std: 0.07253896445035934\n",
      "Similarities: tensor([[1.3515, 0.9590, 0.9426, 0.8503, 0.8446],\n",
      "        [0.9700, 1.4218, 1.3760, 1.4006, 0.1967],\n",
      "        [0.9941, 1.3892, 1.4207, 1.3819, 0.2453],\n",
      "        [0.8974, 1.3994, 1.3670, 1.4269, 0.1531],\n",
      "        [0.8652, 0.1832, 0.2485, 0.1411, 1.4229]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.1297069489955902, 1.4274253845214844]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.7247517108917236\n",
      "Batch 26, Loss: 3.7247517108917236\n",
      "Processing Batch 27/157\n",
      "Batch 27 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.049201227724552155, Std: 0.07343294471502304\n",
      "z_j Mean: 0.04915192723274231, Std: 0.07346595823764801\n",
      "Similarities: tensor([[1.4225, 0.6671, 1.3506, 0.1516, 0.1501],\n",
      "        [0.5978, 1.3571, 0.7268, 1.1836, 1.1123],\n",
      "        [1.3549, 0.8665, 1.4196, 0.2052, 0.1682],\n",
      "        [0.1668, 1.0366, 0.1764, 1.4150, 1.4122],\n",
      "        [0.1711, 1.0057, 0.1721, 1.4025, 1.4172]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.10716594010591507, 1.427775502204895]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.694671154022217\n",
      "Batch 27, Loss: 3.694671154022217\n",
      "Processing Batch 28/157\n",
      "Batch 28 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.048434868454933167, Std: 0.07394073158502579\n",
      "z_j Mean: 0.048484984785318375, Std: 0.07390786707401276\n",
      "Similarities: tensor([[1.4250, 0.1658, 0.1617, 0.7811, 0.2055],\n",
      "        [0.1470, 1.4211, 1.3987, 1.0023, 1.3319],\n",
      "        [0.1368, 1.3824, 1.4142, 0.9996, 1.3466],\n",
      "        [0.6502, 1.0795, 1.0692, 1.3913, 1.2075],\n",
      "        [0.2627, 1.2931, 1.2937, 1.2415, 1.4059]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.08406294137239456, 1.4279729127883911]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.7017714977264404\n",
      "Batch 28, Loss: 3.7017714977264404\n",
      "Processing Batch 29/157\n",
      "Batch 29 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.0502932146191597, Std: 0.07268932461738586\n",
      "z_j Mean: 0.0502978153526783, Std: 0.07268613576889038\n",
      "Similarities: tensor([[1.4270, 1.0926, 1.2456, 0.1064, 0.0845],\n",
      "        [1.0878, 1.3896, 1.2243, 0.6001, 0.4481],\n",
      "        [1.3106, 1.2150, 1.4134, 0.1735, 0.1467],\n",
      "        [0.1550, 0.5725, 0.2907, 1.4097, 1.3065],\n",
      "        [0.0884, 0.3339, 0.1632, 1.3224, 1.4175]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.06949920207262039, 1.4276586771011353]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.6749444007873535\n",
      "Batch 29, Loss: 3.6749444007873535\n",
      "Processing Batch 30/157\n",
      "Batch 30 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.04868737608194351, Std: 0.07377468794584274\n",
      "z_j Mean: 0.04875700920820236, Std: 0.07372867316007614\n",
      "Similarities: tensor([[1.4245, 0.1786, 1.4179, 1.4149, 1.2774],\n",
      "        [0.1966, 1.4192, 0.1541, 0.1969, 0.1307],\n",
      "        [1.4037, 0.1538, 1.4241, 1.4078, 1.3330],\n",
      "        [1.4039, 0.2070, 1.4018, 1.4180, 1.2758],\n",
      "        [1.3275, 0.1724, 1.3779, 1.3681, 1.3883]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.05411244556307793, 1.4277011156082153]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.650862693786621\n",
      "Batch 30, Loss: 3.650862693786621\n",
      "Processing Batch 31/157\n",
      "Batch 31 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.04830088093876839, Std: 0.07402833551168442\n",
      "z_j Mean: 0.048268888145685196, Std: 0.0740491971373558\n",
      "Similarities: tensor([[1.4218, 1.3278, 1.3051, 0.0530, 1.2401],\n",
      "        [1.3513, 1.4241, 1.2254, 0.0705, 1.2005],\n",
      "        [1.3417, 1.2870, 1.3899, 0.0495, 1.3807],\n",
      "        [0.0561, 0.0756, 0.0572, 1.4253, 0.1204],\n",
      "        [1.1734, 1.2254, 1.3084, 0.1313, 1.3864]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0394282191991806, 1.428191065788269]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.6357414722442627\n",
      "Batch 31, Loss: 3.6357414722442627\n",
      "Processing Batch 32/157\n",
      "Batch 32 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.048805177211761475, Std: 0.0736967995762825\n",
      "z_j Mean: 0.048679426312446594, Std: 0.07377992570400238\n",
      "Similarities: tensor([[1.4202, 0.8661, 0.9313, 1.3728, 0.0408],\n",
      "        [1.0365, 1.2964, 1.3130, 1.1329, 0.4924],\n",
      "        [0.6712, 1.3691, 1.3684, 0.7410, 0.7927],\n",
      "        [1.3851, 0.8217, 0.8713, 1.4111, 0.0793],\n",
      "        [0.0361, 0.7519, 0.6174, 0.0883, 1.4115]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.03031415492296219, 1.4270657300949097]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.6282825469970703\n",
      "Batch 32, Loss: 3.6282825469970703\n",
      "Processing Batch 33/157\n",
      "Batch 33 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.04729480296373367, Std: 0.07467518001794815\n",
      "z_j Mean: 0.047421894967556, Std: 0.07459452748298645\n",
      "Similarities: tensor([[1.4001, 1.1235, 0.0316, 0.0380, 1.1209],\n",
      "        [1.1873, 1.3963, 0.2819, 0.3199, 1.1062],\n",
      "        [0.0559, 0.3721, 1.4269, 1.4020, 0.1410],\n",
      "        [0.0620, 0.4243, 1.4037, 1.4227, 0.1721],\n",
      "        [0.8632, 1.1016, 0.3971, 0.4716, 1.3337]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.01783931814134121, 1.4280744791030884]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.6035029888153076\n",
      "Batch 33, Loss: 3.6035029888153076\n",
      "Processing Batch 34/157\n",
      "Batch 34 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.0481521412730217, Std: 0.07412517815828323\n",
      "z_j Mean: 0.04812047258019447, Std: 0.07414574176073074\n",
      "Similarities: tensor([[1.4208, 0.0698, 0.4465, 1.3028, 0.8493],\n",
      "        [0.1682, 1.3485, 1.1524, 0.3303, 0.8702],\n",
      "        [0.2754, 1.2117, 1.3135, 0.4011, 0.9660],\n",
      "        [1.3021, 0.3021, 0.7690, 1.4055, 1.1582],\n",
      "        [0.9930, 0.7432, 1.1783, 1.1808, 1.3736]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.012158980593085289, 1.4272593259811401]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.5995118618011475\n",
      "Batch 34, Loss: 3.5995118618011475\n",
      "Processing Batch 35/157\n",
      "Batch 35 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.0455116331577301, Std: 0.07577526569366455\n",
      "z_j Mean: 0.04542114958167076, Std: 0.07582955062389374\n",
      "Similarities: tensor([[1.4259, 1.4072, 0.0482, 0.0383, 0.0420],\n",
      "        [1.4074, 1.4249, 0.0399, 0.0312, 0.0294],\n",
      "        [0.0355, 0.0310, 1.4146, 1.3746, 1.3959],\n",
      "        [0.0322, 0.0330, 1.3727, 1.4239, 1.3436],\n",
      "        [0.0483, 0.0347, 1.3703, 1.3345, 1.4213]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.008822816424071789, 1.4278231859207153]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.6002635955810547\n",
      "Batch 35, Loss: 3.6002635955810547\n",
      "Processing Batch 36/157\n",
      "Batch 36 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.04642851650714874, Std: 0.07521691173315048\n",
      "z_j Mean: 0.04632420837879181, Std: 0.07528120279312134\n",
      "Similarities: tensor([[1.4254, 0.0337, 1.3464, 1.3331, 0.0173],\n",
      "        [0.0239, 1.4135, 0.0222, 0.0080, 1.2151],\n",
      "        [1.3547, 0.0160, 1.4246, 1.3750, 0.0626],\n",
      "        [1.3419, 0.0096, 1.3589, 1.4279, 0.0214],\n",
      "        [0.0162, 1.0362, 0.0722, 0.0260, 1.3935]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.007439464796334505, 1.4278734922409058]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.5599446296691895\n",
      "Batch 36, Loss: 3.5599446296691895\n",
      "Processing Batch 37/157\n",
      "Batch 37 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.04609750583767891, Std: 0.07542024552822113\n",
      "z_j Mean: 0.046087801456451416, Std: 0.07542618364095688\n",
      "Similarities: tensor([[1.4214, 1.2756, 0.0382, 0.4207, 0.6877],\n",
      "        [1.3380, 1.4259, 0.0941, 0.3350, 0.5458],\n",
      "        [0.0457, 0.0975, 1.4008, 0.9793, 0.7113],\n",
      "        [0.3711, 0.3212, 0.9985, 1.3735, 1.3100],\n",
      "        [0.8122, 0.6850, 0.5469, 1.1768, 1.3565]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0016787702916190028, 1.4275681972503662]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.528806209564209\n",
      "Batch 37, Loss: 3.528806209564209\n",
      "Processing Batch 38/157\n",
      "Batch 38 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.046059124171733856, Std: 0.07544369995594025\n",
      "z_j Mean: 0.046205323189496994, Std: 0.07535424083471298\n",
      "Similarities: tensor([[1.3941, 0.4109, 0.5760, 0.6172, 0.4244],\n",
      "        [0.3953, 1.4200, 0.0037, 1.3080, 0.1319],\n",
      "        [0.6129, 0.0059, 1.4275, 0.0026, 0.9846],\n",
      "        [0.5741, 1.2918, 0.0047, 1.4237, 0.0869],\n",
      "        [0.6943, 0.0949, 1.0945, 0.0967, 1.3898]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [2.8669155653915368e-05, 1.4282246828079224]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.5066118240356445\n",
      "Batch 38, Loss: 3.5066118240356445\n",
      "Processing Batch 39/157\n",
      "Batch 39 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.04539055377244949, Std: 0.07584787160158157\n",
      "z_j Mean: 0.045285701751708984, Std: 0.0759105309844017\n",
      "Similarities: tensor([[1.3218, 0.5630, 0.8761, 0.3876, 1.2151],\n",
      "        [0.4360, 1.4250, 0.7396, 0.0157, 0.4910],\n",
      "        [0.4782, 0.6351, 1.4067, 0.2216, 0.3639],\n",
      "        [0.6398, 0.0197, 0.2269, 1.3946, 0.5576],\n",
      "        [1.3696, 0.4435, 0.4353, 0.4459, 1.4150]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0013370424276217818, 1.4278860092163086]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.479088544845581\n",
      "Batch 39, Loss: 3.479088544845581\n",
      "Processing Batch 40/157\n",
      "Batch 40 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.04498071223497391, Std: 0.07609166949987411\n",
      "z_j Mean: 0.045016780495643616, Std: 0.07607033848762512\n",
      "Similarities: tensor([[1.4261, 0.4615, 0.0185, 1.3908, 0.0250],\n",
      "        [0.4746, 1.3768, 0.4011, 0.6059, 0.4921],\n",
      "        [0.0162, 0.4798, 1.4211, 0.0143, 1.0827],\n",
      "        [1.4013, 0.5244, 0.0170, 1.4203, 0.0038],\n",
      "        [0.0201, 0.6586, 1.1005, 0.0096, 1.4189]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [1.1540883861016482e-05, 1.4278843402862549]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.4943292140960693\n",
      "Batch 40, Loss: 3.4943292140960693\n",
      "Processing Batch 41/157\n",
      "Batch 41 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.04630593955516815, Std: 0.0752924382686615\n",
      "z_j Mean: 0.046336717903614044, Std: 0.07527350634336472\n",
      "Similarities: tensor([[1.3931, 0.2990, 1.2119, 0.3325, 0.0437],\n",
      "        [0.3675, 1.3901, 0.5521, 1.2962, 1.1854],\n",
      "        [1.1611, 0.6832, 1.3970, 0.7538, 0.1624],\n",
      "        [0.3257, 1.4181, 0.5476, 1.4028, 1.1755],\n",
      "        [0.0568, 1.1436, 0.1291, 1.0375, 1.4232]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [9.310345922131091e-05, 1.4283500909805298]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.4901764392852783\n",
      "Batch 41, Loss: 3.4901764392852783\n",
      "Processing Batch 42/157\n",
      "Batch 42 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.04490755498409271, Std: 0.07613487541675568\n",
      "z_j Mean: 0.04496818780899048, Std: 0.07609907537698746\n",
      "Similarities: tensor([[1.4269, 1.2946, 0.8733, 0.1120, 0.0044],\n",
      "        [1.3176, 1.4098, 0.8740, 0.1309, 0.0192],\n",
      "        [0.7252, 0.6565, 1.4047, 0.9297, 0.3103],\n",
      "        [0.0915, 0.0847, 0.7816, 1.4264, 0.8425],\n",
      "        [0.0029, 0.0129, 0.1929, 0.7282, 1.4021]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4278429746627808]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.454031467437744\n",
      "Batch 42, Loss: 3.454031467437744\n",
      "Processing Batch 43/157\n",
      "Batch 43 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.04466014355421066, Std: 0.07628028094768524\n",
      "z_j Mean: 0.0446171872317791, Std: 0.07630541920661926\n",
      "Similarities: tensor([[1.4184, 0.1677, 0.0271, 1.0340, 0.0744],\n",
      "        [0.2066, 1.4049, 0.7132, 0.1940, 1.0389],\n",
      "        [0.0225, 0.7370, 1.4214, 0.0255, 1.1902],\n",
      "        [0.9169, 0.3139, 0.0302, 1.3835, 0.0579],\n",
      "        [0.0775, 1.1150, 1.2526, 0.0205, 1.4141]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.427875280380249]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.4490110874176025\n",
      "Batch 43, Loss: 3.4490110874176025\n",
      "Processing Batch 44/157\n",
      "Batch 44 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.0422353632748127, Std: 0.07764928042888641\n",
      "z_j Mean: 0.042390208691358566, Std: 0.07756484299898148\n",
      "Similarities: tensor([[1.4212, 0.5259, 1.3764, 1.2976, 1.3509],\n",
      "        [0.3181, 1.3780, 0.3149, 0.5556, 0.3835],\n",
      "        [1.4052, 0.5009, 1.4271, 1.3616, 1.4163],\n",
      "        [1.3398, 0.6719, 1.3791, 1.4232, 1.4100],\n",
      "        [1.3876, 0.5547, 1.4173, 1.3904, 1.4270]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4282941818237305]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.4545581340789795\n",
      "Batch 44, Loss: 3.4545581340789795\n",
      "Processing Batch 45/157\n",
      "Batch 45 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.04317937791347504, Std: 0.07712825387716293\n",
      "z_j Mean: 0.043019648641347885, Std: 0.0772174745798111\n",
      "Similarities: tensor([[1.3665, 0.3518, 0.4560, 1.1116, 1.0896],\n",
      "        [0.2935, 1.4066, 1.3886, 0.0978, 0.1031],\n",
      "        [0.3235, 1.3882, 1.4142, 0.1061, 0.1071],\n",
      "        [1.1119, 0.0491, 0.0682, 1.4111, 1.3868],\n",
      "        [1.2259, 0.0974, 0.1344, 1.3931, 1.4257]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4273300170898438]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.4007959365844727\n",
      "Batch 45, Loss: 3.4007959365844727\n",
      "Processing Batch 46/157\n",
      "Batch 46 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.042341042309999466, Std: 0.07759169489145279\n",
      "z_j Mean: 0.042278680950403214, Std: 0.07762569934129715\n",
      "Similarities: tensor([[1.4078e+00, 1.3203e-01, 3.1971e-02, 2.6019e-01, 5.4523e-01],\n",
      "        [1.1821e-01, 1.4231e+00, 1.1504e+00, 6.3843e-01, 2.6982e-04],\n",
      "        [2.3297e-02, 1.1263e+00, 1.4275e+00, 1.0605e+00, 1.6955e-03],\n",
      "        [2.5427e-01, 6.0236e-01, 1.0783e+00, 1.4254e+00, 8.6594e-02],\n",
      "        [6.2738e-01, 1.9242e-03, 0.0000e+00, 7.7052e-02, 1.4240e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4279953241348267]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.4028849601745605\n",
      "Batch 46, Loss: 3.4028849601745605\n",
      "Processing Batch 47/157\n",
      "Batch 47 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.042100027203559875, Std: 0.07772275060415268\n",
      "z_j Mean: 0.04232814535498619, Std: 0.07759873569011688\n",
      "Similarities: tensor([[1.3842, 1.0378, 0.4037, 0.0219, 0.7560],\n",
      "        [0.9102, 1.4001, 0.0713, 0.0397, 1.2628],\n",
      "        [0.4849, 0.0819, 1.4172, 0.0216, 0.0454],\n",
      "        [0.0387, 0.0181, 0.0173, 1.4198, 0.0021],\n",
      "        [0.6182, 1.2827, 0.0463, 0.0031, 1.4265]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4280484914779663]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.3740994930267334\n",
      "Batch 47, Loss: 3.3740994930267334\n",
      "Processing Batch 48/157\n",
      "Batch 48 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.04128070920705795, Std: 0.07816103845834732\n",
      "z_j Mean: 0.041193217039108276, Std: 0.07820719480514526\n",
      "Similarities: tensor([[1.4201e+00, 5.8630e-01, 1.1151e+00, 3.1920e-02, 2.4626e-01],\n",
      "        [5.4094e-01, 1.4172e+00, 9.0268e-01, 1.8017e-02, 7.8920e-04],\n",
      "        [1.1844e+00, 1.0021e+00, 1.4068e+00, 8.2205e-03, 1.5937e-01],\n",
      "        [2.0147e-02, 1.5714e-02, 8.6140e-03, 1.4279e+00, 7.2624e-01],\n",
      "        [2.9890e-01, 1.1754e-02, 2.5607e-01, 5.7036e-01, 1.3951e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4278931617736816]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.346611738204956\n",
      "Batch 48, Loss: 3.346611738204956\n",
      "Processing Batch 49/157\n",
      "Batch 49 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.04015328362584114, Std: 0.0787462443113327\n",
      "z_j Mean: 0.04014153778553009, Std: 0.07875223457813263\n",
      "Similarities: tensor([[1.4136, 0.0203, 0.2135, 1.3510, 1.2284],\n",
      "        [0.0028, 1.4206, 1.3091, 0.0089, 0.2002],\n",
      "        [0.1173, 1.3661, 1.4164, 0.1091, 0.4609],\n",
      "        [1.3791, 0.0131, 0.1679, 1.4253, 1.0977],\n",
      "        [1.1628, 0.2293, 0.5374, 1.0611, 1.4188]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4273567199707031]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.3347830772399902\n",
      "Batch 49, Loss: 3.3347830772399902\n",
      "Processing Batch 50/157\n",
      "Batch 50 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.039408691227436066, Std: 0.07912154495716095\n",
      "z_j Mean: 0.03937828540802002, Std: 0.0791366845369339\n",
      "Similarities: tensor([[1.4249, 1.1706, 0.0097, 0.0592, 1.0270],\n",
      "        [1.2295, 1.4251, 0.0755, 0.0256, 1.3717],\n",
      "        [0.0085, 0.0555, 1.4098, 0.1636, 0.0880],\n",
      "        [0.0589, 0.0273, 0.1822, 1.4050, 0.0619],\n",
      "        [1.0757, 1.3891, 0.1091, 0.0670, 1.4267]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4266878366470337]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.3183836936950684\n",
      "Batch 50, Loss: 3.3183836936950684\n",
      "Processing Batch 51/157\n",
      "Batch 51 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.03832008317112923, Std: 0.07965454459190369\n",
      "z_j Mean: 0.03828694671392441, Std: 0.07967047393321991\n",
      "Similarities: tensor([[1.4239e+00, 4.4095e-02, 0.0000e+00, 1.5379e-01, 8.2040e-02],\n",
      "        [2.9461e-02, 1.4018e+00, 2.6268e-01, 9.7670e-01, 1.3050e+00],\n",
      "        [3.1802e-04, 1.9059e-01, 1.4070e+00, 3.5130e-01, 1.5705e-01],\n",
      "        [1.3189e-01, 8.5987e-01, 4.2692e-01, 1.4108e+00, 7.8670e-01],\n",
      "        [8.3029e-02, 1.3520e+00, 1.8557e-01, 7.8850e-01, 1.4246e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.428297758102417]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.314340591430664\n",
      "Batch 51, Loss: 3.314340591430664\n",
      "Processing Batch 52/157\n",
      "Batch 52 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.037634845823049545, Std: 0.07998061925172806\n",
      "z_j Mean: 0.03755539283156395, Std: 0.08001795411109924\n",
      "Similarities: tensor([[1.4226e+00, 3.1974e-01, 5.0312e-01, 1.0168e+00, 1.2352e+00],\n",
      "        [2.7657e-01, 1.4173e+00, 7.0431e-01, 3.0646e-03, 6.8885e-01],\n",
      "        [6.0904e-01, 6.3243e-01, 1.3708e+00, 6.2181e-01, 5.8431e-01],\n",
      "        [1.0516e+00, 1.2634e-03, 5.6948e-01, 1.4244e+00, 5.5512e-01],\n",
      "        [1.2214e+00, 6.7794e-01, 4.6646e-01, 5.9064e-01, 1.4244e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4280885457992554]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.319119691848755\n",
      "Batch 52, Loss: 3.319119691848755\n",
      "Processing Batch 53/157\n",
      "Batch 53 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.037337783724069595, Std: 0.08011974394321442\n",
      "z_j Mean: 0.037095777690410614, Std: 0.08023209124803543\n",
      "Similarities: tensor([[1.4084, 1.3959, 0.0105, 0.2342, 0.0087],\n",
      "        [1.3047, 1.3287, 0.0050, 0.1612, 0.0076],\n",
      "        [0.0148, 0.0082, 1.4216, 0.0615, 0.6413],\n",
      "        [0.1195, 0.1507, 0.1404, 1.3987, 1.1277],\n",
      "        [0.0043, 0.0054, 0.6881, 0.9674, 1.4232]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4283123016357422]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2779905796051025\n",
      "Batch 53, Loss: 3.2779905796051025\n",
      "Processing Batch 54/157\n",
      "Batch 54 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.0359593890607357, Std: 0.08074786514043808\n",
      "z_j Mean: 0.03611983731389046, Std: 0.08067621290683746\n",
      "Similarities: tensor([[1.4244e+00, 2.4157e-01, 9.4255e-03, 0.0000e+00, 0.0000e+00],\n",
      "        [3.3744e-01, 1.3414e+00, 8.5261e-01, 1.4797e-01, 8.9419e-04],\n",
      "        [3.4708e-03, 9.8949e-01, 1.4192e+00, 7.1772e-01, 2.1629e-04],\n",
      "        [0.0000e+00, 2.2070e-01, 7.4368e-01, 1.4129e+00, 1.3167e-01],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 1.4978e-01, 1.4218e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.428229808807373]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.256619453430176\n",
      "Batch 54, Loss: 3.256619453430176\n",
      "Processing Batch 55/157\n",
      "Batch 55 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.03483319655060768, Std: 0.08124010264873505\n",
      "z_j Mean: 0.03475193679332733, Std: 0.08127490431070328\n",
      "Similarities: tensor([[1.4104e+00, 0.0000e+00, 1.1769e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 1.4129e+00, 0.0000e+00, 1.3621e+00, 4.5564e-02],\n",
      "        [1.2751e+00, 0.0000e+00, 1.4055e+00, 0.0000e+00, 5.1058e-03],\n",
      "        [9.2890e-05, 1.3910e+00, 0.0000e+00, 1.4161e+00, 1.7764e-02],\n",
      "        [0.0000e+00, 1.0179e-02, 1.3157e-02, 2.2240e-03, 1.4109e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4278557300567627]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2550148963928223\n",
      "Batch 55, Loss: 3.2550148963928223\n",
      "Processing Batch 56/157\n",
      "Batch 56 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.03532447665929794, Std: 0.08102766424417496\n",
      "z_j Mean: 0.035218287259340286, Std: 0.08107388019561768\n",
      "Similarities: tensor([[1.4025e+00, 6.3715e-02, 7.1108e-01, 4.6500e-02, 1.6047e-01],\n",
      "        [3.6677e-02, 1.3487e+00, 0.0000e+00, 1.0660e-03, 1.3003e+00],\n",
      "        [6.0724e-01, 7.6975e-03, 1.3663e+00, 5.1866e-01, 2.8616e-03],\n",
      "        [5.0495e-02, 0.0000e+00, 5.0799e-01, 1.3889e+00, 0.0000e+00],\n",
      "        [1.1342e-01, 1.1635e+00, 0.0000e+00, 0.0000e+00, 1.4191e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4283921718597412]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.228231191635132\n",
      "Batch 56, Loss: 3.228231191635132\n",
      "Processing Batch 57/157\n",
      "Batch 57 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.03540734201669693, Std: 0.0809914842247963\n",
      "z_j Mean: 0.035106778144836426, Std: 0.08112223446369171\n",
      "Similarities: tensor([[1.4246, 0.6478, 0.0000, 0.0000, 1.3970],\n",
      "        [0.8491, 1.3856, 0.0000, 0.0000, 0.9737],\n",
      "        [0.0000, 0.0000, 1.3608, 0.2142, 0.0000],\n",
      "        [0.0000, 0.0000, 0.3486, 1.4250, 0.0000],\n",
      "        [1.3903, 0.8447, 0.0000, 0.0000, 1.4134]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4277218580245972]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.216097593307495\n",
      "Batch 57, Loss: 3.216097593307495\n",
      "Processing Batch 58/157\n",
      "Batch 58 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.03230081871151924, Std: 0.08227991312742233\n",
      "z_j Mean: 0.032159678637981415, Std: 0.08233518153429031\n",
      "Similarities: tensor([[1.4031, 1.3876, 1.3199, 0.1284, 0.0036],\n",
      "        [1.3970, 1.4097, 1.4053, 0.1191, 0.0098],\n",
      "        [1.3948, 1.3885, 1.3870, 0.1397, 0.0126],\n",
      "        [0.1442, 0.1123, 0.1528, 1.3481, 0.0866],\n",
      "        [0.0047, 0.0076, 0.0045, 0.2720, 1.4060]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.427678108215332]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.195788860321045\n",
      "Batch 58, Loss: 3.195788860321045\n",
      "Processing Batch 59/157\n",
      "Batch 59 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.03142508864402771, Std: 0.08261838555335999\n",
      "z_j Mean: 0.031627923250198364, Std: 0.08254093676805496\n",
      "Similarities: tensor([[1.4123e+00, 0.0000e+00, 1.2963e-01, 1.7594e-04, 3.7020e-01],\n",
      "        [0.0000e+00, 1.4181e+00, 0.0000e+00, 6.5502e-03, 0.0000e+00],\n",
      "        [9.5486e-02, 0.0000e+00, 1.4093e+00, 0.0000e+00, 7.8983e-01],\n",
      "        [7.3511e-05, 1.2508e-02, 3.9731e-05, 1.4273e+00, 6.2969e-05],\n",
      "        [3.8682e-01, 0.0000e+00, 7.2339e-01, 0.0000e+00, 1.3982e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4273266792297363]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1823883056640625\n",
      "Batch 59, Loss: 3.1823883056640625\n",
      "Processing Batch 60/157\n",
      "Batch 60 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.02968744933605194, Std: 0.08325864374637604\n",
      "z_j Mean: 0.029521482065320015, Std: 0.08331764489412308\n",
      "Similarities: tensor([[1.3025e+00, 1.0709e+00, 1.4109e-01, 6.0258e-01, 9.1757e-02],\n",
      "        [7.6315e-01, 1.3778e+00, 2.6250e-05, 1.3389e-01, 1.1158e-01],\n",
      "        [2.9998e-01, 1.1790e-02, 1.3527e+00, 7.7720e-01, 0.0000e+00],\n",
      "        [7.9296e-01, 9.6417e-02, 8.0077e-01, 1.3843e+00, 1.1869e-01],\n",
      "        [1.4849e-01, 7.1180e-02, 0.0000e+00, 1.1996e-01, 1.3802e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4267573356628418]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.159841299057007\n",
      "Batch 60, Loss: 3.159841299057007\n",
      "Processing Batch 61/157\n",
      "Batch 61 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.028935492038726807, Std: 0.0835229828953743\n",
      "z_j Mean: 0.02937278524041176, Std: 0.08337018638849258\n",
      "Similarities: tensor([[1.3697e+00, 2.8520e-01, 0.0000e+00, 6.3675e-01, 4.0320e-04],\n",
      "        [2.8470e-01, 1.4033e+00, 0.0000e+00, 7.6935e-01, 0.0000e+00],\n",
      "        [9.9738e-03, 0.0000e+00, 1.4264e+00, 4.0117e-02, 4.2718e-03],\n",
      "        [7.8887e-01, 7.6265e-01, 5.0837e-02, 1.3980e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.4231e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4279261827468872]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.150540351867676\n",
      "Batch 61, Loss: 3.150540351867676\n",
      "Processing Batch 62/157\n",
      "Batch 62 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.027642372995615005, Std: 0.08395987004041672\n",
      "z_j Mean: 0.027734484523534775, Std: 0.08392948657274246\n",
      "Similarities: tensor([[1.4257e+00, 0.0000e+00, 3.9247e-03, 1.7076e-03, 2.8773e-04],\n",
      "        [0.0000e+00, 1.3728e+00, 0.0000e+00, 1.4042e+00, 0.0000e+00],\n",
      "        [8.8259e-03, 0.0000e+00, 1.4120e+00, 0.0000e+00, 8.3245e-01],\n",
      "        [1.1028e-02, 1.3176e+00, 0.0000e+00, 1.4219e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 7.9433e-01, 0.0000e+00, 1.4163e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4275490045547485]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1314070224761963\n",
      "Batch 62, Loss: 3.1314070224761963\n",
      "Processing Batch 63/157\n",
      "Batch 63 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.02588069625198841, Std: 0.08451959490776062\n",
      "z_j Mean: 0.026170233264565468, Std: 0.08443038165569305\n",
      "Similarities: tensor([[1.3461, 0.0000, 0.0000, 0.0852, 0.0000],\n",
      "        [0.0000, 1.2102, 0.1804, 0.0000, 0.0924],\n",
      "        [0.0000, 0.0808, 1.3188, 0.0000, 0.4200],\n",
      "        [0.0522, 0.0000, 0.0000, 1.2003, 0.0000],\n",
      "        [0.0000, 0.0101, 0.3469, 0.0000, 1.4030]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.426308512687683]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.137969970703125\n",
      "Batch 63, Loss: 3.137969970703125\n",
      "Processing Batch 64/157\n",
      "Batch 64 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.02291956916451454, Std: 0.085370272397995\n",
      "z_j Mean: 0.023424748331308365, Std: 0.08523301780223846\n",
      "Similarities: tensor([[1.3689e+00, 8.1227e-01, 0.0000e+00, 3.3130e-02, 4.9611e-01],\n",
      "        [6.2236e-01, 1.3750e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 1.2568e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [2.6956e-02, 6.0083e-04, 0.0000e+00, 1.4280e+00, 1.8086e-01],\n",
      "        [8.9219e-01, 2.1674e-01, 0.0000e+00, 1.1593e-01, 1.2023e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285566806793213]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1123225688934326\n",
      "Batch 64, Loss: 3.1123225688934326\n",
      "Processing Batch 65/157\n",
      "Batch 65 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.022360367700457573, Std: 0.08551845699548721\n",
      "z_j Mean: 0.02266477234661579, Std: 0.08543828129768372\n",
      "Similarities: tensor([[1.3819, 0.0451, 0.0000, 1.3925, 0.5518],\n",
      "        [0.0901, 1.3554, 0.0000, 0.0297, 0.5740],\n",
      "        [0.0000, 0.0000, 0.9037, 0.0000, 0.0000],\n",
      "        [1.3807, 0.0114, 0.0000, 1.4193, 0.5880],\n",
      "        [0.5707, 0.5028, 0.0423, 0.4886, 1.4056]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4275527000427246]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1088125705718994\n",
      "Batch 65, Loss: 3.1088125705718994\n",
      "Processing Batch 66/157\n",
      "Batch 66 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.023014891892671585, Std: 0.08534461259841919\n",
      "z_j Mean: 0.02290177159011364, Std: 0.08465703576803207\n",
      "Similarities: tensor([[1.4240, 0.2515, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2966, 1.4099, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4238, 0.0000, 1.3194],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3758, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3598, 0.0000, 1.4208]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4256802797317505]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1151535511016846\n",
      "Batch 66, Loss: 3.1151535511016846\n",
      "Processing Batch 67/157\n",
      "Batch 67 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.02240639738738537, Std: 0.08550640940666199\n",
      "z_j Mean: 0.022325877100229263, Std: 0.08481074869632721\n",
      "Similarities: tensor([[1.3539, 0.0895, 0.0000, 0.6868, 0.0000],\n",
      "        [0.0000, 1.3967, 0.0000, 0.2953, 0.0401],\n",
      "        [0.0000, 0.0000, 1.4247, 0.0000, 0.0000],\n",
      "        [0.7049, 0.0964, 0.0000, 1.3603, 0.3942],\n",
      "        [0.0000, 0.0857, 0.0000, 0.3539, 1.3532]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.096428155899048\n",
      "Batch 67, Loss: 3.096428155899048\n",
      "Processing Batch 68/157\n",
      "Batch 68 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.023118341341614723, Std: 0.08531665056943893\n",
      "z_j Mean: 0.02297084964811802, Std: 0.08535648137331009\n",
      "Similarities: tensor([[1.4102, 0.0228, 0.0000, 1.1533, 0.0000],\n",
      "        [0.0043, 1.4242, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3899, 0.0000, 1.1002],\n",
      "        [1.1079, 0.0000, 0.0000, 1.3977, 0.0000],\n",
      "        [0.0000, 0.0000, 1.1331, 0.0000, 1.4286]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.085127830505371\n",
      "Batch 68, Loss: 3.085127830505371\n",
      "Processing Batch 69/157\n",
      "Batch 69 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.019573993980884552, Std: 0.08619899302721024\n",
      "z_j Mean: 0.019776860252022743, Std: 0.08615266531705856\n",
      "Similarities: tensor([[1.4164, 0.7092, 0.3216, 0.5615, 0.0000],\n",
      "        [0.6190, 1.2186, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3166, 0.0300, 1.4250, 0.6661, 0.4905],\n",
      "        [0.4779, 0.0820, 0.7844, 1.3875, 0.0000],\n",
      "        [0.0000, 0.0000, 0.5161, 0.0000, 1.4262]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1216750144958496\n",
      "Batch 69, Loss: 3.1216750144958496\n",
      "Processing Batch 70/157\n",
      "Batch 70 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.018698271363973618, Std: 0.08568371832370758\n",
      "z_j Mean: 0.018594540655612946, Std: 0.08499108254909515\n",
      "Similarities: tensor([[1.2757, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.5229, 0.0000, 0.0000, 1.2077],\n",
      "        [0.0000, 0.0000, 1.3173, 0.7909, 0.0000],\n",
      "        [0.0000, 0.0000, 0.5200, 1.3845, 0.0000],\n",
      "        [0.0000, 0.6658, 0.0000, 0.0000, 1.4113]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1668782234191895\n",
      "Batch 70, Loss: 3.1668782234191895\n",
      "Processing Batch 71/157\n",
      "Batch 71 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.016600938513875008, Std: 0.08682067692279816\n",
      "z_j Mean: 0.016850024461746216, Std: 0.08677267283201218\n",
      "Similarities: tensor([[1.3163, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4084, 0.0128, 0.0000, 0.8175],\n",
      "        [0.0000, 0.0135, 1.4118, 0.0000, 0.0127],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3972, 0.0000],\n",
      "        [0.0000, 0.9292, 0.0000, 0.0000, 1.3424]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.164912462234497\n",
      "Batch 71, Loss: 3.164912462234497\n",
      "Processing Batch 72/157\n",
      "Batch 72 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.016542475670576096, Std: 0.08683183044195175\n",
      "z_j Mean: 0.016798650845885277, Std: 0.08678262680768967\n",
      "Similarities: tensor([[1.3837, 0.0000, 0.3166, 1.1857, 0.0082],\n",
      "        [0.0000, 1.2270, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2100, 0.0000, 1.3639, 0.1164, 1.1316],\n",
      "        [1.0379, 0.0000, 0.1382, 1.3877, 0.0000],\n",
      "        [0.0247, 0.0000, 1.0531, 0.0000, 1.4210]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.15032958984375\n",
      "Batch 72, Loss: 3.15032958984375\n",
      "Processing Batch 73/157\n",
      "Batch 73 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01780095137655735, Std: 0.08658256381750107\n",
      "z_j Mean: 0.018080785870552063, Std: 0.08652456104755402\n",
      "Similarities: tensor([[1.4172, 0.3022, 0.0000, 0.2983, 0.0000],\n",
      "        [0.2608, 1.3906, 0.0000, 0.0764, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3832, 0.0000, 0.0335],\n",
      "        [0.2988, 0.1342, 0.0000, 1.3941, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0057, 0.0000, 1.4286]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1216557025909424\n",
      "Batch 73, Loss: 3.1216557025909424\n",
      "Processing Batch 74/157\n",
      "Batch 74 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01627938635647297, Std: 0.08546480536460876\n",
      "z_j Mean: 0.0164249949157238, Std: 0.0861484482884407\n",
      "Similarities: tensor([[1.4263, 0.0000, 0.0000, 0.0015, 0.0614],\n",
      "        [0.0000, 1.3733, 0.7050, 0.0000, 0.0000],\n",
      "        [0.0000, 0.9094, 0.7298, 0.0000, 0.0000],\n",
      "        [0.1819, 0.0000, 0.0000, 1.3732, 1.3667],\n",
      "        [0.1462, 0.0000, 0.0000, 1.3587, 1.4154]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.163393020629883\n",
      "Batch 74, Loss: 3.163393020629883\n",
      "Processing Batch 75/157\n",
      "Batch 75 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.016919545829296112, Std: 0.08675913512706757\n",
      "z_j Mean: 0.01682395301759243, Std: 0.08607141673564911\n",
      "Similarities: tensor([[1.3969, 0.0000, 0.0000, 0.3359, 0.7247],\n",
      "        [0.1579, 1.4285, 0.0628, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0631, 1.4177, 0.0000, 0.0000],\n",
      "        [0.3163, 0.0000, 0.0000, 1.4095, 0.5511],\n",
      "        [1.1200, 0.0000, 0.0000, 0.4538, 1.2863]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4284629821777344]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.188223361968994\n",
      "Batch 75, Loss: 3.188223361968994\n",
      "Processing Batch 76/157\n",
      "Batch 76 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01779300533235073, Std: 0.08658420294523239\n",
      "z_j Mean: 0.01811395213007927, Std: 0.08651761710643768\n",
      "Similarities: tensor([[1.3213, 0.9125, 0.0823, 0.0000, 0.3245],\n",
      "        [0.9396, 1.4099, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0645, 0.0000, 1.3729, 0.0000, 0.0827],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4227, 0.0000],\n",
      "        [0.3813, 0.0000, 0.0047, 0.0000, 1.2453]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4255964756011963]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.150501251220703\n",
      "Batch 76, Loss: 3.150501251220703\n",
      "Processing Batch 77/157\n",
      "Batch 77 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.017739426344633102, Std: 0.08659520000219345\n",
      "z_j Mean: 0.017213936895132065, Std: 0.08670120686292648\n",
      "Similarities: tensor([[1.4176e+00, 9.8557e-01, 2.1648e-02, 1.2492e-01, 3.9178e-02],\n",
      "        [1.0055e+00, 1.3777e+00, 3.2955e-02, 2.1398e-02, 3.8193e-02],\n",
      "        [3.8844e-02, 0.0000e+00, 1.3993e+00, 1.1459e+00, 7.1635e-04],\n",
      "        [1.4554e-01, 0.0000e+00, 8.5269e-01, 1.3671e+00, 2.8509e-03],\n",
      "        [3.8000e-02, 0.0000e+00, 3.4856e-02, 0.0000e+00, 1.4121e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.427617073059082]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.114154577255249\n",
      "Batch 77, Loss: 3.114154577255249\n",
      "Processing Batch 78/157\n",
      "Batch 78 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.017935993149876595, Std: 0.08655469119548798\n",
      "z_j Mean: 0.017685795202851295, Std: 0.08660615980625153\n",
      "Similarities: tensor([[1.3811, 0.3800, 0.0456, 0.0000, 0.1774],\n",
      "        [0.3116, 1.4258, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0330, 0.0000, 1.4176, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3879, 0.0067],\n",
      "        [0.1889, 0.0000, 0.0000, 0.0000, 1.4243]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4271273612976074]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0674068927764893\n",
      "Batch 78, Loss: 3.0674068927764893\n",
      "Processing Batch 79/157\n",
      "Batch 79 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01809796877205372, Std: 0.0865209624171257\n",
      "z_j Mean: 0.01833868771791458, Std: 0.08647025376558304\n",
      "Similarities: tensor([[1.4101, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.1287, 0.0086, 0.3746, 0.0000],\n",
      "        [0.0000, 0.2578, 1.3992, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1587, 0.0000, 1.3595, 0.0000],\n",
      "        [0.0784, 0.0000, 0.0000, 0.0000, 1.3723]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0865840911865234\n",
      "Batch 79, Loss: 3.0865840911865234\n",
      "Processing Batch 80/157\n",
      "Batch 80 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.016655150800943375, Std: 0.08681028336286545\n",
      "z_j Mean: 0.016518061980605125, Std: 0.08683647960424423\n",
      "Similarities: tensor([[1.3273, 0.0000, 0.0000, 0.7092, 0.0000],\n",
      "        [0.0000, 1.3805, 0.3656, 0.0000, 0.0000],\n",
      "        [0.0000, 0.5501, 1.4237, 0.0000, 0.0000],\n",
      "        [0.6883, 0.0000, 0.0000, 1.3941, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3851]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285290241241455]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0989325046539307\n",
      "Batch 80, Loss: 3.0989325046539307\n",
      "Processing Batch 81/157\n",
      "Batch 81 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.016545936465263367, Std: 0.08683117479085922\n",
      "z_j Mean: 0.016692709177732468, Std: 0.08609697222709656\n",
      "Similarities: tensor([[1.4165, 0.5488, 0.2973, 0.5955, 0.0000],\n",
      "        [0.5993, 1.4104, 1.2680, 1.4121, 0.0000],\n",
      "        [0.3442, 1.2931, 1.3791, 1.2662, 0.0000],\n",
      "        [0.5761, 1.4113, 1.1903, 1.4233, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4101]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.068718433380127\n",
      "Batch 81, Loss: 3.068718433380127\n",
      "Processing Batch 82/157\n",
      "Batch 82 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.019260559231042862, Std: 0.0862695723772049\n",
      "z_j Mean: 0.019355662167072296, Std: 0.08624828606843948\n",
      "Similarities: tensor([[1.3846, 0.0000, 0.4797, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3909, 0.0000, 0.9105, 0.0000],\n",
      "        [0.2631, 0.0000, 1.3733, 0.0000, 0.0000],\n",
      "        [0.0000, 1.0890, 0.0000, 1.4247, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.2363]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0699667930603027\n",
      "Batch 82, Loss: 3.0699667930603027\n",
      "Processing Batch 83/157\n",
      "Batch 83 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.016928182914853096, Std: 0.08605097234249115\n",
      "z_j Mean: 0.017015717923641205, Std: 0.08674032241106033\n",
      "Similarities: tensor([[1.4231, 1.1524, 0.0000, 0.0000, 0.0000],\n",
      "        [1.2224, 1.4084, 0.0000, 0.0000, 0.0679],\n",
      "        [0.0000, 0.0000, 1.4142, 0.8906, 0.0000],\n",
      "        [0.0000, 0.0000, 1.0975, 1.3833, 0.0000],\n",
      "        [0.0000, 0.1581, 0.0000, 0.0000, 1.4141]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.090458393096924\n",
      "Batch 83, Loss: 3.090458393096924\n",
      "Processing Batch 84/157\n",
      "Batch 84 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.015850171446800232, Std: 0.0841062068939209\n",
      "z_j Mean: 0.015945348888635635, Std: 0.08552776277065277\n",
      "Similarities: tensor([[1.4286, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.2867, 0.0000, 0.3879],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4130, 0.0000],\n",
      "        [0.0000, 0.0000, 0.3343, 0.0000, 1.3954]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0984926223754883\n",
      "Batch 84, Loss: 3.0984926223754883\n",
      "Processing Batch 85/157\n",
      "Batch 85 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.017522118985652924, Std: 0.08663943409919739\n",
      "z_j Mean: 0.016938764601945877, Std: 0.08316291123628616\n",
      "Similarities: tensor([[1.4105, 0.0000, 0.0000, 0.0000, 0.3327],\n",
      "        [0.0000, 1.4187, 0.0542, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3775, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5075, 0.0000, 0.0000, 0.0000, 1.2091]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.114687442779541\n",
      "Batch 85, Loss: 3.114687442779541\n",
      "Processing Batch 86/157\n",
      "Batch 86 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.014124669134616852, Std: 0.08221512287855148\n",
      "z_j Mean: 0.013503779657185078, Std: 0.08082272112369537\n",
      "Similarities: tensor([[1.3916, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.1323, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.2988, 0.5430, 0.0000],\n",
      "        [0.0000, 0.3380, 0.3894, 1.4041, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2179925441741943\n",
      "Batch 86, Loss: 3.2179925441741943\n",
      "Processing Batch 87/157\n",
      "Batch 87 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.016553662717342377, Std: 0.08250384777784348\n",
      "z_j Mean: 0.016997378319501877, Std: 0.0831509530544281\n",
      "Similarities: tensor([[1.3966, 0.4991, 0.9075, 1.1138, 0.0000],\n",
      "        [0.6813, 1.4220, 1.2738, 0.0000, 0.0667],\n",
      "        [1.0387, 1.2657, 1.4243, 0.2808, 0.0000],\n",
      "        [0.7927, 0.0000, 0.2669, 1.3074, 0.0000],\n",
      "        [0.0000, 0.1269, 0.0000, 0.0000, 1.3626]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285025596618652]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1998846530914307\n",
      "Batch 87, Loss: 3.1998846530914307\n",
      "Processing Batch 88/157\n",
      "Batch 88 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.014083793386816978, Std: 0.08222213387489319\n",
      "z_j Mean: 0.014056512154638767, Std: 0.08148105442523956\n",
      "Similarities: tensor([[1.4089, 0.0000, 0.3487, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4285, 0.0000, 0.1960, 0.0000],\n",
      "        [0.6020, 0.0000, 1.2479, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2625, 0.0000, 1.0306, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.200380325317383\n",
      "Batch 88, Loss: 3.200380325317383\n",
      "Processing Batch 89/157\n",
      "Batch 89 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.015496931038796902, Std: 0.08196751028299332\n",
      "z_j Mean: 0.01604391075670719, Std: 0.08260450512170792\n",
      "Similarities: tensor([[1.4182, 0.8676, 0.0000, 0.9041, 0.0000],\n",
      "        [0.8959, 1.4162, 0.0858, 0.1823, 0.0000],\n",
      "        [0.0000, 0.0444, 1.4062, 0.0000, 0.0000],\n",
      "        [1.0392, 0.3182, 0.0000, 1.3953, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.193021535873413\n",
      "Batch 89, Loss: 3.193021535873413\n",
      "Processing Batch 90/157\n",
      "Batch 90 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.015116529539227486, Std: 0.0820385292172432\n",
      "z_j Mean: 0.015218697488307953, Std: 0.08051738142967224\n",
      "Similarities: tensor([[1.4073, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3316, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3716, 0.6400, 0.1562],\n",
      "        [0.0000, 0.0000, 0.5826, 1.3980, 0.0000],\n",
      "        [0.0000, 0.0000, 0.1000, 0.0000, 1.4269]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.22273588180542\n",
      "Batch 90, Loss: 3.22273588180542\n",
      "Processing Batch 91/157\n",
      "Batch 91 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01573532447218895, Std: 0.08339902013540268\n",
      "z_j Mean: 0.015987679362297058, Std: 0.0833510085940361\n",
      "Similarities: tensor([[1.4286, 0.0414, 1.2651, 0.0000, 0.6310],\n",
      "        [0.0254, 1.4252, 0.0225, 0.0000, 0.0112],\n",
      "        [1.3438, 0.0389, 1.3929, 0.0000, 0.8251],\n",
      "        [0.0000, 0.0000, 0.0000, 1.2929, 0.0000],\n",
      "        [1.4286, 0.0414, 1.2651, 0.0000, 0.6310]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2194454669952393\n",
      "Batch 91, Loss: 3.2194454669952393\n",
      "Processing Batch 92/157\n",
      "Batch 92 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01633763313293457, Std: 0.08401286602020264\n",
      "z_j Mean: 0.016358699649572372, Std: 0.08400876820087433\n",
      "Similarities: tensor([[1.3042, 0.3020, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2924, 1.4141, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3563, 0.5092, 0.0000],\n",
      "        [0.0000, 0.0000, 0.3896, 1.3962, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.186552047729492\n",
      "Batch 92, Loss: 3.186552047729492\n",
      "Processing Batch 93/157\n",
      "Batch 93 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.016676217317581177, Std: 0.08467034250497818\n",
      "z_j Mean: 0.01639043167233467, Std: 0.08544357866048813\n",
      "Similarities: tensor([[1.2386, 0.0000, 0.0000, 0.8368, 0.0000],\n",
      "        [0.0000, 1.4286, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4281, 0.4094, 0.0000],\n",
      "        [0.4230, 0.0000, 0.3535, 1.4198, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3807]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.13377046585083\n",
      "Batch 93, Loss: 3.13377046585083\n",
      "Processing Batch 94/157\n",
      "Batch 94 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.014264724217355251, Std: 0.0829303041100502\n",
      "z_j Mean: 0.014095736667513847, Std: 0.08295919746160507\n",
      "Similarities: tensor([[1.2177, 0.4711, 0.0000, 0.0494, 0.0000],\n",
      "        [0.2962, 1.3533, 0.0000, 0.0000, 0.2143],\n",
      "        [0.0000, 0.0000, 1.4255, 0.0000, 0.7823],\n",
      "        [0.0000, 0.0000, 0.0000, 1.2864, 0.0000],\n",
      "        [0.0000, 0.2565, 0.8793, 0.0000, 1.3972]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.323565721511841\n",
      "Batch 94, Loss: 3.323565721511841\n",
      "Processing Batch 95/157\n",
      "Batch 95 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01637118309736252, Std: 0.08327651768922806\n",
      "z_j Mean: 0.016561120748519897, Std: 0.08323895931243896\n",
      "Similarities: tensor([[1.4188, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3698, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4107, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3131]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.171346426010132\n",
      "Batch 95, Loss: 3.171346426010132\n",
      "Processing Batch 96/157\n",
      "Batch 96 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012545396573841572, Std: 0.07945508509874344\n",
      "z_j Mean: 0.01271246001124382, Std: 0.08095097541809082\n",
      "Similarities: tensor([[1.4200, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.278799295425415\n",
      "Batch 96, Loss: 3.278799295425415\n",
      "Processing Batch 97/157\n",
      "Batch 97 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011825214140117168, Std: 0.07801598310470581\n",
      "z_j Mean: 0.01181099098175764, Std: 0.07879666239023209\n",
      "Similarities: tensor([[1.4223, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3891, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4286, 0.4387, 0.0000],\n",
      "        [0.0000, 0.0000, 0.4864, 1.4234, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.298234224319458\n",
      "Batch 97, Loss: 3.298234224319458\n",
      "Processing Batch 98/157\n",
      "Batch 98 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01480114832520485, Std: 0.08209601789712906\n",
      "z_j Mean: 0.015046175569295883, Std: 0.08279207348823547\n",
      "Similarities: tensor([[1.3545, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4286, 0.0000, 0.4509, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.4850, 0.0000, 1.4165, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.220066785812378\n",
      "Batch 98, Loss: 3.220066785812378\n",
      "Processing Batch 99/157\n",
      "Batch 99 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012696711346507072, Std: 0.07943104207515717\n",
      "z_j Mean: 0.012943177483975887, Std: 0.0786186009645462\n",
      "Similarities: tensor([[1.4200, 0.8527, 0.7056, 0.0000, 0.0000],\n",
      "        [0.8177, 1.4118, 0.0000, 0.0000, 0.0000],\n",
      "        [0.7934, 0.0000, 1.4286, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4104]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.3120620250701904\n",
      "Batch 99, Loss: 3.3120620250701904\n",
      "Processing Batch 100/157\n",
      "Batch 100 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01306464709341526, Std: 0.0801367312669754\n",
      "z_j Mean: 0.013465306721627712, Std: 0.0800703763961792\n",
      "Similarities: tensor([[1.3398, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3100, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3976, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4239]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2568063735961914\n",
      "Batch 100, Loss: 3.2568063735961914\n",
      "Processing Batch 101/157\n",
      "Batch 101 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.014316059648990631, Std: 0.08218200504779816\n",
      "z_j Mean: 0.014158129692077637, Std: 0.08294857293367386\n",
      "Similarities: tensor([[1.4004, 0.0000, 0.3171, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4160, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2304, 0.0030, 1.3462, 0.0000, 0.0000],\n",
      "        [0.6806, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3057]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1809136867523193\n",
      "Batch 101, Loss: 3.1809136867523193\n",
      "Processing Batch 102/157\n",
      "Batch 102 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011978273279964924, Std: 0.07877141237258911\n",
      "z_j Mean: 0.012712759897112846, Std: 0.08019331097602844\n",
      "Similarities: tensor([[1.4199, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3977, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4051, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4079, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.311335802078247\n",
      "Batch 102, Loss: 3.311335802078247\n",
      "Processing Batch 103/157\n",
      "Batch 103 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.014130281284451485, Std: 0.08221416175365448\n",
      "z_j Mean: 0.014101834036409855, Std: 0.08221904933452606\n",
      "Similarities: tensor([[1.4064, 0.0000, 0.2520, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4266, 0.1067, 0.5197, 0.7378],\n",
      "        [0.2940, 0.0896, 1.4176, 0.0000, 0.0017],\n",
      "        [0.0000, 0.4894, 0.0000, 1.4230, 1.0018],\n",
      "        [0.0000, 0.2882, 0.0000, 0.9151, 0.5899]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.23583722114563\n",
      "Batch 103, Loss: 3.23583722114563\n",
      "Processing Batch 104/157\n",
      "Batch 104 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012551184743642807, Std: 0.08247002959251404\n",
      "z_j Mean: 0.012892685830593109, Std: 0.08461011201143265\n",
      "Similarities: tensor([[1.3857, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4106, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4218, 0.1439, 1.0835],\n",
      "        [0.0000, 0.0000, 0.2817, 1.3872, 0.9874],\n",
      "        [0.0000, 0.0000, 1.0140, 0.8921, 1.4194]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2574243545532227\n",
      "Batch 104, Loss: 3.2574243545532227\n",
      "Processing Batch 105/157\n",
      "Batch 105 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01313130371272564, Std: 0.08311732858419418\n",
      "z_j Mean: 0.013214075937867165, Std: 0.08383551985025406\n",
      "Similarities: tensor([[1.4135, 0.0000, 0.0000, 0.6123, 0.0000],\n",
      "        [0.0000, 1.3837, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4811, 0.0000, 0.4039, 1.4205, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0552, 0.0000]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2503392696380615\n",
      "Batch 105, Loss: 3.2503392696380615\n",
      "Processing Batch 106/157\n",
      "Batch 106 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.014431623741984367, Std: 0.08436121791601181\n",
      "z_j Mean: 0.014639219269156456, Std: 0.08359841257333755\n",
      "Similarities: tensor([[1.3341, 0.0000, 0.0000, 1.2841, 0.9873],\n",
      "        [0.0000, 1.4128, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [1.2483, 0.0000, 0.0000, 1.3768, 0.7684],\n",
      "        [0.6830, 0.0000, 0.0000, 0.9151, 1.3714]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.242194652557373\n",
      "Batch 106, Loss: 3.242194652557373\n",
      "Processing Batch 107/157\n",
      "Batch 107 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01359580922871828, Std: 0.08304260671138763\n",
      "z_j Mean: 0.014018500223755836, Std: 0.08443085849285126\n",
      "Similarities: tensor([[1.3949, 0.8347, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0222, 0.0798, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3915, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4286, 1.4275]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2457706928253174\n",
      "Batch 107, Loss: 3.2457706928253174\n",
      "Processing Batch 108/157\n",
      "Batch 108 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01514308713376522, Std: 0.08567346632480621\n",
      "z_j Mean: 0.015361730009317398, Std: 0.0870485007762909\n",
      "Similarities: tensor([[1.4255, 0.3092, 0.3050, 0.0756, 0.7399],\n",
      "        [0.2913, 1.4212, 0.9899, 0.1879, 0.9805],\n",
      "        [0.3529, 1.0129, 1.4006, 0.9510, 0.9031],\n",
      "        [0.1001, 0.2593, 1.0375, 1.4220, 0.2462],\n",
      "        [0.5281, 1.2258, 0.9788, 0.2094, 1.3607]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2053887844085693\n",
      "Batch 108, Loss: 3.2053887844085693\n",
      "Processing Batch 109/157\n",
      "Batch 109 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01588064804673195, Std: 0.08625046163797379\n",
      "z_j Mean: 0.015677936375141144, Std: 0.0848608911037445\n",
      "Similarities: tensor([[1.4124, 0.0000, 1.2373, 0.1883, 0.0000],\n",
      "        [0.0000, 1.4286, 0.0000, 0.0000, 0.0000],\n",
      "        [1.2491, 0.0000, 1.4205, 0.4258, 0.0000],\n",
      "        [0.2308, 0.0000, 0.4383, 1.3790, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4286]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1586356163024902\n",
      "Batch 109, Loss: 3.1586356163024902\n",
      "Processing Batch 110/157\n",
      "Batch 110 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.017205756157636642, Std: 0.08670282363891602\n",
      "z_j Mean: 0.017173396423459053, Std: 0.08670923858880997\n",
      "Similarities: tensor([[1.3509, 0.0000, 0.0000, 0.0000, 0.9217],\n",
      "        [0.0000, 1.4182, 0.2884, 1.0340, 0.0000],\n",
      "        [0.0000, 0.3222, 1.4175, 0.0000, 0.0000],\n",
      "        [0.0000, 0.8927, 0.0000, 1.3420, 0.0000],\n",
      "        [0.6671, 0.0000, 0.0000, 0.0000, 1.3768]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.133399724960327\n",
      "Batch 110, Loss: 3.133399724960327\n",
      "Processing Batch 111/157\n",
      "Batch 111 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01593487523496151, Std: 0.08694539964199066\n",
      "z_j Mean: 0.015551210381090641, Std: 0.08560031652450562\n",
      "Similarities: tensor([[1.3624, 0.0147, 0.0000, 0.0609, 0.0000],\n",
      "        [0.0000, 1.4134, 0.0000, 1.1472, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3068, 0.0478, 0.0000],\n",
      "        [0.0000, 1.0890, 0.0000, 1.3517, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4286]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1409265995025635\n",
      "Batch 111, Loss: 3.1409265995025635\n",
      "Processing Batch 112/157\n",
      "Batch 112 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01599450409412384, Std: 0.08480178564786911\n",
      "z_j Mean: 0.016019411385059357, Std: 0.08551391959190369\n",
      "Similarities: tensor([[1.2779, 0.5212, 0.0000, 1.0445, 0.4011],\n",
      "        [0.2981, 1.3895, 0.4144, 0.1747, 0.0702],\n",
      "        [0.0000, 0.4066, 1.4189, 0.0000, 0.0000],\n",
      "        [1.1949, 0.3563, 0.0000, 1.3876, 1.0931],\n",
      "        [0.7130, 0.2337, 0.0000, 1.0581, 1.4189]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1656136512756348\n",
      "Batch 112, Loss: 3.1656136512756348\n",
      "Processing Batch 113/157\n",
      "Batch 113 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01589304208755493, Std: 0.08624818176031113\n",
      "z_j Mean: 0.016242356970906258, Std: 0.08618307113647461\n",
      "Similarities: tensor([[1.3058, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4054, 0.0000, 0.1441, 0.3544],\n",
      "        [0.0000, 0.0000, 1.3920, 0.1114, 0.0000],\n",
      "        [0.0000, 0.1684, 0.0947, 1.4109, 0.1933],\n",
      "        [0.0000, 0.5170, 0.0000, 0.1141, 1.2740]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1407666206359863\n",
      "Batch 113, Loss: 3.1407666206359863\n",
      "Processing Batch 114/157\n",
      "Batch 114 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.015295963734388351, Std: 0.08635608851909637\n",
      "z_j Mean: 0.0151816476136446, Std: 0.08637626469135284\n",
      "Similarities: tensor([[1.4035, 0.0000, 0.0593, 0.0000, 0.4153],\n",
      "        [0.0000, 1.2125, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3669, 0.0000, 1.2860, 0.0000, 1.3599],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4286, 0.0000],\n",
      "        [0.3072, 0.0000, 1.3436, 0.0000, 1.3576]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1586127281188965\n",
      "Batch 114, Loss: 3.1586127281188965\n",
      "Processing Batch 115/157\n",
      "Batch 115 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.015403389930725098, Std: 0.08418918401002884\n",
      "z_j Mean: 0.015492625534534454, Std: 0.08489492535591125\n",
      "Similarities: tensor([[1.3239, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.2190, 0.0459, 0.0000, 1.2446],\n",
      "        [0.0000, 0.1593, 1.3937, 0.0000, 0.0611],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.9132, 0.0230, 0.0000, 1.4169]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1458842754364014\n",
      "Batch 115, Loss: 3.1458842754364014\n",
      "Processing Batch 116/157\n",
      "Batch 116 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01790153980255127, Std: 0.08369354158639908\n",
      "z_j Mean: 0.017951592803001404, Std: 0.08512922376394272\n",
      "Similarities: tensor([[1.3908, 0.0079, 0.0000, 0.0076, 0.0742],\n",
      "        [0.1099, 1.3725, 0.0000, 1.3085, 1.2757],\n",
      "        [0.0000, 0.0000, 1.4280, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3954, 0.0000, 1.4269, 1.3894],\n",
      "        [0.1357, 1.3203, 0.0000, 1.3683, 1.3456]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.199967384338379\n",
      "Batch 116, Loss: 3.199967384338379\n",
      "Processing Batch 117/157\n",
      "Batch 117 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01749747432768345, Std: 0.0852237343788147\n",
      "z_j Mean: 0.017505822703242302, Std: 0.0845027044415474\n",
      "Similarities: tensor([[1.4040, 0.0000, 0.0081, 0.0000, 0.3877],\n",
      "        [0.0000, 1.4286, 0.0000, 0.0000, 0.1102],\n",
      "        [0.0000, 0.0000, 0.9325, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3346, 0.0000, 0.0000, 0.0000, 1.3790]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1755666732788086\n",
      "Batch 117, Loss: 3.1755666732788086\n",
      "Processing Batch 118/157\n",
      "Batch 118 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.017474917694926262, Std: 0.0859416052699089\n",
      "z_j Mean: 0.017787765711545944, Std: 0.08587738126516342\n",
      "Similarities: tensor([[1.3954, 0.0000, 0.0762, 0.0000, 0.8547],\n",
      "        [0.0000, 1.4057, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1444, 0.0000, 1.3791, 0.0000, 0.2332],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4218, 0.0210],\n",
      "        [0.9886, 0.0000, 0.1941, 0.0379, 1.4105]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.145314931869507\n",
      "Batch 118, Loss: 3.145314931869507\n",
      "Processing Batch 119/157\n",
      "Batch 119 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.015169748105108738, Std: 0.08708216995000839\n",
      "z_j Mean: 0.015100385993719101, Std: 0.08639050275087357\n",
      "Similarities: tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 1.3952e+00, 4.3650e-02, 5.7768e-01, 2.1604e-01],\n",
      "        [0.0000e+00, 5.1415e-02, 1.1772e+00, 0.0000e+00, 1.1851e+00],\n",
      "        [0.0000e+00, 8.4076e-01, 0.0000e+00, 1.3860e+00, 3.4343e-04],\n",
      "        [0.0000e+00, 5.9333e-01, 5.2034e-01, 3.6828e-02, 1.0097e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.166872262954712\n",
      "Batch 119, Loss: 3.166872262954712\n",
      "Processing Batch 120/157\n",
      "Batch 120 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.0164167582988739, Std: 0.08472104370594025\n",
      "z_j Mean: 0.016815364360809326, Std: 0.08536095172166824\n",
      "Similarities: tensor([[1.4232, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4211, 0.0000, 0.0897, 0.0865],\n",
      "        [0.0048, 0.0000, 1.3424, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0958, 0.0000, 1.4269, 1.0764],\n",
      "        [0.0000, 0.0755, 0.0000, 1.0834, 1.4286]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1668872833251953\n",
      "Batch 120, Loss: 3.1668872833251953\n",
      "Processing Batch 121/157\n",
      "Batch 121 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.016654042527079582, Std: 0.08539257943630219\n",
      "z_j Mean: 0.01718788966536522, Std: 0.08599946647882462\n",
      "Similarities: tensor([[1.4172, 0.0000, 1.4185, 0.0000, 0.0000],\n",
      "        [0.0000, 1.2936, 0.0000, 0.0418, 0.0000],\n",
      "        [1.4245, 0.0000, 1.4245, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0562, 0.0000, 1.4163, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4286]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.13366961479187\n",
      "Batch 121, Loss: 3.13366961479187\n",
      "Processing Batch 122/157\n",
      "Batch 122 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013263213448226452, Std: 0.08382776379585266\n",
      "z_j Mean: 0.013661891222000122, Std: 0.08448930829763412\n",
      "Similarities: tensor([[1.4101, 0.0000, 0.0000, 0.0000, 0.2607],\n",
      "        [0.0000, 1.4137, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4246, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4286, 0.0000],\n",
      "        [0.3129, 0.0000, 0.0000, 0.0000, 1.4078]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1694040298461914\n",
      "Batch 122, Loss: 3.1694040298461914\n",
      "Processing Batch 123/157\n",
      "Batch 123 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01503424160182476, Std: 0.0842558965086937\n",
      "z_j Mean: 0.015022050589323044, Std: 0.084258072078228\n",
      "Similarities: tensor([[0.9832, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4278, 0.0000, 1.2533, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.2126, 0.0000, 1.3975, 0.0000],\n",
      "        [0.0604, 0.0000, 0.0000, 0.0000, 1.4033]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2287676334381104\n",
      "Batch 123, Loss: 3.2287676334381104\n",
      "Processing Batch 124/157\n",
      "Batch 124 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.015389654785394669, Std: 0.08346351236104965\n",
      "z_j Mean: 0.01485268585383892, Std: 0.08133967220783234\n",
      "Similarities: tensor([[1.4005, 0.0000, 0.4402, 0.0000, 0.8679],\n",
      "        [0.0000, 1.4283, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5457, 0.0000, 1.4087, 0.0000, 0.2336],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.9160, 0.0000, 0.1735, 0.0000, 1.2319]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2473790645599365\n",
      "Batch 124, Loss: 3.2473790645599365\n",
      "Processing Batch 125/157\n",
      "Batch 125 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01586099900305271, Std: 0.08337520807981491\n",
      "z_j Mean: 0.015734795480966568, Std: 0.08192217350006104\n",
      "Similarities: tensor([[1.3871, 0.0000, 0.0732, 0.0499, 0.0000],\n",
      "        [0.0000, 1.4219, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1696, 0.0000, 1.3493, 0.4639, 0.0000],\n",
      "        [0.0097, 0.0000, 0.1972, 1.3692, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.219940185546875\n",
      "Batch 125, Loss: 3.219940185546875\n",
      "Processing Batch 126/157\n",
      "Batch 126 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013664226979017258, Std: 0.08229290693998337\n",
      "z_j Mean: 0.014102966524660587, Std: 0.08147303014993668\n",
      "Similarities: tensor([[1.2201, 0.0000, 0.0000, 0.0000, 1.0484],\n",
      "        [0.0000, 1.0723, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0283, 0.0000, 1.3973, 0.0000, 0.0000],\n",
      "        [0.0000, 0.6150, 0.0000, 0.0000, 0.0000],\n",
      "        [0.7741, 0.0000, 0.0000, 0.0000, 1.4270]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2301177978515625\n",
      "Batch 126, Loss: 3.2301177978515625\n",
      "Processing Batch 127/157\n",
      "Batch 127 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.016565965488553047, Std: 0.08323799073696136\n",
      "z_j Mean: 0.016658999025821686, Std: 0.08467373251914978\n",
      "Similarities: tensor([[0.7935, 0.0000, 0.1088, 0.0205, 0.3172],\n",
      "        [0.0000, 1.4193, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1868, 0.0000, 1.4244, 0.4892, 0.4737],\n",
      "        [0.5740, 0.0000, 0.5628, 1.3668, 0.5199],\n",
      "        [0.3320, 0.0000, 0.4804, 0.2320, 1.3637]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1737849712371826\n",
      "Batch 127, Loss: 3.1737849712371826\n",
      "Processing Batch 128/157\n",
      "Batch 128 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.014909650199115276, Std: 0.08427803963422775\n",
      "z_j Mean: 0.015154086984694004, Std: 0.08423442393541336\n",
      "Similarities: tensor([[1.2860, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4743, 0.5475, 0.0177, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1871, 1.4177, 0.0000, 1.2511],\n",
      "        [0.0068, 0.0000, 0.0000, 1.4248, 0.0000],\n",
      "        [0.0000, 0.0000, 1.2888, 0.0000, 1.4162]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.218886137008667\n",
      "Batch 128, Loss: 3.218886137008667\n",
      "Processing Batch 129/157\n",
      "Batch 129 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.018744342029094696, Std: 0.08567366003990173\n",
      "z_j Mean: 0.01832452230155468, Std: 0.08576443791389465\n",
      "Similarities: tensor([[1.4007, 0.0856, 0.0000, 0.0000, 0.1564],\n",
      "        [0.0709, 1.4071, 0.0000, 0.0000, 1.1123],\n",
      "        [0.0000, 0.0000, 1.2632, 0.1895, 0.0000],\n",
      "        [0.0000, 0.0000, 0.1624, 1.4038, 0.0000],\n",
      "        [0.3603, 0.9103, 0.0000, 0.0000, 1.3261]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1386938095092773\n",
      "Batch 129, Loss: 3.1386938095092773\n",
      "Processing Batch 130/157\n",
      "Batch 130 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.016130734235048294, Std: 0.0825875923037529\n",
      "z_j Mean: 0.01634395867586136, Std: 0.08254565298557281\n",
      "Similarities: tensor([[1.4121, 0.0000, 1.3765, 0.8404, 0.6946],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [1.3682, 0.0000, 1.3985, 0.9699, 0.3960],\n",
      "        [0.9022, 0.0000, 0.9620, 1.4187, 0.0422],\n",
      "        [0.5755, 0.0000, 0.4338, 0.0581, 1.4067]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2965497970581055\n",
      "Batch 130, Loss: 3.2965497970581055\n",
      "Processing Batch 131/157\n",
      "Batch 131 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01478253398090601, Std: 0.0843004360795021\n",
      "z_j Mean: 0.015347545966506004, Std: 0.08419938385486603\n",
      "Similarities: tensor([[1.4186, 0.1814, 0.4386, 0.0000, 0.7585],\n",
      "        [0.1946, 1.3925, 0.0383, 0.0000, 0.3174],\n",
      "        [0.6203, 0.0123, 1.3994, 0.0000, 0.6949],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4177, 0.0000],\n",
      "        [0.6769, 0.0908, 0.5158, 0.0000, 1.2543]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.263179302215576\n",
      "Batch 131, Loss: 3.263179302215576\n",
      "Processing Batch 132/157\n",
      "Batch 132 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.016805116087198257, Std: 0.0846448540687561\n",
      "z_j Mean: 0.016413282603025436, Std: 0.08326823264360428\n",
      "Similarities: tensor([[1.2940e+00, 0.0000e+00, 8.4574e-01, 5.5917e-02, 1.5295e-05],\n",
      "        [0.0000e+00, 1.1396e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [1.1026e+00, 0.0000e+00, 1.3874e+00, 7.4275e-02, 2.0316e-05],\n",
      "        [2.6957e-01, 0.0000e+00, 2.0879e-01, 1.3228e+00, 1.0327e-05],\n",
      "        [1.1091e-01, 0.0000e+00, 8.5904e-02, 1.5534e-02, 1.4020e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2648000717163086\n",
      "Batch 132, Loss: 3.2648000717163086\n",
      "Processing Batch 133/157\n",
      "Batch 133 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.014084378257393837, Std: 0.08296112716197968\n",
      "z_j Mean: 0.014257499016821384, Std: 0.08293154835700989\n",
      "Similarities: tensor([[1.4206e+00, 1.3257e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [9.8654e-02, 1.1766e+00, 1.3149e-01, 9.5024e-02, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 1.3429e+00, 1.3105e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 1.3199e+00, 1.4234e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.4138e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2538654804229736\n",
      "Batch 133, Loss: 3.2538654804229736\n",
      "Processing Batch 134/157\n",
      "Batch 134 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.014299905858933926, Std: 0.08365713059902191\n",
      "z_j Mean: 0.014680350199341774, Std: 0.08285772800445557\n",
      "Similarities: tensor([[1.4112, 1.2464, 0.0000, 0.0336, 1.0809],\n",
      "        [0.7739, 1.2362, 0.0000, 0.1360, 0.4622],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0514, 0.0686, 0.0000, 1.3247, 0.0000],\n",
      "        [1.1929, 0.8655, 0.0000, 0.0000, 1.3742]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.3301198482513428\n",
      "Batch 134, Loss: 3.3301198482513428\n",
      "Processing Batch 135/157\n",
      "Batch 135 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01580413617193699, Std: 0.08555397391319275\n",
      "z_j Mean: 0.01618463173508644, Std: 0.08689925074577332\n",
      "Similarities: tensor([[1.4285, 1.4035, 0.0000, 0.4386, 1.1228],\n",
      "        [1.3675, 1.3941, 0.0000, 0.7659, 1.2349],\n",
      "        [0.0000, 0.0000, 1.2509, 0.0000, 0.0000],\n",
      "        [0.5873, 0.7039, 0.0000, 1.3961, 1.0205],\n",
      "        [1.1083, 1.0862, 0.0000, 0.9117, 1.4271]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.267493963241577\n",
      "Batch 135, Loss: 3.267493963241577\n",
      "Processing Batch 136/157\n",
      "Batch 136 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.014710760675370693, Std: 0.08136546611785889\n",
      "z_j Mean: 0.014949161559343338, Std: 0.08280964940786362\n",
      "Similarities: tensor([[1.4201, 0.7129, 0.2306, 0.4852, 1.0807],\n",
      "        [0.6635, 1.4043, 0.0473, 1.2320, 0.5470],\n",
      "        [0.5596, 0.1901, 1.3090, 0.0000, 0.4209],\n",
      "        [0.3952, 1.1162, 0.0000, 1.3606, 0.3039],\n",
      "        [1.1394, 0.5649, 0.2913, 0.3197, 1.4101]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.285146713256836\n",
      "Batch 136, Loss: 3.285146713256836\n",
      "Processing Batch 137/157\n",
      "Batch 137 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.014973210170865059, Std: 0.08280530571937561\n",
      "z_j Mean: 0.015323448926210403, Std: 0.08347569406032562\n",
      "Similarities: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4234, 0.1625, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1395, 1.4183, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3811, 1.3695],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4190, 1.4227]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.269658327102661\n",
      "Batch 137, Loss: 3.269658327102661\n",
      "Processing Batch 138/157\n",
      "Batch 138 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.015897877514362335, Std: 0.08263273537158966\n",
      "z_j Mean: 0.016406066715717316, Std: 0.08399952948093414\n",
      "Similarities: tensor([[1.4215, 0.2486, 0.0000, 0.0000, 0.4528],\n",
      "        [0.2888, 1.1895, 0.1921, 0.5941, 0.0000],\n",
      "        [0.0000, 0.4556, 1.4079, 0.1389, 0.0000],\n",
      "        [0.0000, 0.1046, 0.4883, 1.3616, 0.0000],\n",
      "        [0.5452, 0.0015, 0.0000, 0.0000, 1.4226]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.3238842487335205\n",
      "Batch 138, Loss: 3.3238842487335205\n",
      "Processing Batch 139/157\n",
      "Batch 139 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.016885388642549515, Std: 0.08317376673221588\n",
      "z_j Mean: 0.016860712319612503, Std: 0.08390944451093674\n",
      "Similarities: tensor([[1.3447, 0.8391, 0.6784, 1.2075, 0.0000],\n",
      "        [1.0635, 1.4131, 0.4801, 0.5909, 0.0000],\n",
      "        [0.5284, 0.4665, 1.3784, 0.3671, 0.0700],\n",
      "        [0.9651, 0.4844, 0.6211, 1.3523, 0.0000],\n",
      "        [0.0000, 0.0000, 0.1031, 0.0000, 1.4286]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2534916400909424\n",
      "Batch 139, Loss: 3.2534916400909424\n",
      "Processing Batch 140/157\n",
      "Batch 140 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.015556953847408295, Std: 0.08488316088914871\n",
      "z_j Mean: 0.015587439760565758, Std: 0.08487756550312042\n",
      "Similarities: tensor([[1.3175, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4117, 0.2107, 0.2543, 0.0000],\n",
      "        [0.0000, 0.2081, 1.4267, 1.4128, 0.0000],\n",
      "        [0.0000, 0.2375, 1.3876, 1.4241, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.0736]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2693841457366943\n",
      "Batch 140, Loss: 3.2693841457366943\n",
      "Processing Batch 141/157\n",
      "Batch 141 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.016865324229002, Std: 0.08463288098573685\n",
      "z_j Mean: 0.016945049166679382, Std: 0.08461695164442062\n",
      "Similarities: tensor([[1.4150, 0.0000, 0.0000, 1.3992, 0.0000],\n",
      "        [0.0000, 1.2586, 0.2922, 0.0000, 0.0238],\n",
      "        [0.0000, 0.3883, 1.3865, 0.0000, 0.1836],\n",
      "        [1.3986, 0.0000, 0.0000, 1.4034, 0.0000],\n",
      "        [0.0000, 0.3606, 0.4293, 0.0000, 1.0264]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2273991107940674\n",
      "Batch 141, Loss: 3.2273991107940674\n",
      "Processing Batch 142/157\n",
      "Batch 142 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.015861429274082184, Std: 0.08482678234577179\n",
      "z_j Mean: 0.01598428562283516, Std: 0.08408082276582718\n",
      "Similarities: tensor([[1.3222, 0.0000, 1.0595, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3897, 0.0000, 1.1920, 1.3285],\n",
      "        [1.1195, 0.0000, 1.3765, 0.0000, 0.0000],\n",
      "        [0.0000, 1.0766, 0.0000, 1.3428, 0.9732],\n",
      "        [0.0000, 1.3349, 0.0000, 1.0206, 1.4219]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.181149959564209\n",
      "Batch 142, Loss: 3.181149959564209\n",
      "Processing Batch 143/157\n",
      "Batch 143 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01562533527612686, Std: 0.08700156956911087\n",
      "z_j Mean: 0.0156183410435915, Std: 0.08629835397005081\n",
      "Similarities: tensor([[1.3990e+00, 7.3179e-01, 0.0000e+00, 2.0092e-04, 1.0526e+00],\n",
      "        [6.5014e-01, 1.3680e+00, 0.0000e+00, 0.0000e+00, 9.2163e-01],\n",
      "        [0.0000e+00, 0.0000e+00, 1.4286e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 1.4108e+00, 0.0000e+00],\n",
      "        [1.0259e+00, 1.0452e+00, 0.0000e+00, 9.2744e-05, 1.4158e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1703739166259766\n",
      "Batch 143, Loss: 3.1703739166259766\n",
      "Processing Batch 144/157\n",
      "Batch 144 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012785874307155609, Std: 0.08462631702423096\n",
      "z_j Mean: 0.013027044013142586, Std: 0.08386479318141937\n",
      "Similarities: tensor([[1.3024, 0.0000, 0.0000, 0.1112, 0.0000],\n",
      "        [0.0000, 1.1693, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4253, 0.0000, 0.0000],\n",
      "        [0.1215, 0.0000, 0.0000, 1.3090, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4286]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1929666996002197\n",
      "Batch 144, Loss: 3.1929666996002197\n",
      "Processing Batch 145/157\n",
      "Batch 145 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.010977040976285934, Std: 0.08342920988798141\n",
      "z_j Mean: 0.011276746168732643, Std: 0.08555709570646286\n",
      "Similarities: tensor([[1.4286, 0.0000, 0.0000, 1.4286, 0.0000],\n",
      "        [0.0000, 1.4286, 0.8097, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.1769, 0.0000, 0.0000],\n",
      "        [1.4286, 0.0000, 0.0000, 1.4286, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4206]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2252280712127686\n",
      "Batch 145, Loss: 3.2252280712127686\n",
      "Processing Batch 146/157\n",
      "Batch 146 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011051474139094353, Std: 0.08414795249700546\n",
      "z_j Mean: 0.010971367359161377, Std: 0.08342995494604111\n",
      "Similarities: tensor([[1.4243, 0.0103, 0.0000, 0.0000, 1.1299],\n",
      "        [0.0000, 1.4125, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4286, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.2179, 0.0000],\n",
      "        [1.1942, 0.0000, 0.0000, 0.0000, 1.4286]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2042672634124756\n",
      "Batch 146, Loss: 3.2042672634124756\n",
      "Processing Batch 147/157\n",
      "Batch 147 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012627789750695229, Std: 0.08608020842075348\n",
      "z_j Mean: 0.012245794758200645, Std: 0.08542376756668091\n",
      "Similarities: tensor([[1.3651, 0.0000, 0.0000, 1.2826, 0.0000],\n",
      "        [0.0000, 1.4153, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3792, 0.0000, 0.0000],\n",
      "        [1.4078, 0.0000, 0.0000, 1.4237, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.2242]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.19966459274292\n",
      "Batch 147, Loss: 3.19966459274292\n",
      "Processing Batch 148/157\n",
      "Batch 148 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011608939617872238, Std: 0.08551265299320221\n",
      "z_j Mean: 0.012042181566357613, Std: 0.08616409450769424\n",
      "Similarities: tensor([[1.4286, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.2545, 0.8785, 0.8785, 1.3550],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.0044, 1.4286, 1.4286, 0.4763],\n",
      "        [0.0000, 1.0044, 1.4286, 1.4286, 0.4763]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.192639112472534\n",
      "Batch 148, Loss: 3.192639112472534\n",
      "Processing Batch 149/157\n",
      "Batch 149 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.010761951096355915, Std: 0.08490747213363647\n",
      "z_j Mean: 0.010699506849050522, Std: 0.08419343829154968\n",
      "Similarities: tensor([[1.3777, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3168, 0.5540, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4286, 0.0000, 0.0000],\n",
      "        [0.0140, 0.0000, 0.0000, 1.4277, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3919]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.268401622772217\n",
      "Batch 149, Loss: 3.268401622772217\n",
      "Processing Batch 150/157\n",
      "Batch 150 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011195910163223743, Std: 0.08556771278381348\n",
      "z_j Mean: 0.011008710600435734, Std: 0.08487582951784134\n",
      "Similarities: tensor([[1.3499, 0.0000, 0.0000, 0.0000, 0.7006],\n",
      "        [0.0000, 1.4279, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4286, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3700, 0.0000],\n",
      "        [0.6812, 0.0000, 0.0000, 0.0000, 1.4146]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.234297752380371\n",
      "Batch 150, Loss: 3.234297752380371\n",
      "Processing Batch 151/157\n",
      "Batch 151 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011621654033660889, Std: 0.08622182905673981\n",
      "z_j Mean: 0.01155148260295391, Std: 0.08623126149177551\n",
      "Similarities: tensor([[1.4128, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.2854, 0.0000, 0.0000, 1.1634],\n",
      "        [0.0000, 0.0000, 1.3928, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0592, 0.0000],\n",
      "        [0.0000, 1.4085, 0.0000, 0.0000, 1.4286]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1991376876831055\n",
      "Batch 151, Loss: 3.1991376876831055\n",
      "Processing Batch 152/157\n",
      "Batch 152 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012410715222358704, Std: 0.08681776374578476\n",
      "z_j Mean: 0.012752732262015343, Std: 0.08606179058551788\n",
      "Similarities: tensor([[1.3745, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4286, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4286, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4286, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4005]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1982064247131348\n",
      "Batch 152, Loss: 3.1982064247131348\n",
      "Processing Batch 153/157\n",
      "Batch 153 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011819431558251381, Std: 0.08619493991136551\n",
      "z_j Mean: 0.011850854381918907, Std: 0.0847623348236084\n",
      "Similarities: tensor([[1.3829, 0.0000, 0.0000, 0.0000, 1.1734],\n",
      "        [0.0000, 1.4258, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4212, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4286, 0.0000],\n",
      "        [1.3914, 0.0000, 0.0000, 0.0000, 1.1931]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2622716426849365\n",
      "Batch 153, Loss: 3.2622716426849365\n",
      "Processing Batch 154/157\n",
      "Batch 154 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01338166557252407, Std: 0.08525319397449493\n",
      "z_j Mean: 0.01345529779791832, Std: 0.08595473319292068\n",
      "Similarities: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4285, 0.0000, 1.3827, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4010, 0.0000, 1.3789],\n",
      "        [0.0000, 1.3853, 0.0000, 1.4286, 0.0000],\n",
      "        [0.0000, 0.0000, 1.2316, 0.0000, 1.4120]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.219459056854248\n",
      "Batch 154, Loss: 3.219459056854248\n",
      "Processing Batch 155/157\n",
      "Batch 155 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013094160705804825, Std: 0.08741839230060577\n",
      "z_j Mean: 0.013131842017173767, Std: 0.08671160042285919\n",
      "Similarities: tensor([[1.4284, 0.0000, 0.2300, 0.0000, 1.4286],\n",
      "        [0.0000, 1.4034, 0.9644, 1.3025, 0.0000],\n",
      "        [0.6278, 0.2982, 1.1650, 0.3501, 0.6110],\n",
      "        [0.0000, 1.2215, 1.0484, 1.4282, 0.0000],\n",
      "        [1.4284, 0.0000, 0.2300, 0.0000, 1.4286]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2288408279418945\n",
      "Batch 155, Loss: 3.2288408279418945\n",
      "Processing Batch 156/157\n",
      "Batch 156 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013401959091424942, Std: 0.08453092724084854\n",
      "z_j Mean: 0.01332630030810833, Std: 0.08381775766611099\n",
      "Similarities: tensor([[1.4261, 0.7075, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5884, 1.4041, 0.0482, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0288, 1.3732, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.1671, 1.4043],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4286, 1.2984]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2475061416625977\n",
      "Batch 156, Loss: 3.2475061416625977\n",
      "Processing Batch 157/157\n",
      "Batch 157 loaded: x.shape=torch.Size([16, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([16, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([16, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([16, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([16, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([16, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.015246061608195305, Std: 0.08708479255437851\n",
      "z_j Mean: 0.015261603519320488, Std: 0.08708206564188004\n",
      "Similarities: tensor([[1.4278, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4208, 1.3221, 0.9869, 0.0000],\n",
      "        [0.0000, 1.3199, 1.4279, 1.1903, 0.0000],\n",
      "        [0.0000, 0.9836, 1.1243, 1.4241, 0.0000],\n",
      "        [0.3958, 0.0000, 0.0000, 0.0000, 1.3400]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 1.8890923261642456\n",
      "Batch 157, Loss: 1.8890923261642456\n",
      "Epoch 1/10, Loss: 3.3693\n",
      "Entered Epoch 2\n",
      "Starting Training Loop with 157 batches.\n",
      "Processing Batch 1/157\n",
      "Batch 1 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.014944138005375862, Std: 0.08570839464664459\n",
      "z_j Mean: 0.014940867200493813, Std: 0.08570896089076996\n",
      "Similarities: tensor([[1.4247, 0.0000, 0.0000, 0.9661, 0.0000],\n",
      "        [0.0000, 0.5841, 0.2182, 0.0000, 0.2684],\n",
      "        [0.0000, 0.0000, 1.4247, 0.0000, 0.0000],\n",
      "        [0.9373, 0.0000, 0.0000, 1.4278, 0.0000],\n",
      "        [0.0000, 0.8839, 0.0000, 0.0000, 1.1952]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1275601387023926\n",
      "Batch 1, Loss: 3.1275601387023926\n",
      "Processing Batch 2/157\n",
      "Batch 2 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012506761588156223, Std: 0.08538594841957092\n",
      "z_j Mean: 0.01208433136343956, Std: 0.08400583267211914\n",
      "Similarities: tensor([[1.3652, 0.0199, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0324, 1.4214, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4203, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4201]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.260160207748413\n",
      "Batch 2, Loss: 3.260160207748413\n",
      "Processing Batch 3/157\n",
      "Batch 3 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.014368731528520584, Std: 0.08651521801948547\n",
      "z_j Mean: 0.014464084059000015, Std: 0.08720216900110245\n",
      "Similarities: tensor([[1.3964e+00, 0.0000e+00, 5.4760e-02, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 1.4211e+00, 0.0000e+00, 1.1225e+00, 0.0000e+00],\n",
      "        [1.9980e-02, 0.0000e+00, 1.4057e+00, 2.4266e-02, 1.3024e-03],\n",
      "        [0.0000e+00, 1.0237e+00, 0.0000e+00, 1.4223e+00, 4.1569e-01],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 3.6062e-01, 1.4255e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1913387775421143\n",
      "Batch 3, Loss: 3.1913387775421143\n",
      "Processing Batch 4/157\n",
      "Batch 4 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.014972534961998463, Std: 0.0871162936091423\n",
      "z_j Mean: 0.015007670037448406, Std: 0.0871102511882782\n",
      "Similarities: tensor([[1.4261, 0.4410, 0.0000, 0.0000, 0.2843],\n",
      "        [0.8468, 1.3560, 0.0000, 0.0000, 0.1711],\n",
      "        [0.0000, 0.0000, 1.3119, 0.2439, 0.0000],\n",
      "        [0.0000, 0.0000, 0.4166, 1.1520, 0.0000],\n",
      "        [0.2976, 0.0932, 0.0000, 0.0000, 1.4284]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.144601583480835\n",
      "Batch 4, Loss: 3.144601583480835\n",
      "Processing Batch 5/157\n",
      "Batch 5 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.016097985208034515, Std: 0.08691534399986267\n",
      "z_j Mean: 0.01610613614320755, Std: 0.0869138315320015\n",
      "Similarities: tensor([[1.4222, 1.2921, 0.0000, 0.0000, 1.0532],\n",
      "        [1.3756, 1.4120, 0.0000, 0.0000, 1.1500],\n",
      "        [0.0157, 0.0000, 1.2800, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4193, 0.0000],\n",
      "        [1.0918, 1.2462, 0.0000, 0.0000, 1.3871]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1457409858703613\n",
      "Batch 5, Loss: 3.1457409858703613\n",
      "Processing Batch 6/157\n",
      "Batch 6 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.015815136954188347, Std: 0.08696726709604263\n",
      "z_j Mean: 0.014942575246095657, Std: 0.08712144941091537\n",
      "Similarities: tensor([[1.1303, 0.0000, 0.9526, 0.7210, 0.0000],\n",
      "        [0.0000, 1.3748, 0.0000, 0.0000, 0.6854],\n",
      "        [1.0909, 0.0000, 1.3396, 1.1866, 0.0000],\n",
      "        [0.5903, 0.0000, 1.3257, 1.3921, 0.0000],\n",
      "        [0.0000, 0.9098, 0.0000, 0.0000, 1.2968]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1810977458953857\n",
      "Batch 6, Loss: 3.1810977458953857\n",
      "Processing Batch 7/157\n",
      "Batch 7 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013823985122144222, Std: 0.0873059406876564\n",
      "z_j Mean: 0.01418672688305378, Std: 0.08724772185087204\n",
      "Similarities: tensor([[1.4227, 0.2369, 0.0000, 0.0000, 0.0825],\n",
      "        [0.2088, 1.4285, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4150, 0.0000, 0.2869],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3335, 0.0000],\n",
      "        [0.0749, 0.0000, 0.3365, 0.0000, 1.4220]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1676950454711914\n",
      "Batch 7, Loss: 3.1676950454711914\n",
      "Processing Batch 8/157\n",
      "Batch 8 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011774308979511261, Std: 0.08690637350082397\n",
      "z_j Mean: 0.011878719553351402, Std: 0.08689215779304504\n",
      "Similarities: tensor([[1.4186, 0.0000, 0.0000, 0.2953, 0.0000],\n",
      "        [0.0000, 1.3923, 0.0386, 0.0000, 0.2572],\n",
      "        [0.0000, 0.0206, 1.4069, 0.0000, 0.1055],\n",
      "        [0.2005, 0.0000, 0.0000, 1.4286, 0.0000],\n",
      "        [0.0000, 0.2412, 0.1905, 0.0000, 1.3943]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.246958017349243\n",
      "Batch 8, Loss: 3.246958017349243\n",
      "Processing Batch 9/157\n",
      "Batch 9 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.014546561054885387, Std: 0.08718845248222351\n",
      "z_j Mean: 0.015095165930688381, Std: 0.08709513396024704\n",
      "Similarities: tensor([[1.3821, 0.0000, 0.1181, 0.8126, 0.0000],\n",
      "        [0.0000, 1.4160, 0.0000, 0.0000, 1.1369],\n",
      "        [0.5005, 0.0000, 1.1309, 1.0456, 0.0000],\n",
      "        [1.2625, 0.0000, 0.5579, 1.1330, 0.0000],\n",
      "        [0.0000, 0.8756, 0.0000, 0.0000, 1.3263]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.126657009124756\n",
      "Batch 9, Loss: 3.126657009124756\n",
      "Processing Batch 10/157\n",
      "Batch 10 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012744400650262833, Std: 0.08747007697820663\n",
      "z_j Mean: 0.0127762071788311, Std: 0.08746543526649475\n",
      "Similarities: tensor([[1.4012, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2665, 1.2446, 0.0267, 0.6647, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4114, 0.0000, 0.0000],\n",
      "        [0.0000, 0.5439, 0.0000, 1.4286, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4283]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.204501152038574\n",
      "Batch 10, Loss: 3.204501152038574\n",
      "Processing Batch 11/157\n",
      "Batch 11 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013993533328175545, Std: 0.08657670021057129\n",
      "z_j Mean: 0.013616083189845085, Std: 0.08592940866947174\n",
      "Similarities: tensor([[0.0000, 1.1031, 1.0379, 0.0000, 0.1286],\n",
      "        [0.0000, 1.3589, 0.8975, 0.0000, 0.1871],\n",
      "        [0.0000, 0.6564, 1.4192, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.1404, 0.0000],\n",
      "        [0.0000, 0.2601, 0.0000, 0.0000, 1.4269]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1653668880462646\n",
      "Batch 11, Loss: 3.1653668880462646\n",
      "Processing Batch 12/157\n",
      "Batch 12 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.014516773633658886, Std: 0.08719341456890106\n",
      "z_j Mean: 0.014650123193860054, Std: 0.08717110753059387\n",
      "Similarities: tensor([[1.4035, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.2931, 0.0000, 0.8396, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4286, 0.0000, 0.0000],\n",
      "        [0.0000, 0.9011, 0.0000, 1.4025, 0.2970],\n",
      "        [0.0000, 0.0000, 0.0000, 0.1453, 1.4108]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1812684535980225\n",
      "Batch 12, Loss: 3.1812684535980225\n",
      "Processing Batch 13/157\n",
      "Batch 13 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01381186954677105, Std: 0.08730786293745041\n",
      "z_j Mean: 0.013923663645982742, Std: 0.08729010075330734\n",
      "Similarities: tensor([[1.4220e+00, 1.0807e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [1.1041e+00, 1.4105e+00, 1.4126e-01, 0.0000e+00, 6.1746e-02],\n",
      "        [0.0000e+00, 6.4796e-02, 1.2234e+00, 0.0000e+00, 8.6737e-02],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 1.3948e+00, 5.0310e-01],\n",
      "        [0.0000e+00, 5.1520e-03, 1.5777e-02, 5.6170e-04, 1.0143e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2032976150512695\n",
      "Batch 13, Loss: 3.2032976150512695\n",
      "Processing Batch 14/157\n",
      "Batch 14 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.014622556045651436, Std: 0.0857638493180275\n",
      "z_j Mean: 0.014719254337251186, Std: 0.08645626157522202\n",
      "Similarities: tensor([[1.3015, 0.4748, 0.6396, 0.0000, 0.1311],\n",
      "        [0.5629, 1.3279, 0.0368, 0.0000, 0.0456],\n",
      "        [0.6776, 0.0388, 1.4193, 0.0000, 0.0319],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4286, 0.0000],\n",
      "        [0.1837, 0.0574, 0.0489, 0.0000, 1.3961]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1900718212127686\n",
      "Batch 14, Loss: 3.1900718212127686\n",
      "Processing Batch 15/157\n",
      "Batch 15 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013326255604624748, Std: 0.08738330751657486\n",
      "z_j Mean: 0.013761267997324467, Std: 0.08731585741043091\n",
      "Similarities: tensor([[1.4230, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.2766, 0.0000, 0.2773, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3799, 0.0000, 0.7249],\n",
      "        [0.0000, 0.4379, 0.0000, 0.9679, 0.0000],\n",
      "        [0.0000, 0.1135, 0.8806, 0.0000, 1.3588]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1755728721618652\n",
      "Batch 15, Loss: 3.1755728721618652\n",
      "Processing Batch 16/157\n",
      "Batch 16 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.014200444333255291, Std: 0.08724549412727356\n",
      "z_j Mean: 0.014309154823422432, Std: 0.0872277319431305\n",
      "Similarities: tensor([[1.3887, 0.0000, 0.0000, 0.9802, 0.0000],\n",
      "        [0.0000, 1.3942, 0.5291, 0.0000, 0.0000],\n",
      "        [0.0000, 0.4586, 1.1917, 0.0000, 0.0000],\n",
      "        [0.7520, 0.0000, 0.0000, 1.4169, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4286]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.129357099533081\n",
      "Batch 16, Loss: 3.129357099533081\n",
      "Processing Batch 17/157\n",
      "Batch 17 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01391826756298542, Std: 0.08729095757007599\n",
      "z_j Mean: 0.01371964905411005, Std: 0.08662053197622299\n",
      "Similarities: tensor([[1.3379, 0.0000, 0.0391, 0.3586, 0.0000],\n",
      "        [0.0000, 1.4279, 0.0000, 0.0000, 0.5744],\n",
      "        [0.2308, 0.0000, 1.4057, 0.0468, 0.0804],\n",
      "        [0.5323, 0.0000, 0.0918, 1.3652, 0.5570],\n",
      "        [0.0000, 0.7316, 0.1406, 0.5706, 1.3740]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.184335470199585\n",
      "Batch 17, Loss: 3.184335470199585\n",
      "Processing Batch 18/157\n",
      "Batch 18 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.014850916340947151, Std: 0.08713711053133011\n",
      "z_j Mean: 0.014516403898596764, Std: 0.08719347417354584\n",
      "Similarities: tensor([[1.3616, 0.0000, 0.6174, 0.0312, 0.0000],\n",
      "        [0.2122, 1.4286, 0.0000, 0.4359, 0.0000],\n",
      "        [0.5759, 0.0000, 1.4252, 0.0000, 0.0000],\n",
      "        [0.1052, 0.3290, 0.0000, 1.3062, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4133]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1104679107666016\n",
      "Batch 18, Loss: 3.1104679107666016\n",
      "Processing Batch 19/157\n",
      "Batch 19 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.014110969379544258, Std: 0.08726001530885696\n",
      "z_j Mean: 0.014398811385035515, Std: 0.08721296489238739\n",
      "Similarities: tensor([[1.1951, 0.2088, 0.6159, 0.0000, 0.0000],\n",
      "        [0.2026, 0.7088, 0.0307, 0.0000, 0.0000],\n",
      "        [1.2050, 0.0553, 1.3269, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2603, 0.0000, 1.4262, 0.1947],\n",
      "        [0.0000, 0.0000, 0.0000, 0.1529, 1.4238]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1352503299713135\n",
      "Batch 19, Loss: 3.1352503299713135\n",
      "Processing Batch 20/157\n",
      "Batch 20 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013654690235853195, Std: 0.087332583963871\n",
      "z_j Mean: 0.013540586456656456, Std: 0.08735035359859467\n",
      "Similarities: tensor([[1.4220, 0.0155, 0.0282, 0.0000, 0.0064],\n",
      "        [0.0350, 1.3838, 0.6392, 0.3551, 1.4140],\n",
      "        [0.0959, 1.1860, 1.3713, 0.1169, 0.8301],\n",
      "        [0.0000, 0.3287, 0.0481, 1.4160, 0.5247],\n",
      "        [0.0373, 1.3408, 0.6625, 0.5003, 1.4031]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1605281829833984\n",
      "Batch 20, Loss: 3.1605281829833984\n",
      "Processing Batch 21/157\n",
      "Batch 21 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.014297528192400932, Std: 0.0872296392917633\n",
      "z_j Mean: 0.014295116066932678, Std: 0.08723003417253494\n",
      "Similarities: tensor([[1.3086, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4271, 0.0000, 0.0000, 0.3717],\n",
      "        [0.0000, 0.0000, 1.3337, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4286, 0.0000],\n",
      "        [0.0000, 0.3384, 0.0000, 0.0000, 1.4286]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1578052043914795\n",
      "Batch 21, Loss: 3.1578052043914795\n",
      "Processing Batch 22/157\n",
      "Batch 22 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.014300765469670296, Std: 0.08722910284996033\n",
      "z_j Mean: 0.014128380455076694, Std: 0.08584665507078171\n",
      "Similarities: tensor([[1.3093, 0.0000, 0.0000, 0.0000, 0.6797],\n",
      "        [0.0000, 1.3416, 0.0000, 1.0349, 0.0000],\n",
      "        [0.2112, 0.0000, 1.4079, 0.0000, 0.2140],\n",
      "        [0.0000, 1.1243, 0.0000, 1.4096, 0.0000],\n",
      "        [1.0323, 0.0000, 0.3967, 0.0000, 1.3892]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1884162425994873\n",
      "Batch 22, Loss: 3.1884162425994873\n",
      "Processing Batch 23/157\n",
      "Batch 23 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013365605846047401, Std: 0.08667587488889694\n",
      "z_j Mean: 0.013772523030638695, Std: 0.08661214262247086\n",
      "Similarities: tensor([[1.4216, 0.2264, 0.0000, 0.0699, 0.0000],\n",
      "        [0.0000, 1.2697, 1.1880, 0.0000, 0.3233],\n",
      "        [0.0000, 1.4052, 1.4207, 0.0000, 0.4374],\n",
      "        [0.0377, 0.0000, 0.0000, 1.4227, 0.0000],\n",
      "        [0.0000, 0.6636, 0.7004, 0.0000, 1.3235]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.13746976852417\n",
      "Batch 23, Loss: 3.13746976852417\n",
      "Processing Batch 24/157\n",
      "Batch 24 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013544153422117233, Std: 0.0873498022556305\n",
      "z_j Mean: 0.013857034966349602, Std: 0.08730070292949677\n",
      "Similarities: tensor([[1.4286, 0.0000, 0.0000, 1.2455, 1.4237],\n",
      "        [0.0000, 1.4248, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4203, 0.0000, 0.0000],\n",
      "        [1.2016, 0.0000, 0.0000, 1.4260, 1.1975],\n",
      "        [1.3713, 0.0000, 0.0000, 1.1955, 1.3986]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.188279867172241\n",
      "Batch 24, Loss: 3.188279867172241\n",
      "Processing Batch 25/157\n",
      "Batch 25 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013725635595619678, Std: 0.0873214602470398\n",
      "z_j Mean: 0.013883586972951889, Std: 0.0872964859008789\n",
      "Similarities: tensor([[1.4261, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.0025, 0.5970, 0.4048, 0.1562],\n",
      "        [0.0000, 0.7592, 1.3846, 0.0851, 0.0000],\n",
      "        [0.0000, 0.0798, 0.0373, 1.4022, 0.1403],\n",
      "        [0.0000, 0.2836, 0.0000, 0.0701, 1.3606]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.239483118057251\n",
      "Batch 25, Loss: 3.239483118057251\n",
      "Processing Batch 26/157\n",
      "Batch 26 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012718121521174908, Std: 0.08606690913438797\n",
      "z_j Mean: 0.01322980597615242, Std: 0.0873979702591896\n",
      "Similarities: tensor([[1.4268e+00, 1.4119e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [1.4193e+00, 1.4268e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 1.4003e+00, 0.0000e+00, 2.5712e-02],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 1.3611e+00, 1.3780e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 1.3273e-03, 1.3277e+00, 1.4089e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2405247688293457\n",
      "Batch 26, Loss: 3.2405247688293457\n",
      "Processing Batch 27/157\n",
      "Batch 27 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013745431788265705, Std: 0.08661644905805588\n",
      "z_j Mean: 0.013940110802650452, Std: 0.08658532053232193\n",
      "Similarities: tensor([[0.7396, 0.0000, 0.0000, 0.0000, 0.4168],\n",
      "        [0.0000, 1.4271, 0.5890, 0.8979, 0.0000],\n",
      "        [0.0000, 0.2559, 1.3433, 1.2240, 0.0000],\n",
      "        [0.0000, 0.8205, 1.3571, 1.4082, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4124]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2230677604675293\n",
      "Batch 27, Loss: 3.2230677604675293\n",
      "Processing Batch 28/157\n",
      "Batch 28 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.015362564474344254, Std: 0.08704835921525955\n",
      "z_j Mean: 0.015497284941375256, Std: 0.08702447265386581\n",
      "Similarities: tensor([[1.3998, 0.0000, 0.6470, 0.0000, 0.4706],\n",
      "        [0.0000, 1.3901, 0.0000, 0.1006, 0.0000],\n",
      "        [0.8140, 0.0000, 1.4256, 0.0000, 1.4251],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4281, 0.0000],\n",
      "        [0.7307, 0.0000, 1.4157, 0.0000, 1.4286]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.171011209487915\n",
      "Batch 28, Loss: 3.171011209487915\n",
      "Processing Batch 29/157\n",
      "Batch 29 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013873767107725143, Std: 0.08659598231315613\n",
      "z_j Mean: 0.014052176848053932, Std: 0.08726949989795685\n",
      "Similarities: tensor([[1.4158, 0.0574, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4252, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4065, 0.0000, 0.9719],\n",
      "        [0.0000, 0.0000, 0.4280, 0.0841, 0.3625],\n",
      "        [0.0000, 0.0000, 1.1741, 0.0000, 1.0593]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.187100410461426\n",
      "Batch 29, Loss: 3.187100410461426\n",
      "Processing Batch 30/157\n",
      "Batch 30 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013836424797773361, Std: 0.08730397373437881\n",
      "z_j Mean: 0.01396182831376791, Std: 0.08728399872779846\n",
      "Similarities: tensor([[1.4090, 0.0000, 1.3482, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4286, 0.0000, 0.0000, 0.0000],\n",
      "        [1.3664, 0.0000, 1.4175, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.2534, 0.1531],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4224]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1634814739227295\n",
      "Batch 30, Loss: 3.1634814739227295\n",
      "Processing Batch 31/157\n",
      "Batch 31 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.014178161509335041, Std: 0.08654665946960449\n",
      "z_j Mean: 0.014283088967204094, Std: 0.08723200857639313\n",
      "Similarities: tensor([[1.4169, 0.0000, 0.0000, 0.0446, 0.0000],\n",
      "        [0.0000, 1.3942, 0.0000, 0.0999, 0.1421],\n",
      "        [0.0000, 0.0000, 1.4079, 0.0000, 0.2854],\n",
      "        [0.0811, 0.0916, 0.0000, 1.3409, 0.0228],\n",
      "        [0.0000, 0.1780, 0.5196, 0.0000, 1.4118]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1564085483551025\n",
      "Batch 31, Loss: 3.1564085483551025\n",
      "Processing Batch 32/157\n",
      "Batch 32 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.014726988039910793, Std: 0.08715815097093582\n",
      "z_j Mean: 0.015106194652616978, Std: 0.08709321916103363\n",
      "Similarities: tensor([[1.3581, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4281, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3939, 0.0535, 0.9051],\n",
      "        [0.0000, 0.0000, 0.4139, 1.2553, 0.0573],\n",
      "        [0.0000, 0.0000, 0.9326, 0.0046, 1.4172]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.197802782058716\n",
      "Batch 32, Loss: 3.197802782058716\n",
      "Processing Batch 33/157\n",
      "Batch 33 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.014468370005488396, Std: 0.0857900008559227\n",
      "z_j Mean: 0.014681910164654255, Std: 0.08503889292478561\n",
      "Similarities: tensor([[1.3895, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4286, 0.0000, 0.0000, 1.4286],\n",
      "        [0.0000, 0.0000, 1.4248, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4286, 0.0000, 0.0000, 1.4286]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.3104302883148193\n",
      "Batch 33, Loss: 3.3104302883148193\n",
      "Processing Batch 34/157\n",
      "Batch 34 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012509803287684917, Std: 0.08538550138473511\n",
      "z_j Mean: 0.01294095441699028, Std: 0.08603368699550629\n",
      "Similarities: tensor([[1.3422, 0.0000, 0.0000, 0.4490, 0.6864],\n",
      "        [0.0000, 1.0621, 0.0000, 0.4416, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.6676, 0.0912, 0.0000, 1.2407, 0.2700],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2062840461730957\n",
      "Batch 34, Loss: 3.2062840461730957\n",
      "Processing Batch 35/157\n",
      "Batch 35 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013205298222601414, Std: 0.0867004469037056\n",
      "z_j Mean: 0.013046588748693466, Std: 0.08742550760507584\n",
      "Similarities: tensor([[1.2202, 0.0000, 0.1385, 0.0000, 1.2870],\n",
      "        [0.0000, 1.2695, 0.0000, 0.5153, 0.0000],\n",
      "        [1.3149, 0.0000, 0.6138, 0.0000, 0.9483],\n",
      "        [0.0000, 0.4336, 0.0000, 0.7067, 0.0000],\n",
      "        [0.9578, 0.0000, 0.1101, 0.0000, 1.3122]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.223288059234619\n",
      "Batch 35, Loss: 3.223288059234619\n",
      "Processing Batch 36/157\n",
      "Batch 36 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.014919381588697433, Std: 0.08712542057037354\n",
      "z_j Mean: 0.01526721753180027, Std: 0.08706513047218323\n",
      "Similarities: tensor([[1.3381, 0.0000, 0.0000, 0.4152, 0.3832],\n",
      "        [0.0000, 1.4070, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.2876, 0.0000, 0.0000],\n",
      "        [0.4339, 0.0000, 0.0000, 1.4141, 0.0000],\n",
      "        [0.4452, 0.0224, 0.0000, 0.0000, 1.4107]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.13423228263855\n",
      "Batch 36, Loss: 3.13423228263855\n",
      "Processing Batch 37/157\n",
      "Batch 37 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013837321661412716, Std: 0.08589405566453934\n",
      "z_j Mean: 0.014187250286340714, Std: 0.08654516190290451\n",
      "Similarities: tensor([[1.4218e+00, 2.1235e-02, 1.1381e+00, 1.2537e+00, 0.0000e+00],\n",
      "        [1.0243e-02, 1.3391e+00, 3.3401e-04, 4.5305e-02, 1.0614e+00],\n",
      "        [1.1368e+00, 1.0555e-03, 1.4277e+00, 1.0365e+00, 0.0000e+00],\n",
      "        [1.2885e+00, 9.8035e-02, 9.8023e-01, 1.3932e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 6.7416e-01, 0.0000e+00, 0.0000e+00, 1.4286e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2286581993103027\n",
      "Batch 37, Loss: 3.2286581993103027\n",
      "Processing Batch 38/157\n",
      "Batch 38 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013988020829856396, Std: 0.08727981150150299\n",
      "z_j Mean: 0.014395032078027725, Std: 0.08721359819173813\n",
      "Similarities: tensor([[1.3329, 1.0048, 1.3226, 0.0000, 0.0000],\n",
      "        [0.8867, 1.3030, 0.9026, 0.0821, 0.0000],\n",
      "        [1.2877, 0.9958, 1.3826, 0.0000, 0.1606],\n",
      "        [0.0000, 0.0947, 0.0000, 1.4225, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0325, 0.0000, 1.4185]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1727194786071777\n",
      "Batch 38, Loss: 3.1727194786071777\n",
      "Processing Batch 39/157\n",
      "Batch 39 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.014120849780738354, Std: 0.08655603229999542\n",
      "z_j Mean: 0.014389744028449059, Std: 0.08651172369718552\n",
      "Similarities: tensor([[1.2951, 0.0000, 0.0000, 0.1595, 0.2306],\n",
      "        [0.0000, 1.3903, 0.0000, 0.0000, 1.0617],\n",
      "        [0.0000, 0.0000, 1.4244, 0.0000, 0.0000],\n",
      "        [0.0700, 0.0000, 0.0000, 0.9277, 0.0000],\n",
      "        [0.0696, 0.9626, 0.0000, 0.0106, 1.2407]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1984260082244873\n",
      "Batch 39, Loss: 3.1984260082244873\n",
      "Processing Batch 40/157\n",
      "Batch 40 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.015133270993828773, Std: 0.08708851784467697\n",
      "z_j Mean: 0.015304945409297943, Std: 0.08705850690603256\n",
      "Similarities: tensor([[1.4121, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3123, 0.0356, 0.0000, 0.3845],\n",
      "        [0.0181, 0.0061, 1.3931, 0.0000, 0.0248],\n",
      "        [0.0000, 0.1287, 0.0000, 1.4130, 0.0000],\n",
      "        [0.0000, 0.3064, 0.0747, 0.0000, 1.3746]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1432089805603027\n",
      "Batch 40, Loss: 3.1432089805603027\n",
      "Processing Batch 41/157\n",
      "Batch 41 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.016303684562444687, Std: 0.08687698841094971\n",
      "z_j Mean: 0.016279054805636406, Std: 0.08617614954710007\n",
      "Similarities: tensor([[1.4182, 0.0000, 0.0000, 0.0946, 0.9917],\n",
      "        [0.0000, 1.4113, 1.3994, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3435, 1.3647, 0.0000, 0.0000],\n",
      "        [0.0463, 0.0000, 0.0000, 1.1861, 0.0031],\n",
      "        [1.1107, 0.0000, 0.0000, 0.0000, 1.3941]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.148423194885254\n",
      "Batch 41, Loss: 3.148423194885254\n",
      "Processing Batch 42/157\n",
      "Batch 42 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.017291825264692307, Std: 0.08668570220470428\n",
      "z_j Mean: 0.017039377242326736, Std: 0.08673568069934845\n",
      "Similarities: tensor([[1.3849, 1.3791, 0.0000, 0.8694, 0.2269],\n",
      "        [1.3375, 1.3975, 0.0000, 0.7926, 0.2642],\n",
      "        [0.0000, 0.0000, 1.4132, 0.0025, 0.3121],\n",
      "        [0.7159, 0.7823, 0.0060, 1.4166, 0.5650],\n",
      "        [0.2135, 0.2098, 0.3916, 0.5589, 1.4082]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1728968620300293\n",
      "Batch 42, Loss: 3.1728968620300293\n",
      "Processing Batch 43/157\n",
      "Batch 43 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.017450721934437752, Std: 0.08665385097265244\n",
      "z_j Mean: 0.01770949177443981, Std: 0.08660132437944412\n",
      "Similarities: tensor([[1.3903, 0.0000, 0.0000, 0.0000, 0.1175],\n",
      "        [0.0000, 1.3294, 0.6115, 0.0000, 0.0622],\n",
      "        [0.0000, 0.4586, 1.4117, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3722, 0.0000],\n",
      "        [0.0618, 0.0803, 0.0000, 0.0000, 1.4067]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1866512298583984\n",
      "Batch 43, Loss: 3.1866512298583984\n",
      "Processing Batch 44/157\n",
      "Batch 44 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.019744541496038437, Std: 0.0861600786447525\n",
      "z_j Mean: 0.019704408943653107, Std: 0.08616926521062851\n",
      "Similarities: tensor([[1.2394, 0.7625, 0.0000, 0.5777, 0.0000],\n",
      "        [0.5077, 1.4192, 0.0000, 1.4278, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3628, 0.0000, 0.1380],\n",
      "        [0.1400, 1.2886, 0.0000, 1.3655, 0.0000],\n",
      "        [0.0000, 0.0000, 0.1100, 0.0000, 1.3901]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1682751178741455\n",
      "Batch 44, Loss: 3.1682751178741455\n",
      "Processing Batch 45/157\n",
      "Batch 45 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.016024434939026833, Std: 0.08622386306524277\n",
      "z_j Mean: 0.015800487250089645, Std: 0.08696992695331573\n",
      "Similarities: tensor([[1.2211, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3951, 0.1376, 0.2333, 0.6014],\n",
      "        [0.0000, 0.0519, 1.2850, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2242, 0.0676, 1.3688, 0.4280],\n",
      "        [0.0000, 0.6661, 0.0000, 0.5098, 1.4157]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.17033052444458\n",
      "Batch 45, Loss: 3.17033052444458\n",
      "Processing Batch 46/157\n",
      "Batch 46 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.014950715005397797, Std: 0.08641653507947922\n",
      "z_j Mean: 0.01531005185097456, Std: 0.0863535925745964\n",
      "Similarities: tensor([[1.4086, 1.3716, 0.9265, 0.0000, 0.0000],\n",
      "        [1.3981, 1.4245, 0.9131, 0.0000, 0.0000],\n",
      "        [0.8972, 0.9141, 1.3882, 0.7497, 0.0000],\n",
      "        [0.0000, 0.0413, 0.9055, 1.0318, 0.0000],\n",
      "        [0.0712, 0.0000, 0.0000, 0.0000, 1.4286]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.14656400680542\n",
      "Batch 46, Loss: 3.14656400680542\n",
      "Processing Batch 47/157\n",
      "Batch 47 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01743096485733986, Std: 0.0866578221321106\n",
      "z_j Mean: 0.01743146777153015, Std: 0.08665772527456284\n",
      "Similarities: tensor([[1.2780, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3980, 0.0000, 0.5222, 1.1115],\n",
      "        [0.0143, 0.0113, 1.4230, 0.0000, 0.0047],\n",
      "        [0.0000, 0.6087, 0.0000, 1.3889, 1.1466],\n",
      "        [0.0000, 0.9294, 0.0000, 1.1946, 1.3709]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.113346576690674\n",
      "Batch 47, Loss: 3.113346576690674\n",
      "Processing Batch 48/157\n",
      "Batch 48 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.015443745069205761, Std: 0.08632978051900864\n",
      "z_j Mean: 0.015823280438780785, Std: 0.08626101166009903\n",
      "Similarities: tensor([[1.4093, 0.7542, 0.0176, 0.7413, 0.0098],\n",
      "        [0.8555, 1.3404, 0.0236, 0.9962, 0.0609],\n",
      "        [0.0235, 0.0088, 1.2858, 0.2389, 0.5312],\n",
      "        [0.5704, 1.1259, 0.0478, 1.1411, 0.2560],\n",
      "        [0.0519, 0.0526, 0.2698, 0.5609, 1.1966]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.173092842102051\n",
      "Batch 48, Loss: 3.173092842102051\n",
      "Processing Batch 49/157\n",
      "Batch 49 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.017052343115210533, Std: 0.08602645248174667\n",
      "z_j Mean: 0.01725948229432106, Std: 0.08669213950634003\n",
      "Similarities: tensor([[1.3348, 0.0000, 0.1808, 0.2731, 0.0024],\n",
      "        [0.0000, 1.1524, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1555, 0.0000, 1.4067, 1.3269, 0.0000],\n",
      "        [0.4683, 0.0000, 1.2748, 1.3792, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3862]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.155432939529419\n",
      "Batch 49, Loss: 3.155432939529419\n",
      "Processing Batch 50/157\n",
      "Batch 50 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.015425771474838257, Std: 0.08703718334436417\n",
      "z_j Mean: 0.015627466142177582, Std: 0.08700118213891983\n",
      "Similarities: tensor([[1.4208, 0.8452, 0.0000, 0.0807, 0.0000],\n",
      "        [1.0159, 1.4073, 0.0131, 0.3676, 0.0022],\n",
      "        [0.0000, 0.0181, 1.3936, 0.0000, 0.2996],\n",
      "        [0.0913, 0.4902, 0.0000, 1.3777, 0.0000],\n",
      "        [0.0000, 0.0065, 0.0435, 0.0000, 1.3552]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.094970464706421\n",
      "Batch 50, Loss: 3.094970464706421\n",
      "Processing Batch 51/157\n",
      "Batch 51 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013901456259191036, Std: 0.08588369935750961\n",
      "z_j Mean: 0.014025525189936161, Std: 0.08586352318525314\n",
      "Similarities: tensor([[1.4020, 0.0000, 0.1582, 0.0480, 0.0000],\n",
      "        [0.0000, 1.3688, 0.3262, 0.0000, 0.0000],\n",
      "        [0.2644, 0.0000, 1.3973, 0.0126, 0.0000],\n",
      "        [0.0423, 0.0000, 0.0641, 0.0650, 0.7218],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0268, 1.3898]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1657676696777344\n",
      "Batch 51, Loss: 3.1657676696777344\n",
      "Processing Batch 52/157\n",
      "Batch 52 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01330519374459982, Std: 0.08454622328281403\n",
      "z_j Mean: 0.01312720775604248, Std: 0.08600545674562454\n",
      "Similarities: tensor([[1.3720, 0.0000, 0.6876, 0.0000, 0.0000],\n",
      "        [0.0000, 1.2850, 0.0000, 0.0000, 1.1704],\n",
      "        [0.4040, 0.0000, 1.4097, 0.0000, 0.0000],\n",
      "        [0.0148, 0.0000, 0.0000, 0.8565, 0.0000],\n",
      "        [0.0000, 1.3270, 0.0000, 0.0000, 1.4235]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.173826217651367\n",
      "Batch 52, Loss: 3.173826217651367\n",
      "Processing Batch 53/157\n",
      "Batch 53 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01407371275126934, Std: 0.08656369894742966\n",
      "z_j Mean: 0.014048958197236061, Std: 0.08656772971153259\n",
      "Similarities: tensor([[1.2448, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3051, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3993, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4286, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3568]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1376936435699463\n",
      "Batch 53, Loss: 3.1376936435699463\n",
      "Processing Batch 54/157\n",
      "Batch 54 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01345088705420494, Std: 0.0859554260969162\n",
      "z_j Mean: 0.013135412707924843, Std: 0.08529148995876312\n",
      "Similarities: tensor([[1.2636, 0.0000, 0.0000, 0.0155, 1.3031],\n",
      "        [0.0000, 1.3964, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4189, 0.0000, 0.0000],\n",
      "        [0.5277, 0.0000, 0.0000, 1.4153, 0.0275],\n",
      "        [1.4041, 0.0000, 0.0000, 0.4162, 1.2586]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1486001014709473\n",
      "Batch 54, Loss: 3.1486001014709473\n",
      "Processing Batch 55/157\n",
      "Batch 55 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.014052627608180046, Std: 0.08585909008979797\n",
      "z_j Mean: 0.014235639944672585, Std: 0.0851147472858429\n",
      "Similarities: tensor([[1.4196, 0.0000, 0.7843, 0.4655, 0.0000],\n",
      "        [0.0000, 1.3769, 0.0000, 0.0000, 0.0000],\n",
      "        [0.8335, 0.0000, 1.4227, 0.8394, 0.0000],\n",
      "        [0.4926, 0.0000, 0.8113, 1.3651, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4285]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1711957454681396\n",
      "Batch 55, Loss: 3.1711957454681396\n",
      "Processing Batch 56/157\n",
      "Batch 56 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013693015091121197, Std: 0.08662474900484085\n",
      "z_j Mean: 0.01332178246229887, Std: 0.0845436081290245\n",
      "Similarities: tensor([[1.2896, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4251, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4048, 0.0000, 0.5276],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4167, 0.0000],\n",
      "        [0.0000, 0.0000, 0.6848, 0.0000, 1.3762]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2158279418945312\n",
      "Batch 56, Loss: 3.2158279418945312\n",
      "Processing Batch 57/157\n",
      "Batch 57 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01339101605117321, Std: 0.08667195588350296\n",
      "z_j Mean: 0.01392731536179781, Std: 0.08587950468063354\n",
      "Similarities: tensor([[1.4183, 0.0000, 1.2267, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4285, 0.1201, 0.0000, 0.0000],\n",
      "        [0.9800, 0.4568, 1.3780, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3406, 0.7872],\n",
      "        [0.0000, 0.0000, 0.0000, 0.8570, 1.3949]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.183328628540039\n",
      "Batch 57, Loss: 3.183328628540039\n",
      "Processing Batch 58/157\n",
      "Batch 58 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.014646240510046482, Std: 0.0871717557311058\n",
      "z_j Mean: 0.014796528965234756, Std: 0.08714637905359268\n",
      "Similarities: tensor([[1.3873, 1.2357, 0.0049, 0.0000, 0.3399],\n",
      "        [1.3198, 1.3941, 0.0729, 0.0000, 0.3427],\n",
      "        [0.0376, 0.1076, 1.4203, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3280, 0.0000],\n",
      "        [0.2863, 0.2868, 0.0000, 0.0000, 1.3979]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1552889347076416\n",
      "Batch 58, Loss: 3.1552889347076416\n",
      "Processing Batch 59/157\n",
      "Batch 59 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.015482855960726738, Std: 0.08632276952266693\n",
      "z_j Mean: 0.015408401377499104, Std: 0.08633609116077423\n",
      "Similarities: tensor([[1.4225, 0.0000, 0.0000, 0.0292, 0.0000],\n",
      "        [0.0238, 1.4240, 0.0000, 0.8977, 0.0000],\n",
      "        [0.0000, 0.0056, 1.3615, 0.0369, 0.0000],\n",
      "        [0.0279, 0.9349, 0.0000, 1.4259, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3352]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.110621452331543\n",
      "Batch 59, Loss: 3.110621452331543\n",
      "Processing Batch 60/157\n",
      "Batch 60 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.015119054354727268, Std: 0.08709098398685455\n",
      "z_j Mean: 0.015019236132502556, Std: 0.08710826188325882\n",
      "Similarities: tensor([[1.4010, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4263, 0.0970, 0.0000, 0.0093],\n",
      "        [0.0000, 0.0552, 1.4286, 0.0000, 0.1369],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4173, 0.0000],\n",
      "        [0.0000, 0.0037, 0.0960, 0.0000, 1.4247]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1805949211120605\n",
      "Batch 60, Loss: 3.1805949211120605\n",
      "Processing Batch 61/157\n",
      "Batch 61 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.014021696522831917, Std: 0.08727440237998962\n",
      "z_j Mean: 0.01388348825275898, Std: 0.0872965008020401\n",
      "Similarities: tensor([[1.4139, 1.4099, 0.0000, 0.0000, 0.0238],\n",
      "        [1.3228, 1.3294, 0.0000, 0.0000, 0.0041],\n",
      "        [0.0000, 0.0000, 1.4039, 0.0448, 0.0736],\n",
      "        [0.0000, 0.0000, 0.0842, 1.3501, 0.1082],\n",
      "        [0.0000, 0.0623, 0.0000, 0.0000, 0.8089]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.179673910140991\n",
      "Batch 61, Loss: 3.179673910140991\n",
      "Processing Batch 62/157\n",
      "Batch 62 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.015582999214529991, Std: 0.08700916171073914\n",
      "z_j Mean: 0.015274805016815662, Std: 0.08706380426883698\n",
      "Similarities: tensor([[1.4209, 0.2483, 0.0000, 0.1738, 0.0000],\n",
      "        [0.1799, 1.3990, 0.0431, 0.0315, 0.0422],\n",
      "        [0.0000, 0.0233, 1.4009, 0.0000, 1.3310],\n",
      "        [0.1864, 0.0585, 0.0000, 1.4116, 0.0000],\n",
      "        [0.0000, 0.0175, 1.2552, 0.0000, 1.4232]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.157254695892334\n",
      "Batch 62, Loss: 3.157254695892334\n",
      "Processing Batch 63/157\n",
      "Batch 63 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01462840847671032, Std: 0.08717475086450577\n",
      "z_j Mean: 0.01479218527674675, Std: 0.08714710921049118\n",
      "Similarities: tensor([[1.0504, 1.1447, 0.2006, 0.0000, 0.0000],\n",
      "        [0.1855, 1.3562, 0.0354, 0.0000, 0.0000],\n",
      "        [0.3864, 0.1178, 1.3559, 1.3708, 0.4868],\n",
      "        [0.7198, 0.2195, 1.2532, 1.2340, 0.3817],\n",
      "        [0.0000, 0.0000, 0.8353, 0.5908, 1.4186]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4282869100570679]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.164438009262085\n",
      "Batch 63, Loss: 3.164438009262085\n",
      "Processing Batch 64/157\n",
      "Batch 64 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.015260404907166958, Std: 0.08706633001565933\n",
      "z_j Mean: 0.015228858217597008, Std: 0.08707185089588165\n",
      "Similarities: tensor([[1.3976, 1.3719, 1.2206, 0.0000, 0.0000],\n",
      "        [1.2838, 1.4278, 1.3696, 0.0000, 0.0000],\n",
      "        [1.0125, 1.3472, 1.4259, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4271, 0.7270],\n",
      "        [0.0000, 0.0000, 0.0000, 0.7661, 1.3873]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.162513494491577\n",
      "Batch 64, Loss: 3.162513494491577\n",
      "Processing Batch 65/157\n",
      "Batch 65 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01451726071536541, Std: 0.0871933326125145\n",
      "z_j Mean: 0.014470748603343964, Std: 0.0872010663151741\n",
      "Similarities: tensor([[1.3485, 0.0000, 0.0621, 0.9490, 0.1279],\n",
      "        [0.0000, 1.4213, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0349, 0.0000, 1.4207, 0.3297, 1.1012],\n",
      "        [0.7493, 0.0000, 0.3024, 1.4248, 0.1580],\n",
      "        [0.1082, 0.0000, 1.1361, 0.1574, 1.4244]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1170339584350586\n",
      "Batch 65, Loss: 3.1170339584350586\n",
      "Processing Batch 66/157\n",
      "Batch 66 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013354012742638588, Std: 0.0866776630282402\n",
      "z_j Mean: 0.01365252397954464, Std: 0.08663114160299301\n",
      "Similarities: tensor([[1.3504, 0.0000, 0.0000, 0.9545, 1.3944],\n",
      "        [0.0000, 1.4137, 0.2727, 0.1323, 0.0000],\n",
      "        [0.2564, 0.2721, 1.4250, 0.7305, 0.0000],\n",
      "        [0.8547, 0.1898, 1.0939, 1.3051, 0.6307],\n",
      "        [1.3353, 0.0000, 0.0000, 1.0137, 1.4025]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.169025421142578\n",
      "Batch 66, Loss: 3.169025421142578\n",
      "Processing Batch 67/157\n",
      "Batch 67 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.017054151743650436, Std: 0.08602609485387802\n",
      "z_j Mean: 0.017311055213212967, Std: 0.08597476035356522\n",
      "Similarities: tensor([[1.4253, 1.0792, 0.8580, 0.0000, 0.0000],\n",
      "        [1.0601, 1.4210, 1.1297, 0.0000, 0.0000],\n",
      "        [0.9073, 1.2922, 1.4191, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4278, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3804]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.105882406234741\n",
      "Batch 67, Loss: 3.105882406234741\n",
      "Processing Batch 68/157\n",
      "Batch 68 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.014308766461908817, Std: 0.08365561813116074\n",
      "z_j Mean: 0.01437747199088335, Std: 0.0836438313126564\n",
      "Similarities: tensor([[1.2881, 1.2221, 0.3671, 0.0000, 1.1698],\n",
      "        [1.1665, 1.3649, 0.1126, 0.0000, 1.1940],\n",
      "        [0.4092, 0.0977, 1.4136, 0.0000, 0.0771],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.7439, 0.8800, 0.0439, 0.0000, 1.1882]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.195167303085327\n",
      "Batch 68, Loss: 3.195167303085327\n",
      "Processing Batch 69/157\n",
      "Batch 69 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.014490576460957527, Std: 0.08435111492872238\n",
      "z_j Mean: 0.014352674596011639, Std: 0.08437468856573105\n",
      "Similarities: tensor([[1.3630, 0.5221, 0.6942, 0.8236, 1.3600],\n",
      "        [0.8939, 1.4286, 0.0000, 0.0000, 0.6853],\n",
      "        [0.4744, 0.0000, 1.3931, 0.3126, 0.3163],\n",
      "        [1.0055, 0.0000, 0.4916, 1.1868, 1.2076],\n",
      "        [1.2268, 0.1993, 0.7345, 0.8779, 1.2784]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.220543384552002\n",
      "Batch 69, Loss: 3.220543384552002\n",
      "Processing Batch 70/157\n",
      "Batch 70 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013687431812286377, Std: 0.08520463109016418\n",
      "z_j Mean: 0.013689419254660606, Std: 0.08591774851083755\n",
      "Similarities: tensor([[1.4029, 1.2810, 0.0000, 0.0000, 0.3694],\n",
      "        [1.3415, 1.4187, 0.1890, 0.1696, 0.3046],\n",
      "        [0.0846, 0.2345, 1.3148, 1.2884, 0.0000],\n",
      "        [0.0000, 0.0618, 1.3933, 1.3820, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3486]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.202786684036255\n",
      "Batch 70, Loss: 3.202786684036255\n",
      "Processing Batch 71/157\n",
      "Batch 71 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01351185142993927, Std: 0.0866531953215599\n",
      "z_j Mean: 0.013348272070288658, Std: 0.08525843173265457\n",
      "Similarities: tensor([[1.4203, 0.0000, 0.0000, 0.0000, 0.9437],\n",
      "        [0.0000, 1.3675, 0.0000, 0.4425, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4221, 0.0277, 0.0000],\n",
      "        [0.0000, 0.3183, 0.0000, 1.4222, 0.0000],\n",
      "        [0.9570, 0.0000, 0.0000, 0.0000, 1.4191]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1953845024108887\n",
      "Batch 71, Loss: 3.1953845024108887\n",
      "Processing Batch 72/157\n",
      "Batch 72 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.014166081324219704, Std: 0.08440622687339783\n",
      "z_j Mean: 0.014116499572992325, Std: 0.0844145342707634\n",
      "Similarities: tensor([[1.3900, 0.0000, 0.0387, 0.0641, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0063, 0.0000, 1.2930, 0.2805, 0.0344],\n",
      "        [0.0044, 0.0000, 0.1222, 1.3874, 1.2289],\n",
      "        [0.0000, 0.0000, 0.0109, 1.2212, 1.3596]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.18865704536438\n",
      "Batch 72, Loss: 3.18865704536438\n",
      "Processing Batch 73/157\n",
      "Batch 73 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.017151489853858948, Std: 0.08671357482671738\n",
      "z_j Mean: 0.017115818336606026, Std: 0.08601384609937668\n",
      "Similarities: tensor([[1.4286, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3351, 0.6970, 0.9849, 0.0000],\n",
      "        [0.0000, 0.7319, 1.3743, 0.7715, 0.0000],\n",
      "        [0.0000, 0.8476, 0.6976, 1.3849, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4011]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1023366451263428\n",
      "Batch 73, Loss: 3.1023366451263428\n",
      "Processing Batch 74/157\n",
      "Batch 74 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.014793703332543373, Std: 0.08429846912622452\n",
      "z_j Mean: 0.015273809432983398, Std: 0.08636000752449036\n",
      "Similarities: tensor([[1.3011, 0.0000, 0.0000, 0.2530, 0.0000],\n",
      "        [0.0000, 1.4236, 0.0000, 0.0000, 0.4339],\n",
      "        [0.0000, 0.0000, 1.3030, 0.0000, 0.0000],\n",
      "        [1.0820, 0.0000, 0.0000, 1.2966, 0.0000],\n",
      "        [0.0000, 0.4502, 0.0000, 0.0000, 1.3925]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.203533411026001\n",
      "Batch 74, Loss: 3.203533411026001\n",
      "Processing Batch 75/157\n",
      "Batch 75 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.015269331634044647, Std: 0.08565105497837067\n",
      "z_j Mean: 0.01559810247272253, Std: 0.0863020122051239\n",
      "Similarities: tensor([[1.4166e+00, 0.0000e+00, 6.9370e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 1.4262e+00, 0.0000e+00, 4.4972e-03, 7.0266e-01],\n",
      "        [5.3376e-01, 0.0000e+00, 1.3974e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [8.0263e-03, 0.0000e+00, 0.0000e+00, 1.4174e+00, 3.7684e-01],\n",
      "        [5.6576e-04, 8.1268e-01, 0.0000e+00, 3.1069e-01, 1.4097e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1220316886901855\n",
      "Batch 75, Loss: 3.1220316886901855\n",
      "Processing Batch 76/157\n",
      "Batch 76 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.015074366703629494, Std: 0.08424872905015945\n",
      "z_j Mean: 0.015136217698454857, Std: 0.08495919406414032\n",
      "Similarities: tensor([[1.4286e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.1165e-02],\n",
      "        [0.0000e+00, 9.3284e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 1.4144e+00, 4.6618e-01, 8.3686e-03],\n",
      "        [0.0000e+00, 0.0000e+00, 5.2937e-01, 1.3645e+00, 4.8722e-01],\n",
      "        [0.0000e+00, 0.0000e+00, 4.3159e-04, 4.6216e-01, 1.3056e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1492156982421875\n",
      "Batch 76, Loss: 3.1492156982421875\n",
      "Processing Batch 77/157\n",
      "Batch 77 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.014744771644473076, Std: 0.0850280150771141\n",
      "z_j Mean: 0.014775644987821579, Std: 0.08430163562297821\n",
      "Similarities: tensor([[1.4246, 0.7258, 1.3596, 0.3348, 0.9850],\n",
      "        [0.6764, 1.4067, 0.4455, 0.0000, 0.0000],\n",
      "        [1.4146, 0.5006, 1.4174, 0.4089, 1.1220],\n",
      "        [0.1880, 0.0000, 0.2607, 1.4124, 0.9054],\n",
      "        [1.2045, 0.0000, 1.3342, 0.5434, 1.2872]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.148439884185791\n",
      "Batch 77, Loss: 3.148439884185791\n",
      "Processing Batch 78/157\n",
      "Batch 78 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013114240020513535, Std: 0.08238235861063004\n",
      "z_j Mean: 0.013156833127140999, Std: 0.08384452015161514\n",
      "Similarities: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.1336],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0171, 1.4045]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.203803539276123\n",
      "Batch 78, Loss: 3.203803539276123\n",
      "Processing Batch 79/157\n",
      "Batch 79 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.014468275010585785, Std: 0.08362817019224167\n",
      "z_j Mean: 0.014321209862828255, Std: 0.08365347981452942\n",
      "Similarities: tensor([[1.1694, 0.0743, 0.4329, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3483, 0.3322, 0.0000, 0.5384],\n",
      "        [0.0000, 0.4540, 1.3772, 0.0000, 0.0950],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4275, 0.0000],\n",
      "        [0.0000, 0.3326, 0.0457, 0.0000, 1.3958]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1769347190856934\n",
      "Batch 79, Loss: 3.1769347190856934\n",
      "Processing Batch 80/157\n",
      "Batch 80 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013796934857964516, Std: 0.08590054512023926\n",
      "z_j Mean: 0.01346215233206749, Std: 0.08452136814594269\n",
      "Similarities: tensor([[1.3282, 0.9195, 0.0520, 0.0000, 0.0000],\n",
      "        [0.8951, 1.4123, 0.5213, 0.0000, 0.0000],\n",
      "        [0.1087, 0.5241, 1.3794, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3501]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1816494464874268\n",
      "Batch 80, Loss: 3.1816494464874268\n",
      "Processing Batch 81/157\n",
      "Batch 81 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.014439687132835388, Std: 0.0857948362827301\n",
      "z_j Mean: 0.014233063906431198, Std: 0.08582935482263565\n",
      "Similarities: tensor([[1.1451, 0.0000, 0.8303, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4153, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4474, 0.0000, 1.4042, 0.0000, 0.0233],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4286, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0092, 0.0000, 1.3782]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.154161214828491\n",
      "Batch 81, Loss: 3.154161214828491\n",
      "Processing Batch 82/157\n",
      "Batch 82 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.014705456793308258, Std: 0.08211321383714676\n",
      "z_j Mean: 0.014853064902126789, Std: 0.08208663761615753\n",
      "Similarities: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3770, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4284, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4014, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0492, 0.0000, 1.2933]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1535658836364746\n",
      "Batch 82, Loss: 3.1535658836364746\n",
      "Processing Batch 83/157\n",
      "Batch 83 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.014972828328609467, Std: 0.08570338785648346\n",
      "z_j Mean: 0.014397339895367622, Std: 0.08508753776550293\n",
      "Similarities: tensor([[1.3998, 0.0000, 0.0033, 0.0648, 0.0000],\n",
      "        [0.0000, 1.2913, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1076, 0.0000, 1.4286, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0133, 0.0000, 1.4180, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4201]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.15065860748291\n",
      "Batch 83, Loss: 3.15065860748291\n",
      "Processing Batch 84/157\n",
      "Batch 84 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012338203378021717, Std: 0.07793648540973663\n",
      "z_j Mean: 0.012289410457015038, Std: 0.07949508726596832\n",
      "Similarities: tensor([[1.4286, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3510, 1.2977, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4144, 1.4274, 0.0000, 0.0000],\n",
      "        [0.2085, 0.0000, 0.0000, 0.9730, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3679]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.3440606594085693\n",
      "Batch 84, Loss: 3.3440606594085693\n",
      "Processing Batch 85/157\n",
      "Batch 85 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013145910575985909, Std: 0.08384623378515244\n",
      "z_j Mean: 0.012525154277682304, Std: 0.08173049241304398\n",
      "Similarities: tensor([[1.4192, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4101, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.1265, 0.9438],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3459, 1.4114]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.196521043777466\n",
      "Batch 85, Loss: 3.196521043777466\n",
      "Processing Batch 86/157\n",
      "Batch 86 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.014302334748208523, Std: 0.08365671336650848\n",
      "z_j Mean: 0.014127487316727638, Std: 0.08441269397735596\n",
      "Similarities: tensor([[1.4286, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3931, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4012, 1.0238, 0.9932],\n",
      "        [0.0000, 0.0000, 1.1027, 1.4192, 1.3929],\n",
      "        [0.0000, 0.0000, 1.1413, 1.3997, 1.4011]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1821506023406982\n",
      "Batch 86, Loss: 3.1821506023406982\n",
      "Processing Batch 87/157\n",
      "Batch 87 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.014624260365962982, Std: 0.08212772011756897\n",
      "z_j Mean: 0.015248959884047508, Std: 0.08421730250120163\n",
      "Similarities: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.5190, 1.2543, 0.0000, 0.5124],\n",
      "        [0.0000, 0.5269, 1.3583, 0.0000, 1.0369],\n",
      "        [0.0711, 0.0000, 0.0000, 1.3829, 0.0000],\n",
      "        [0.0000, 0.7425, 0.9359, 0.0000, 1.4260]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.268526792526245\n",
      "Batch 87, Loss: 3.268526792526245\n",
      "Processing Batch 88/157\n",
      "Batch 88 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013160943053662777, Std: 0.08384387940168381\n",
      "z_j Mean: 0.013008886948227882, Std: 0.08459232747554779\n",
      "Similarities: tensor([[1.4286, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3565, 0.3779, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1207, 1.4286, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.1815, 0.2633],\n",
      "        [0.0000, 0.0000, 0.0000, 0.4107, 1.2099]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1941444873809814\n",
      "Batch 88, Loss: 3.1941444873809814\n",
      "Processing Batch 89/157\n",
      "Batch 89 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013778338208794594, Std: 0.08374462276697159\n",
      "z_j Mean: 0.01387340109795332, Std: 0.0851745456457138\n",
      "Similarities: tensor([[1.3805, 0.0876, 0.0000, 0.7715, 0.0000],\n",
      "        [0.0399, 1.3592, 0.0000, 0.6548, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4286, 0.0000, 0.7440],\n",
      "        [0.7786, 0.7929, 0.0000, 1.3732, 0.0000],\n",
      "        [0.0000, 0.0000, 1.0282, 0.0000, 1.2267]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1692075729370117\n",
      "Batch 89, Loss: 3.1692075729370117\n",
      "Processing Batch 90/157\n",
      "Batch 90 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013513190671801567, Std: 0.08305609226226807\n",
      "z_j Mean: 0.014172524213790894, Std: 0.08583937585353851\n",
      "Similarities: tensor([[1.4286, 1.4286, 0.0000, 0.0000, 0.0000],\n",
      "        [1.4286, 1.4286, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.8958, 0.0128, 0.0000],\n",
      "        [0.0000, 0.0000, 0.1934, 1.3510, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.2867]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2137792110443115\n",
      "Batch 90, Loss: 3.2137792110443115\n",
      "Processing Batch 91/157\n",
      "Batch 91 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01498254295438528, Std: 0.08353755623102188\n",
      "z_j Mean: 0.015624080784618855, Std: 0.08414850383996964\n",
      "Similarities: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4246, 1.3353, 0.0000, 1.2839],\n",
      "        [0.0000, 1.4245, 1.3960, 0.0000, 1.2696],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4011, 0.0000],\n",
      "        [0.0000, 1.3097, 1.2276, 0.0000, 1.4268]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2002575397491455\n",
      "Batch 91, Loss: 3.2002575397491455\n",
      "Processing Batch 92/157\n",
      "Batch 92 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012457717210054398, Std: 0.08248420804738998\n",
      "z_j Mean: 0.012088209390640259, Std: 0.08104655891656876\n",
      "Similarities: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3711, 0.0000, 0.0403, 0.0000],\n",
      "        [0.0000, 0.0000, 0.9885, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0184, 0.0000, 1.4115, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.261445999145508\n",
      "Batch 92, Loss: 3.261445999145508\n",
      "Processing Batch 93/157\n",
      "Batch 93 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.014600081369280815, Std: 0.08287190645933151\n",
      "z_j Mean: 0.014721340499818325, Std: 0.08358398824930191\n",
      "Similarities: tensor([[1.4149, 0.0000, 0.7176, 0.0000, 0.0107],\n",
      "        [0.0000, 1.3687, 0.0000, 0.0000, 1.4285],\n",
      "        [0.8824, 0.0000, 1.4072, 0.0000, 0.0057],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3919, 0.0000],\n",
      "        [0.0000, 1.3687, 0.0000, 0.0000, 1.4285]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.183875799179077\n",
      "Batch 93, Loss: 3.183875799179077\n",
      "Processing Batch 94/157\n",
      "Batch 94 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012577791698276997, Std: 0.08097200840711594\n",
      "z_j Mean: 0.012262076139450073, Std: 0.07872772216796875\n",
      "Similarities: tensor([[1.3358, 0.0000, 1.3358, 0.4642, 0.0000],\n",
      "        [1.4286, 0.0000, 1.4286, 0.0000, 0.0000],\n",
      "        [1.4286, 0.0000, 1.4286, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.2429, 0.1159],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4232]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.296501398086548\n",
      "Batch 94, Loss: 3.296501398086548\n",
      "Processing Batch 95/157\n",
      "Batch 95 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012054290622472763, Std: 0.08328036218881607\n",
      "z_j Mean: 0.012235807254910469, Std: 0.0839838981628418\n",
      "Similarities: tensor([[1.3824, 0.0000, 0.0000, 0.2500, 0.0000],\n",
      "        [0.0000, 1.4208, 0.0000, 0.3778, 0.5869],\n",
      "        [0.0000, 0.0000, 1.2696, 0.0000, 0.0000],\n",
      "        [0.1202, 0.2047, 0.0000, 1.3925, 0.0000],\n",
      "        [0.0000, 0.5764, 0.0000, 0.0000, 1.4249]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.204606771469116\n",
      "Batch 95, Loss: 3.204606771469116\n",
      "Processing Batch 96/157\n",
      "Batch 96 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01275910995900631, Std: 0.08317527920007706\n",
      "z_j Mean: 0.01255803368985653, Std: 0.08320587873458862\n",
      "Similarities: tensor([[1.1407, 0.1493, 0.5199, 0.0000, 0.0000],\n",
      "        [0.0149, 1.3895, 0.6041, 0.0000, 0.0000],\n",
      "        [0.0000, 0.4103, 1.4286, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4286]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2076847553253174\n",
      "Batch 96, Loss: 3.2076847553253174\n",
      "Processing Batch 97/157\n",
      "Batch 97 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012598361819982529, Std: 0.082462839782238\n",
      "z_j Mean: 0.012213759124279022, Std: 0.08102772384881973\n",
      "Similarities: tensor([[1.3845, 0.0262, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3549, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3726, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4200, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.22412371635437\n",
      "Batch 97, Loss: 3.22412371635437\n",
      "Processing Batch 98/157\n",
      "Batch 98 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011756256222724915, Std: 0.07957568764686584\n",
      "z_j Mean: 0.011720940470695496, Std: 0.0803442895412445\n",
      "Similarities: tensor([[1.3577, 0.0000, 0.0000, 0.0098, 0.0000],\n",
      "        [0.0000, 1.3984, 0.3071, 0.0568, 0.0000],\n",
      "        [0.0000, 0.1589, 1.0849, 0.0000, 0.0000],\n",
      "        [0.0387, 0.0709, 0.0000, 1.3619, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4285]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2582597732543945\n",
      "Batch 98, Loss: 3.2582597732543945\n",
      "Processing Batch 99/157\n",
      "Batch 99 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011430647224187851, Std: 0.08263281732797623\n",
      "z_j Mean: 0.011126919649541378, Std: 0.08193258196115494\n",
      "Similarities: tensor([[1.2578, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5287, 1.3916, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3594, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4286, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.238020896911621\n",
      "Batch 99, Loss: 3.238020896911621\n",
      "Processing Batch 100/157\n",
      "Batch 100 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011975478380918503, Std: 0.0840214192867279\n",
      "z_j Mean: 0.012010176666080952, Std: 0.08401645720005035\n",
      "Similarities: tensor([[1.4245, 0.0000, 0.7964, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3986, 0.0000, 0.4565, 1.1462],\n",
      "        [0.7107, 0.0000, 1.1132, 0.0000, 0.1765],\n",
      "        [0.0000, 0.2926, 0.0000, 1.3943, 0.0000],\n",
      "        [0.0000, 1.3091, 0.0000, 0.0000, 1.4021]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2022876739501953\n",
      "Batch 100, Loss: 3.2022876739501953\n",
      "Processing Batch 101/157\n",
      "Batch 101 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012001588940620422, Std: 0.08255180716514587\n",
      "z_j Mean: 0.012155972421169281, Std: 0.08326558023691177\n",
      "Similarities: tensor([[1.3767, 0.0000, 0.0000, 0.0000, 0.1776],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4286, 0.0000, 0.0000],\n",
      "        [0.0000, 0.5524, 0.0000, 1.4243, 0.0000],\n",
      "        [0.0000, 0.5353, 0.0000, 0.1684, 1.3245]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1860272884368896\n",
      "Batch 101, Loss: 3.1860272884368896\n",
      "Processing Batch 102/157\n",
      "Batch 102 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012270740233361721, Std: 0.08251222968101501\n",
      "z_j Mean: 0.01203933171927929, Std: 0.08180347830057144\n",
      "Similarities: tensor([[1.3692, 0.0000, 0.0000, 0.0000, 0.8266],\n",
      "        [0.0000, 0.9605, 0.9031, 0.0000, 0.0923],\n",
      "        [0.0000, 1.4187, 1.4286, 0.0000, 0.0846],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4286, 0.0000],\n",
      "        [0.4649, 0.0140, 0.0000, 0.0000, 1.1904]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2427496910095215\n",
      "Batch 102, Loss: 3.2427496910095215\n",
      "Processing Batch 103/157\n",
      "Batch 103 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01101021096110344, Std: 0.08120004087686539\n",
      "z_j Mean: 0.01056178379803896, Std: 0.07897383719682693\n",
      "Similarities: tensor([[1.3869, 0.0000, 0.6070, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4286, 0.0000, 0.0000, 0.0000],\n",
      "        [0.6052, 0.0000, 1.3994, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.2596, 0.5929],\n",
      "        [0.0000, 0.0000, 0.0000, 0.3337, 1.3319]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2733330726623535\n",
      "Batch 103, Loss: 3.2733330726623535\n",
      "Processing Batch 104/157\n",
      "Batch 104 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013077687472105026, Std: 0.08671978861093521\n",
      "z_j Mean: 0.012963229790329933, Std: 0.08531783521175385\n",
      "Similarities: tensor([[1.4190, 0.8242, 1.1900, 0.0000, 0.0000],\n",
      "        [0.7611, 1.4224, 0.5348, 0.0000, 0.0137],\n",
      "        [1.3074, 0.7803, 1.3912, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3635, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.1531]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1759722232818604\n",
      "Batch 104, Loss: 3.1759722232818604\n",
      "Processing Batch 105/157\n",
      "Batch 105 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012810874730348587, Std: 0.08243007957935333\n",
      "z_j Mean: 0.013102669268846512, Std: 0.08385300636291504\n",
      "Similarities: tensor([[1.4277, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4102, 0.0000, 0.1299, 0.1613],\n",
      "        [0.0000, 0.0000, 1.4093, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0813, 0.0000, 1.4001, 1.3666],\n",
      "        [0.0000, 0.0820, 0.0000, 1.2729, 1.3982]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.26141357421875\n",
      "Batch 105, Loss: 3.26141357421875\n",
      "Processing Batch 106/157\n",
      "Batch 106 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012593703344464302, Std: 0.0853731632232666\n",
      "z_j Mean: 0.012723682448267937, Std: 0.08535388857126236\n",
      "Similarities: tensor([[1.3339, 0.0000, 0.0000, 0.6415, 1.0858],\n",
      "        [0.0000, 1.4031, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3037, 0.0000, 0.0000],\n",
      "        [0.2885, 0.0000, 0.0783, 1.3281, 1.0379],\n",
      "        [1.0918, 0.0000, 0.0000, 0.9730, 1.3758]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1952342987060547\n",
      "Batch 106, Loss: 3.1952342987060547\n",
      "Processing Batch 107/157\n",
      "Batch 107 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012638624757528305, Std: 0.08319367468357086\n",
      "z_j Mean: 0.012921404093503952, Std: 0.0831502228975296\n",
      "Similarities: tensor([[1.3958, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4286, 0.0000, 0.0000, 1.4286],\n",
      "        [0.0000, 0.0000, 1.4286, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4235, 0.0000],\n",
      "        [0.0000, 1.4286, 0.0000, 0.0000, 1.4286]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1884689331054688\n",
      "Batch 107, Loss: 3.1884689331054688\n",
      "Processing Batch 108/157\n",
      "Batch 108 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011879823170602322, Std: 0.0840349942445755\n",
      "z_j Mean: 0.01194178406149149, Std: 0.08329657465219498\n",
      "Similarities: tensor([[1.3872, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.2851, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4286, 0.0000, 1.0960],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4286, 0.0000],\n",
      "        [0.0000, 0.0000, 1.0928, 0.0000, 1.4275]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.21463942527771\n",
      "Batch 108, Loss: 3.21463942527771\n",
      "Processing Batch 109/157\n",
      "Batch 109 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013837536796927452, Std: 0.08446071296930313\n",
      "z_j Mean: 0.013347374275326729, Std: 0.0823449045419693\n",
      "Similarities: tensor([[1.4222, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4286, 0.3179, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3748, 1.3943, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.2399, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4286]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2289979457855225\n",
      "Batch 109, Loss: 3.2289979457855225\n",
      "Processing Batch 110/157\n",
      "Batch 110 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012380942702293396, Std: 0.08249575644731522\n",
      "z_j Mean: 0.012461718171834946, Std: 0.08248360455036163\n",
      "Similarities: tensor([[1.4286, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [1.4286, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3448, 0.7177, 0.0023],\n",
      "        [0.0000, 0.0000, 0.8325, 1.4115, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3861]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2657904624938965\n",
      "Batch 110, Loss: 3.2657904624938965\n",
      "Processing Batch 111/157\n",
      "Batch 111 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013816935941576958, Std: 0.08446408063173294\n",
      "z_j Mean: 0.01394779421389103, Std: 0.08444257080554962\n",
      "Similarities: tensor([[1.4286, 0.0000, 0.0000, 0.0000, 1.2809],\n",
      "        [0.0000, 1.4216, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3804, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4203, 0.3918],\n",
      "        [0.9579, 0.0000, 0.0000, 0.7421, 1.3282]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2127225399017334\n",
      "Batch 111, Loss: 3.2127225399017334\n",
      "Processing Batch 112/157\n",
      "Batch 112 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01253546867519617, Std: 0.08320928364992142\n",
      "z_j Mean: 0.012236492708325386, Std: 0.08251731097698212\n",
      "Similarities: tensor([[1.4280, 0.0963, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0988, 1.4137, 0.0000, 0.8481, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4286, 0.0000, 1.4286],\n",
      "        [0.0000, 0.5961, 0.0000, 1.3823, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4286, 0.0000, 1.4286]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2414069175720215\n",
      "Batch 112, Loss: 3.2414069175720215\n",
      "Processing Batch 113/157\n",
      "Batch 113 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01476303767412901, Std: 0.0843038484454155\n",
      "z_j Mean: 0.014375060796737671, Std: 0.08291125297546387\n",
      "Similarities: tensor([[1.4286, 0.0000, 0.0000, 1.1461, 0.0000],\n",
      "        [0.0000, 1.4002, 1.0692, 0.0000, 0.3134],\n",
      "        [0.0000, 1.0234, 1.3271, 0.0000, 0.5584],\n",
      "        [0.7903, 0.0000, 0.0000, 0.8539, 0.0000],\n",
      "        [0.0000, 0.3345, 0.4050, 0.0000, 1.4198]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.226135730743408\n",
      "Batch 113, Loss: 3.226135730743408\n",
      "Processing Batch 114/157\n",
      "Batch 114 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.014085624366998672, Std: 0.0836934819817543\n",
      "z_j Mean: 0.014387451112270355, Std: 0.08508920669555664\n",
      "Similarities: tensor([[1.4285, 0.0000, 0.0000, 1.1894, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3971, 0.1117, 0.0403],\n",
      "        [1.1902, 0.0000, 0.0701, 1.4261, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0154, 0.0000, 1.3389]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2258052825927734\n",
      "Batch 114, Loss: 3.2258052825927734\n",
      "Processing Batch 115/157\n",
      "Batch 115 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01577446609735489, Std: 0.0848429948091507\n",
      "z_j Mean: 0.015893060714006424, Std: 0.08482085913419724\n",
      "Similarities: tensor([[0.0537, 0.0481, 0.0000, 0.2992, 0.0000],\n",
      "        [0.0000, 1.4264, 0.0000, 0.4497, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4217, 0.0000, 0.0000],\n",
      "        [0.0000, 0.4865, 0.0000, 1.3949, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4201]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1616530418395996\n",
      "Batch 115, Loss: 3.1616530418395996\n",
      "Processing Batch 116/157\n",
      "Batch 116 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01402375753968954, Std: 0.08657181262969971\n",
      "z_j Mean: 0.013791973702609539, Std: 0.08518777042627335\n",
      "Similarities: tensor([[1.3532, 1.0573, 0.1037, 0.0000, 0.0000],\n",
      "        [0.2911, 0.7481, 0.9882, 0.0000, 0.7721],\n",
      "        [0.0758, 0.3156, 1.3275, 0.2000, 0.2445],\n",
      "        [0.0000, 0.0000, 0.0000, 0.7268, 0.0000],\n",
      "        [0.0000, 0.1344, 0.1514, 0.0000, 1.1349]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.160855531692505\n",
      "Batch 116, Loss: 3.160855531692505\n",
      "Processing Batch 117/157\n",
      "Batch 117 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013075842522084713, Std: 0.0867200642824173\n",
      "z_j Mean: 0.012683754786849022, Std: 0.0853598341345787\n",
      "Similarities: tensor([[1.3478, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4243, 0.8062, 0.0000],\n",
      "        [0.0000, 0.0000, 0.6025, 1.2236, 0.0000],\n",
      "        [0.0000, 0.0000, 0.8250, 0.4669, 1.1645]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1755168437957764\n",
      "Batch 117, Loss: 3.1755168437957764\n",
      "Processing Batch 118/157\n",
      "Batch 118 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013342900201678276, Std: 0.08525928109884262\n",
      "z_j Mean: 0.0133628249168396, Std: 0.08453712612390518\n",
      "Similarities: tensor([[1.4286, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3997, 0.9653, 0.0000, 0.0000],\n",
      "        [0.0000, 1.0654, 1.4214, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4118, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3915]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1426048278808594\n",
      "Batch 118, Loss: 3.1426048278808594\n",
      "Processing Batch 119/157\n",
      "Batch 119 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013008251786231995, Std: 0.08602353185415268\n",
      "z_j Mean: 0.014119815081357956, Std: 0.08655619621276855\n",
      "Similarities: tensor([[0.9991, 0.0000, 0.0000, 0.0472, 0.0000],\n",
      "        [0.0000, 1.3406, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0480, 1.3490, 0.0644, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0165, 1.3361, 0.2908],\n",
      "        [0.0000, 0.0000, 0.0000, 0.2044, 1.4203]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.137141704559326\n",
      "Batch 119, Loss: 3.137141704559326\n",
      "Processing Batch 120/157\n",
      "Batch 120 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013123095035552979, Std: 0.08457467705011368\n",
      "z_j Mean: 0.012682631611824036, Std: 0.08318697661161423\n",
      "Similarities: tensor([[1.3952, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4264, 0.4016, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1457, 1.1980, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3493]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2095632553100586\n",
      "Batch 120, Loss: 3.2095632553100586\n",
      "Processing Batch 121/157\n",
      "Batch 121 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013216997496783733, Std: 0.08669865876436234\n",
      "z_j Mean: 0.013099316507577896, Std: 0.08600971102714539\n",
      "Similarities: tensor([[1.4145, 0.0695, 0.0944, 0.0000, 0.3602],\n",
      "        [0.1461, 1.4189, 0.0758, 1.0334, 0.2895],\n",
      "        [0.1620, 0.0917, 1.4060, 0.0000, 0.3211],\n",
      "        [0.0000, 0.4805, 0.0000, 1.2033, 0.0000],\n",
      "        [0.5084, 0.1944, 0.2640, 0.0000, 1.4245]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1532626152038574\n",
      "Batch 121, Loss: 3.1532626152038574\n",
      "Processing Batch 122/157\n",
      "Batch 122 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.014695525169372559, Std: 0.08716346323490143\n",
      "z_j Mean: 0.014234032481908798, Std: 0.08724001795053482\n",
      "Similarities: tensor([[1.4285, 0.0284, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0581, 1.4207, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.1886, 0.4609, 0.0000],\n",
      "        [0.0000, 0.0000, 0.3598, 1.1065, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3891]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0815000534057617\n",
      "Batch 122, Loss: 3.0815000534057617\n",
      "Processing Batch 123/157\n",
      "Batch 123 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012532523833215237, Std: 0.08680026233196259\n",
      "z_j Mean: 0.012545371428132057, Std: 0.08679840713739395\n",
      "Similarities: tensor([[1.4250, 0.0000, 0.0000, 0.1470, 1.0026],\n",
      "        [0.0000, 1.3795, 0.0000, 0.3234, 0.0000],\n",
      "        [0.0000, 0.0000, 1.2959, 0.2646, 0.0000],\n",
      "        [0.0000, 0.5241, 0.0000, 1.2584, 0.0000],\n",
      "        [0.5631, 0.0000, 0.0000, 0.2907, 1.1998]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.13508939743042\n",
      "Batch 123, Loss: 3.13508939743042\n",
      "Processing Batch 124/157\n",
      "Batch 124 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012871171347796917, Std: 0.08675068616867065\n",
      "z_j Mean: 0.01289415918290615, Std: 0.08674726635217667\n",
      "Similarities: tensor([[1.4086, 0.0000, 0.5477, 0.0000, 0.0050],\n",
      "        [0.0000, 1.4279, 0.0000, 0.0000, 0.5422],\n",
      "        [0.5412, 0.0000, 1.4177, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4182, 0.0000],\n",
      "        [0.0000, 0.1869, 0.0000, 0.0000, 1.3663]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.122732162475586\n",
      "Batch 124, Loss: 3.122732162475586\n",
      "Processing Batch 125/157\n",
      "Batch 125 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.014706523157656193, Std: 0.08645842969417572\n",
      "z_j Mean: 0.01474963128566742, Std: 0.08645108342170715\n",
      "Similarities: tensor([[0.8337, 0.0000, 0.0000, 0.2515, 0.4850],\n",
      "        [0.0000, 1.2558, 0.0000, 0.6546, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4168, 0.0000, 0.0000],\n",
      "        [0.0195, 0.6441, 0.0000, 1.4240, 0.0000],\n",
      "        [0.0300, 0.0000, 0.0000, 0.9448, 0.0000]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.151315927505493\n",
      "Batch 125, Loss: 3.151315927505493\n",
      "Processing Batch 126/157\n",
      "Batch 126 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013232031837105751, Std: 0.08527655154466629\n",
      "z_j Mean: 0.013448284938931465, Std: 0.08666308969259262\n",
      "Similarities: tensor([[1.3735, 0.9012, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3847, 0.0247, 0.0000],\n",
      "        [0.0065, 0.0000, 0.0000, 1.3941, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3331]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2032158374786377\n",
      "Batch 126, Loss: 3.2032158374786377\n",
      "Processing Batch 127/157\n",
      "Batch 127 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012378860265016556, Std: 0.08682230859994888\n",
      "z_j Mean: 0.012878037057816982, Std: 0.08745049685239792\n",
      "Similarities: tensor([[1.4026, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4284, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4265, 0.2585, 0.1232],\n",
      "        [0.0000, 0.0000, 0.3063, 1.3874, 0.0923],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3955]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.113447904586792\n",
      "Batch 127, Loss: 3.113447904586792\n",
      "Processing Batch 128/157\n",
      "Batch 128 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.014867574907839298, Std: 0.08713427186012268\n",
      "z_j Mean: 0.014781944453716278, Std: 0.08714884519577026\n",
      "Similarities: tensor([[1.1977, 0.0000, 0.4026, 0.5641, 0.0000],\n",
      "        [0.0000, 1.3933, 0.0000, 0.0000, 0.8375],\n",
      "        [0.6361, 0.0000, 1.3401, 0.0521, 0.0000],\n",
      "        [0.2233, 0.0000, 0.1550, 1.4091, 0.0000],\n",
      "        [0.0000, 0.8920, 0.0000, 0.0000, 1.4136]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1196250915527344\n",
      "Batch 128, Loss: 3.1196250915527344\n",
      "Processing Batch 129/157\n",
      "Batch 129 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012379646301269531, Std: 0.08468670397996902\n",
      "z_j Mean: 0.01278688758611679, Std: 0.08605671674013138\n",
      "Similarities: tensor([[1.3873, 0.9958, 0.1841, 1.1332, 0.0000],\n",
      "        [1.2462, 1.2358, 0.1490, 1.0073, 0.0000],\n",
      "        [0.5395, 0.3343, 1.3527, 0.4433, 0.0000],\n",
      "        [1.1968, 0.4925, 0.1329, 1.4215, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4263]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.199025869369507\n",
      "Batch 129, Loss: 3.199025869369507\n",
      "Processing Batch 130/157\n",
      "Batch 130 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013936329632997513, Std: 0.08728807419538498\n",
      "z_j Mean: 0.013613530434668064, Std: 0.08663728088140488\n",
      "Similarities: tensor([[0.0000, 1.3718, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3718, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3524, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4212, 1.4184],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4093, 1.4263]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1068837642669678\n",
      "Batch 130, Loss: 3.1068837642669678\n",
      "Processing Batch 131/157\n",
      "Batch 131 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013611173257231712, Std: 0.08663764595985413\n",
      "z_j Mean: 0.013178639113903046, Std: 0.08528482168912888\n",
      "Similarities: tensor([[1.4038, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4286, 1.4286, 0.0000, 0.7069],\n",
      "        [0.0000, 1.4286, 1.4286, 0.0000, 0.7069],\n",
      "        [0.0750, 0.0000, 0.0000, 1.4053, 1.1547],\n",
      "        [0.0518, 0.7570, 0.7570, 1.1710, 1.3803]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.171213150024414\n",
      "Batch 131, Loss: 3.171213150024414\n",
      "Processing Batch 132/157\n",
      "Batch 132 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01241263560950756, Std: 0.08751777559518814\n",
      "z_j Mean: 0.012268759310245514, Std: 0.08753807097673416\n",
      "Similarities: tensor([[1.4057, 1.2966, 0.0000, 0.0116, 0.0000],\n",
      "        [1.3168, 1.4061, 0.0000, 0.0306, 0.0912],\n",
      "        [0.0000, 0.0000, 1.3721, 0.0000, 0.0000],\n",
      "        [0.0483, 0.1370, 0.0000, 1.3040, 0.0000],\n",
      "        [0.0708, 0.2479, 0.0000, 0.0265, 1.3417]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.117586374282837\n",
      "Batch 132, Loss: 3.117586374282837\n",
      "Processing Batch 133/157\n",
      "Batch 133 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012147128582000732, Std: 0.08614936470985413\n",
      "z_j Mean: 0.012039536610245705, Std: 0.0854530856013298\n",
      "Similarities: tensor([[1.4086, 0.0000, 0.0000, 0.0262, 0.0000],\n",
      "        [0.0000, 1.4230, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4255, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4120, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4286]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1697607040405273\n",
      "Batch 133, Loss: 3.1697607040405273\n",
      "Processing Batch 134/157\n",
      "Batch 134 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.014451625756919384, Std: 0.08650141209363937\n",
      "z_j Mean: 0.014421330764889717, Std: 0.08579792082309723\n",
      "Similarities: tensor([[1.4261, 0.0159, 0.1976, 0.0000, 0.3185],\n",
      "        [0.0090, 1.4212, 0.0000, 0.0000, 0.0968],\n",
      "        [0.1813, 0.0000, 1.4101, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4286, 0.0000],\n",
      "        [0.4751, 0.0000, 0.0000, 0.0000, 1.2147]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1324639320373535\n",
      "Batch 134, Loss: 3.1324639320373535\n",
      "Processing Batch 135/157\n",
      "Batch 135 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013212844729423523, Std: 0.08527952432632446\n",
      "z_j Mean: 0.013066430576145649, Std: 0.08458344638347626\n",
      "Similarities: tensor([[1.4082, 0.2180, 0.0000, 1.0568, 0.0816],\n",
      "        [0.3841, 1.2197, 0.0000, 0.0745, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4107, 0.0000, 0.0000],\n",
      "        [1.1626, 0.0458, 0.0000, 1.4142, 0.2654],\n",
      "        [0.0941, 0.0000, 0.0000, 0.2536, 1.3943]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.193284034729004\n",
      "Batch 135, Loss: 3.193284034729004\n",
      "Processing Batch 136/157\n",
      "Batch 136 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01337924599647522, Std: 0.08596660941839218\n",
      "z_j Mean: 0.01398005336523056, Std: 0.08657888323068619\n",
      "Similarities: tensor([[1.4285, 0.0000, 0.0000, 0.0000, 0.7617],\n",
      "        [0.0000, 1.4166, 0.0000, 1.1979, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4161, 0.0000, 0.0000],\n",
      "        [0.0000, 0.9559, 0.0000, 1.4104, 0.0000],\n",
      "        [0.0675, 0.0000, 0.0000, 0.0000, 1.2388]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1598994731903076\n",
      "Batch 136, Loss: 3.1598994731903076\n",
      "Processing Batch 137/157\n",
      "Batch 137 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01279480941593647, Std: 0.08390053361654282\n",
      "z_j Mean: 0.012708278372883797, Std: 0.08318306505680084\n",
      "Similarities: tensor([[1.1874, 0.1688, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4082, 1.3888, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4256, 0.0000, 0.7135],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3933, 0.0668],\n",
      "        [0.0000, 0.0000, 0.7944, 0.0318, 1.4145]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.253918409347534\n",
      "Batch 137, Loss: 3.253918409347534\n",
      "Processing Batch 138/157\n",
      "Batch 138 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011661224998533726, Std: 0.08035297691822052\n",
      "z_j Mean: 0.01175096444785595, Std: 0.08109614998102188\n",
      "Similarities: tensor([[1.3781, 0.0000, 0.0000, 1.4252, 0.0000],\n",
      "        [0.0000, 1.3993, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4257, 0.0000, 0.0000],\n",
      "        [1.2741, 0.0000, 0.0000, 1.4158, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.243593454360962\n",
      "Batch 138, Loss: 3.243593454360962\n",
      "Processing Batch 139/157\n",
      "Batch 139 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012867577373981476, Std: 0.08242125064134598\n",
      "z_j Mean: 0.013013450428843498, Std: 0.08165416121482849\n",
      "Similarities: tensor([[1.3887, 0.6716, 0.0000, 0.7914, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4286, 0.0000, 0.0000],\n",
      "        [0.6140, 0.9542, 0.0000, 1.0191, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2882559299468994\n",
      "Batch 139, Loss: 3.2882559299468994\n",
      "Processing Batch 140/157\n",
      "Batch 140 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011393781751394272, Std: 0.08189589530229568\n",
      "z_j Mean: 0.011347309686243534, Std: 0.08039791136980057\n",
      "Similarities: tensor([[1.3376, 0.1042, 0.0000, 1.1667, 0.0000],\n",
      "        [0.1051, 1.4204, 0.0000, 0.4593, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4261, 0.0000, 0.0000],\n",
      "        [1.1361, 0.6509, 0.0000, 1.3863, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4286]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2418837547302246\n",
      "Batch 140, Loss: 3.2418837547302246\n",
      "Processing Batch 141/157\n",
      "Batch 141 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.010759265162050724, Std: 0.07894716411828995\n",
      "z_j Mean: 0.010740426369011402, Std: 0.08048124611377716\n",
      "Similarities: tensor([[1.4138, 0.0000, 0.3523, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.6264, 0.0000, 1.4233, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4044]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.3080062866210938\n",
      "Batch 141, Loss: 3.3080062866210938\n",
      "Processing Batch 142/157\n",
      "Batch 142 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012220462784171104, Std: 0.08470983058214188\n",
      "z_j Mean: 0.01224590465426445, Std: 0.0839824229478836\n",
      "Similarities: tensor([[1.4070, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4282, 0.0023, 0.0000, 0.0345],\n",
      "        [0.0000, 0.0000, 1.4262, 0.0000, 0.0171],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1784346103668213\n",
      "Batch 142, Loss: 3.1784346103668213\n",
      "Processing Batch 143/157\n",
      "Batch 143 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.010569158010184765, Std: 0.0812586322426796\n",
      "z_j Mean: 0.010656960308551788, Std: 0.08124716579914093\n",
      "Similarities: tensor([[1.4221, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4283, 1.1202, 0.0000, 0.0000],\n",
      "        [0.0000, 1.0328, 1.1990, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4286, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2596545219421387\n",
      "Batch 143, Loss: 3.2596545219421387\n",
      "Processing Batch 144/157\n",
      "Batch 144 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.009646188467741013, Std: 0.08061973005533218\n",
      "z_j Mean: 0.010152000933885574, Std: 0.0820591002702713\n",
      "Similarities: tensor([[1.3860, 1.1050, 0.0000, 0.9724, 0.8091],\n",
      "        [1.2830, 1.4102, 0.0000, 1.4098, 1.1731],\n",
      "        [0.0000, 0.0000, 1.4283, 0.0000, 0.0000],\n",
      "        [1.1785, 1.3868, 0.0000, 1.4286, 1.1887],\n",
      "        [0.9826, 1.1562, 0.0000, 1.1910, 1.4286]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2695703506469727\n",
      "Batch 144, Loss: 3.2695703506469727\n",
      "Processing Batch 145/157\n",
      "Batch 145 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.009892415255308151, Std: 0.0805898904800415\n",
      "z_j Mean: 0.00951545313000679, Std: 0.077548086643219\n",
      "Similarities: tensor([[1.4281, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3630, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.4053633213043213\n",
      "Batch 145, Loss: 3.4053633213043213\n",
      "Processing Batch 146/157\n",
      "Batch 146 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.009934262372553349, Std: 0.08282605558633804\n",
      "z_j Mean: 0.009857799857854843, Std: 0.08209496736526489\n",
      "Similarities: tensor([[1.4220, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4259, 0.0000, 0.9172, 0.2486],\n",
      "        [0.0000, 0.0000, 1.4286, 0.0000, 0.0000],\n",
      "        [0.0000, 0.9649, 0.0000, 1.4284, 1.0142],\n",
      "        [0.0000, 0.2939, 0.0000, 1.0201, 1.4281]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2651891708374023\n",
      "Batch 146, Loss: 3.2651891708374023\n",
      "Processing Batch 147/157\n",
      "Batch 147 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.010993726551532745, Std: 0.08269207924604416\n",
      "z_j Mean: 0.011391290463507175, Std: 0.08263824880123138\n",
      "Similarities: tensor([[1.3947, 0.0000, 0.1717, 1.3933, 0.2559],\n",
      "        [1.1635, 0.0000, 0.0000, 1.2865, 0.0000],\n",
      "        [0.2893, 0.0000, 1.3964, 0.0000, 0.6937],\n",
      "        [1.2920, 0.0000, 0.0000, 1.4286, 0.0000],\n",
      "        [0.5587, 0.0000, 0.7988, 0.0587, 1.4045]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2995920181274414\n",
      "Batch 147, Loss: 3.2995920181274414\n",
      "Processing Batch 148/157\n",
      "Batch 148 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.010267402045428753, Std: 0.08278541266918182\n",
      "z_j Mean: 0.009733959101140499, Std: 0.08060918003320694\n",
      "Similarities: tensor([[1.4268, 0.0000, 0.0000, 0.0000, 0.7564],\n",
      "        [0.0000, 1.4217, 1.3011, 0.0000, 0.0000],\n",
      "        [0.0000, 1.2655, 1.4215, 0.2208, 0.0000],\n",
      "        [0.0000, 0.0754, 0.4226, 1.2361, 0.0000],\n",
      "        [0.8803, 0.0000, 0.0000, 0.0000, 1.4225]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.317307472229004\n",
      "Batch 148, Loss: 3.317307472229004\n",
      "Processing Batch 149/157\n",
      "Batch 149 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01060730591416359, Std: 0.08274254947900772\n",
      "z_j Mean: 0.010601334273815155, Std: 0.08274330943822861\n",
      "Similarities: tensor([[1.4286, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.1941, 0.6790, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.9768, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3097, 0.0000],\n",
      "        [0.0000, 0.1041, 0.0000, 0.0000, 1.3446]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.3312337398529053\n",
      "Batch 149, Loss: 3.3312337398529053\n",
      "Processing Batch 150/157\n",
      "Batch 150 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.009514832869172096, Std: 0.07675696909427643\n",
      "z_j Mean: 0.009868675842881203, Std: 0.07828756421804428\n",
      "Similarities: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4286, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.1960, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.4149630069732666\n",
      "Batch 150, Loss: 3.4149630069732666\n",
      "Processing Batch 151/157\n",
      "Batch 151 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.010507350787520409, Std: 0.08275529742240906\n",
      "z_j Mean: 0.010238127782940865, Std: 0.07978524267673492\n",
      "Similarities: tensor([[1.4285, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4286, 1.4286, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4286, 1.4286, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3987, 0.0164],\n",
      "        [0.0000, 0.0000, 0.0000, 0.1107, 1.4286]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.323528528213501\n",
      "Batch 151, Loss: 3.323528528213501\n",
      "Processing Batch 152/157\n",
      "Batch 152 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.010091284289956093, Std: 0.0798039510846138\n",
      "z_j Mean: 0.010052518919110298, Std: 0.07980883866548538\n",
      "Similarities: tensor([[1.0542, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4270, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.7439, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4286]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.3455970287323\n",
      "Batch 152, Loss: 3.3455970287323\n",
      "Processing Batch 153/157\n",
      "Batch 153 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.010038698092103004, Std: 0.07981058210134506\n",
      "z_j Mean: 0.009914778172969818, Std: 0.07828173786401749\n",
      "Similarities: tensor([[1.3929, 0.6407, 0.0000, 0.0000, 0.8433],\n",
      "        [0.7597, 1.4175, 0.0000, 0.0000, 1.3475],\n",
      "        [0.0000, 0.0000, 1.4047, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4184, 0.0000],\n",
      "        [0.9276, 1.3579, 0.0000, 0.0291, 1.4161]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.320686101913452\n",
      "Batch 153, Loss: 3.320686101913452\n",
      "Processing Batch 154/157\n",
      "Batch 154 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01022929698228836, Std: 0.08130212128162384\n",
      "z_j Mean: 0.009901217184960842, Std: 0.07982775568962097\n",
      "Similarities: tensor([[1.4286, 0.0000, 0.0000, 1.4286, 0.0000],\n",
      "        [0.0000, 1.4286, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.2438]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.370648145675659\n",
      "Batch 154, Loss: 3.370648145675659\n",
      "Processing Batch 155/157\n",
      "Batch 155 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.009777411818504333, Std: 0.07751549035310745\n",
      "z_j Mean: 0.010307662189006805, Std: 0.07977628707885742\n",
      "Similarities: tensor([[1.4201, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4286, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3894, 1.3198],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.3847978115081787\n",
      "Batch 155, Loss: 3.3847978115081787\n",
      "Processing Batch 156/157\n",
      "Batch 156 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.009593045338988304, Std: 0.07513965666294098\n",
      "z_j Mean: 0.009655620902776718, Std: 0.07593976706266403\n",
      "Similarities: tensor([[1.3848, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3498, 0.3396, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.0341, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4271]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.3620760440826416\n",
      "Batch 156, Loss: 3.3620760440826416\n",
      "Processing Batch 157/157\n",
      "Batch 157 loaded: x.shape=torch.Size([16, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([16, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([16, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([16, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([16, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([16, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.010984677821397781, Std: 0.08489448577165604\n",
      "z_j Mean: 0.01213337853550911, Std: 0.08757296949625015\n",
      "Similarities: tensor([[1.3205, 0.7968, 0.0000, 1.1558, 0.0000],\n",
      "        [1.2299, 1.2781, 0.0000, 1.2094, 0.0000],\n",
      "        [0.0000, 0.0000, 1.2801, 0.0000, 0.0000],\n",
      "        [1.1018, 1.1148, 0.0000, 1.4262, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4286]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 1.8699153661727905\n",
      "Batch 157, Loss: 1.8699153661727905\n",
      "Epoch 2/10, Loss: 3.1887\n",
      "Entered Epoch 3\n",
      "Starting Training Loop with 157 batches.\n",
      "Processing Batch 1/157\n",
      "Batch 1 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011473298072814941, Std: 0.0796169862151146\n",
      "z_j Mean: 0.011463934555649757, Std: 0.0803813636302948\n",
      "Similarities: tensor([[1.2867, 0.0000, 0.5955, 0.0000, 1.2007],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3647, 0.0000, 1.3867, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3135, 0.0000, 0.0454, 0.0000, 0.6398]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.3435068130493164\n",
      "Batch 1, Loss: 3.3435068130493164\n",
      "Processing Batch 2/157\n",
      "Batch 2 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.010232638567686081, Std: 0.07745671272277832\n",
      "z_j Mean: 0.010786985047161579, Std: 0.0773814395070076\n",
      "Similarities: tensor([[1.4080, 0.3801, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0968, 1.3881, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4286, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.3932859897613525\n",
      "Batch 2, Loss: 3.3932859897613525\n",
      "Processing Batch 3/157\n",
      "Batch 3 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011476187966763973, Std: 0.08113549649715424\n",
      "z_j Mean: 0.011529850773513317, Std: 0.08187684416770935\n",
      "Similarities: tensor([[1.4114, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1761, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4286, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1128, 0.0000, 1.4202, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4286]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.3161730766296387\n",
      "Batch 3, Loss: 3.3161730766296387\n",
      "Processing Batch 4/157\n",
      "Batch 4 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.010615559294819832, Std: 0.07896661758422852\n",
      "z_j Mean: 0.010963277891278267, Std: 0.08045118302106857\n",
      "Similarities: tensor([[1.4086, 0.0000, 0.0000, 0.0000, 0.5172],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4286, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.6390, 0.0000, 0.0000, 0.0000, 1.3558]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.3172435760498047\n",
      "Batch 4, Loss: 3.3172435760498047\n",
      "Processing Batch 5/157\n",
      "Batch 5 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.009303871542215347, Std: 0.07435958087444305\n",
      "z_j Mean: 0.008965429849922657, Std: 0.07357612252235413\n",
      "Similarities: tensor([[1.4286, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4026, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4286, 1.0211],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0191, 1.4265]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.449800968170166\n",
      "Batch 5, Loss: 3.449800968170166\n",
      "Processing Batch 6/157\n",
      "Batch 6 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011438900604844093, Std: 0.08188960701227188\n",
      "z_j Mean: 0.01153326965868473, Std: 0.08187636733055115\n",
      "Similarities: tensor([[1.1567, 1.1522, 0.0000, 0.0000, 0.0000],\n",
      "        [0.8165, 1.3064, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4089, 0.5122, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.1073, 0.0000, 1.4235]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.277784585952759\n",
      "Batch 6, Loss: 3.277784585952759\n",
      "Processing Batch 7/157\n",
      "Batch 7 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.009654700756072998, Std: 0.07673949748277664\n",
      "z_j Mean: 0.010745702311396599, Std: 0.08123547583818436\n",
      "Similarities: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4265, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4142, 0.0000, 0.7350],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4234, 0.0000],\n",
      "        [0.0000, 0.0000, 0.7452, 0.0000, 1.3976]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.330238103866577\n",
      "Batch 7, Loss: 3.330238103866577\n",
      "Processing Batch 8/157\n",
      "Batch 8 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.010156788863241673, Std: 0.07902694493532181\n",
      "z_j Mean: 0.009927095845341682, Std: 0.07828018069267273\n",
      "Similarities: tensor([[1.4032, 1.0751, 0.8154, 0.0000, 0.1761],\n",
      "        [0.8044, 1.3938, 0.3518, 0.0000, 0.0000],\n",
      "        [0.9564, 0.4838, 1.4254, 0.0000, 0.1893],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4286, 0.0000],\n",
      "        [0.4147, 0.0000, 0.3428, 0.0000, 1.3288]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.355027437210083\n",
      "Batch 8, Loss: 3.355027437210083\n",
      "Processing Batch 9/157\n",
      "Batch 9 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011225219815969467, Std: 0.082660973072052\n",
      "z_j Mean: 0.011273530311882496, Std: 0.08116389811038971\n",
      "Similarities: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4284, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4244, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0209, 0.0000, 1.4286, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3046]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.295114755630493\n",
      "Batch 9, Loss: 3.295114755630493\n",
      "Processing Batch 10/157\n",
      "Batch 10 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.009979818016290665, Std: 0.0774896889925003\n",
      "z_j Mean: 0.010361576452851295, Std: 0.07664724439382553\n",
      "Similarities: tensor([[1.2937, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4286, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4272, 0.0000, 0.3271],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4286, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0924, 0.0000, 0.8756]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.390526533126831\n",
      "Batch 10, Loss: 3.390526533126831\n",
      "Processing Batch 11/157\n",
      "Batch 11 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.009608553722500801, Std: 0.07594573497772217\n",
      "z_j Mean: 0.009903158992528915, Std: 0.07749952375888824\n",
      "Similarities: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.9036, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3603, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4286, 1.4286]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.3867878913879395\n",
      "Batch 11, Loss: 3.3867878913879395\n",
      "Processing Batch 12/157\n",
      "Batch 12 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.010966189205646515, Std: 0.0796884223818779\n",
      "z_j Mean: 0.011233208701014519, Std: 0.08041393756866455\n",
      "Similarities: tensor([[1.4113, 0.0000, 0.0000, 0.2600, 1.2075],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4286, 1.4286, 0.0000, 0.0000],\n",
      "        [0.3851, 0.0000, 0.0000, 1.4008, 0.4111],\n",
      "        [1.0744, 0.0000, 0.0000, 0.3075, 1.4286]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.34934139251709\n",
      "Batch 12, Loss: 3.34934139251709\n",
      "Processing Batch 13/157\n",
      "Batch 13 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.00889488123357296, Std: 0.074409618973732\n",
      "z_j Mean: 0.00903122965246439, Std: 0.07520925253629684\n",
      "Similarities: tensor([[1.3948, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4286, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4286, 1.4286, 0.0000],\n",
      "        [0.0000, 0.9153, 1.0969, 1.0969, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.3928351402282715\n",
      "Batch 13, Loss: 3.3928351402282715\n",
      "Processing Batch 14/157\n",
      "Batch 14 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012189824134111404, Std: 0.08252421766519547\n",
      "z_j Mean: 0.011916451156139374, Std: 0.08107199519872665\n",
      "Similarities: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4250, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4284, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3806, 0.3916],\n",
      "        [0.0000, 0.0000, 0.0000, 0.4709, 1.4136]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.277062177658081\n",
      "Batch 14, Loss: 3.277062177658081\n",
      "Processing Batch 15/157\n",
      "Batch 15 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011095458641648293, Std: 0.08043306320905685\n",
      "z_j Mean: 0.011188952252268791, Std: 0.0804200991988182\n",
      "Similarities: tensor([[1.3858, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4285, 1.3118, 0.0000, 1.4062],\n",
      "        [0.0000, 1.4153, 1.3808, 0.0000, 1.4280],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4176, 1.3760, 0.0000, 1.4273]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.3930745124816895\n",
      "Batch 15, Loss: 3.3930745124816895\n",
      "Processing Batch 16/157\n",
      "Batch 16 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.010999946855008602, Std: 0.08194972574710846\n",
      "z_j Mean: 0.011132478713989258, Std: 0.08193182945251465\n",
      "Similarities: tensor([[1.4286, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3960, 0.0000, 0.0000, 0.3445],\n",
      "        [0.0000, 0.0000, 1.4286, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4286, 0.0000],\n",
      "        [0.0000, 0.6506, 0.0000, 0.0000, 1.3197]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.241668701171875\n",
      "Batch 16, Loss: 3.241668701171875\n",
      "Processing Batch 17/157\n",
      "Batch 17 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011903990991413593, Std: 0.0810738205909729\n",
      "z_j Mean: 0.012509421445429325, Std: 0.083213210105896\n",
      "Similarities: tensor([[1.3487, 0.2893, 0.0000, 1.1426, 0.0000],\n",
      "        [0.0000, 1.4036, 0.0000, 0.0445, 0.0000],\n",
      "        [0.0000, 0.0000, 1.1507, 0.0000, 0.6787],\n",
      "        [1.2188, 0.0000, 0.0000, 1.4160, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4286]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.3094708919525146\n",
      "Batch 17, Loss: 3.3094708919525146\n",
      "Processing Batch 18/157\n",
      "Batch 18 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011051849462091923, Std: 0.07890673726797104\n",
      "z_j Mean: 0.011401792988181114, Std: 0.07962725311517715\n",
      "Similarities: tensor([[1.4286, 0.0000, 0.1440, 0.0000, 0.9017],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0520, 0.0000, 1.4145, 1.1592, 0.1000],\n",
      "        [0.0000, 0.0000, 1.1709, 1.4110, 0.0000],\n",
      "        [0.9696, 0.0000, 0.1735, 0.0000, 1.4254]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.3415212631225586\n",
      "Batch 18, Loss: 3.3415212631225586\n",
      "Processing Batch 19/157\n",
      "Batch 19 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013051921501755714, Std: 0.08239225298166275\n",
      "z_j Mean: 0.0127513213083148, Std: 0.08243931829929352\n",
      "Similarities: tensor([[1.3649, 0.0000, 0.0000, 0.5864, 1.1176],\n",
      "        [0.8410, 1.0014, 0.1835, 0.3721, 0.8536],\n",
      "        [0.0000, 0.0000, 1.4201, 0.0000, 0.0000],\n",
      "        [0.1279, 0.0366, 0.0000, 1.3474, 0.1066],\n",
      "        [1.0646, 0.0000, 0.0000, 0.4520, 1.3563]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.3211147785186768\n",
      "Batch 19, Loss: 3.3211147785186768\n",
      "Processing Batch 20/157\n",
      "Batch 20 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011782030574977398, Std: 0.08109164237976074\n",
      "z_j Mean: 0.01198594644665718, Std: 0.08255408704280853\n",
      "Similarities: tensor([[1.3525e+00, 1.1572e+00, 2.9036e-02, 0.0000e+00, 0.0000e+00],\n",
      "        [1.1429e+00, 1.4229e+00, 3.0829e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [1.9315e-04, 2.5180e-01, 1.4097e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 1.4286e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.4164e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.3219754695892334\n",
      "Batch 20, Loss: 3.3219754695892334\n",
      "Processing Batch 21/157\n",
      "Batch 21 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013195812702178955, Std: 0.08236932754516602\n",
      "z_j Mean: 0.01258416660130024, Std: 0.08021359145641327\n",
      "Similarities: tensor([[1.3991, 0.0000, 0.1495, 0.0169, 0.0000],\n",
      "        [0.0087, 1.0306, 0.0000, 0.4509, 0.0000],\n",
      "        [0.0809, 0.0000, 1.3924, 1.1933, 0.0000],\n",
      "        [0.0315, 0.0000, 0.9988, 1.3589, 0.0000],\n",
      "        [0.0284, 0.4913, 0.0000, 0.0000, 1.4259]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.27140212059021\n",
      "Batch 21, Loss: 3.27140212059021\n",
      "Processing Batch 22/157\n",
      "Batch 22 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.010312020778656006, Std: 0.07744617015123367\n",
      "z_j Mean: 0.011089380830526352, Std: 0.07967137545347214\n",
      "Similarities: tensor([[1.4090, 0.3194, 0.0000, 0.9922, 0.0000],\n",
      "        [0.2714, 1.4098, 0.0000, 0.0131, 0.0312],\n",
      "        [0.0000, 0.0000, 1.3944, 0.0000, 0.0000],\n",
      "        [0.8982, 0.0076, 0.0000, 1.4101, 0.0000],\n",
      "        [0.0000, 0.0000, 0.1185, 0.0000, 1.4005]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.3482749462127686\n",
      "Batch 22, Loss: 3.3482749462127686\n",
      "Processing Batch 23/157\n",
      "Batch 23 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01120411790907383, Std: 0.07732214033603668\n",
      "z_j Mean: 0.011018877848982811, Std: 0.07655549794435501\n",
      "Similarities: tensor([[1.4129, 0.0000, 0.0000, 0.8099, 0.0000],\n",
      "        [0.0245, 1.4230, 0.0000, 0.0074, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.8242, 0.0000, 0.0000, 1.4191, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4286]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.3369674682617188\n",
      "Batch 23, Loss: 3.3369674682617188\n",
      "Processing Batch 24/157\n",
      "Batch 24 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01024748757481575, Std: 0.07823888212442398\n",
      "z_j Mean: 0.010407106950879097, Std: 0.07899437099695206\n",
      "Similarities: tensor([[1.4280, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4188, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.3233373165130615\n",
      "Batch 24, Loss: 3.3233373165130615\n",
      "Processing Batch 25/157\n",
      "Batch 25 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011555408127605915, Std: 0.0826154574751854\n",
      "z_j Mean: 0.011443527415394783, Std: 0.08263102918863297\n",
      "Similarities: tensor([[1.4286, 0.0000, 0.0000, 0.3607, 1.0828],\n",
      "        [0.0000, 1.3526, 0.0000, 0.0337, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3779, 0.7974, 0.5908],\n",
      "        [0.1952, 0.5119, 0.6213, 1.3382, 0.1480],\n",
      "        [1.0871, 0.0000, 0.4157, 0.2745, 1.3729]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1999194622039795\n",
      "Batch 25, Loss: 3.1999194622039795\n",
      "Processing Batch 26/157\n",
      "Batch 26 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013729337602853775, Std: 0.08078470826148987\n",
      "z_j Mean: 0.014126741327345371, Std: 0.08146890252828598\n",
      "Similarities: tensor([[1.4286e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 1.4286e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 1.4100e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 1.4286e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 1.0749e-05, 0.0000e+00, 1.4282e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.3187179565429688\n",
      "Batch 26, Loss: 3.3187179565429688\n",
      "Processing Batch 27/157\n",
      "Batch 27 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011874220333993435, Std: 0.08182761073112488\n",
      "z_j Mean: 0.01182851754128933, Std: 0.08032851666212082\n",
      "Similarities: tensor([[0.6298, 0.1402, 0.0000, 0.0000, 1.3357],\n",
      "        [0.0000, 1.4135, 0.3662, 0.0000, 0.3840],\n",
      "        [0.0000, 0.3943, 1.3909, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4286, 0.0000],\n",
      "        [0.0652, 0.6210, 0.7082, 0.0000, 0.5899]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.300126075744629\n",
      "Batch 27, Loss: 3.300126075744629\n",
      "Processing Batch 28/157\n",
      "Batch 28 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.010796628892421722, Std: 0.07894206792116165\n",
      "z_j Mean: 0.011173313483595848, Std: 0.08117775619029999\n",
      "Similarities: tensor([[1.4280, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0249, 0.0000, 1.4000, 0.0000, 0.7654],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3625, 0.0000],\n",
      "        [0.0130, 0.0000, 0.4747, 0.0000, 1.3430]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.3222928047180176\n",
      "Batch 28, Loss: 3.3222928047180176\n",
      "Processing Batch 29/157\n",
      "Batch 29 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.008967027068138123, Std: 0.07761342078447342\n",
      "z_j Mean: 0.009068691171705723, Std: 0.07838427275419235\n",
      "Similarities: tensor([[1.3863, 0.0000, 1.1592, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4259, 0.0000],\n",
      "        [1.1805, 0.0000, 1.0960, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4286, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4286]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.315088987350464\n",
      "Batch 29, Loss: 3.315088987350464\n",
      "Processing Batch 30/157\n",
      "Batch 30 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.009727328084409237, Std: 0.07752178609371185\n",
      "z_j Mean: 0.009524639695882797, Std: 0.07833016663789749\n",
      "Similarities: tensor([[1.4244, 0.8822, 0.0141, 0.0000, 0.0000],\n",
      "        [0.7670, 1.4108, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.1354, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.3643829822540283\n",
      "Batch 30, Loss: 3.3643829822540283\n",
      "Processing Batch 31/157\n",
      "Batch 31 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011273741722106934, Std: 0.08484102785587311\n",
      "z_j Mean: 0.010713683441281319, Std: 0.08272884041070938\n",
      "Similarities: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3856, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4278, 0.6858, 1.1025],\n",
      "        [0.0000, 0.2989, 0.4246, 1.3911, 0.7475],\n",
      "        [0.0000, 0.0110, 1.1887, 0.6148, 1.3729]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2909865379333496\n",
      "Batch 31, Loss: 3.2909865379333496\n",
      "Processing Batch 32/157\n",
      "Batch 32 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.010498381219804287, Std: 0.0820154994726181\n",
      "z_j Mean: 0.010785909369587898, Std: 0.08271945267915726\n",
      "Similarities: tensor([[1.4170, 0.0000, 1.3570, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3857, 0.0000, 0.7700, 0.2895],\n",
      "        [1.2892, 0.0000, 1.4286, 0.0000, 0.0000],\n",
      "        [0.0000, 0.6486, 0.0000, 1.4244, 0.1633],\n",
      "        [0.0000, 0.1562, 0.0000, 0.0944, 1.4061]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2667784690856934\n",
      "Batch 32, Loss: 3.2667784690856934\n",
      "Processing Batch 33/157\n",
      "Batch 33 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.010430816560983658, Std: 0.08052196353673935\n",
      "z_j Mean: 0.010784029960632324, Std: 0.08197842538356781\n",
      "Similarities: tensor([[1.4286, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3293, 0.6401, 0.0000, 0.0716],\n",
      "        [0.0000, 0.9199, 1.3587, 0.0517, 0.2284],\n",
      "        [0.0000, 0.0000, 0.0337, 0.8563, 0.0000],\n",
      "        [0.0000, 0.1442, 0.0745, 0.0000, 1.3250]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.311908483505249\n",
      "Batch 33, Loss: 3.311908483505249\n",
      "Processing Batch 34/157\n",
      "Batch 34 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011162315495312214, Std: 0.08340461552143097\n",
      "z_j Mean: 0.011280309408903122, Std: 0.0841175764799118\n",
      "Similarities: tensor([[1.4286, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4286, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3870, 0.1075, 1.1165],\n",
      "        [0.0000, 0.0000, 0.0095, 1.3936, 0.0052],\n",
      "        [0.0000, 0.0000, 1.0942, 0.0273, 1.4166]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.259202480316162\n",
      "Batch 34, Loss: 3.259202480316162\n",
      "Processing Batch 35/157\n",
      "Batch 35 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01193128153681755, Std: 0.08329807221889496\n",
      "z_j Mean: 0.011979221366345882, Std: 0.083291195333004\n",
      "Similarities: tensor([[1.4125, 0.0000, 0.0000, 0.1295, 0.0000],\n",
      "        [0.0000, 1.4286, 0.0000, 0.0000, 0.8436],\n",
      "        [0.0000, 0.0000, 1.4279, 0.0000, 0.0000],\n",
      "        [0.0916, 0.0000, 0.0000, 1.3599, 0.0000],\n",
      "        [0.0000, 0.5775, 0.0990, 0.0000, 1.3914]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2089180946350098\n",
      "Batch 35, Loss: 3.2089180946350098\n",
      "Processing Batch 36/157\n",
      "Batch 36 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011507207527756691, Std: 0.0840868353843689\n",
      "z_j Mean: 0.011649534106254578, Std: 0.08479023724794388\n",
      "Similarities: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3751, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3508, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4286, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4270]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1935267448425293\n",
      "Batch 36, Loss: 3.1935267448425293\n",
      "Processing Batch 37/157\n",
      "Batch 37 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012914180755615234, Std: 0.08603771030902863\n",
      "z_j Mean: 0.012749401852488518, Std: 0.08606228232383728\n",
      "Similarities: tensor([[1.3738, 1.1630, 0.0000, 0.0710, 0.0000],\n",
      "        [0.8844, 1.3291, 0.0000, 0.0344, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3744, 0.8221, 0.6337],\n",
      "        [0.0000, 0.0000, 0.9538, 1.3527, 0.2542],\n",
      "        [0.0000, 0.0000, 0.6699, 0.0244, 1.3882]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.162546396255493\n",
      "Batch 37, Loss: 3.162546396255493\n",
      "Processing Batch 38/157\n",
      "Batch 38 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01270916499197483, Std: 0.08463788032531738\n",
      "z_j Mean: 0.012859170325100422, Std: 0.08604595065116882\n",
      "Similarities: tensor([[1.1236, 0.0000, 0.0000, 1.0634, 0.0000],\n",
      "        [0.0000, 1.2975, 1.0306, 0.0000, 0.0000],\n",
      "        [0.0000, 0.5470, 1.3616, 0.0697, 0.0000],\n",
      "        [1.1415, 0.0000, 0.0000, 1.4056, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3769]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1702980995178223\n",
      "Batch 38, Loss: 3.1702980995178223\n",
      "Processing Batch 39/157\n",
      "Batch 39 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.0118987075984478, Std: 0.08475562930107117\n",
      "z_j Mean: 0.011622488498687744, Std: 0.08334173262119293\n",
      "Similarities: tensor([[1.1060, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4171, 1.3842, 0.0982],\n",
      "        [0.0000, 0.0000, 1.4177, 1.4286, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0162, 0.0000, 1.4030]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2248806953430176\n",
      "Batch 39, Loss: 3.2248806953430176\n",
      "Processing Batch 40/157\n",
      "Batch 40 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012058351188898087, Std: 0.0854504331946373\n",
      "z_j Mean: 0.011933382600545883, Std: 0.08329777419567108\n",
      "Similarities: tensor([[1.3467, 0.1541, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2382, 1.4005, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.1332, 0.0057, 0.0000],\n",
      "        [0.0000, 0.0000, 0.1505, 1.4175, 0.0000],\n",
      "        [0.0000, 0.0000, 0.1538, 0.0000, 1.3884]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.177345037460327\n",
      "Batch 40, Loss: 3.177345037460327\n",
      "Processing Batch 41/157\n",
      "Batch 41 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012073487974703312, Std: 0.0825413316488266\n",
      "z_j Mean: 0.012129007838666439, Std: 0.08326951414346695\n",
      "Similarities: tensor([[1.3935, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3680, 0.0543, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3975, 1.4266, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0512, 0.0000, 0.0000, 0.0000, 1.4185]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2220401763916016\n",
      "Batch 41, Loss: 3.2220401763916016\n",
      "Processing Batch 42/157\n",
      "Batch 42 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.0124307069927454, Std: 0.08467922359704971\n",
      "z_j Mean: 0.012200472876429558, Std: 0.08252264559268951\n",
      "Similarities: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4286, 0.0000, 0.0000],\n",
      "        [0.0000, 1.0541, 0.0000, 1.3928, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2157673835754395\n",
      "Batch 42, Loss: 3.2157673835754395\n",
      "Processing Batch 43/157\n",
      "Batch 43 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011317593976855278, Std: 0.08338368684053421\n",
      "z_j Mean: 0.011783920228481293, Std: 0.0861998051404953\n",
      "Similarities: tensor([[1.2234, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.2373, 0.6636, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1797, 1.4151, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.2851, 0.1668],\n",
      "        [0.0000, 0.0000, 0.0000, 0.3141, 1.4257]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.192089796066284\n",
      "Batch 43, Loss: 3.192089796066284\n",
      "Processing Batch 44/157\n",
      "Batch 44 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.010289231315255165, Std: 0.08054018020629883\n",
      "z_j Mean: 0.010539110749959946, Std: 0.08126254379749298\n",
      "Similarities: tensor([[1.2355, 0.0000, 0.0000, 1.0371, 1.1163],\n",
      "        [0.0000, 1.4157, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0868, 1.2988, 0.0000, 0.0000],\n",
      "        [1.4232, 0.0000, 0.0000, 1.4135, 0.4002],\n",
      "        [0.5165, 0.0000, 0.0816, 0.1969, 1.4245]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.3140289783477783\n",
      "Batch 44, Loss: 3.3140289783477783\n",
      "Processing Batch 45/157\n",
      "Batch 45 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.010838493704795837, Std: 0.0827125683426857\n",
      "z_j Mean: 0.010950833559036255, Std: 0.0834326520562172\n",
      "Similarities: tensor([[1.4139, 1.3415, 0.0000, 0.0000, 0.9786],\n",
      "        [1.3522, 1.4279, 0.0000, 0.0000, 1.2019],\n",
      "        [0.0000, 0.0000, 1.2909, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4286, 0.0000],\n",
      "        [0.4295, 0.5760, 0.3832, 0.0000, 0.6721]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.276604652404785\n",
      "Batch 45, Loss: 3.276604652404785\n",
      "Processing Batch 46/157\n",
      "Batch 46 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.010523367673158646, Std: 0.08348765224218369\n",
      "z_j Mean: 0.010445916093885899, Std: 0.08349738270044327\n",
      "Similarities: tensor([[1.4271, 0.1463, 1.3807, 0.0000, 0.0000],\n",
      "        [0.2209, 1.3901, 0.4768, 0.0000, 0.0000],\n",
      "        [1.4150, 0.3445, 1.4244, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4275, 1.4153],\n",
      "        [0.0000, 0.0000, 0.0000, 1.2654, 1.3239]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2727713584899902\n",
      "Batch 46, Loss: 3.2727713584899902\n",
      "Processing Batch 47/157\n",
      "Batch 47 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.010712379589676857, Std: 0.08562960475683212\n",
      "z_j Mean: 0.010628102347254753, Std: 0.08347438275814056\n",
      "Similarities: tensor([[1.3419, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4285, 0.0000, 0.0000, 1.3957],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.2640, 0.0000],\n",
      "        [0.0000, 1.3918, 0.0000, 0.0000, 1.4286]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.198920726776123\n",
      "Batch 47, Loss: 3.198920726776123\n",
      "Processing Batch 48/157\n",
      "Batch 48 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.0103948675096035, Std: 0.08423159271478653\n",
      "z_j Mean: 0.010251092724502087, Std: 0.0835215225815773\n",
      "Similarities: tensor([[1.2347, 0.0000, 0.0000, 0.3286, 0.9901],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4286, 0.0000, 0.0000],\n",
      "        [0.2963, 0.0000, 0.0000, 1.4214, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.28818941116333\n",
      "Batch 48, Loss: 3.28818941116333\n",
      "Processing Batch 49/157\n",
      "Batch 49 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.010158151388168335, Std: 0.08569712191820145\n",
      "z_j Mean: 0.010210208594799042, Std: 0.0842541828751564\n",
      "Similarities: tensor([[0.0000, 0.0000, 0.0000, 0.5466, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4188, 0.0000, 1.4188],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4034, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4286, 0.0000, 1.4286]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.295478582382202\n",
      "Batch 49, Loss: 3.295478582382202\n",
      "Processing Batch 50/157\n",
      "Batch 50 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.010181903839111328, Std: 0.08425760269165039\n",
      "z_j Mean: 0.010454037226736546, Std: 0.08566153049468994\n",
      "Similarities: tensor([[1.4284, 0.3701, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4849, 1.4222, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4231, 0.0000, 1.3688],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3912, 0.0000, 1.4232]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.3099756240844727\n",
      "Batch 50, Loss: 3.3099756240844727\n",
      "Processing Batch 51/157\n",
      "Batch 51 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.009757496416568756, Std: 0.0835806354880333\n",
      "z_j Mean: 0.010349858552217484, Std: 0.08567418158054352\n",
      "Similarities: tensor([[1.4209, 1.4286, 0.0000, 0.0000, 0.0000],\n",
      "        [1.3754, 1.3828, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4286, 0.0000, 0.0000],\n",
      "        [0.1382, 0.0000, 0.0000, 1.3786, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3925]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2585418224334717\n",
      "Batch 51, Loss: 3.2585418224334717\n",
      "Processing Batch 52/157\n",
      "Batch 52 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.010200805962085724, Std: 0.08569205552339554\n",
      "z_j Mean: 0.01015947200357914, Std: 0.08498167991638184\n",
      "Similarities: tensor([[1.3471, 0.0000, 0.5688, 0.0000, 0.6689],\n",
      "        [0.0000, 1.3962, 0.5736, 0.0000, 0.4440],\n",
      "        [0.8691, 0.6834, 1.4256, 0.0000, 1.4151],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4015, 0.0000],\n",
      "        [0.9867, 0.5325, 1.3870, 0.0000, 1.4275]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2775824069976807\n",
      "Batch 52, Loss: 3.2775824069976807\n",
      "Processing Batch 53/157\n",
      "Batch 53 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01061059907078743, Std: 0.08635210245847702\n",
      "z_j Mean: 0.010615160688757896, Std: 0.08564171195030212\n",
      "Similarities: tensor([[0.9316, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3826, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4285, 0.0000, 0.7813],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4255, 0.9021],\n",
      "        [0.0000, 0.0000, 0.9246, 0.7245, 1.3977]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2393407821655273\n",
      "Batch 53, Loss: 3.2393407821655273\n",
      "Processing Batch 54/157\n",
      "Batch 54 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.010406224057078362, Std: 0.08423018455505371\n",
      "z_j Mean: 0.010636290535330772, Std: 0.08420144021511078\n",
      "Similarities: tensor([[1.4122, 0.0000, 0.2961, 0.8693, 0.0000],\n",
      "        [0.0000, 1.3199, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2649, 0.0000, 1.4286, 0.0000, 0.0000],\n",
      "        [0.6037, 0.0000, 0.0000, 1.3939, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4252]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.195913553237915\n",
      "Batch 54, Loss: 3.195913553237915\n",
      "Processing Batch 55/157\n",
      "Batch 55 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01111975871026516, Std: 0.08557765185832977\n",
      "z_j Mean: 0.011412488296627998, Std: 0.08695463091135025\n",
      "Similarities: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4286, 0.0000, 1.4286, 1.1056],\n",
      "        [0.0000, 0.0000, 1.0027, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4286, 0.0000, 1.4286, 1.1056],\n",
      "        [0.0000, 0.7841, 0.0000, 0.7841, 1.3631]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.246910572052002\n",
      "Batch 55, Loss: 3.246910572052002\n",
      "Processing Batch 56/157\n",
      "Batch 56 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.009676621295511723, Std: 0.08359003812074661\n",
      "z_j Mean: 0.009804105386137962, Std: 0.08284156024456024\n",
      "Similarities: tensor([[1.4011, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3566, 0.0000, 0.0000, 0.0483],\n",
      "        [0.0000, 0.0000, 1.4221, 0.7872, 0.3959],\n",
      "        [0.0000, 0.0000, 0.6497, 1.1994, 1.2640],\n",
      "        [0.0000, 0.3071, 0.0597, 0.2921, 1.3070]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2624967098236084\n",
      "Batch 56, Loss: 3.2624967098236084\n",
      "Processing Batch 57/157\n",
      "Batch 57 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.00996303465217352, Std: 0.08428376168012619\n",
      "z_j Mean: 0.009907703846693039, Std: 0.0842902883887291\n",
      "Similarities: tensor([[1.4076, 0.0000, 0.5126, 1.4267, 0.0000],\n",
      "        [0.0000, 1.2366, 0.0000, 0.0000, 0.2731],\n",
      "        [0.7180, 0.0000, 1.4136, 0.4903, 0.0000],\n",
      "        [1.4240, 0.0000, 0.6547, 1.4184, 0.0000],\n",
      "        [0.0000, 0.0139, 0.0000, 0.0000, 1.3554]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2919960021972656\n",
      "Batch 57, Loss: 3.2919960021972656\n",
      "Processing Batch 58/157\n",
      "Batch 58 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.008851147256791592, Std: 0.08071090281009674\n",
      "z_j Mean: 0.009393399581313133, Std: 0.0828891471028328\n",
      "Similarities: tensor([[1.4281, 0.0000, 0.0000, 0.0000, 0.4262],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4134, 0.0000, 0.8902],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0510, 0.0000],\n",
      "        [0.2931, 0.0000, 0.7669, 0.0000, 1.4176]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2527570724487305\n",
      "Batch 58, Loss: 3.2527570724487305\n",
      "Processing Batch 59/157\n",
      "Batch 59 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.00952810700982809, Std: 0.08360709249973297\n",
      "z_j Mean: 0.0096468236297369, Std: 0.08359348028898239\n",
      "Similarities: tensor([[1.4094, 0.0000, 0.0000, 1.2342, 0.0000],\n",
      "        [0.0000, 1.4274, 1.4188, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4284, 1.4223, 0.0000, 0.0000],\n",
      "        [1.3749, 0.0000, 0.0000, 1.4230, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.289093255996704\n",
      "Batch 59, Loss: 3.289093255996704\n",
      "Processing Batch 60/157\n",
      "Batch 60 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.010402755811810493, Std: 0.08495223522186279\n",
      "z_j Mean: 0.01040280144661665, Std: 0.08423061668872833\n",
      "Similarities: tensor([[1.3884, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4280, 0.9451, 0.0000, 0.0000],\n",
      "        [1.0776, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4218, 1.3795],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3765, 1.4098]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.3245646953582764\n",
      "Batch 60, Loss: 3.3245646953582764\n",
      "Processing Batch 61/157\n",
      "Batch 61 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.009711367078125477, Std: 0.08503405749797821\n",
      "z_j Mean: 0.009401468560099602, Std: 0.08288823068141937\n",
      "Similarities: tensor([[1.2348, 0.0000, 0.9412, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4053, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.8806, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4168, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4286]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.3177032470703125\n",
      "Batch 61, Loss: 3.3177032470703125\n",
      "Processing Batch 62/157\n",
      "Batch 62 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.009985530748963356, Std: 0.08571740984916687\n",
      "z_j Mean: 0.0097957169637084, Std: 0.08430337905883789\n",
      "Similarities: tensor([[1.4285, 0.9982, 0.0000, 1.0071, 0.0000],\n",
      "        [1.0202, 1.4109, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4183, 0.0000, 0.0000],\n",
      "        [0.9976, 0.0000, 0.0000, 1.4286, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4286]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.332458972930908\n",
      "Batch 62, Loss: 3.332458972930908\n",
      "Processing Batch 63/157\n",
      "Batch 63 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.0102139413356781, Std: 0.08710356801748276\n",
      "z_j Mean: 0.010275331325829029, Std: 0.08639264106750488\n",
      "Similarities: tensor([[1.4103, 0.0000, 0.9563, 1.2203, 1.4148],\n",
      "        [0.0000, 1.3286, 0.0000, 0.0000, 0.0000],\n",
      "        [1.2994, 0.0000, 1.3828, 0.8775, 1.2867],\n",
      "        [1.1776, 0.0000, 0.6401, 1.4283, 1.1866],\n",
      "        [1.4279, 0.0000, 1.0841, 1.1785, 1.4285]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.276707410812378\n",
      "Batch 63, Loss: 3.276707410812378\n",
      "Processing Batch 64/157\n",
      "Batch 64 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.009571819566190243, Std: 0.08504988253116608\n",
      "z_j Mean: 0.009737000800669193, Std: 0.08503112196922302\n",
      "Similarities: tensor([[1.4273, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4199, 0.8590, 0.0000, 1.2458],\n",
      "        [0.0000, 0.8719, 1.4225, 0.0000, 0.2112],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4097, 0.0000],\n",
      "        [0.0000, 1.2513, 0.3555, 0.0000, 1.3685]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.3275439739227295\n",
      "Batch 64, Loss: 3.3275439739227295\n",
      "Processing Batch 65/157\n",
      "Batch 65 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.010316124185919762, Std: 0.08638777583837509\n",
      "z_j Mean: 0.010371552780270576, Std: 0.08638113737106323\n",
      "Similarities: tensor([[1.3756, 1.1480, 1.1585, 1.1125, 0.7984],\n",
      "        [1.0113, 1.4168, 0.4559, 1.3897, 1.0793],\n",
      "        [1.2382, 0.4811, 1.4277, 0.4571, 0.3480],\n",
      "        [0.9171, 1.3266, 0.3702, 1.3900, 0.7673],\n",
      "        [0.7332, 1.0389, 0.3493, 0.9510, 1.4251]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2650318145751953\n",
      "Batch 65, Loss: 3.2650318145751953\n",
      "Processing Batch 66/157\n",
      "Batch 66 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.009740299545228481, Std: 0.08574563264846802\n",
      "z_j Mean: 0.010006614960730076, Std: 0.08499981462955475\n",
      "Similarities: tensor([[1.4286, 1.3968, 0.0000, 0.0000, 0.0000],\n",
      "        [1.4285, 1.4004, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4238, 0.0000, 0.0061],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4208, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0616, 0.0000, 1.3176]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2522764205932617\n",
      "Batch 66, Loss: 3.2522764205932617\n",
      "Processing Batch 67/157\n",
      "Batch 67 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.010919949039816856, Std: 0.08701787143945694\n",
      "z_j Mean: 0.010705973953008652, Std: 0.08704445511102676\n",
      "Similarities: tensor([[1.4279, 0.0000, 1.0296, 0.2509, 0.0000],\n",
      "        [0.0000, 1.3946, 0.0000, 0.7706, 0.0000],\n",
      "        [1.0591, 0.0000, 1.4286, 0.0000, 0.0000],\n",
      "        [1.2337, 0.3485, 0.7989, 0.9186, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3363]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2482945919036865\n",
      "Batch 67, Loss: 3.2482945919036865\n",
      "Processing Batch 68/157\n",
      "Batch 68 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.010085804387927055, Std: 0.08570567518472672\n",
      "z_j Mean: 0.010393760167062283, Std: 0.08708228915929794\n",
      "Similarities: tensor([[1.3752, 0.3870, 1.1159, 0.0000, 1.4038],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [1.1693, 0.0000, 1.4284, 0.0000, 1.1648],\n",
      "        [0.0000, 0.0000, 0.0000, 1.2924, 0.0000],\n",
      "        [1.2763, 0.6417, 1.0357, 0.0000, 1.3277]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.242253541946411\n",
      "Batch 68, Loss: 3.242253541946411\n",
      "Processing Batch 69/157\n",
      "Batch 69 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.010963577777147293, Std: 0.087012380361557\n",
      "z_j Mean: 0.01060677319765091, Std: 0.08635256439447403\n",
      "Similarities: tensor([[1.4135, 0.9192, 1.0982, 0.6916, 0.0000],\n",
      "        [0.7966, 1.4252, 0.0736, 1.1204, 0.0628],\n",
      "        [1.0711, 0.0301, 1.2553, 0.0000, 0.0367],\n",
      "        [0.6575, 1.3092, 0.0000, 1.3934, 0.4977],\n",
      "        [0.0000, 0.1011, 0.0000, 0.5748, 1.4079]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1917319297790527\n",
      "Batch 69, Loss: 3.1917319297790527\n",
      "Processing Batch 70/157\n",
      "Batch 70 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.010370392352342606, Std: 0.08567169308662415\n",
      "z_j Mean: 0.01051427237689495, Std: 0.08636387437582016\n",
      "Similarities: tensor([[1.3651, 0.0000, 0.7019, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4345, 0.0000, 1.3362, 0.1207, 0.4101],\n",
      "        [0.0000, 0.0000, 0.0128, 1.3634, 0.0000],\n",
      "        [0.0000, 0.0000, 0.7562, 0.0000, 1.2660]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2316954135894775\n",
      "Batch 70, Loss: 3.2316954135894775\n",
      "Processing Batch 71/157\n",
      "Batch 71 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.010863144882023335, Std: 0.08702497184276581\n",
      "z_j Mean: 0.010916123166680336, Std: 0.08560387045145035\n",
      "Similarities: tensor([[1.4033, 1.3872, 0.0000, 0.4985, 1.2845],\n",
      "        [1.4027, 1.4121, 0.0000, 0.2314, 1.4206],\n",
      "        [0.0000, 0.0000, 1.4226, 1.1061, 0.0000],\n",
      "        [0.3524, 0.3197, 1.1187, 1.4230, 0.1692],\n",
      "        [1.3620, 1.3818, 0.0000, 0.1445, 1.4251]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2610390186309814\n",
      "Batch 71, Loss: 3.2610390186309814\n",
      "Processing Batch 72/157\n",
      "Batch 72 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01062752865254879, Std: 0.08635001629590988\n",
      "z_j Mean: 0.010745041072368622, Std: 0.08703963458538055\n",
      "Similarities: tensor([[1.4139, 0.0000, 0.0000, 0.0000, 0.9769],\n",
      "        [0.0000, 1.3972, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4266, 1.3746, 1.0145],\n",
      "        [0.0000, 0.0000, 1.4286, 1.3767, 1.0161],\n",
      "        [1.0739, 0.0000, 0.9370, 0.9031, 1.4245]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.236388921737671\n",
      "Batch 72, Loss: 3.236388921737671\n",
      "Processing Batch 73/157\n",
      "Batch 73 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011111916974186897, Std: 0.08699356019496918\n",
      "z_j Mean: 0.011473635211586952, Std: 0.08764583617448807\n",
      "Similarities: tensor([[1.4008, 1.4284, 0.0000, 0.7811, 0.0000],\n",
      "        [1.3508, 1.3197, 0.0000, 1.1870, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3293, 0.1013, 0.2785],\n",
      "        [0.8916, 0.7909, 0.2732, 1.4164, 0.0000],\n",
      "        [0.0000, 0.0000, 0.2727, 0.0060, 1.4250]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2479026317596436\n",
      "Batch 73, Loss: 3.2479026317596436\n",
      "Processing Batch 74/157\n",
      "Batch 74 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012152297422289848, Std: 0.08755431324243546\n",
      "z_j Mean: 0.012154927477240562, Std: 0.0875539481639862\n",
      "Similarities: tensor([[1.4286, 0.0000, 0.0000, 0.0000, 0.8699],\n",
      "        [0.0000, 1.3646, 0.1505, 0.0450, 0.6392],\n",
      "        [0.0000, 0.4106, 1.3844, 0.3178, 0.0000],\n",
      "        [0.0000, 0.2163, 0.2194, 1.3670, 0.0000],\n",
      "        [1.0623, 0.4189, 0.0000, 0.0000, 1.4045]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.248718738555908\n",
      "Batch 74, Loss: 3.248718738555908\n",
      "Processing Batch 75/157\n",
      "Batch 75 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011774515733122826, Std: 0.08760593086481094\n",
      "z_j Mean: 0.011687319725751877, Std: 0.08761759847402573\n",
      "Similarities: tensor([[1.3910, 0.2883, 0.0797, 0.0000, 1.2504],\n",
      "        [0.3521, 1.3886, 0.3650, 0.0000, 0.0397],\n",
      "        [0.0043, 0.2033, 1.3739, 0.5605, 0.0000],\n",
      "        [0.0000, 0.0000, 0.4918, 1.3935, 0.0000],\n",
      "        [1.3598, 0.1441, 0.0359, 0.0000, 1.3873]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.275521755218506\n",
      "Batch 75, Loss: 3.275521755218506\n",
      "Processing Batch 76/157\n",
      "Batch 76 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012029818259179592, Std: 0.08757122606039047\n",
      "z_j Mean: 0.012205368839204311, Std: 0.0875469297170639\n",
      "Similarities: tensor([[1.4286, 0.0000, 0.0000, 0.0000, 0.1722],\n",
      "        [0.0000, 1.3707, 0.3610, 0.0000, 0.8284],\n",
      "        [0.0000, 0.4727, 1.3517, 0.4901, 0.4635],\n",
      "        [0.0000, 0.0000, 0.3454, 1.4167, 0.4186],\n",
      "        [0.0623, 0.7345, 0.5088, 0.4189, 1.3947]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.173367738723755\n",
      "Batch 76, Loss: 3.173367738723755\n",
      "Processing Batch 77/157\n",
      "Batch 77 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012261101976037025, Std: 0.08683902025222778\n",
      "z_j Mean: 0.012536603957414627, Std: 0.08679966628551483\n",
      "Similarities: tensor([[1.4200, 1.2898, 0.8788, 0.4750, 0.0000],\n",
      "        [1.0729, 1.2821, 0.0000, 0.0000, 0.0000],\n",
      "        [0.8327, 0.4499, 1.4244, 1.1745, 0.0000],\n",
      "        [0.3551, 0.1919, 0.9919, 1.4026, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2566304206848145\n",
      "Batch 77, Loss: 3.2566304206848145\n",
      "Processing Batch 78/157\n",
      "Batch 78 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012282611802220345, Std: 0.08753612637519836\n",
      "z_j Mean: 0.012327630072832108, Std: 0.08612371236085892\n",
      "Similarities: tensor([[1.3433, 0.0655, 0.0050, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4286, 0.1097, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2768, 1.3825, 0.5981, 0.0000],\n",
      "        [0.0000, 0.0000, 0.2904, 1.1829, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4286]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2706780433654785\n",
      "Batch 78, Loss: 3.2706780433654785\n",
      "Processing Batch 79/157\n",
      "Batch 79 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012151876464486122, Std: 0.0861486941576004\n",
      "z_j Mean: 0.012010907754302025, Std: 0.08545711636543274\n",
      "Similarities: tensor([[1.4047, 0.5061, 0.0000, 0.5983, 0.0000],\n",
      "        [0.8840, 1.2873, 0.2011, 0.0000, 0.0000],\n",
      "        [0.1408, 0.6646, 1.4286, 0.0000, 0.0000],\n",
      "        [0.5790, 0.0000, 0.0000, 1.3768, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3973]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.265866756439209\n",
      "Batch 79, Loss: 3.265866756439209\n",
      "Processing Batch 80/157\n",
      "Batch 80 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012543033808469772, Std: 0.08609259873628616\n",
      "z_j Mean: 0.012407062575221062, Std: 0.08611230552196503\n",
      "Similarities: tensor([[1.4004, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4214, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.1601]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2241098880767822\n",
      "Batch 80, Loss: 3.2241098880767822\n",
      "Processing Batch 81/157\n",
      "Batch 81 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01228083111345768, Std: 0.08613040298223495\n",
      "z_j Mean: 0.011998835951089859, Std: 0.08474151045084\n",
      "Similarities: tensor([[1.4129, 0.0000, 0.0065, 1.1852, 0.0000],\n",
      "        [0.0000, 1.4285, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0958, 0.0000, 1.4286, 0.0000, 0.0000],\n",
      "        [1.2061, 0.0000, 0.0000, 1.3785, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.311276435852051\n",
      "Batch 81, Loss: 3.311276435852051\n",
      "Processing Batch 82/157\n",
      "Batch 82 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013069067150354385, Std: 0.08458304405212402\n",
      "z_j Mean: 0.01333562470972538, Std: 0.08526040613651276\n",
      "Similarities: tensor([[1.4250, 0.0833, 0.0000, 0.3611, 0.4671],\n",
      "        [0.0940, 1.4083, 0.0000, 0.0000, 1.3214],\n",
      "        [0.0000, 0.0000, 1.4286, 0.0000, 0.0000],\n",
      "        [0.2774, 0.0000, 0.0000, 1.3763, 0.0000],\n",
      "        [0.4756, 1.3301, 0.0000, 0.0000, 1.4213]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.3134713172912598\n",
      "Batch 82, Loss: 3.3134713172912598\n",
      "Processing Batch 83/157\n",
      "Batch 83 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011797365732491016, Std: 0.08476979285478592\n",
      "z_j Mean: 0.011793851852416992, Std: 0.08548734337091446\n",
      "Similarities: tensor([[1.4084e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [2.1583e-02, 1.4014e+00, 0.0000e+00, 1.4156e+00, 5.4721e-01],\n",
      "        [0.0000e+00, 0.0000e+00, 1.3898e+00, 0.0000e+00, 3.8502e-01],\n",
      "        [2.1829e-04, 1.3693e+00, 0.0000e+00, 1.4077e+00, 6.3268e-01],\n",
      "        [0.0000e+00, 6.4163e-01, 2.5176e-01, 6.9718e-01, 1.4123e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.211754322052002\n",
      "Batch 83, Loss: 3.211754322052002\n",
      "Processing Batch 84/157\n",
      "Batch 84 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01165495440363884, Std: 0.08478949218988419\n",
      "z_j Mean: 0.011978301219642162, Std: 0.0847444161772728\n",
      "Similarities: tensor([[1.4031, 0.0000, 0.0959, 0.7348, 0.0499],\n",
      "        [0.0000, 1.3043, 0.0000, 0.0000, 0.2853],\n",
      "        [0.0000, 0.0000, 1.4286, 0.0000, 0.0000],\n",
      "        [0.6597, 0.0000, 0.0000, 1.3553, 0.0300],\n",
      "        [0.0952, 0.3570, 0.0000, 0.0848, 1.4217]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2294061183929443\n",
      "Batch 84, Loss: 3.2294061183929443\n",
      "Processing Batch 85/157\n",
      "Batch 85 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011520957574248314, Std: 0.08623533695936203\n",
      "z_j Mean: 0.012029437348246574, Std: 0.08687141537666321\n",
      "Similarities: tensor([[1.4286, 0.0000, 1.4286, 0.6323, 0.6243],\n",
      "        [0.0000, 1.4286, 0.0000, 0.0000, 0.0000],\n",
      "        [1.4286, 0.0000, 1.4286, 0.6323, 0.6243],\n",
      "        [0.7708, 0.0000, 0.7708, 1.2606, 0.7077],\n",
      "        [0.7122, 0.0000, 0.7122, 0.7221, 1.4250]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2444968223571777\n",
      "Batch 85, Loss: 3.2444968223571777\n",
      "Processing Batch 86/157\n",
      "Batch 86 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012278317473828793, Std: 0.08613076061010361\n",
      "z_j Mean: 0.012680646032094955, Std: 0.08747934550046921\n",
      "Similarities: tensor([[1.4165, 1.2961, 1.0472, 0.0000, 0.0000],\n",
      "        [1.3368, 1.4189, 0.8793, 0.0000, 0.0000],\n",
      "        [1.0384, 0.8469, 1.4223, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4187, 1.4138],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4276, 1.4285]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1747982501983643\n",
      "Batch 86, Loss: 3.1747982501983643\n",
      "Processing Batch 87/157\n",
      "Batch 87 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012561105191707611, Std: 0.08608996868133545\n",
      "z_j Mean: 0.012838546186685562, Std: 0.08604902774095535\n",
      "Similarities: tensor([[1.4089, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4286, 1.4262, 0.8858, 0.0000],\n",
      "        [0.0000, 1.3079, 1.3386, 1.1802, 0.0000],\n",
      "        [0.0000, 0.4662, 0.5339, 1.2939, 0.0187],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3761]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1735305786132812\n",
      "Batch 87, Loss: 3.1735305786132812\n",
      "Processing Batch 88/157\n",
      "Batch 88 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013100278563797474, Std: 0.08741747587919235\n",
      "z_j Mean: 0.013282827101647854, Std: 0.08738992363214493\n",
      "Similarities: tensor([[1.3519, 0.3425, 0.3773, 0.2048, 0.1031],\n",
      "        [0.2545, 1.3211, 0.0000, 0.3814, 1.3417],\n",
      "        [0.2212, 0.0000, 1.3694, 0.0000, 0.0000],\n",
      "        [0.1192, 0.1417, 0.0000, 1.3429, 0.4193],\n",
      "        [0.2390, 0.8852, 0.0164, 0.5758, 1.2070]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.162546396255493\n",
      "Batch 88, Loss: 3.162546396255493\n",
      "Processing Batch 89/157\n",
      "Batch 89 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012629365548491478, Std: 0.08536789566278458\n",
      "z_j Mean: 0.012638572603464127, Std: 0.08607863634824753\n",
      "Similarities: tensor([[1.4277, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4280, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.2098, 0.2989, 0.7248],\n",
      "        [0.0000, 0.0000, 0.2615, 1.2262, 0.2539],\n",
      "        [0.0000, 0.0000, 0.5579, 0.5726, 1.4137]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2183024883270264\n",
      "Batch 89, Loss: 3.2183024883270264\n",
      "Processing Batch 90/157\n",
      "Batch 90 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012223859317600727, Std: 0.08613850921392441\n",
      "z_j Mean: 0.011980229988694191, Std: 0.08617273718118668\n",
      "Similarities: tensor([[1.3755, 0.0000, 0.1042, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3237, 0.7752, 0.2052, 0.0000],\n",
      "        [0.0299, 0.8194, 1.4253, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4286, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4286]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.202971935272217\n",
      "Batch 90, Loss: 3.202971935272217\n",
      "Processing Batch 91/157\n",
      "Batch 91 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.0120183564722538, Std: 0.08473873883485794\n",
      "z_j Mean: 0.011882022023200989, Std: 0.08403468877077103\n",
      "Similarities: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3935, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3754, 0.0742, 1.4156],\n",
      "        [0.0000, 0.0868, 0.0000, 1.3250, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3194, 0.0000, 1.4286]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2430057525634766\n",
      "Batch 91, Loss: 3.2430057525634766\n",
      "Processing Batch 92/157\n",
      "Batch 92 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012250804342329502, Std: 0.08684048056602478\n",
      "z_j Mean: 0.012297747656702995, Std: 0.08612798899412155\n",
      "Similarities: tensor([[1.4200, 0.0520, 1.3990, 1.4100, 0.0043],\n",
      "        [0.0000, 1.2443, 0.0000, 0.0000, 1.2171],\n",
      "        [1.4144, 0.0945, 1.4211, 1.3442, 0.0078],\n",
      "        [1.2779, 0.0000, 1.2465, 1.3661, 0.0000],\n",
      "        [0.0000, 1.1115, 0.0000, 0.0000, 1.4274]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2104275226593018\n",
      "Batch 92, Loss: 3.2104275226593018\n",
      "Processing Batch 93/157\n",
      "Batch 93 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01255895011126995, Std: 0.08466029912233353\n",
      "z_j Mean: 0.012681100517511368, Std: 0.08464208990335464\n",
      "Similarities: tensor([[1.4286, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4134, 0.0000, 0.9037, 0.0000],\n",
      "        [0.0000, 0.0174, 1.3814, 0.2240, 0.8669],\n",
      "        [0.0000, 0.6328, 0.2247, 1.3948, 0.5198],\n",
      "        [0.0000, 0.0341, 0.5987, 0.3585, 1.4048]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1866211891174316\n",
      "Batch 93, Loss: 3.1866211891174316\n",
      "Processing Batch 94/157\n",
      "Batch 94 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.010787725448608398, Std: 0.08418217301368713\n",
      "z_j Mean: 0.010947700589895248, Std: 0.0863099992275238\n",
      "Similarities: tensor([[1.2472, 0.1240, 0.0000, 0.0000, 0.1258],\n",
      "        [0.0000, 1.4112, 0.0000, 0.0000, 0.4649],\n",
      "        [0.0000, 0.0000, 1.4207, 0.4123, 0.0000],\n",
      "        [0.0000, 0.0000, 0.4752, 1.4286, 0.0000],\n",
      "        [0.2537, 0.4356, 0.0000, 0.0000, 1.4178]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2736306190490723\n",
      "Batch 94, Loss: 3.2736306190490723\n",
      "Processing Batch 95/157\n",
      "Batch 95 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011476589366793633, Std: 0.08553051948547363\n",
      "z_j Mean: 0.010942082852125168, Std: 0.08416224271059036\n",
      "Similarities: tensor([[1.4050e+00, 0.0000e+00, 1.1780e-01, 0.0000e+00, 1.2846e+00],\n",
      "        [0.0000e+00, 1.3662e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [8.1074e-02, 0.0000e+00, 1.3963e+00, 0.0000e+00, 9.5607e-04],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 1.4286e+00, 0.0000e+00],\n",
      "        [1.2033e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.4028e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.237321376800537\n",
      "Batch 95, Loss: 3.237321376800537\n",
      "Processing Batch 96/157\n",
      "Batch 96 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01207086443901062, Std: 0.08544865995645523\n",
      "z_j Mean: 0.012026162818074226, Std: 0.08545496314764023\n",
      "Similarities: tensor([[1.3650, 0.9477, 0.0127, 0.0000, 0.6433],\n",
      "        [1.0192, 1.3345, 0.1902, 0.0000, 0.4487],\n",
      "        [0.0000, 0.3868, 1.4087, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0017, 0.0000],\n",
      "        [0.3176, 0.0524, 0.0000, 0.0000, 1.4246]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2093780040740967\n",
      "Batch 96, Loss: 3.2093780040740967\n",
      "Processing Batch 97/157\n",
      "Batch 97 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012179268524050713, Std: 0.08685053884983063\n",
      "z_j Mean: 0.011748889461159706, Std: 0.08620458841323853\n",
      "Similarities: tensor([[1.3841, 1.3592, 1.2683, 0.0000, 0.4333],\n",
      "        [1.3795, 1.4113, 1.3960, 0.0000, 0.0572],\n",
      "        [1.3916, 1.3736, 1.4286, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4082, 0.0000],\n",
      "        [0.0836, 0.0786, 0.0000, 0.0000, 1.2815]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2045905590057373\n",
      "Batch 97, Loss: 3.2045905590057373\n",
      "Processing Batch 98/157\n",
      "Batch 98 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011250435374677181, Std: 0.08697575330734253\n",
      "z_j Mean: 0.011062005534768105, Std: 0.08558513224124908\n",
      "Similarities: tensor([[1.2772, 0.0000, 1.1907, 0.0000, 1.1963],\n",
      "        [0.0000, 1.4286, 0.0000, 1.4275, 0.0000],\n",
      "        [1.3314, 0.0000, 1.3876, 0.0000, 1.0338],\n",
      "        [0.0000, 1.4240, 0.0000, 1.4273, 0.0000],\n",
      "        [0.8958, 0.0000, 1.0423, 0.0000, 1.3592]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.266103506088257\n",
      "Batch 98, Loss: 3.266103506088257\n",
      "Processing Batch 99/157\n",
      "Batch 99 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012709593400359154, Std: 0.08747513592243195\n",
      "z_j Mean: 0.01261758804321289, Std: 0.08748845756053925\n",
      "Similarities: tensor([[1.2919, 0.6067, 0.0000, 0.0000, 0.7643],\n",
      "        [0.3299, 1.4101, 0.0000, 0.0000, 1.3319],\n",
      "        [0.0000, 0.0000, 1.4118, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4286, 0.0000],\n",
      "        [0.3721, 1.0453, 0.0000, 0.0000, 1.3695]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.250309944152832\n",
      "Batch 99, Loss: 3.250309944152832\n",
      "Processing Batch 100/157\n",
      "Batch 100 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013507593423128128, Std: 0.08594653010368347\n",
      "z_j Mean: 0.013226032257080078, Std: 0.0852774828672409\n",
      "Similarities: tensor([[1.3900, 0.0000, 0.0000, 0.0000, 0.3672],\n",
      "        [0.0000, 1.4228, 0.0000, 0.0000, 1.1928],\n",
      "        [0.0000, 0.0000, 1.3946, 1.1078, 0.0000],\n",
      "        [0.0000, 0.0000, 1.1648, 1.3722, 0.0000],\n",
      "        [0.2817, 1.1692, 0.0000, 0.0000, 1.4265]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.234583854675293\n",
      "Batch 100, Loss: 3.234583854675293\n",
      "Processing Batch 101/157\n",
      "Batch 101 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013863594271242619, Std: 0.08659761399030685\n",
      "z_j Mean: 0.014004206284880638, Std: 0.0865749791264534\n",
      "Similarities: tensor([[1.3988, 0.0000, 0.8757, 0.3774, 0.9618],\n",
      "        [0.0000, 1.4233, 0.0000, 0.0000, 0.0000],\n",
      "        [0.7050, 0.0000, 1.4031, 0.0000, 1.0331],\n",
      "        [0.5669, 0.0000, 0.0000, 1.4154, 0.0571],\n",
      "        [0.5570, 0.0000, 0.8813, 0.0150, 1.3160]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1566147804260254\n",
      "Batch 101, Loss: 3.1566147804260254\n",
      "Processing Batch 102/157\n",
      "Batch 102 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012597186490893364, Std: 0.08679089695215225\n",
      "z_j Mean: 0.012076199054718018, Std: 0.08544791489839554\n",
      "Similarities: tensor([[1.4232, 0.0000, 0.9425, 0.0000, 0.3477],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.9514, 0.0000, 1.4268, 0.1709, 1.0022],\n",
      "        [0.0000, 0.0000, 0.1774, 1.4267, 0.6641],\n",
      "        [0.1565, 0.0000, 0.8341, 0.7916, 1.3606]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.250751256942749\n",
      "Batch 102, Loss: 3.250751256942749\n",
      "Processing Batch 103/157\n",
      "Batch 103 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011769029311835766, Std: 0.08549077063798904\n",
      "z_j Mean: 0.01203513890504837, Std: 0.08616507798433304\n",
      "Similarities: tensor([[1.2474e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 1.3745e+00, 0.0000e+00, 1.2791e+00, 2.8225e-01],\n",
      "        [4.3417e-01, 0.0000e+00, 1.4286e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 1.0018e+00, 0.0000e+00, 1.3733e+00, 1.2638e-01],\n",
      "        [0.0000e+00, 1.6332e-03, 0.0000e+00, 1.2112e-03, 1.0622e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.215744733810425\n",
      "Batch 103, Loss: 3.215744733810425\n",
      "Processing Batch 104/157\n",
      "Batch 104 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011845714412629604, Std: 0.0847630500793457\n",
      "z_j Mean: 0.011362908408045769, Std: 0.0841064527630806\n",
      "Similarities: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4278, 0.0000, 0.0000, 0.0000],\n",
      "        [0.7534, 0.0000, 1.4125, 0.0000, 0.6351],\n",
      "        [0.0000, 1.3919, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.6435, 0.0000, 1.4044]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2449357509613037\n",
      "Batch 104, Loss: 3.2449357509613037\n",
      "Processing Batch 105/157\n",
      "Batch 105 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01075020432472229, Std: 0.08562486618757248\n",
      "z_j Mean: 0.010270923376083374, Std: 0.08424679934978485\n",
      "Similarities: tensor([[1.3254, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3236, 0.0000, 0.0000, 0.3902],\n",
      "        [0.0000, 0.0000, 1.3904, 0.0000, 1.3011],\n",
      "        [0.0000, 0.0000, 0.0000, 1.1598, 0.0000],\n",
      "        [0.0000, 0.6627, 1.1116, 0.0000, 1.3445]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.288008689880371\n",
      "Batch 105, Loss: 3.288008689880371\n",
      "Processing Batch 106/157\n",
      "Batch 106 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012905148789286613, Std: 0.08532664179801941\n",
      "z_j Mean: 0.01308309193700552, Std: 0.08601217716932297\n",
      "Similarities: tensor([[1.2801, 0.0000, 0.0000, 0.0000, 1.0490],\n",
      "        [0.0000, 1.4054, 0.0000, 0.9214, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4170, 0.0000, 0.0000],\n",
      "        [0.0000, 0.9941, 0.0000, 1.4052, 0.0000],\n",
      "        [1.0501, 0.0000, 0.0000, 0.0000, 1.3396]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.147758722305298\n",
      "Batch 106, Loss: 3.147758722305298\n",
      "Processing Batch 107/157\n",
      "Batch 107 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011011356487870216, Std: 0.08342468738555908\n",
      "z_j Mean: 0.010024301707744598, Std: 0.08207480609416962\n",
      "Similarities: tensor([[1.4204, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4245, 0.0000, 1.4013, 0.2514],\n",
      "        [0.0000, 0.0000, 1.4207, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3786, 0.0000, 1.4237, 0.4864],\n",
      "        [0.0000, 0.4496, 0.0000, 0.6028, 1.3256]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.290518283843994\n",
      "Batch 107, Loss: 3.290518283843994\n",
      "Processing Batch 108/157\n",
      "Batch 108 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.010295652784407139, Std: 0.0812937468290329\n",
      "z_j Mean: 0.010133814066648483, Std: 0.08055988699197769\n",
      "Similarities: tensor([[1.4239, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1746, 0.0000, 0.7882, 0.3177],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0920, 0.0000, 1.4279, 1.3512],\n",
      "        [0.0000, 0.0466, 0.0000, 1.3585, 1.4271]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.324448347091675\n",
      "Batch 108, Loss: 3.324448347091675\n",
      "Processing Batch 109/157\n",
      "Batch 109 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011442500166594982, Std: 0.08481843024492264\n",
      "z_j Mean: 0.011329646222293377, Std: 0.08411094546318054\n",
      "Similarities: tensor([[1.4273, 0.0000, 0.0000, 0.0591, 0.0000],\n",
      "        [0.0000, 1.3971, 0.8351, 0.0000, 0.0000],\n",
      "        [0.0000, 0.6079, 1.4009, 0.0000, 0.0000],\n",
      "        [0.1291, 0.0000, 0.0000, 1.2825, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.7581, 0.0000]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.267293930053711\n",
      "Batch 109, Loss: 3.267293930053711\n",
      "Processing Batch 110/157\n",
      "Batch 110 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01205211691558361, Std: 0.08756816387176514\n",
      "z_j Mean: 0.012246765196323395, Std: 0.08684104681015015\n",
      "Similarities: tensor([[1.4195, 1.0415, 0.0000, 0.0000, 0.0000],\n",
      "        [0.9307, 1.4059, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.1541, 1.1581, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3108, 1.4055, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4066]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.201568126678467\n",
      "Batch 110, Loss: 3.201568126678467\n",
      "Processing Batch 111/157\n",
      "Batch 111 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01178798545151949, Std: 0.08619924634695053\n",
      "z_j Mean: 0.011434763669967651, Std: 0.08409671485424042\n",
      "Similarities: tensor([[1.4240, 0.0000, 0.0000, 0.8479, 0.0000],\n",
      "        [0.0000, 1.4185, 1.3600, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3887, 1.4238, 0.0000, 0.0000],\n",
      "        [1.2018, 0.0000, 0.0000, 1.2703, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.2959]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285622835159302]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2660064697265625\n",
      "Batch 111, Loss: 3.2660064697265625\n",
      "Processing Batch 112/157\n",
      "Batch 112 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011295306496322155, Std: 0.08555465191602707\n",
      "z_j Mean: 0.01144818589091301, Std: 0.08553432673215866\n",
      "Similarities: tensor([[0.0000, 0.0000, 0.0000, 0.2932, 0.3705],\n",
      "        [0.0000, 1.3800, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4178, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3907, 1.3644],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4215, 1.4183]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2531681060791016\n",
      "Batch 112, Loss: 3.2531681060791016\n",
      "Processing Batch 113/157\n",
      "Batch 113 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012319064699113369, Std: 0.08683081716299057\n",
      "z_j Mean: 0.012340955436229706, Std: 0.08752791583538055\n",
      "Similarities: tensor([[1.3978, 0.0000, 0.5700, 1.2046, 0.0000],\n",
      "        [0.0000, 1.4184, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3698, 0.0000, 1.4191, 0.0608, 0.0000],\n",
      "        [1.1583, 0.0000, 0.0000, 1.3840, 0.0418],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4123]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2392563819885254\n",
      "Batch 113, Loss: 3.2392563819885254\n",
      "Processing Batch 114/157\n",
      "Batch 114 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012212229892611504, Std: 0.08684590458869934\n",
      "z_j Mean: 0.01165889110416174, Std: 0.08550585806369781\n",
      "Similarities: tensor([[1.4174, 1.3981, 0.0000, 0.0000, 0.0000],\n",
      "        [1.3854, 1.4231, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4193, 0.0000, 0.7695],\n",
      "        [0.0159, 0.1744, 0.0625, 1.3349, 0.1796],\n",
      "        [0.0000, 0.0000, 0.8926, 0.0000, 1.4264]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285675287246704]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2758140563964844\n",
      "Batch 114, Loss: 3.2758140563964844\n",
      "Processing Batch 115/157\n",
      "Batch 115 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012416367419064045, Std: 0.08611096441745758\n",
      "z_j Mean: 0.012537196278572083, Std: 0.08679958432912827\n",
      "Similarities: tensor([[1.3198, 0.0000, 0.0000, 1.1801, 1.3305],\n",
      "        [0.0000, 1.4163, 0.0000, 0.1955, 0.0000],\n",
      "        [0.0851, 0.0000, 1.4224, 0.0000, 0.0000],\n",
      "        [1.2139, 0.1765, 0.0000, 1.4158, 1.2662],\n",
      "        [1.3993, 0.1150, 0.0196, 1.2773, 1.4057]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.281599521636963\n",
      "Batch 115, Loss: 3.281599521636963\n",
      "Processing Batch 116/157\n",
      "Batch 116 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01223062165081501, Std: 0.08684331923723221\n",
      "z_j Mean: 0.012300948612391949, Std: 0.08612753450870514\n",
      "Similarities: tensor([[1.4266, 0.0000, 0.0000, 0.0000, 0.2704],\n",
      "        [0.0000, 1.3279, 1.2221, 0.0000, 1.1083],\n",
      "        [0.0000, 1.3717, 1.4071, 0.0000, 0.8225],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3413, 0.0000],\n",
      "        [0.3631, 0.8622, 0.8511, 0.0000, 1.4172]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.428494930267334]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2588229179382324\n",
      "Batch 116, Loss: 3.2588229179382324\n",
      "Processing Batch 117/157\n",
      "Batch 117 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011600226163864136, Std: 0.08622471988201141\n",
      "z_j Mean: 0.011554408818483353, Std: 0.08623086661100388\n",
      "Similarities: tensor([[1.3481, 0.0000, 0.4293, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4274, 0.0000, 0.7706, 0.7814],\n",
      "        [0.1387, 0.0000, 1.4194, 0.0000, 0.0000],\n",
      "        [0.0000, 0.7495, 0.0000, 1.4261, 1.4279],\n",
      "        [0.0000, 0.7354, 0.0000, 1.4234, 1.4265]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2185580730438232\n",
      "Batch 117, Loss: 3.2185580730438232\n",
      "Processing Batch 118/157\n",
      "Batch 118 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01203833520412445, Std: 0.08687018603086472\n",
      "z_j Mean: 0.011643953621387482, Std: 0.08692393451929092\n",
      "Similarities: tensor([[1.4223, 0.0000, 0.0000, 0.0452, 1.2480],\n",
      "        [0.0000, 1.2458, 0.0000, 0.3820, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3495, 0.0000, 0.0000],\n",
      "        [0.0546, 0.3634, 0.0000, 1.4209, 0.0000],\n",
      "        [1.1589, 0.0000, 0.0000, 0.0000, 1.4253]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.208178997039795\n",
      "Batch 118, Loss: 3.208178997039795\n",
      "Processing Batch 119/157\n",
      "Batch 119 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011100640520453453, Std: 0.08341284841299057\n",
      "z_j Mean: 0.01155979372560978, Std: 0.08551930636167526\n",
      "Similarities: tensor([[1.4194, 0.2072, 0.0528, 0.0000, 1.1889],\n",
      "        [0.2003, 1.3690, 0.5444, 0.0000, 0.1441],\n",
      "        [0.0073, 0.4310, 1.2938, 0.0214, 0.0024],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4257, 0.0000],\n",
      "        [1.0551, 0.1801, 0.0256, 0.0000, 1.3902]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.264082431793213\n",
      "Batch 119, Loss: 3.264082431793213\n",
      "Processing Batch 120/157\n",
      "Batch 120 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011098265647888184, Std: 0.08558043837547302\n",
      "z_j Mean: 0.011208873242139816, Std: 0.08556601405143738\n",
      "Similarities: tensor([[1.3158, 0.0000, 0.0000, 1.0664, 0.0000],\n",
      "        [0.0000, 1.4071, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.1738, 0.0000, 0.0000],\n",
      "        [0.6122, 0.0000, 0.0000, 1.4286, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4249]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.221580982208252\n",
      "Batch 120, Loss: 3.221580982208252\n",
      "Processing Batch 121/157\n",
      "Batch 121 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.010530179366469383, Std: 0.08421477675437927\n",
      "z_j Mean: 0.010592242702841759, Std: 0.08420699834823608\n",
      "Similarities: tensor([[1.4199, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4127, 1.3305, 0.0546, 0.0000],\n",
      "        [0.0000, 1.3588, 1.3846, 0.0217, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3720, 0.6632],\n",
      "        [0.0000, 0.0000, 0.0000, 0.5783, 1.4286]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.263200283050537\n",
      "Batch 121, Loss: 3.263200283050537\n",
      "Processing Batch 122/157\n",
      "Batch 122 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.010814346373081207, Std: 0.08345045894384384\n",
      "z_j Mean: 0.010746188461780548, Std: 0.08490947633981705\n",
      "Similarities: tensor([[1.4282, 0.0000, 0.7617, 0.0000, 0.0731],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.2643, 0.0000],\n",
      "        [0.0924, 0.1654, 0.0367, 0.0000, 1.3844]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2546420097351074\n",
      "Batch 122, Loss: 3.2546420097351074\n",
      "Processing Batch 123/157\n",
      "Batch 123 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.010864272713661194, Std: 0.08489444106817245\n",
      "z_j Mean: 0.010708646848797798, Std: 0.08419226855039597\n",
      "Similarities: tensor([[1.4059, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4018, 0.0000, 0.0000, 0.7954],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4132, 0.0110],\n",
      "        [0.0000, 0.8487, 0.0000, 0.0000, 1.4188]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.20635724067688\n",
      "Batch 123, Loss: 3.20635724067688\n",
      "Processing Batch 124/157\n",
      "Batch 124 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.010561703704297543, Std: 0.08635809272527695\n",
      "z_j Mean: 0.010447438806295395, Std: 0.0870758593082428\n",
      "Similarities: tensor([[1.4286, 0.0000, 0.0000, 0.0000, 1.4104],\n",
      "        [0.0000, 1.4214, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3583, 0.0000, 0.0159],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4204, 0.0000],\n",
      "        [1.4286, 0.0000, 0.0000, 0.0000, 1.4104]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1959359645843506\n",
      "Batch 124, Loss: 3.1959359645843506\n",
      "Processing Batch 125/157\n",
      "Batch 125 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011168419383466244, Std: 0.08698632568120956\n",
      "z_j Mean: 0.010786501690745354, Std: 0.08490435779094696\n",
      "Similarities: tensor([[1.4205, 0.0000, 0.2882, 0.0000, 0.5345],\n",
      "        [0.0341, 1.3799, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1968, 0.0000, 1.4257, 0.1539, 1.3862],\n",
      "        [0.0000, 0.0000, 0.0201, 1.4166, 0.0190],\n",
      "        [0.6668, 0.0000, 1.3694, 0.1369, 1.4202]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2052083015441895\n",
      "Batch 125, Loss: 3.2052083015441895\n",
      "Processing Batch 126/157\n",
      "Batch 126 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.009629787877202034, Std: 0.08137531578540802\n",
      "z_j Mean: 0.010450545698404312, Std: 0.08349680155515671\n",
      "Similarities: tensor([[1.4023, 1.0906, 1.0108, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [1.1940, 0.8172, 1.3845, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.1448, 1.4061, 0.1131],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3614]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.280520439147949\n",
      "Batch 126, Loss: 3.280520439147949\n",
      "Processing Batch 127/157\n",
      "Batch 127 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.009562689810991287, Std: 0.08062967658042908\n",
      "z_j Mean: 0.009500136598944664, Std: 0.08063707500696182\n",
      "Similarities: tensor([[1.4286, 0.0000, 0.0000, 0.0000, 0.6035],\n",
      "        [0.0000, 1.3370, 0.4972, 0.0000, 0.0000],\n",
      "        [0.0000, 0.8927, 1.4106, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5804, 0.0000, 0.0217, 0.0000, 1.3833]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.329500198364258\n",
      "Batch 127, Loss: 3.329500198364258\n",
      "Processing Batch 128/157\n",
      "Batch 128 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.010179078206419945, Std: 0.08425794541835785\n",
      "z_j Mean: 0.009913293644785881, Std: 0.0828285664319992\n",
      "Similarities: tensor([[1.3622, 0.0000, 0.0000, 0.0000, 0.3539],\n",
      "        [0.0000, 1.4165, 0.0000, 0.0000, 0.0159],\n",
      "        [0.0000, 0.0000, 1.0207, 0.0000, 0.0987],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4285, 0.0000],\n",
      "        [0.0724, 0.0542, 0.0000, 0.0000, 0.9901]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.272094249725342\n",
      "Batch 128, Loss: 3.272094249725342\n",
      "Processing Batch 129/157\n",
      "Batch 129 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011021366342902184, Std: 0.08487418293952942\n",
      "z_j Mean: 0.011236999183893204, Std: 0.0848459005355835\n",
      "Similarities: tensor([[1.3236, 0.0000, 0.0000, 0.9040, 0.0000],\n",
      "        [0.0000, 1.3391, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4265, 0.0000, 0.0000],\n",
      "        [0.3437, 0.0000, 0.0000, 1.1956, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1769046783447266\n",
      "Batch 129, Loss: 3.1769046783447266\n",
      "Processing Batch 130/157\n",
      "Batch 130 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.010609004646539688, Std: 0.08347681164741516\n",
      "z_j Mean: 0.01086345687508583, Std: 0.08344407379627228\n",
      "Similarities: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4278, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.2933, 0.0000, 0.9876],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4286, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3294, 0.0000, 1.4286]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2256386280059814\n",
      "Batch 130, Loss: 3.2256386280059814\n",
      "Processing Batch 131/157\n",
      "Batch 131 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.010178262367844582, Std: 0.0813085287809372\n",
      "z_j Mean: 0.010309960693120956, Std: 0.0835142731666565\n",
      "Similarities: tensor([[1.3996, 0.0000, 1.1720, 0.0000, 0.7731],\n",
      "        [0.0000, 0.9297, 0.0000, 0.8943, 0.0000],\n",
      "        [1.2604, 0.0000, 1.4118, 0.0000, 1.2160],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4195, 0.0000],\n",
      "        [1.3922, 0.0000, 1.3194, 0.0000, 1.1676]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.289496421813965\n",
      "Batch 131, Loss: 3.289496421813965\n",
      "Processing Batch 132/157\n",
      "Batch 132 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01051435898989439, Std: 0.08421675860881805\n",
      "z_j Mean: 0.010633227415382862, Std: 0.08492369204759598\n",
      "Similarities: tensor([[1.3851, 0.2345, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.1901, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4286, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.2178, 1.4285]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2792415618896484\n",
      "Batch 132, Loss: 3.2792415618896484\n",
      "Processing Batch 133/157\n",
      "Batch 133 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.010078666731715202, Std: 0.08206814527511597\n",
      "z_j Mean: 0.010249316692352295, Std: 0.0820470005273819\n",
      "Similarities: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.2368, 0.1731, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3712, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3712, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4261]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.296513080596924\n",
      "Batch 133, Loss: 3.296513080596924\n",
      "Processing Batch 134/157\n",
      "Batch 134 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011730778031051159, Std: 0.08620704710483551\n",
      "z_j Mean: 0.011535951867699623, Std: 0.08552252501249313\n",
      "Similarities: tensor([[1.3957, 0.5024, 0.0000, 0.4132, 0.3867],\n",
      "        [0.0000, 1.2119, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4277, 0.0000, 0.0000],\n",
      "        [0.1950, 0.0000, 0.0000, 1.3756, 1.0578],\n",
      "        [0.1727, 0.0000, 0.0000, 0.6250, 1.1207]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2204792499542236\n",
      "Batch 134, Loss: 3.2204792499542236\n",
      "Processing Batch 135/157\n",
      "Batch 135 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.010189078748226166, Std: 0.08055291324853897\n",
      "z_j Mean: 0.010537480935454369, Std: 0.08275146782398224\n",
      "Similarities: tensor([[1.4253, 0.0000, 0.0000, 0.0000, 0.0275],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4206, 0.0000],\n",
      "        [0.0024, 0.0000, 0.0000, 0.0000, 1.4132]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2586331367492676\n",
      "Batch 135, Loss: 3.2586331367492676\n",
      "Processing Batch 136/157\n",
      "Batch 136 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011483920738101006, Std: 0.08481282740831375\n",
      "z_j Mean: 0.01195958536118269, Std: 0.08688106387853622\n",
      "Similarities: tensor([[1.3389, 0.0000, 0.9027, 0.0000, 0.0000],\n",
      "        [0.0000, 1.2971, 0.0000, 0.0533, 0.9113],\n",
      "        [1.1571, 0.0000, 1.4242, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0053, 0.0000, 1.3759, 0.0821],\n",
      "        [0.0000, 1.0828, 0.0000, 0.1383, 1.4189]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.164189100265503\n",
      "Batch 136, Loss: 3.164189100265503\n",
      "Processing Batch 137/157\n",
      "Batch 137 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011595216579735279, Std: 0.08551451563835144\n",
      "z_j Mean: 0.011743854731321335, Std: 0.08620526641607285\n",
      "Similarities: tensor([[1.4279, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4112, 0.7422, 1.0513, 0.0083],\n",
      "        [0.0000, 0.7073, 1.4084, 1.0679, 0.2253],\n",
      "        [0.0000, 1.0450, 1.1115, 1.4261, 0.0376],\n",
      "        [0.0000, 0.0000, 0.2530, 0.0246, 1.4212]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1953961849212646\n",
      "Batch 137, Loss: 3.1953961849212646\n",
      "Processing Batch 138/157\n",
      "Batch 138 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011898664757609367, Std: 0.08547281473875046\n",
      "z_j Mean: 0.011944638565182686, Std: 0.08474916964769363\n",
      "Similarities: tensor([[1.4191, 0.0000, 0.0000, 0.0000, 0.3801],\n",
      "        [0.0000, 1.3946, 0.0569, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1680, 1.4022, 1.0960, 0.0000],\n",
      "        [0.0000, 0.0000, 1.0932, 1.4241, 0.0000],\n",
      "        [0.6036, 0.0000, 0.0000, 0.0000, 1.4219]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.234811305999756\n",
      "Batch 138, Loss: 3.234811305999756\n",
      "Processing Batch 139/157\n",
      "Batch 139 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011285383254289627, Std: 0.08338805288076401\n",
      "z_j Mean: 0.011275844648480415, Std: 0.08265408128499985\n",
      "Similarities: tensor([[1.4110, 0.6365, 0.4622, 0.0915, 0.0000],\n",
      "        [0.5837, 1.4063, 1.2703, 0.5952, 0.0182],\n",
      "        [0.4155, 1.3056, 1.4218, 0.9763, 0.2241],\n",
      "        [0.1308, 0.7180, 0.9196, 1.4091, 0.1929],\n",
      "        [0.0000, 0.1373, 0.2485, 0.2845, 1.3462]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285683631896973]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.260622262954712\n",
      "Batch 139, Loss: 3.260622262954712\n",
      "Processing Batch 140/157\n",
      "Batch 140 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011162586510181427, Std: 0.08485572785139084\n",
      "z_j Mean: 0.010626422241330147, Std: 0.08347459137439728\n",
      "Similarities: tensor([[1.4253, 0.0000, 0.0000, 0.0000, 0.6233],\n",
      "        [0.0000, 1.4210, 0.1137, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3918, 0.0295, 0.0041],\n",
      "        [0.0000, 0.0000, 0.3507, 1.4286, 0.1970],\n",
      "        [0.6153, 0.0000, 0.0431, 0.1757, 1.4238]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.259152889251709\n",
      "Batch 140, Loss: 3.259152889251709\n",
      "Processing Batch 141/157\n",
      "Batch 141 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.010772604495286942, Std: 0.08197993040084839\n",
      "z_j Mean: 0.01081916969269514, Std: 0.08197379857301712\n",
      "Similarities: tensor([[1.4286e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 9.2787e-01],\n",
      "        [0.0000e+00, 1.4281e+00, 7.8001e-01, 1.3475e+00, 6.4747e-04],\n",
      "        [0.0000e+00, 7.6677e-01, 1.3983e+00, 8.1012e-01, 9.7798e-02],\n",
      "        [0.0000e+00, 1.4286e+00, 7.7463e-01, 1.3379e+00, 0.0000e+00],\n",
      "        [9.8639e-01, 3.4578e-01, 3.8671e-01, 3.2382e-01, 1.1903e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.294827938079834\n",
      "Batch 141, Loss: 3.294827938079834\n",
      "Processing Batch 142/157\n",
      "Batch 142 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01218966394662857, Std: 0.0854317918419838\n",
      "z_j Mean: 0.011803770437836647, Std: 0.08476890623569489\n",
      "Similarities: tensor([[1.3125, 0.0000, 0.0000, 0.0289, 0.4595],\n",
      "        [0.0000, 1.1526, 0.8546, 0.2022, 0.0000],\n",
      "        [0.0000, 1.0118, 1.3358, 0.0000, 0.0000],\n",
      "        [0.0000, 0.6168, 0.0000, 0.6734, 0.1152],\n",
      "        [0.0471, 0.0000, 0.0000, 0.0000, 1.3797]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4283961057662964]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2098171710968018\n",
      "Batch 142, Loss: 3.2098171710968018\n",
      "Processing Batch 143/157\n",
      "Batch 143 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011967000551521778, Std: 0.08617457747459412\n",
      "z_j Mean: 0.011756622232496738, Std: 0.08477545529603958\n",
      "Similarities: tensor([[1.4145, 0.8778, 0.9665, 0.0051, 0.0000],\n",
      "        [0.9891, 1.4196, 1.3582, 0.0000, 0.0000],\n",
      "        [1.0418, 1.2975, 1.4286, 0.0000, 0.0000],\n",
      "        [0.0451, 0.0000, 0.0000, 1.3998, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4139]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2235238552093506\n",
      "Batch 143, Loss: 3.2235238552093506\n",
      "Processing Batch 144/157\n",
      "Batch 144 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012376890517771244, Std: 0.08611664921045303\n",
      "z_j Mean: 0.01237686537206173, Std: 0.08468711376190186\n",
      "Similarities: tensor([[1.1422, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4252, 0.4551, 0.3604, 0.7175],\n",
      "        [0.4429, 0.3980, 1.4059, 0.0000, 0.6278],\n",
      "        [0.0000, 0.3073, 0.0000, 1.4057, 0.0000],\n",
      "        [0.0000, 0.5572, 0.9196, 0.0000, 1.3206]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2477211952209473\n",
      "Batch 144, Loss: 3.2477211952209473\n",
      "Processing Batch 145/157\n",
      "Batch 145 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011444158852100372, Std: 0.08336640149354935\n",
      "z_j Mean: 0.011522623710334301, Std: 0.08335559070110321\n",
      "Similarities: tensor([[1.4274, 0.0000, 1.3627, 0.0000, 0.0136],\n",
      "        [0.0000, 1.3965, 0.0049, 0.0000, 0.6020],\n",
      "        [1.3914, 0.0000, 1.4189, 0.0000, 0.0739],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0044, 0.7004, 0.1359, 0.0000, 1.4249]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.311110734939575\n",
      "Batch 145, Loss: 3.311110734939575\n",
      "Processing Batch 146/157\n",
      "Batch 146 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011876380071043968, Std: 0.08547592163085938\n",
      "z_j Mean: 0.011781161651015282, Std: 0.08548909425735474\n",
      "Similarities: tensor([[1.4286, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4121, 0.8158, 1.1210, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.1596, 0.7069, 1.4271, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2656362056732178\n",
      "Batch 146, Loss: 3.2656362056732178\n",
      "Processing Batch 147/157\n",
      "Batch 147 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012500984594225883, Std: 0.08680480718612671\n",
      "z_j Mean: 0.012386739253997803, Std: 0.08752144873142242\n",
      "Similarities: tensor([[1.2595, 0.0000, 0.0000, 1.1374, 0.0000],\n",
      "        [0.0102, 0.6675, 0.1527, 0.2419, 0.0000],\n",
      "        [0.0000, 0.0000, 1.0184, 0.0000, 0.9705],\n",
      "        [1.3499, 0.0069, 0.0000, 1.4152, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4286]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.213200330734253\n",
      "Batch 147, Loss: 3.213200330734253\n",
      "Processing Batch 148/157\n",
      "Batch 148 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01284035388380289, Std: 0.08675525337457657\n",
      "z_j Mean: 0.01237674430012703, Std: 0.08611667156219482\n",
      "Similarities: tensor([[1.4000, 0.5557, 0.0690, 0.0000, 0.0000],\n",
      "        [1.0225, 1.3216, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4280, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.184837818145752\n",
      "Batch 148, Loss: 3.184837818145752\n",
      "Processing Batch 149/157\n",
      "Batch 149 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.0124686099588871, Std: 0.0861034169793129\n",
      "z_j Mean: 0.01248871348798275, Std: 0.08538858592510223\n",
      "Similarities: tensor([[1.3955, 0.0000, 0.4165, 1.1081, 0.4591],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5736, 0.0000, 1.4159, 0.0413, 0.2960],\n",
      "        [0.9039, 0.0000, 0.0000, 1.4227, 0.3232],\n",
      "        [1.0034, 0.0000, 0.2837, 1.0820, 0.9318]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1859638690948486\n",
      "Batch 149, Loss: 3.1859638690948486\n",
      "Processing Batch 150/157\n",
      "Batch 150 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012175414711236954, Std: 0.08614537119865417\n",
      "z_j Mean: 0.012227596715092659, Std: 0.08613797277212143\n",
      "Similarities: tensor([[1.4234, 0.0000, 0.0000, 1.1224, 1.3384],\n",
      "        [0.0000, 1.2591, 0.0635, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4225, 0.0000, 0.0000],\n",
      "        [1.1566, 0.0000, 0.0000, 1.2130, 1.1180],\n",
      "        [1.4249, 0.0000, 0.0000, 1.1351, 1.3525]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2449758052825928\n",
      "Batch 150, Loss: 3.2449758052825928\n",
      "Processing Batch 151/157\n",
      "Batch 151 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011951854452490807, Std: 0.08546539396047592\n",
      "z_j Mean: 0.01223740354180336, Std: 0.0861365869641304\n",
      "Similarities: tensor([[1.4227, 0.7251, 0.1888, 0.0000, 0.0847],\n",
      "        [0.5768, 1.4198, 1.2672, 0.0000, 0.0000],\n",
      "        [0.0861, 1.0213, 1.1634, 0.0000, 0.7426],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3670, 0.0000],\n",
      "        [0.6121, 0.2801, 0.1365, 0.0000, 1.3338]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.224508285522461\n",
      "Batch 151, Loss: 3.224508285522461\n",
      "Processing Batch 152/157\n",
      "Batch 152 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01186802051961422, Std: 0.0854770839214325\n",
      "z_j Mean: 0.01199314184486866, Std: 0.08474230766296387\n",
      "Similarities: tensor([[1.4286, 0.5174, 0.0000, 0.0000, 0.0000],\n",
      "        [0.6395, 1.4209, 0.8759, 0.0000, 0.0000],\n",
      "        [0.0000, 1.1064, 1.3751, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0357, 1.4224, 0.7879],\n",
      "        [0.0000, 0.0000, 0.0000, 0.3051, 0.9478]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2206919193267822\n",
      "Batch 152, Loss: 3.2206919193267822\n",
      "Processing Batch 153/157\n",
      "Batch 153 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011916623450815678, Std: 0.08475310355424881\n",
      "z_j Mean: 0.011806667782366276, Std: 0.08476850390434265\n",
      "Similarities: tensor([[1.1394, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4112, 0.0000, 0.8546, 1.3814],\n",
      "        [0.0803, 0.0000, 1.4045, 0.0000, 0.0000],\n",
      "        [0.0000, 1.1003, 0.0000, 1.3620, 1.2533],\n",
      "        [0.0000, 1.3420, 0.0000, 1.0090, 1.4277]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2690441608428955\n",
      "Batch 153, Loss: 3.2690441608428955\n",
      "Processing Batch 154/157\n",
      "Batch 154 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011628285981714725, Std: 0.08260522782802582\n",
      "z_j Mean: 0.012019138783216476, Std: 0.08328544348478317\n",
      "Similarities: tensor([[1.4204, 0.1907, 0.0000, 1.2763, 1.1384],\n",
      "        [0.0743, 1.3592, 0.7754, 0.1023, 0.1888],\n",
      "        [0.0000, 0.0218, 0.9432, 0.0000, 0.0000],\n",
      "        [1.3099, 0.2137, 0.0000, 1.3982, 1.1782],\n",
      "        [1.0454, 0.2768, 0.0042, 1.2294, 1.4162]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2838640213012695\n",
      "Batch 154, Loss: 3.2838640213012695\n",
      "Processing Batch 155/157\n",
      "Batch 155 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012272031977772713, Std: 0.08470237255096436\n",
      "z_j Mean: 0.012756011448800564, Std: 0.08534906059503555\n",
      "Similarities: tensor([[1.4105, 0.0000, 0.2128, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4131, 0.7865, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1187, 0.1476, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3981, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4164]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2374935150146484\n",
      "Batch 155, Loss: 3.2374935150146484\n",
      "Processing Batch 156/157\n",
      "Batch 156 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01249169372022152, Std: 0.08610007166862488\n",
      "z_j Mean: 0.012071770615875721, Std: 0.08615995943546295\n",
      "Similarities: tensor([[1.3987, 0.0000, 0.0000, 1.2983, 0.0000],\n",
      "        [0.8525, 0.0000, 0.0000, 0.5032, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4017, 0.0000, 0.0000],\n",
      "        [1.2986, 0.0000, 0.0000, 1.4111, 0.0000],\n",
      "        [0.0228, 0.0000, 0.0000, 0.0000, 0.0000]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2491252422332764\n",
      "Batch 156, Loss: 3.2491252422332764\n",
      "Processing Batch 157/157\n",
      "Batch 157 loaded: x.shape=torch.Size([16, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([16, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([16, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([16, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([16, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([16, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.015567249618470669, Std: 0.08702791482210159\n",
      "z_j Mean: 0.015034683980047703, Std: 0.08712154626846313\n",
      "Similarities: tensor([[1.4074e+00, 2.3525e-01, 9.7848e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [1.8303e-01, 1.4246e+00, 7.8382e-02, 0.0000e+00, 0.0000e+00],\n",
      "        [1.1190e+00, 7.9288e-02, 1.3562e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 4.4423e-04, 0.0000e+00, 1.3568e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.4222e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4246265888214111]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 1.8364325761795044\n",
      "Batch 157, Loss: 1.8364325761795044\n",
      "Epoch 3/10, Loss: 3.2519\n",
      "Entered Epoch 4\n",
      "Starting Training Loop with 157 batches.\n",
      "Processing Batch 1/157\n",
      "Batch 1 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012667366303503513, Std: 0.0853622630238533\n",
      "z_j Mean: 0.012658417224884033, Std: 0.08464548736810684\n",
      "Similarities: tensor([[1.3798, 0.0000, 1.0262, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4149, 0.0000, 0.0000, 0.0000],\n",
      "        [1.2107, 0.0000, 1.4286, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4248, 0.1677],\n",
      "        [0.0000, 0.0000, 0.0000, 0.2331, 1.3198]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.198800563812256\n",
      "Batch 1, Loss: 3.198800563812256\n",
      "Processing Batch 2/157\n",
      "Batch 2 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012525358237326145, Std: 0.08466527611017227\n",
      "z_j Mean: 0.012527203187346458, Std: 0.08538295328617096\n",
      "Similarities: tensor([[1.4265, 0.5057, 1.3432, 0.0000, 0.0000],\n",
      "        [0.5601, 1.4275, 0.3962, 0.0000, 0.0000],\n",
      "        [1.3525, 0.3882, 1.4251, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3630]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285080432891846]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.176283359527588\n",
      "Batch 2, Loss: 3.176283359527588\n",
      "Processing Batch 3/157\n",
      "Batch 3 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01347130537033081, Std: 0.08736106753349304\n",
      "z_j Mean: 0.01333559025079012, Std: 0.08597338944673538\n",
      "Similarities: tensor([[1.3885e+00, 5.3625e-01, 3.7923e-04, 0.0000e+00, 0.0000e+00],\n",
      "        [7.6251e-01, 1.4235e+00, 2.4156e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [1.6804e-02, 3.3523e-01, 1.4106e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 1.4030e+00, 1.2951e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 1.2784e+00, 1.4286e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1588611602783203\n",
      "Batch 3, Loss: 3.1588611602783203\n",
      "Processing Batch 4/157\n",
      "Batch 4 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013548053801059723, Std: 0.08664754033088684\n",
      "z_j Mean: 0.013409135863184929, Std: 0.08666915446519852\n",
      "Similarities: tensor([[1.3426, 0.0701, 0.7077, 0.0000, 0.0000],\n",
      "        [0.3643, 1.4118, 0.8314, 0.0000, 0.0000],\n",
      "        [1.0622, 0.7103, 1.4234, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3489, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3953]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.428300142288208]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.123039722442627\n",
      "Batch 4, Loss: 3.123039722442627\n",
      "Processing Batch 5/157\n",
      "Batch 5 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013160252012312412, Std: 0.08670730143785477\n",
      "z_j Mean: 0.013153540901839733, Std: 0.08670831471681595\n",
      "Similarities: tensor([[1.3884, 0.5518, 1.0401, 0.0000, 1.1961],\n",
      "        [0.5645, 1.3447, 0.2685, 0.0000, 0.5245],\n",
      "        [0.9577, 0.3168, 1.4102, 0.0000, 1.0139],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4197, 0.0000],\n",
      "        [1.1061, 0.5838, 1.0546, 0.0000, 1.3055]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.189453363418579\n",
      "Batch 5, Loss: 3.189453363418579\n",
      "Processing Batch 6/157\n",
      "Batch 6 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01257091760635376, Std: 0.08679470419883728\n",
      "z_j Mean: 0.012306222692131996, Std: 0.08612678200006485\n",
      "Similarities: tensor([[1.4262, 0.0127, 0.9378, 0.0709, 0.8382],\n",
      "        [0.1092, 1.4242, 0.0594, 0.0058, 0.0522],\n",
      "        [0.9205, 0.0109, 1.4264, 0.0000, 1.0717],\n",
      "        [0.3374, 0.0000, 0.0000, 1.3037, 0.2848],\n",
      "        [0.9380, 0.0046, 1.0041, 0.0663, 1.4092]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1672234535217285\n",
      "Batch 6, Loss: 3.1672234535217285\n",
      "Processing Batch 7/157\n",
      "Batch 7 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012941116467118263, Std: 0.08744119107723236\n",
      "z_j Mean: 0.012516777962446213, Std: 0.08609642088413239\n",
      "Similarities: tensor([[1.4204e+00, 0.0000e+00, 3.2925e-04, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 1.1901e+00, 0.0000e+00, 4.9231e-01, 1.3910e+00],\n",
      "        [1.6352e-01, 0.0000e+00, 1.3715e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 3.5245e-01, 0.0000e+00, 1.4167e+00, 5.1438e-01],\n",
      "        [0.0000e+00, 1.1452e+00, 0.0000e+00, 4.9786e-01, 1.4067e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1442322731018066\n",
      "Batch 7, Loss: 3.1442322731018066\n",
      "Processing Batch 8/157\n",
      "Batch 8 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01239074021577835, Std: 0.08752088248729706\n",
      "z_j Mean: 0.012343006208539009, Std: 0.08752762526273727\n",
      "Similarities: tensor([[1.4284e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.6154e-01],\n",
      "        [0.0000e+00, 1.3749e+00, 1.0110e-01, 1.3054e-03, 5.4854e-01],\n",
      "        [0.0000e+00, 0.0000e+00, 1.0943e+00, 9.2741e-01, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 1.2218e+00, 1.3779e+00, 0.0000e+00],\n",
      "        [5.6174e-01, 3.5353e-01, 0.0000e+00, 0.0000e+00, 1.3863e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.215392827987671\n",
      "Batch 8, Loss: 3.215392827987671\n",
      "Processing Batch 9/157\n",
      "Batch 9 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012832666747272015, Std: 0.08745717257261276\n",
      "z_j Mean: 0.013049520552158356, Std: 0.08742507547140121\n",
      "Similarities: tensor([[1.3971, 0.1299, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2210, 1.3638, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4232, 1.1407, 0.0842],\n",
      "        [0.0000, 0.0000, 1.1756, 1.3456, 0.0392],\n",
      "        [0.0000, 0.0000, 0.0504, 0.0053, 1.4273]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1089634895324707\n",
      "Batch 9, Loss: 3.1089634895324707\n",
      "Processing Batch 10/157\n",
      "Batch 10 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012763436883687973, Std: 0.08746730536222458\n",
      "z_j Mean: 0.012340161949396133, Std: 0.08682781457901001\n",
      "Similarities: tensor([[1.0671, 0.0000, 0.0529, 0.2020, 0.0000],\n",
      "        [0.0000, 1.3814, 0.0000, 0.0000, 1.2761],\n",
      "        [1.3999, 0.0000, 0.2436, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3073, 0.0000],\n",
      "        [0.0000, 1.3590, 0.0000, 0.0000, 1.4213]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1851179599761963\n",
      "Batch 10, Loss: 3.1851179599761963\n",
      "Processing Batch 11/157\n",
      "Batch 11 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011841322295367718, Std: 0.08689726144075394\n",
      "z_j Mean: 0.01182558387517929, Std: 0.08759904652833939\n",
      "Similarities: tensor([[1.4277, 0.9459, 0.8241, 0.4850, 0.0398],\n",
      "        [0.9237, 1.4104, 1.3842, 0.2241, 0.1834],\n",
      "        [0.7829, 1.3624, 1.4286, 0.0000, 0.0690],\n",
      "        [0.6856, 0.2201, 0.0000, 1.3876, 0.0000],\n",
      "        [0.0000, 0.3325, 0.0000, 0.0000, 1.3967]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1811890602111816\n",
      "Batch 11, Loss: 3.1811890602111816\n",
      "Processing Batch 12/157\n",
      "Batch 12 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012029523961246014, Std: 0.08687140792608261\n",
      "z_j Mean: 0.012083008885383606, Std: 0.08686397969722748\n",
      "Similarities: tensor([[1.4177, 0.0000, 0.6041, 0.0990, 0.0000],\n",
      "        [0.0000, 0.9881, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4992, 0.0000, 1.3806, 0.0000, 1.1460],\n",
      "        [0.2090, 0.0000, 0.0000, 1.2455, 0.0000],\n",
      "        [0.0000, 1.1474, 0.5409, 0.0000, 0.7746]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.180215835571289\n",
      "Batch 12, Loss: 3.180215835571289\n",
      "Processing Batch 13/157\n",
      "Batch 13 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011580133810639381, Std: 0.0862274169921875\n",
      "z_j Mean: 0.011524399742484093, Std: 0.08623488992452621\n",
      "Similarities: tensor([[1.0613, 0.9880, 0.0000, 0.1218, 0.0000],\n",
      "        [0.4590, 1.3920, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3495, 0.0000, 0.0000],\n",
      "        [0.3062, 0.0000, 0.0000, 1.4257, 1.1231],\n",
      "        [0.0000, 0.0000, 0.0000, 0.9786, 1.4069]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1916186809539795\n",
      "Batch 13, Loss: 3.1916186809539795\n",
      "Processing Batch 14/157\n",
      "Batch 14 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01205480843782425, Std: 0.08686789870262146\n",
      "z_j Mean: 0.012206480838358402, Std: 0.08614096790552139\n",
      "Similarities: tensor([[1.3653, 1.3810, 0.4643, 1.1475, 0.0000],\n",
      "        [1.3256, 1.4134, 0.4209, 1.0402, 0.0000],\n",
      "        [0.3646, 0.3493, 1.4171, 0.4906, 0.0000],\n",
      "        [1.0609, 1.0163, 0.5814, 1.4276, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.0910]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1378560066223145\n",
      "Batch 14, Loss: 3.1378560066223145\n",
      "Processing Batch 15/157\n",
      "Batch 15 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011958818882703781, Std: 0.08546442538499832\n",
      "z_j Mean: 0.011974094435572624, Std: 0.08617358654737473\n",
      "Similarities: tensor([[1.3984, 0.0000, 0.0000, 0.0074, 0.0000],\n",
      "        [0.0000, 1.4123, 0.0000, 0.0000, 1.0251],\n",
      "        [0.0000, 0.0000, 1.3929, 1.0205, 0.0000],\n",
      "        [0.0000, 0.0000, 0.9522, 1.4111, 0.0000],\n",
      "        [0.0000, 0.9813, 0.0000, 0.0000, 1.4255]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.155949592590332\n",
      "Batch 15, Loss: 3.155949592590332\n",
      "Processing Batch 16/157\n",
      "Batch 16 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012601956725120544, Std: 0.08679020404815674\n",
      "z_j Mean: 0.012769760563969612, Std: 0.08605926483869553\n",
      "Similarities: tensor([[1.4002, 0.0000, 0.7681, 0.0000, 0.0000],\n",
      "        [0.0000, 1.1154, 0.0000, 1.3607, 0.7438],\n",
      "        [1.0799, 0.0000, 1.2540, 0.0000, 0.0000],\n",
      "        [0.0000, 0.9572, 0.0000, 1.3910, 0.6383],\n",
      "        [0.0000, 1.1992, 0.0000, 0.6761, 1.3666]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.167145252227783\n",
      "Batch 16, Loss: 3.167145252227783\n",
      "Processing Batch 17/157\n",
      "Batch 17 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011523050256073475, Std: 0.0840846598148346\n",
      "z_j Mean: 0.01129284966737032, Std: 0.0841159000992775\n",
      "Similarities: tensor([[1.4033, 0.0000, 1.1237, 0.0000, 0.0000],\n",
      "        [0.0599, 1.4121, 0.6183, 0.6355, 0.8735],\n",
      "        [0.9279, 0.6688, 1.4144, 0.6207, 0.3280],\n",
      "        [0.0000, 0.5041, 0.4141, 1.4103, 0.2472],\n",
      "        [0.0000, 0.9938, 0.4337, 0.4900, 1.3785]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2624998092651367\n",
      "Batch 17, Loss: 3.2624998092651367\n",
      "Processing Batch 18/157\n",
      "Batch 18 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.010983574204146862, Std: 0.08195192366838455\n",
      "z_j Mean: 0.011292269453406334, Std: 0.08265183866024017\n",
      "Similarities: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4201, 0.0000, 0.0000, 0.0041],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0641, 0.0000, 0.0000, 1.3943]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.254558801651001\n",
      "Batch 18, Loss: 3.254558801651001\n",
      "Processing Batch 19/157\n",
      "Batch 19 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.010865634307265282, Std: 0.0834437906742096\n",
      "z_j Mean: 0.011033458635210991, Std: 0.08342175930738449\n",
      "Similarities: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3357, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3987, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.1178, 1.3107],\n",
      "        [0.0000, 0.0000, 0.0000, 1.1701, 1.4286]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.240319013595581\n",
      "Batch 19, Loss: 3.240319013595581\n",
      "Processing Batch 20/157\n",
      "Batch 20 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011925140395760536, Std: 0.08618037402629852\n",
      "z_j Mean: 0.011521334759891033, Std: 0.08552449941635132\n",
      "Similarities: tensor([[7.6423e-01, 7.2939e-02, 1.8788e-01, 1.8088e-02, 1.4550e-01],\n",
      "        [2.5084e-01, 1.3887e+00, 3.7219e-03, 1.3855e+00, 2.9423e-04],\n",
      "        [2.1326e-02, 2.0354e-03, 1.4267e+00, 5.0475e-04, 1.4156e+00],\n",
      "        [6.7577e-02, 1.2712e+00, 1.0027e-03, 1.3992e+00, 7.9266e-05],\n",
      "        [0.0000e+00, 0.0000e+00, 1.4111e+00, 0.0000e+00, 1.4267e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.189744472503662\n",
      "Batch 20, Loss: 3.189744472503662\n",
      "Processing Batch 21/157\n",
      "Batch 21 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012015283107757568, Std: 0.0861678496003151\n",
      "z_j Mean: 0.011319754645228386, Std: 0.08338339626789093\n",
      "Similarities: tensor([[1.1930, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3640, 0.6408, 0.0000, 0.0000],\n",
      "        [0.0000, 1.0013, 1.4171, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.2781, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4286]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.223743200302124\n",
      "Batch 21, Loss: 3.223743200302124\n",
      "Processing Batch 22/157\n",
      "Batch 22 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013373672030866146, Std: 0.08737606555223465\n",
      "z_j Mean: 0.012816292233765125, Std: 0.0867588073015213\n",
      "Similarities: tensor([[1.4245, 0.0000, 0.0000, 0.0000, 0.2073],\n",
      "        [0.0000, 1.4204, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4286, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4189, 0.0000],\n",
      "        [0.4926, 0.0140, 0.0000, 0.0000, 1.1467]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1981096267700195\n",
      "Batch 22, Loss: 3.1981096267700195\n",
      "Processing Batch 23/157\n",
      "Batch 23 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011820510029792786, Std: 0.08331387490034103\n",
      "z_j Mean: 0.012023456394672394, Std: 0.08401455730199814\n",
      "Similarities: tensor([[1.4064, 0.0000, 0.4549, 0.1052, 0.1010],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5194, 0.0000, 1.3516, 1.1353, 0.9932],\n",
      "        [0.1193, 0.0000, 1.1233, 1.3731, 1.0823],\n",
      "        [0.0978, 0.0000, 1.2500, 1.2437, 1.3962]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.191171646118164\n",
      "Batch 23, Loss: 3.191171646118164\n",
      "Processing Batch 24/157\n",
      "Batch 24 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01152392290532589, Std: 0.0818776860833168\n",
      "z_j Mean: 0.01150207407772541, Std: 0.08113182336091995\n",
      "Similarities: tensor([[1.4286, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4286, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.2796, 1.1488, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4077]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.309321880340576\n",
      "Batch 24, Loss: 3.309321880340576\n",
      "Processing Batch 25/157\n",
      "Batch 25 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011913569644093513, Std: 0.0825645700097084\n",
      "z_j Mean: 0.012061337009072304, Std: 0.08400912582874298\n",
      "Similarities: tensor([[1.4286, 0.0000, 0.9333, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4210, 0.0000, 0.6499, 0.0000],\n",
      "        [0.5927, 0.0048, 1.3463, 0.0245, 0.0000],\n",
      "        [0.0000, 0.6153, 0.0000, 1.4277, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4286]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2687182426452637\n",
      "Batch 25, Loss: 3.2687182426452637\n",
      "Processing Batch 26/157\n",
      "Batch 26 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011975333094596863, Std: 0.08546210825443268\n",
      "z_j Mean: 0.011822015047073364, Std: 0.08476635813713074\n",
      "Similarities: tensor([[1.4286, 0.0000, 0.0000, 1.3097, 0.0000],\n",
      "        [0.0000, 1.4286, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.6754, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.5706, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4182]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.247312545776367\n",
      "Batch 26, Loss: 3.247312545776367\n",
      "Processing Batch 27/157\n",
      "Batch 27 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011416220106184483, Std: 0.08409923315048218\n",
      "z_j Mean: 0.011972666718065739, Std: 0.08474521338939667\n",
      "Similarities: tensor([[1.4286, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3599, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4265, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3494, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2004847526550293\n",
      "Batch 27, Loss: 3.2004847526550293\n",
      "Processing Batch 28/157\n",
      "Batch 28 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011537954211235046, Std: 0.08335347473621368\n",
      "z_j Mean: 0.011708050966262817, Std: 0.08332975208759308\n",
      "Similarities: tensor([[1.4206, 0.0000, 0.0000, 0.0847, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4234, 0.0000, 1.0532],\n",
      "        [0.2602, 0.0000, 0.0000, 1.1165, 0.0000],\n",
      "        [0.0000, 0.0000, 1.0901, 0.0000, 1.4223]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.291323184967041\n",
      "Batch 28, Loss: 3.291323184967041\n",
      "Processing Batch 29/157\n",
      "Batch 29 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011402115225791931, Std: 0.0862511396408081\n",
      "z_j Mean: 0.011585754342377186, Std: 0.08551579713821411\n",
      "Similarities: tensor([[1.3804, 0.0000, 0.0000, 0.4959, 0.5415],\n",
      "        [0.0000, 1.4155, 0.0000, 0.9949, 1.1567],\n",
      "        [0.0000, 0.0000, 1.0755, 0.0000, 0.0000],\n",
      "        [0.4270, 0.9733, 0.0000, 1.3502, 0.8120],\n",
      "        [0.4549, 1.3054, 0.0000, 0.8787, 1.3963]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.222688913345337\n",
      "Batch 29, Loss: 3.222688913345337\n",
      "Processing Batch 30/157\n",
      "Batch 30 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01138683408498764, Std: 0.08554251492023468\n",
      "z_j Mean: 0.011099468916654587, Std: 0.08414163440465927\n",
      "Similarities: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4248, 0.0000, 0.8991, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3478, 0.0000, 0.0000],\n",
      "        [0.0000, 0.9494, 0.0000, 1.3932, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.214914083480835\n",
      "Batch 30, Loss: 3.214914083480835\n",
      "Processing Batch 31/157\n",
      "Batch 31 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.010721661150455475, Std: 0.08419061452150345\n",
      "z_j Mean: 0.010106826201081276, Std: 0.08131743967533112\n",
      "Similarities: tensor([[1.4250, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3462, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4254, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4275, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4275, 0.0000]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2153916358947754\n",
      "Batch 31, Loss: 3.2153916358947754\n",
      "Processing Batch 32/157\n",
      "Batch 32 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011899130418896675, Std: 0.08547275513410568\n",
      "z_j Mean: 0.011748982593417168, Std: 0.0840533897280693\n",
      "Similarities: tensor([[1.3408, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4097, 0.0000, 0.4058, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3490, 0.0000, 0.1570],\n",
      "        [0.0000, 0.1765, 0.0000, 1.3952, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3769]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1775424480438232\n",
      "Batch 32, Loss: 3.1775424480438232\n",
      "Processing Batch 33/157\n",
      "Batch 33 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.010010428726673126, Std: 0.0842781513929367\n",
      "z_j Mean: 0.009767886251211166, Std: 0.0835794135928154\n",
      "Similarities: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4043, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3353, 0.0000, 0.0000],\n",
      "        [1.4286, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4286]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.256087064743042\n",
      "Batch 33, Loss: 3.256087064743042\n",
      "Processing Batch 34/157\n",
      "Batch 34 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.010264803655445576, Std: 0.0820450633764267\n",
      "z_j Mean: 0.00981670431792736, Std: 0.07983819395303726\n",
      "Similarities: tensor([[1.1514, 0.0000, 0.0000, 1.4285, 0.0025],\n",
      "        [0.0000, 1.3637, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.8437, 0.0000, 0.0000, 1.1465, 0.1148],\n",
      "        [0.0447, 0.0000, 0.0000, 0.0069, 1.4238]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.295498847961426\n",
      "Batch 34, Loss: 3.295498847961426\n",
      "Processing Batch 35/157\n",
      "Batch 35 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011333044618368149, Std: 0.08554966002702713\n",
      "z_j Mean: 0.011521387845277786, Std: 0.08552449941635132\n",
      "Similarities: tensor([[1.3952, 0.3758, 0.0000, 0.0871, 0.0000],\n",
      "        [0.2391, 0.1529, 0.0000, 0.8550, 0.0000],\n",
      "        [0.0000, 0.0000, 1.0523, 1.0758, 0.0000],\n",
      "        [0.0818, 0.0000, 0.1145, 1.1899, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3542]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2185940742492676\n",
      "Batch 35, Loss: 3.2185940742492676\n",
      "Processing Batch 36/157\n",
      "Batch 36 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.010009780526161194, Std: 0.08355078846216202\n",
      "z_j Mean: 0.010233364067971706, Std: 0.08425137400627136\n",
      "Similarities: tensor([[1.3877, 0.0000, 0.4535, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4286, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5481, 0.0000, 1.4286, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2396862506866455\n",
      "Batch 36, Loss: 3.2396862506866455\n",
      "Processing Batch 37/157\n",
      "Batch 37 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011753211729228497, Std: 0.08620399236679077\n",
      "z_j Mean: 0.011846484616398811, Std: 0.08548007160425186\n",
      "Similarities: tensor([[1.4286, 0.0000, 0.0000, 0.0000, 1.4286],\n",
      "        [0.0000, 0.0000, 0.3273, 0.5516, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3834, 1.1383, 0.0000],\n",
      "        [0.0000, 0.0000, 0.9880, 1.2934, 0.0000],\n",
      "        [1.4286, 0.0000, 0.0000, 0.0000, 1.4286]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2070157527923584\n",
      "Batch 37, Loss: 3.2070157527923584\n",
      "Processing Batch 38/157\n",
      "Batch 38 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.010516895912587643, Std: 0.08201313018798828\n",
      "z_j Mean: 0.010778781026601791, Std: 0.08272038400173187\n",
      "Similarities: tensor([[1.4238, 0.0000, 1.2002, 0.0000, 0.0067],\n",
      "        [0.0000, 1.3990, 0.0381, 0.0000, 1.2944],\n",
      "        [1.1834, 0.0028, 1.4173, 0.0000, 0.0940],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3194, 0.1161, 0.0000, 1.4095]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.258601188659668\n",
      "Batch 38, Loss: 3.258601188659668\n",
      "Processing Batch 39/157\n",
      "Batch 39 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011138916015625, Std: 0.08485884219408035\n",
      "z_j Mean: 0.010831493884325027, Std: 0.08344823122024536\n",
      "Similarities: tensor([[1.1736, 0.0000, 0.0000, 0.2732, 0.0000],\n",
      "        [0.0000, 1.4118, 0.0000, 0.0000, 0.1625],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4044, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4286]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.28029203414917\n",
      "Batch 39, Loss: 3.28029203414917\n",
      "Processing Batch 40/157\n",
      "Batch 40 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012049831449985504, Std: 0.0840107873082161\n",
      "z_j Mean: 0.012495588511228561, Std: 0.08466967195272446\n",
      "Similarities: tensor([[1.4210, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4195, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.2120, 0.0171, 0.0031],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3759, 0.4774],\n",
      "        [0.0000, 0.0000, 0.0000, 0.7931, 1.2120]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.302846670150757\n",
      "Batch 40, Loss: 3.302846670150757\n",
      "Processing Batch 41/157\n",
      "Batch 41 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011333927512168884, Std: 0.08411037176847458\n",
      "z_j Mean: 0.011589123867452145, Std: 0.08479851484298706\n",
      "Similarities: tensor([[1.4286, 0.0000, 0.6821, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4087, 0.0000, 0.0000, 1.3602],\n",
      "        [0.6629, 0.0000, 1.4263, 0.3958, 0.0000],\n",
      "        [0.0000, 0.0000, 0.4008, 1.2898, 0.0000],\n",
      "        [0.0000, 1.1725, 0.0000, 0.0000, 1.4138]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.313267707824707\n",
      "Batch 41, Loss: 3.313267707824707\n",
      "Processing Batch 42/157\n",
      "Batch 42 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012492732144892216, Std: 0.08750638365745544\n",
      "z_j Mean: 0.012366553768515587, Std: 0.08611813932657242\n",
      "Similarities: tensor([[1.3747, 0.9794, 0.8505, 0.0299, 1.1339],\n",
      "        [0.5744, 1.2834, 0.2008, 0.0000, 0.2981],\n",
      "        [0.9559, 0.2942, 1.3951, 0.2278, 1.2728],\n",
      "        [0.0463, 0.0000, 0.3096, 1.4200, 0.1473],\n",
      "        [1.0718, 0.2943, 1.3017, 0.1161, 1.3912]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.201685667037964\n",
      "Batch 42, Loss: 3.201685667037964\n",
      "Processing Batch 43/157\n",
      "Batch 43 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.010948698036372662, Std: 0.08416138589382172\n",
      "z_j Mean: 0.011272836476564407, Std: 0.08411857485771179\n",
      "Similarities: tensor([[0.9961, 0.0530, 0.0000, 0.2750, 0.1806],\n",
      "        [0.0792, 1.4107, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4276, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3959, 0.9169],\n",
      "        [0.0000, 0.0000, 0.0000, 0.9902, 1.4226]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.276555061340332\n",
      "Batch 43, Loss: 3.276555061340332\n",
      "Processing Batch 44/157\n",
      "Batch 44 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01166374608874321, Std: 0.08550519496202469\n",
      "z_j Mean: 0.01187186036258936, Std: 0.08689309656620026\n",
      "Similarities: tensor([[1.4146, 0.0000, 0.0000, 0.9560, 0.8620],\n",
      "        [0.0000, 1.4284, 1.4244, 0.3069, 0.0000],\n",
      "        [0.0000, 1.3771, 1.3796, 0.2972, 0.0000],\n",
      "        [0.7648, 0.5025, 0.5034, 1.3797, 0.3214],\n",
      "        [0.8253, 0.0000, 0.0000, 0.4414, 1.4190]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2496705055236816\n",
      "Batch 44, Loss: 3.2496705055236816\n",
      "Processing Batch 45/157\n",
      "Batch 45 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012826723977923393, Std: 0.08533846586942673\n",
      "z_j Mean: 0.013082239776849747, Std: 0.08529966324567795\n",
      "Similarities: tensor([[1.3291, 0.0000, 0.0000, 1.0765, 0.0000],\n",
      "        [0.0000, 1.3194, 0.0000, 0.0000, 0.4718],\n",
      "        [0.0000, 0.0000, 1.4286, 0.0000, 0.0000],\n",
      "        [0.6376, 0.0000, 0.0000, 1.3939, 0.0000],\n",
      "        [0.0000, 0.5778, 0.0000, 0.0000, 1.3938]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.244312286376953\n",
      "Batch 45, Loss: 3.244312286376953\n",
      "Processing Batch 46/157\n",
      "Batch 46 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01207099948078394, Std: 0.08473125845193863\n",
      "z_j Mean: 0.012213064357638359, Std: 0.08325722813606262\n",
      "Similarities: tensor([[1.2789, 0.0000, 0.3966, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4034, 0.0000, 0.3677, 0.0000],\n",
      "        [0.3897, 0.0000, 1.4286, 0.0000, 0.0000],\n",
      "        [0.3135, 0.0123, 0.0000, 0.9525, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.1231]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.191709518432617\n",
      "Batch 46, Loss: 3.191709518432617\n",
      "Processing Batch 47/157\n",
      "Batch 47 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01253597717732191, Std: 0.08609363436698914\n",
      "z_j Mean: 0.012104837223887444, Std: 0.08544386178255081\n",
      "Similarities: tensor([[1.2921, 0.0000, 0.5383, 0.0000, 0.1215],\n",
      "        [0.0000, 1.4251, 0.0000, 0.0000, 0.0000],\n",
      "        [0.9748, 0.0000, 1.4286, 0.0000, 0.1375],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4041, 0.0000],\n",
      "        [0.2111, 0.0000, 0.1489, 0.0000, 1.3950]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2049217224121094\n",
      "Batch 47, Loss: 3.2049217224121094\n",
      "Processing Batch 48/157\n",
      "Batch 48 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011178072541952133, Std: 0.08485369384288788\n",
      "z_j Mean: 0.011371856555342674, Std: 0.0841052457690239\n",
      "Similarities: tensor([[1.2080, 0.0497, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3222, 0.0000, 0.5729, 0.6182],\n",
      "        [0.0000, 0.0000, 1.3155, 0.0000, 0.0000],\n",
      "        [0.0000, 0.5353, 0.0000, 1.4286, 0.0000],\n",
      "        [0.0000, 0.4682, 0.0000, 0.0000, 1.4212]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2394697666168213\n",
      "Batch 48, Loss: 3.2394697666168213\n",
      "Processing Batch 49/157\n",
      "Batch 49 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011941644363105297, Std: 0.08617809414863586\n",
      "z_j Mean: 0.011941813863813877, Std: 0.08617807179689407\n",
      "Similarities: tensor([[0.0000, 0.0000, 0.0000, 0.0303, 0.0000],\n",
      "        [0.0000, 1.3686, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4286, 0.0000, 1.4286],\n",
      "        [0.0000, 0.0000, 0.2829, 1.4001, 0.2829],\n",
      "        [0.0000, 0.0000, 1.4286, 0.0000, 1.4286]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1880416870117188\n",
      "Batch 49, Loss: 3.1880416870117188\n",
      "Processing Batch 50/157\n",
      "Batch 50 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011313898488879204, Std: 0.08555219322443008\n",
      "z_j Mean: 0.011116757988929749, Std: 0.08413935452699661\n",
      "Similarities: tensor([[1.4286, 0.9156, 0.0000, 0.0000, 0.0000],\n",
      "        [0.8407, 1.3817, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4033, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.2677]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2072436809539795\n",
      "Batch 50, Loss: 3.2072436809539795\n",
      "Processing Batch 51/157\n",
      "Batch 51 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011887178756296635, Std: 0.0854744166135788\n",
      "z_j Mean: 0.011469538323581219, Std: 0.08553146570920944\n",
      "Similarities: tensor([[1.3635, 1.0470, 0.0034, 0.0000, 0.0000],\n",
      "        [1.1254, 1.3630, 0.0017, 0.0000, 0.0000],\n",
      "        [0.0284, 0.0039, 1.3535, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0883, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4286]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2348835468292236\n",
      "Batch 51, Loss: 3.2348835468292236\n",
      "Processing Batch 52/157\n",
      "Batch 52 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01155282836407423, Std: 0.08480347692966461\n",
      "z_j Mean: 0.012164643034338951, Std: 0.08755259215831757\n",
      "Similarities: tensor([[1.4017, 0.9962, 0.0000, 0.0000, 1.1400],\n",
      "        [0.9648, 1.4255, 0.0000, 0.0000, 0.9387],\n",
      "        [0.0000, 0.0000, 1.4092, 1.3025, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3075, 1.3933, 0.0000],\n",
      "        [1.1945, 1.0156, 0.0000, 0.0000, 1.3637]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.211162805557251\n",
      "Batch 52, Loss: 3.211162805557251\n",
      "Processing Batch 53/157\n",
      "Batch 53 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011824455112218857, Std: 0.08689956367015839\n",
      "z_j Mean: 0.011216642335057259, Std: 0.08484859764575958\n",
      "Similarities: tensor([[1.4139, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3836, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3400, 0.2234],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.0043]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.162719249725342\n",
      "Batch 53, Loss: 3.162719249725342\n",
      "Processing Batch 54/157\n",
      "Batch 54 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012227754108607769, Std: 0.0875438004732132\n",
      "z_j Mean: 0.011977693997323513, Std: 0.08757837116718292\n",
      "Similarities: tensor([[1.4278, 0.0000, 0.5792, 0.5698, 0.0389],\n",
      "        [0.0000, 0.8692, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5432, 0.0000, 1.4263, 1.4273, 0.1681],\n",
      "        [0.2248, 0.0000, 0.8688, 0.9481, 0.3410],\n",
      "        [0.0883, 0.0352, 0.4875, 0.5868, 1.0636]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.202629566192627\n",
      "Batch 54, Loss: 3.202629566192627\n",
      "Processing Batch 55/157\n",
      "Batch 55 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011586597189307213, Std: 0.084075927734375\n",
      "z_j Mean: 0.011568387039005756, Std: 0.08407843858003616\n",
      "Similarities: tensor([[1.4060, 0.7131, 0.0000, 0.0000, 0.0000],\n",
      "        [0.7294, 1.2713, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4106, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3721]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.25534987449646\n",
      "Batch 55, Loss: 3.25534987449646\n",
      "Processing Batch 56/157\n",
      "Batch 56 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011387422680854797, Std: 0.08554244041442871\n",
      "z_j Mean: 0.01152345072478056, Std: 0.08623500913381577\n",
      "Similarities: tensor([[1.3838, 0.0512, 1.2476, 0.0000, 0.0000],\n",
      "        [0.0935, 1.3525, 0.0000, 0.0000, 0.0000],\n",
      "        [1.0666, 0.0000, 1.3734, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3925, 0.9366],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0533, 1.2596]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.153582811355591\n",
      "Batch 56, Loss: 3.153582811355591\n",
      "Processing Batch 57/157\n",
      "Batch 57 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012613829225301743, Std: 0.08678848296403885\n",
      "z_j Mean: 0.012591630220413208, Std: 0.08679170161485672\n",
      "Similarities: tensor([[1.2443, 0.0000, 0.0935, 0.1072, 0.0101],\n",
      "        [0.0000, 1.3980, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2353, 0.0000, 1.3944, 0.4558, 0.0430],\n",
      "        [0.3859, 0.0000, 0.7880, 1.2119, 0.1186],\n",
      "        [0.0753, 0.0000, 0.1272, 0.4610, 1.3916]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1312336921691895\n",
      "Batch 57, Loss: 3.1312336921691895\n",
      "Processing Batch 58/157\n",
      "Batch 58 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013101622462272644, Std: 0.0867161750793457\n",
      "z_j Mean: 0.012909326702356339, Std: 0.0867450162768364\n",
      "Similarities: tensor([[1.4248, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4093, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4027, 0.1029, 0.0000],\n",
      "        [0.0000, 0.0000, 0.1581, 1.4156, 0.3939],\n",
      "        [0.0000, 0.0000, 0.0000, 0.1902, 1.4034]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1214935779571533\n",
      "Batch 58, Loss: 3.1214935779571533\n",
      "Processing Batch 59/157\n",
      "Batch 59 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012391936033964157, Std: 0.08752071112394333\n",
      "z_j Mean: 0.012460388243198395, Std: 0.08751098811626434\n",
      "Similarities: tensor([[1.3563, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1783, 1.3589, 0.0411, 0.0000, 1.2251],\n",
      "        [0.0000, 0.1858, 1.3479, 0.0000, 0.3814],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3030, 0.0000],\n",
      "        [0.1316, 1.2281, 0.0711, 0.0000, 1.3905]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1406753063201904\n",
      "Batch 59, Loss: 3.1406753063201904\n",
      "Processing Batch 60/157\n",
      "Batch 60 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012329012155532837, Std: 0.08752959966659546\n",
      "z_j Mean: 0.011962490156292915, Std: 0.08546391129493713\n",
      "Similarities: tensor([[1.1900, 0.0000, 1.2975, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3755, 0.0000, 0.0000, 0.5378],\n",
      "        [0.9859, 0.0000, 1.3985, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0994],\n",
      "        [0.0205, 0.9131, 0.0043, 0.0000, 1.3984]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.187190294265747\n",
      "Batch 60, Loss: 3.187190294265747\n",
      "Processing Batch 61/157\n",
      "Batch 61 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01191824208945036, Std: 0.08758648484945297\n",
      "z_j Mean: 0.01161203347146511, Std: 0.08551223576068878\n",
      "Similarities: tensor([[1.3559, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4229, 1.0551, 0.0000, 0.0000],\n",
      "        [0.0000, 0.9539, 0.9327, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0330, 0.0000, 0.0000, 0.0000, 1.4039]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.171292543411255\n",
      "Batch 61, Loss: 3.171292543411255\n",
      "Processing Batch 62/157\n",
      "Batch 62 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011721129529178143, Std: 0.0869135633111\n",
      "z_j Mean: 0.011582154780626297, Std: 0.08551628142595291\n",
      "Similarities: tensor([[1.3470, 0.0000, 0.0503, 0.0000, 0.9646],\n",
      "        [0.0000, 1.4286, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0332, 0.0000, 1.4248, 0.0000, 0.0504],\n",
      "        [0.0000, 1.0179, 0.0000, 1.0023, 0.0000],\n",
      "        [1.1358, 0.0000, 0.0214, 0.0000, 1.3530]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1481032371520996\n",
      "Batch 62, Loss: 3.1481032371520996\n",
      "Processing Batch 63/157\n",
      "Batch 63 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01096315123140812, Std: 0.08630804717540741\n",
      "z_j Mean: 0.010965036228299141, Std: 0.08630780130624771\n",
      "Similarities: tensor([[1.3825, 0.0000, 0.0000, 0.0000, 0.8685],\n",
      "        [0.0000, 1.4282, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3608, 0.5383, 0.0000],\n",
      "        [0.0000, 0.0000, 0.5607, 1.4227, 0.0000],\n",
      "        [0.9320, 0.0000, 0.0000, 0.0000, 0.5605]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1905417442321777\n",
      "Batch 63, Loss: 3.1905417442321777\n",
      "Processing Batch 64/157\n",
      "Batch 64 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011166144162416458, Std: 0.08557160943746567\n",
      "z_j Mean: 0.011214249767363071, Std: 0.08484891057014465\n",
      "Similarities: tensor([[1.4164, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3900, 0.0000, 0.0000, 0.5775],\n",
      "        [0.0000, 0.0000, 1.3999, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.4983, 0.0000, 0.0000, 1.3910]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.150949478149414\n",
      "Batch 64, Loss: 3.150949478149414\n",
      "Processing Batch 65/157\n",
      "Batch 65 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011438730172812939, Std: 0.08624628931283951\n",
      "z_j Mean: 0.011638319119811058, Std: 0.08621957898139954\n",
      "Similarities: tensor([[1.3844, 0.0065, 0.7516, 0.4567, 1.2445],\n",
      "        [0.0000, 1.4249, 0.0000, 0.0000, 0.0000],\n",
      "        [1.1908, 0.0000, 1.2672, 0.8883, 1.2926],\n",
      "        [0.0000, 0.0000, 0.9416, 1.3472, 0.6316],\n",
      "        [1.2351, 0.0000, 1.1327, 1.0136, 1.4072]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1319572925567627\n",
      "Batch 65, Loss: 3.1319572925567627\n",
      "Processing Batch 66/157\n",
      "Batch 66 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.010305806063115597, Std: 0.08567949384450912\n",
      "z_j Mean: 0.010417572222650051, Std: 0.08566597104072571\n",
      "Similarities: tensor([[0.8926, 0.0000, 0.2369, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4267, 0.0000, 0.0000, 0.1101],\n",
      "        [0.0920, 0.0000, 1.4090, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4284, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.256643533706665\n",
      "Batch 66, Loss: 3.256643533706665\n",
      "Processing Batch 67/157\n",
      "Batch 67 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011364876292645931, Std: 0.08696086704730988\n",
      "z_j Mean: 0.011443253606557846, Std: 0.08764981478452682\n",
      "Similarities: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3336, 0.0436, 0.0027, 0.0000],\n",
      "        [0.0000, 0.4102, 1.4214, 0.4191, 0.1070],\n",
      "        [0.0000, 0.0000, 0.6705, 1.3098, 0.0932],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4062]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.139474630355835\n",
      "Batch 67, Loss: 3.139474630355835\n",
      "Processing Batch 68/157\n",
      "Batch 68 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011313248425722122, Std: 0.08626283705234528\n",
      "z_j Mean: 0.01112205721437931, Std: 0.0855773538351059\n",
      "Similarities: tensor([[1.4107, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4268, 0.0743, 0.0000, 0.0465],\n",
      "        [0.0000, 0.0000, 1.4070, 0.0000, 0.0000],\n",
      "        [0.0000, 0.4251, 0.2327, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4136]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1265127658843994\n",
      "Batch 68, Loss: 3.1265127658843994\n",
      "Processing Batch 69/157\n",
      "Batch 69 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.010046557523310184, Std: 0.08207208663225174\n",
      "z_j Mean: 0.010288577526807785, Std: 0.08204209059476852\n",
      "Similarities: tensor([[1.4257, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3392, 0.0000, 1.2191, 0.2972],\n",
      "        [0.0000, 0.0000, 1.2731, 0.0000, 0.0000],\n",
      "        [0.0000, 0.9661, 0.0000, 1.4122, 0.2689],\n",
      "        [0.0000, 0.4314, 0.0000, 0.2527, 1.4267]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.302375316619873\n",
      "Batch 69, Loss: 3.302375316619873\n",
      "Processing Batch 70/157\n",
      "Batch 70 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011409811675548553, Std: 0.0869549885392189\n",
      "z_j Mean: 0.011649931780993938, Std: 0.08762258291244507\n",
      "Similarities: tensor([[1.4281, 0.0000, 0.3417, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4262, 0.0000, 0.0766, 0.0000],\n",
      "        [1.0375, 0.0000, 0.2482, 0.0000, 0.7061],\n",
      "        [0.0000, 0.2398, 0.0000, 1.3675, 0.0000],\n",
      "        [0.0000, 0.0000, 0.9906, 0.0000, 1.4238]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.13179349899292\n",
      "Batch 70, Loss: 3.13179349899292\n",
      "Processing Batch 71/157\n",
      "Batch 71 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.010827935300767422, Std: 0.08561506122350693\n",
      "z_j Mean: 0.010865263640880585, Std: 0.08561033755540848\n",
      "Similarities: tensor([[1.3582, 0.0000, 0.0000, 0.2309, 0.0000],\n",
      "        [0.0000, 1.2927, 0.2824, 0.3529, 0.7139],\n",
      "        [0.0000, 0.4321, 1.3432, 0.0000, 0.2898],\n",
      "        [0.0792, 0.1696, 0.0000, 1.2201, 0.0000],\n",
      "        [0.0000, 0.6344, 0.4009, 0.0000, 1.3969]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1853420734405518\n",
      "Batch 71, Loss: 3.1853420734405518\n",
      "Processing Batch 72/157\n",
      "Batch 72 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.010934291407465935, Std: 0.08701606094837189\n",
      "z_j Mean: 0.011234891600906849, Std: 0.0876767709851265\n",
      "Similarities: tensor([[1.3844, 0.0000, 0.0000, 1.0100, 0.0000],\n",
      "        [0.0000, 1.3525, 0.0000, 0.0000, 1.0264],\n",
      "        [0.0000, 0.0000, 1.4223, 0.0000, 0.0000],\n",
      "        [0.8931, 0.0000, 0.0000, 1.3958, 0.0000],\n",
      "        [0.0000, 1.1892, 0.0000, 0.0000, 1.4134]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1586105823516846\n",
      "Batch 72, Loss: 3.1586105823516846\n",
      "Processing Batch 73/157\n",
      "Batch 73 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01148993894457817, Std: 0.08694443106651306\n",
      "z_j Mean: 0.01146065816283226, Std: 0.08694829791784286\n",
      "Similarities: tensor([[1.3462, 1.0729, 1.4024, 0.0000, 0.0000],\n",
      "        [0.4767, 1.3775, 0.9687, 0.0000, 0.0000],\n",
      "        [0.4220, 1.1861, 0.9475, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4260, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4286]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1643385887145996\n",
      "Batch 73, Loss: 3.1643385887145996\n",
      "Processing Batch 74/157\n",
      "Batch 74 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012151621282100677, Std: 0.08614873141050339\n",
      "z_j Mean: 0.012227410450577736, Std: 0.08542639762163162\n",
      "Similarities: tensor([[1.0676, 0.0000, 0.7362, 0.9983, 0.2232],\n",
      "        [0.0000, 1.4248, 0.0000, 0.0000, 0.5928],\n",
      "        [0.6242, 0.0000, 1.3541, 0.0000, 0.0000],\n",
      "        [0.3639, 0.0000, 0.0000, 1.4268, 0.2485],\n",
      "        [0.0021, 0.6539, 0.0000, 0.0102, 1.3813]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1937849521636963\n",
      "Batch 74, Loss: 3.1937849521636963\n",
      "Processing Batch 75/157\n",
      "Batch 75 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011935748159885406, Std: 0.08617890626192093\n",
      "z_j Mean: 0.011417752131819725, Std: 0.08624906837940216\n",
      "Similarities: tensor([[1.1850, 0.0000, 0.3436, 0.0348, 0.0254],\n",
      "        [0.0000, 1.3991, 0.0000, 0.7940, 0.0000],\n",
      "        [0.3015, 0.0000, 1.3516, 0.0000, 0.0000],\n",
      "        [0.0000, 0.7671, 0.0000, 1.3480, 0.0000],\n",
      "        [0.0808, 0.0000, 0.0000, 0.0141, 1.4083]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2003800868988037\n",
      "Batch 75, Loss: 3.2003800868988037\n",
      "Processing Batch 76/157\n",
      "Batch 76 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011363806203007698, Std: 0.08625619858503342\n",
      "z_j Mean: 0.010810386389493942, Std: 0.08561728894710541\n",
      "Similarities: tensor([[1.4282, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3398, 1.1232, 0.0071, 0.0000],\n",
      "        [0.0000, 0.7413, 1.4126, 0.0177, 0.0000],\n",
      "        [0.0000, 0.0000, 0.1319, 1.3729, 0.4701],\n",
      "        [0.0000, 0.0000, 0.0286, 0.5906, 1.4223]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.178718090057373\n",
      "Batch 76, Loss: 3.178718090057373\n",
      "Processing Batch 77/157\n",
      "Batch 77 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012298177927732468, Std: 0.08612792193889618\n",
      "z_j Mean: 0.012086339294910431, Std: 0.0861579179763794\n",
      "Similarities: tensor([[1.4253, 0.7976, 0.7518, 0.0000, 0.0000],\n",
      "        [0.9543, 1.0933, 0.0000, 0.0000, 0.0000],\n",
      "        [0.7721, 0.0796, 1.4229, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3756, 0.0825],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0635, 1.4153]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.179875373840332\n",
      "Batch 77, Loss: 3.179875373840332\n",
      "Processing Batch 78/157\n",
      "Batch 78 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011551852338016033, Std: 0.08693622052669525\n",
      "z_j Mean: 0.011338727548718452, Std: 0.08696428686380386\n",
      "Similarities: tensor([[1.3986, 0.0069, 0.0000, 0.0000, 0.6350],\n",
      "        [0.0316, 1.4180, 0.0000, 0.0000, 0.4041],\n",
      "        [0.0000, 0.0000, 1.2954, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4256, 0.0000],\n",
      "        [0.5422, 0.4886, 0.0000, 0.0000, 1.4117]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.102539539337158\n",
      "Batch 78, Loss: 3.102539539337158\n",
      "Processing Batch 79/157\n",
      "Batch 79 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012351909652352333, Std: 0.0875263661146164\n",
      "z_j Mean: 0.012658867053687572, Std: 0.08678192645311356\n",
      "Similarities: tensor([[1.4139, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3226, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4257, 0.1925, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3506, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4096]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1150035858154297\n",
      "Batch 79, Loss: 3.1150035858154297\n",
      "Processing Batch 80/157\n",
      "Batch 80 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011584068648517132, Std: 0.08551602810621262\n",
      "z_j Mean: 0.011596690863370895, Std: 0.08551431447267532\n",
      "Similarities: tensor([[0.0000, 0.0000, 0.7890, 0.0000, 0.6447],\n",
      "        [0.0000, 1.4285, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4284, 0.0000, 0.3654],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4194, 0.0000],\n",
      "        [0.0000, 0.0000, 0.3070, 0.0000, 1.4235]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1597440242767334\n",
      "Batch 80, Loss: 3.1597440242767334\n",
      "Processing Batch 81/157\n",
      "Batch 81 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01094810664653778, Std: 0.08630994707345963\n",
      "z_j Mean: 0.010758167132735252, Std: 0.08272306621074677\n",
      "Similarities: tensor([[1.3926, 0.0319, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4271, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4206, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4201, 0.0000],\n",
      "        [0.0194, 0.0000, 0.0000, 0.0000, 0.0000]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2373058795928955\n",
      "Batch 81, Loss: 3.2373058795928955\n",
      "Processing Batch 82/157\n",
      "Batch 82 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011310823261737823, Std: 0.08766700327396393\n",
      "z_j Mean: 0.011421632021665573, Std: 0.08765263110399246\n",
      "Similarities: tensor([[1.3917, 0.3153, 0.0000, 0.5472, 0.3782],\n",
      "        [0.2186, 1.4167, 0.0000, 0.2999, 1.4021],\n",
      "        [0.0000, 0.0000, 1.4237, 0.0000, 0.0000],\n",
      "        [0.4379, 0.3304, 0.0000, 1.4232, 0.3963],\n",
      "        [0.2076, 1.3574, 0.0000, 0.2848, 1.3430]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0906577110290527\n",
      "Batch 82, Loss: 3.0906577110290527\n",
      "Processing Batch 83/157\n",
      "Batch 83 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011321218684315681, Std: 0.08555122464895248\n",
      "z_j Mean: 0.011330250650644302, Std: 0.08626061677932739\n",
      "Similarities: tensor([[1.4190, 0.2192, 0.0000, 0.0555, 0.0018],\n",
      "        [0.1999, 1.3707, 0.0000, 0.0000, 0.1656],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0647, 0.0000, 0.0000, 0.8880, 0.7087],\n",
      "        [0.0069, 0.0814, 0.6594, 0.0947, 1.3982]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.173180341720581\n",
      "Batch 83, Loss: 3.173180341720581\n",
      "Processing Batch 84/157\n",
      "Batch 84 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.010443296283483505, Std: 0.08707636594772339\n",
      "z_j Mean: 0.010189601220190525, Std: 0.08569339662790298\n",
      "Similarities: tensor([[1.4210, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3354, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4285, 0.0000, 0.9139],\n",
      "        [0.0000, 0.0000, 0.0000, 0.9816, 0.0000],\n",
      "        [0.0000, 0.0000, 1.1295, 0.0000, 1.3980]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2023508548736572\n",
      "Batch 84, Loss: 3.2023508548736572\n",
      "Processing Batch 85/157\n",
      "Batch 85 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011500769294798374, Std: 0.08764228969812393\n",
      "z_j Mean: 0.011653324589133263, Std: 0.08762212097644806\n",
      "Similarities: tensor([[1.4281, 0.0000, 0.0000, 1.4285, 0.0000],\n",
      "        [0.0000, 1.4252, 0.7835, 0.0000, 0.0000],\n",
      "        [0.0000, 0.8082, 1.3981, 0.0000, 0.0000],\n",
      "        [1.4285, 0.0000, 0.0000, 1.4279, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.2928]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1730775833129883\n",
      "Batch 85, Loss: 3.1730775833129883\n",
      "Processing Batch 86/157\n",
      "Batch 86 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011270077899098396, Std: 0.08767225593328476\n",
      "z_j Mean: 0.011357984505593777, Std: 0.08766090124845505\n",
      "Similarities: tensor([[1.3491, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.2504, 0.6246, 1.0887, 0.0000],\n",
      "        [0.0000, 1.3073, 1.2851, 0.9499, 0.2307],\n",
      "        [0.0000, 1.2120, 0.6893, 1.3108, 0.0000],\n",
      "        [0.0000, 0.3734, 0.6202, 0.0000, 1.4150]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1196610927581787\n",
      "Batch 86, Loss: 3.1196610927581787\n",
      "Processing Batch 87/157\n",
      "Batch 87 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011695330031216145, Std: 0.08761652559041977\n",
      "z_j Mean: 0.011824248358607292, Std: 0.08759922534227371\n",
      "Similarities: tensor([[1.4169, 0.0000, 0.0000, 0.2678, 0.0000],\n",
      "        [0.0000, 1.3189, 0.0000, 0.0000, 0.6589],\n",
      "        [0.0000, 0.0000, 1.4120, 0.0000, 0.0000],\n",
      "        [0.0986, 0.0000, 0.0037, 1.4015, 0.0000],\n",
      "        [0.1364, 0.1648, 0.0000, 0.0000, 1.3820]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1710166931152344\n",
      "Batch 87, Loss: 3.1710166931152344\n",
      "Processing Batch 88/157\n",
      "Batch 88 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012585091404616833, Std: 0.08749314397573471\n",
      "z_j Mean: 0.012811059132218361, Std: 0.08746033906936646\n",
      "Similarities: tensor([[1.4230, 0.7948, 0.0000, 0.0000, 0.0000],\n",
      "        [0.6987, 1.4096, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3839, 0.0000, 0.3387],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3287, 0.0000],\n",
      "        [0.0000, 0.0000, 0.8159, 0.0000, 1.3639]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.160050868988037\n",
      "Batch 88, Loss: 3.160050868988037\n",
      "Processing Batch 89/157\n",
      "Batch 89 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012125572189688683, Std: 0.08755801618099213\n",
      "z_j Mean: 0.012069843709468842, Std: 0.08756572008132935\n",
      "Similarities: tensor([[1.4168, 0.0000, 0.0000, 0.9436, 0.5530],\n",
      "        [0.0000, 1.0777, 0.0000, 0.1863, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3125, 0.0373, 0.0000],\n",
      "        [0.5563, 0.1893, 0.0000, 1.3216, 1.2906],\n",
      "        [0.6360, 0.0000, 0.1678, 1.2398, 1.4120]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.164383888244629\n",
      "Batch 89, Loss: 3.164383888244629\n",
      "Processing Batch 90/157\n",
      "Batch 90 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01158764585852623, Std: 0.08551554381847382\n",
      "z_j Mean: 0.011672417633235455, Std: 0.08478709310293198\n",
      "Similarities: tensor([[1.4271, 0.5191, 0.0000, 0.0000, 0.6700],\n",
      "        [0.9796, 0.8350, 0.0000, 0.0000, 0.9154],\n",
      "        [0.0000, 0.0000, 1.4275, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3936, 0.0000],\n",
      "        [0.6183, 0.7374, 0.0000, 0.0000, 1.3004]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2382729053497314\n",
      "Batch 90, Loss: 3.2382729053497314\n",
      "Processing Batch 91/157\n",
      "Batch 91 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012262866832315922, Std: 0.08753889054059982\n",
      "z_j Mean: 0.012430589646100998, Std: 0.08681491762399673\n",
      "Similarities: tensor([[1.4117, 0.0000, 1.2865, 0.1952, 0.0000],\n",
      "        [0.0000, 1.1227, 0.0000, 0.0000, 0.5335],\n",
      "        [1.2931, 0.0000, 1.4269, 0.1405, 0.0000],\n",
      "        [0.3979, 0.0000, 0.1477, 1.4244, 0.0000],\n",
      "        [0.0000, 1.0953, 0.0000, 0.0000, 1.4286]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1575541496276855\n",
      "Batch 91, Loss: 3.1575541496276855\n",
      "Processing Batch 92/157\n",
      "Batch 92 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011411603540182114, Std: 0.08765393495559692\n",
      "z_j Mean: 0.011625973507761955, Std: 0.08692634105682373\n",
      "Similarities: tensor([[1.4214, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.2944, 0.8147, 0.0000, 1.2996],\n",
      "        [0.0000, 0.4816, 1.4054, 0.0000, 0.7165],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4134, 0.0000],\n",
      "        [0.0000, 1.2625, 0.8505, 0.0000, 1.3421]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1766881942749023\n",
      "Batch 92, Loss: 3.1766881942749023\n",
      "Processing Batch 93/157\n",
      "Batch 93 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011112157255411148, Std: 0.08628898859024048\n",
      "z_j Mean: 0.01157735288143158, Std: 0.08622778952121735\n",
      "Similarities: tensor([[1.0118, 0.0000, 0.0000, 0.0000, 1.0272],\n",
      "        [0.0000, 1.4277, 0.0000, 0.0874, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4082, 0.2894, 0.0000],\n",
      "        [0.0000, 0.0816, 0.5542, 1.4100, 0.0000],\n",
      "        [0.7507, 0.0000, 0.0066, 0.0000, 1.4129]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.208784580230713\n",
      "Batch 93, Loss: 3.208784580230713\n",
      "Processing Batch 94/157\n",
      "Batch 94 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011898316442966461, Std: 0.08688947558403015\n",
      "z_j Mean: 0.011705849319696426, Std: 0.08549943566322327\n",
      "Similarities: tensor([[1.4136, 0.0000, 0.0000, 0.0000, 0.7607],\n",
      "        [0.0000, 1.4213, 0.1402, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2830, 1.4286, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4140, 0.0000],\n",
      "        [0.6792, 0.0000, 0.0000, 0.0000, 1.4276]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1834027767181396\n",
      "Batch 94, Loss: 3.1834027767181396\n",
      "Processing Batch 95/157\n",
      "Batch 95 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012195499613881111, Std: 0.08754830062389374\n",
      "z_j Mean: 0.011851000599563122, Std: 0.0861905962228775\n",
      "Similarities: tensor([[1.3335, 0.0000, 0.0000, 0.0202, 0.0000],\n",
      "        [0.0000, 1.3979, 0.0000, 0.0236, 1.3685],\n",
      "        [0.0000, 0.0000, 1.3597, 0.0000, 0.0000],\n",
      "        [0.1927, 0.0000, 0.0000, 1.4179, 0.0000],\n",
      "        [0.0000, 1.3271, 0.0000, 0.0262, 1.4129]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1158573627471924\n",
      "Batch 95, Loss: 3.1158573627471924\n",
      "Processing Batch 96/157\n",
      "Batch 96 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.010733969509601593, Std: 0.08562690019607544\n",
      "z_j Mean: 0.010686554946005344, Std: 0.08563283085823059\n",
      "Similarities: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3953, 0.2373, 0.0000, 1.1894],\n",
      "        [0.0000, 0.2157, 1.4188, 0.0000, 0.1266],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.2784, 0.0000, 0.0000, 1.3735]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2192187309265137\n",
      "Batch 96, Loss: 3.2192187309265137\n",
      "Processing Batch 97/157\n",
      "Batch 97 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012125655077397823, Std: 0.08615238964557648\n",
      "z_j Mean: 0.011900528334081173, Std: 0.08618377894163132\n",
      "Similarities: tensor([[1.3679e+00, 0.0000e+00, 6.5727e-01, 4.1059e-01, 0.0000e+00],\n",
      "        [0.0000e+00, 1.4231e+00, 0.0000e+00, 0.0000e+00, 1.3695e+00],\n",
      "        [7.2867e-01, 0.0000e+00, 1.4261e+00, 2.6252e-01, 0.0000e+00],\n",
      "        [3.4685e-01, 0.0000e+00, 8.5158e-04, 8.7960e-01, 0.0000e+00],\n",
      "        [0.0000e+00, 1.4067e+00, 0.0000e+00, 0.0000e+00, 1.4281e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.178022861480713\n",
      "Batch 97, Loss: 3.178022861480713\n",
      "Processing Batch 98/157\n",
      "Batch 98 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01129069086164236, Std: 0.08626580238342285\n",
      "z_j Mean: 0.011302292346954346, Std: 0.08626428246498108\n",
      "Similarities: tensor([[1.0180, 0.6280, 0.0000, 0.0000, 1.3287],\n",
      "        [0.0000, 0.9159, 0.0000, 0.1076, 0.1193],\n",
      "        [0.0000, 0.0000, 1.2785, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0385, 0.2637, 1.4044, 0.0000],\n",
      "        [0.6886, 0.7493, 0.1241, 0.0000, 1.3597]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.242306709289551\n",
      "Batch 98, Loss: 3.242306709289551\n",
      "Processing Batch 99/157\n",
      "Batch 99 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011777915991842747, Std: 0.08760546892881393\n",
      "z_j Mean: 0.011865408159792423, Std: 0.08759365975856781\n",
      "Similarities: tensor([[1.3949, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4185, 0.0000, 0.1043, 0.0000],\n",
      "        [0.1859, 0.0000, 0.9683, 0.8536, 1.0709],\n",
      "        [0.0000, 0.0056, 0.5750, 1.3163, 0.8830],\n",
      "        [0.0527, 0.0000, 0.8177, 0.9332, 1.4077]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1405465602874756\n",
      "Batch 99, Loss: 3.1405465602874756\n",
      "Processing Batch 100/157\n",
      "Batch 100 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011809839867055416, Std: 0.08619625866413116\n",
      "z_j Mean: 0.011971386149525642, Std: 0.0868794396519661\n",
      "Similarities: tensor([[1.4168, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3323, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0405, 0.0000, 1.2562, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4269]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.209186315536499\n",
      "Batch 100, Loss: 3.209186315536499\n",
      "Processing Batch 101/157\n",
      "Batch 101 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012384496629238129, Std: 0.08682151138782501\n",
      "z_j Mean: 0.01217340026050806, Std: 0.08685136586427689\n",
      "Similarities: tensor([[1.4281, 0.0000, 0.0000, 1.3153, 0.0000],\n",
      "        [0.0000, 1.4210, 1.4017, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4089, 1.4256, 0.0000, 0.0000],\n",
      "        [1.3302, 0.0000, 0.0000, 1.4202, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4286]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.185969829559326\n",
      "Batch 101, Loss: 3.185969829559326\n",
      "Processing Batch 102/157\n",
      "Batch 102 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01167899277061224, Std: 0.086214080452919\n",
      "z_j Mean: 0.011372249573469162, Std: 0.08625508099794388\n",
      "Similarities: tensor([[1.4064, 0.6434, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1882, 1.3189, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4286, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4032, 1.4242],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4159, 1.4118]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.217489004135132\n",
      "Batch 102, Loss: 3.217489004135132\n",
      "Processing Batch 103/157\n",
      "Batch 103 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01105781551450491, Std: 0.08414711803197861\n",
      "z_j Mean: 0.010953416116535664, Std: 0.08488298207521439\n",
      "Similarities: tensor([[1.4173, 0.0071, 0.0425, 0.0000, 0.9432],\n",
      "        [0.0627, 1.3068, 1.1502, 0.0000, 0.0000],\n",
      "        [0.1421, 1.0488, 1.3968, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.2964, 0.0000],\n",
      "        [1.0237, 0.0000, 0.0000, 0.0000, 1.4222]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2755141258239746\n",
      "Batch 103, Loss: 3.2755141258239746\n",
      "Processing Batch 104/157\n",
      "Batch 104 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011932148598134518, Std: 0.08758458495140076\n",
      "z_j Mean: 0.012126455083489418, Std: 0.08755789697170258\n",
      "Similarities: tensor([[1.4023, 0.0000, 0.0000, 0.7654, 0.0000],\n",
      "        [0.0000, 1.4156, 0.0740, 0.0000, 0.0237],\n",
      "        [0.0000, 0.1019, 1.3741, 0.0000, 0.1539],\n",
      "        [0.7927, 0.0000, 0.0000, 1.4173, 0.0000],\n",
      "        [0.0000, 0.0273, 0.1285, 0.0000, 1.4127]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1871092319488525\n",
      "Batch 104, Loss: 3.1871092319488525\n",
      "Processing Batch 105/157\n",
      "Batch 105 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011015225201845169, Std: 0.08487498760223389\n",
      "z_j Mean: 0.010932727716863155, Std: 0.08488564938306808\n",
      "Similarities: tensor([[1.4243, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [1.1896, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0194, 0.0000, 1.4243, 0.7327, 0.0000],\n",
      "        [0.0000, 0.0000, 0.4597, 1.3717, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4258]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.3122992515563965\n",
      "Batch 105, Loss: 3.3122992515563965\n",
      "Processing Batch 106/157\n",
      "Batch 106 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012056617066264153, Std: 0.087567538022995\n",
      "z_j Mean: 0.011928044259548187, Std: 0.08688540756702423\n",
      "Similarities: tensor([[1.4232, 0.9925, 0.0000, 0.0000, 0.0000],\n",
      "        [0.8879, 1.4205, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4274, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4028]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1427910327911377\n",
      "Batch 106, Loss: 3.1427910327911377\n",
      "Processing Batch 107/157\n",
      "Batch 107 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013708778657019138, Std: 0.08591466397047043\n",
      "z_j Mean: 0.013838534243404865, Std: 0.0866016149520874\n",
      "Similarities: tensor([[1.3687e+00, 1.1036e+00, 0.0000e+00, 0.0000e+00, 1.4849e-02],\n",
      "        [4.3669e-01, 8.5626e-01, 0.0000e+00, 0.0000e+00, 8.7505e-01],\n",
      "        [1.0171e-02, 0.0000e+00, 1.3955e+00, 0.0000e+00, 3.7361e-04],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 1.4286e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 3.6348e-01, 0.0000e+00, 0.0000e+00, 1.4203e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.184574842453003\n",
      "Batch 107, Loss: 3.184574842453003\n",
      "Processing Batch 108/157\n",
      "Batch 108 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011900801211595535, Std: 0.08403202891349792\n",
      "z_j Mean: 0.012408626265823841, Std: 0.08611208200454712\n",
      "Similarities: tensor([[1.4254, 0.9971, 0.1094, 0.0000, 0.8299],\n",
      "        [1.1872, 1.3103, 0.0000, 0.2631, 1.1569],\n",
      "        [0.1789, 0.0000, 1.4107, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0208, 0.0000, 1.4286, 0.0000],\n",
      "        [0.9338, 1.3901, 0.0000, 0.1100, 1.3136]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.206818103790283\n",
      "Batch 108, Loss: 3.206818103790283\n",
      "Processing Batch 109/157\n",
      "Batch 109 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011834913864731789, Std: 0.08404133468866348\n",
      "z_j Mean: 0.011828168295323849, Std: 0.08404228091239929\n",
      "Similarities: tensor([[1.4198, 0.0000, 1.2374, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3619, 0.0000, 0.0637, 0.2781],\n",
      "        [1.1302, 0.0000, 1.3875, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3289, 0.0000, 1.3635, 0.2884],\n",
      "        [0.0000, 0.5201, 0.0000, 0.0196, 1.2776]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.238111734390259\n",
      "Batch 109, Loss: 3.238111734390259\n",
      "Processing Batch 110/157\n",
      "Batch 110 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012759732082486153, Std: 0.08746784180402756\n",
      "z_j Mean: 0.012900370173156261, Std: 0.08674634248018265\n",
      "Similarities: tensor([[1.3849, 0.0000, 0.0000, 0.0000, 0.5072],\n",
      "        [0.0000, 1.4250, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4286, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4284, 0.2402],\n",
      "        [0.5431, 0.0000, 0.0000, 0.1605, 1.3610]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1279752254486084\n",
      "Batch 110, Loss: 3.1279752254486084\n",
      "Processing Batch 111/157\n",
      "Batch 111 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012222623452544212, Std: 0.0839858129620552\n",
      "z_j Mean: 0.012558612041175365, Std: 0.0860903337597847\n",
      "Similarities: tensor([[1.0468, 0.0000, 0.0000, 0.2149, 0.4592],\n",
      "        [0.0000, 1.4047, 0.0000, 0.0183, 0.3479],\n",
      "        [0.0000, 0.0000, 1.3775, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0204, 0.0000, 1.4196, 0.0000],\n",
      "        [0.8511, 0.3787, 0.0000, 0.0000, 1.3703]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2555932998657227\n",
      "Batch 111, Loss: 3.2555932998657227\n",
      "Processing Batch 112/157\n",
      "Batch 112 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012721393257379532, Std: 0.08677277714014053\n",
      "z_j Mean: 0.012376207858324051, Std: 0.0861167460680008\n",
      "Similarities: tensor([[1.2718, 0.2194, 0.0143, 1.2861, 0.0000],\n",
      "        [0.2812, 1.3068, 0.0161, 0.4333, 0.0000],\n",
      "        [0.0000, 0.3020, 1.3825, 0.0000, 0.0000],\n",
      "        [1.0435, 0.2116, 0.0060, 1.4014, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3708]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.160357713699341\n",
      "Batch 112, Loss: 3.160357713699341\n",
      "Processing Batch 113/157\n",
      "Batch 113 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012618020176887512, Std: 0.08536957204341888\n",
      "z_j Mean: 0.01282314583659172, Std: 0.0860513225197792\n",
      "Similarities: tensor([[1.3968e+00, 2.2833e-01, 0.0000e+00, 0.0000e+00, 1.2008e+00],\n",
      "        [1.7437e-01, 1.2856e+00, 1.4835e-01, 0.0000e+00, 1.5759e-01],\n",
      "        [7.9665e-04, 1.9450e-01, 1.4033e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 1.4286e+00, 0.0000e+00],\n",
      "        [1.1520e+00, 2.1502e-01, 0.0000e+00, 0.0000e+00, 1.2829e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1647720336914062\n",
      "Batch 113, Loss: 3.1647720336914062\n",
      "Processing Batch 114/157\n",
      "Batch 114 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012966067530214787, Std: 0.08602990210056305\n",
      "z_j Mean: 0.012897448614239693, Std: 0.08532780408859253\n",
      "Similarities: tensor([[1.4084, 0.0000, 0.0000, 0.0000, 0.1916],\n",
      "        [0.0000, 1.3871, 0.0000, 0.9951, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4179, 0.0000, 0.0000],\n",
      "        [0.0000, 1.2173, 0.0000, 1.3763, 0.0000],\n",
      "        [0.1704, 0.0000, 0.0000, 0.0000, 1.3759]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1378533840179443\n",
      "Batch 114, Loss: 3.1378533840179443\n",
      "Processing Batch 115/157\n",
      "Batch 115 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012211955152451992, Std: 0.08614019304513931\n",
      "z_j Mean: 0.012342432513833046, Std: 0.08612159639596939\n",
      "Similarities: tensor([[0.8938, 0.0000, 0.0000, 0.0000, 0.9765],\n",
      "        [0.0000, 1.4274, 1.3897, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3808, 1.4286, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2435, 0.0000, 0.0000, 0.0000, 1.3206]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.222153902053833\n",
      "Batch 115, Loss: 3.222153902053833\n",
      "Processing Batch 116/157\n",
      "Batch 116 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011670200154185295, Std: 0.08621527254581451\n",
      "z_j Mean: 0.011443871073424816, Std: 0.08553490787744522\n",
      "Similarities: tensor([[1.4286, 0.0000, 0.9961, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4247, 0.0000, 0.0000, 1.1416],\n",
      "        [0.2880, 0.0000, 1.1801, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3833, 0.0000],\n",
      "        [0.0000, 0.9623, 0.0000, 0.0000, 1.4195]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2013766765594482\n",
      "Batch 116, Loss: 3.2013766765594482\n",
      "Processing Batch 117/157\n",
      "Batch 117 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012177855707705021, Std: 0.08614501357078552\n",
      "z_j Mean: 0.012232297100126743, Std: 0.08542570471763611\n",
      "Similarities: tensor([[1.4034, 0.0983, 0.0000, 1.0511, 0.0455],\n",
      "        [0.1326, 1.3042, 0.0000, 0.2227, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4286, 0.0000, 0.0000],\n",
      "        [1.1212, 0.1953, 0.0000, 1.3953, 0.2130],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.2317]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.139821767807007\n",
      "Batch 117, Loss: 3.139821767807007\n",
      "Processing Batch 118/157\n",
      "Batch 118 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011699656024575233, Std: 0.08621127903461456\n",
      "z_j Mean: 0.011865555308759212, Std: 0.08618859946727753\n",
      "Similarities: tensor([[1.4239, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4286, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4276, 0.6510, 0.0000],\n",
      "        [0.0000, 0.0000, 1.0032, 1.3604, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.2522]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1921048164367676\n",
      "Batch 118, Loss: 3.1921048164367676\n",
      "Processing Batch 119/157\n",
      "Batch 119 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011270938441157341, Std: 0.0876721441745758\n",
      "z_j Mean: 0.010930484160780907, Std: 0.0863121896982193\n",
      "Similarities: tensor([[1.4205, 0.9543, 0.0000, 0.3680, 1.3849],\n",
      "        [0.6255, 1.1724, 0.0000, 0.7075, 0.7358],\n",
      "        [0.0000, 0.0000, 1.4218, 0.0000, 0.0000],\n",
      "        [0.7222, 1.0577, 0.0000, 1.3485, 0.8301],\n",
      "        [1.1172, 1.2732, 0.0000, 0.5210, 1.1831]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1793103218078613\n",
      "Batch 119, Loss: 3.1793103218078613\n",
      "Processing Batch 120/157\n",
      "Batch 120 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011818839237093925, Std: 0.08690032362937927\n",
      "z_j Mean: 0.011723789386451244, Std: 0.08620800077915192\n",
      "Similarities: tensor([[1.4228, 0.0000, 0.9095, 0.0000, 0.0000],\n",
      "        [0.0033, 1.3699, 0.0000, 0.0000, 0.0000],\n",
      "        [0.8980, 0.0065, 1.4263, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3960, 1.4068],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3600, 1.4219]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.144415855407715\n",
      "Batch 120, Loss: 3.144415855407715\n",
      "Processing Batch 121/157\n",
      "Batch 121 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011035694740712643, Std: 0.08487232029438019\n",
      "z_j Mean: 0.010620273649692535, Std: 0.08420346677303314\n",
      "Similarities: tensor([[1.2942, 0.6941, 0.0000, 0.0000, 0.7634],\n",
      "        [0.9783, 1.4109, 0.0000, 0.0000, 1.1469],\n",
      "        [0.0000, 0.0000, 1.1122, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4044, 0.0000],\n",
      "        [1.0744, 1.0002, 0.0000, 0.0000, 1.4132]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.240093231201172\n",
      "Batch 121, Loss: 3.240093231201172\n",
      "Processing Batch 122/157\n",
      "Batch 122 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011496156454086304, Std: 0.0869436040520668\n",
      "z_j Mean: 0.01157383993268013, Std: 0.0876326635479927\n",
      "Similarities: tensor([[1.3837, 0.3332, 0.0578, 0.3885, 0.0000],\n",
      "        [0.3208, 1.4272, 0.0000, 1.3809, 0.0000],\n",
      "        [0.3124, 0.0000, 1.3393, 0.0000, 0.0000],\n",
      "        [0.0760, 1.1705, 0.0000, 1.2242, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4280]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.174950122833252\n",
      "Batch 122, Loss: 3.174950122833252\n",
      "Processing Batch 123/157\n",
      "Batch 123 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011726519092917442, Std: 0.08620762825012207\n",
      "z_j Mean: 0.011897780001163483, Std: 0.0854729413986206\n",
      "Similarities: tensor([[1.4272, 0.0000, 0.3680, 1.2492, 0.0000],\n",
      "        [0.0000, 0.4543, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1851, 0.0000, 1.1705, 0.3708, 0.2235],\n",
      "        [1.0897, 0.0000, 0.6082, 1.3836, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0487, 0.0000, 1.3962]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.167832612991333\n",
      "Batch 123, Loss: 3.167832612991333\n",
      "Processing Batch 124/157\n",
      "Batch 124 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.010578304529190063, Std: 0.08635606616735458\n",
      "z_j Mean: 0.010664299130439758, Std: 0.08634547889232635\n",
      "Similarities: tensor([[0.7656, 0.0000, 0.7648, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4041, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4286, 0.0000, 1.4241, 0.0000, 0.0995],\n",
      "        [1.1832, 0.0000, 0.0000, 1.4178, 0.1163],\n",
      "        [0.0000, 0.0000, 0.1320, 0.0386, 1.4194]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2050693035125732\n",
      "Batch 124, Loss: 3.2050693035125732\n",
      "Processing Batch 125/157\n",
      "Batch 125 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01149025745689869, Std: 0.08623944222927094\n",
      "z_j Mean: 0.01167622022330761, Std: 0.08691960573196411\n",
      "Similarities: tensor([[1.4243, 1.3203, 0.0000, 0.0000, 0.0000],\n",
      "        [1.3169, 1.4280, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4161, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.2358, 0.5385],\n",
      "        [0.0000, 0.0000, 0.0000, 0.1793, 0.9331]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2113943099975586\n",
      "Batch 125, Loss: 3.2113943099975586\n",
      "Processing Batch 126/157\n",
      "Batch 126 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011405927129089832, Std: 0.08765468001365662\n",
      "z_j Mean: 0.01109091192483902, Std: 0.08699623495340347\n",
      "Similarities: tensor([[1.3405, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.1086, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4087, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.9661, 0.0000],\n",
      "        [0.0000, 0.0000, 0.6150, 0.0000, 1.2426]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1582486629486084\n",
      "Batch 126, Loss: 3.1582486629486084\n",
      "Processing Batch 127/157\n",
      "Batch 127 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011662378907203674, Std: 0.08692146837711334\n",
      "z_j Mean: 0.011922257952392101, Std: 0.0875859335064888\n",
      "Similarities: tensor([[1.0229, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3743, 0.2592, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.2204, 0.0000, 0.0000],\n",
      "        [0.2247, 0.0000, 0.0000, 1.3950, 1.3702],\n",
      "        [0.2301, 0.0000, 0.0000, 1.4286, 1.4032]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1341054439544678\n",
      "Batch 127, Loss: 3.1341054439544678\n",
      "Processing Batch 128/157\n",
      "Batch 128 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011754665523767471, Std: 0.08690903335809708\n",
      "z_j Mean: 0.011471057310700417, Std: 0.08624199777841568\n",
      "Similarities: tensor([[1.3762, 0.0023, 0.0000, 0.0000, 0.2620],\n",
      "        [0.0000, 1.4204, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4218, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.8856, 0.0000, 0.0270],\n",
      "        [0.4717, 0.0000, 0.0000, 0.0000, 1.2902]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1303024291992188\n",
      "Batch 128, Loss: 3.1303024291992188\n",
      "Processing Batch 129/157\n",
      "Batch 129 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011448569595813751, Std: 0.08764912188053131\n",
      "z_j Mean: 0.011327143758535385, Std: 0.08696579188108444\n",
      "Similarities: tensor([[1.3365, 0.0000, 0.0000, 0.0000, 0.6440],\n",
      "        [0.0000, 0.8541, 0.8646, 0.0000, 0.0000],\n",
      "        [0.0000, 0.6027, 1.4229, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3786, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.2752]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0932505130767822\n",
      "Batch 129, Loss: 3.0932505130767822\n",
      "Processing Batch 130/157\n",
      "Batch 130 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.010916130617260933, Std: 0.08631400763988495\n",
      "z_j Mean: 0.011032762005925179, Std: 0.08558890223503113\n",
      "Similarities: tensor([[1.3509, 0.0000, 1.2760, 0.0000, 0.0000],\n",
      "        [0.2579, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [1.1461, 0.0000, 1.4024, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4024, 1.0632],\n",
      "        [0.0000, 0.0000, 0.0000, 1.1940, 1.4246]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.206254720687866\n",
      "Batch 130, Loss: 3.206254720687866\n",
      "Processing Batch 131/157\n",
      "Batch 131 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.010372844524681568, Std: 0.08277226239442825\n",
      "z_j Mean: 0.010289628058671951, Std: 0.08278265595436096\n",
      "Similarities: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4216, 0.2658, 0.0000],\n",
      "        [0.0000, 0.0000, 0.1725, 1.4068, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4248]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.299837827682495\n",
      "Batch 131, Loss: 3.299837827682495\n",
      "Processing Batch 132/157\n",
      "Batch 132 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011320054531097412, Std: 0.0841122418642044\n",
      "z_j Mean: 0.011564644053578377, Std: 0.08480186015367508\n",
      "Similarities: tensor([[1.4172, 0.0000, 1.0001, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4256, 0.0000, 0.0000, 0.0000],\n",
      "        [0.9948, 0.0000, 1.4192, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4229, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3990]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.216052770614624\n",
      "Batch 132, Loss: 3.216052770614624\n",
      "Processing Batch 133/157\n",
      "Batch 133 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01183617115020752, Std: 0.08404116332530975\n",
      "z_j Mean: 0.011704053729772568, Std: 0.08405965566635132\n",
      "Similarities: tensor([[1.3571, 0.0212, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2017, 1.4118, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4067, 0.0000, 0.1884],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.2842, 0.0000, 1.2077]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285715818405151]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2610278129577637\n",
      "Batch 133, Loss: 3.2610278129577637\n",
      "Processing Batch 134/157\n",
      "Batch 134 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.010478314012289047, Std: 0.08201806247234344\n",
      "z_j Mean: 0.011332755908370018, Std: 0.08554970473051071\n",
      "Similarities: tensor([[1.4286, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.1123, 1.3767, 0.0000, 0.0000],\n",
      "        [0.0000, 1.1559, 1.4012, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4262, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.258894205093384\n",
      "Batch 134, Loss: 3.258894205093384\n",
      "Processing Batch 135/157\n",
      "Batch 135 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01266828179359436, Std: 0.08678054809570312\n",
      "z_j Mean: 0.013069073669612408, Std: 0.08672108501195908\n",
      "Similarities: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3783, 0.4436, 0.0000, 0.0000],\n",
      "        [0.0000, 0.5863, 1.2127, 0.0000, 0.1126],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4286, 0.3868],\n",
      "        [0.0000, 0.0000, 0.0470, 0.2835, 1.4246]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2299110889434814\n",
      "Batch 135, Loss: 3.2299110889434814\n",
      "Processing Batch 136/157\n",
      "Batch 136 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011924762278795242, Std: 0.08688585460186005\n",
      "z_j Mean: 0.011940871365368366, Std: 0.08617819845676422\n",
      "Similarities: tensor([[0.0000, 0.0000, 0.7403, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4031, 0.0000, 1.3172, 0.0105],\n",
      "        [0.0000, 0.0000, 1.3806, 0.0000, 0.0000],\n",
      "        [0.0000, 1.2278, 0.0000, 1.4041, 0.0000],\n",
      "        [0.0000, 0.0798, 0.0000, 0.0000, 1.3896]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.222320079803467\n",
      "Batch 136, Loss: 3.222320079803467\n",
      "Processing Batch 137/157\n",
      "Batch 137 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011438802815973759, Std: 0.0826316848397255\n",
      "z_j Mean: 0.01150626502931118, Std: 0.08335785567760468\n",
      "Similarities: tensor([[1.4286e+00, 0.0000e+00, 7.6308e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 1.4245e+00, 0.0000e+00, 8.9610e-01, 0.0000e+00],\n",
      "        [7.9288e-01, 0.0000e+00, 1.4276e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 8.2873e-01, 0.0000e+00, 1.4154e+00, 9.8179e-02],\n",
      "        [9.9270e-04, 0.0000e+00, 0.0000e+00, 2.6545e-01, 1.1785e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.252784252166748\n",
      "Batch 137, Loss: 3.252784252166748\n",
      "Processing Batch 138/157\n",
      "Batch 138 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013570282608270645, Std: 0.08734574168920517\n",
      "z_j Mean: 0.01360239926725626, Std: 0.08663902431726456\n",
      "Similarities: tensor([[1.1188, 0.2834, 0.0000, 0.0000, 0.5679],\n",
      "        [0.4208, 1.4268, 0.0000, 0.0000, 1.1926],\n",
      "        [0.0000, 0.0000, 1.3859, 0.6544, 0.0000],\n",
      "        [0.0000, 0.0000, 0.3943, 1.2471, 0.0000],\n",
      "        [0.5404, 1.1995, 0.0000, 0.0000, 1.3821]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.18857741355896\n",
      "Batch 138, Loss: 3.18857741355896\n",
      "Processing Batch 139/157\n",
      "Batch 139 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013498827815055847, Std: 0.08665522933006287\n",
      "z_j Mean: 0.013471202924847603, Std: 0.08736108243465424\n",
      "Similarities: tensor([[1.4224, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4108, 0.0000, 1.2432, 0.7584],\n",
      "        [0.0000, 0.0000, 1.3927, 0.0593, 0.0000],\n",
      "        [0.0000, 0.8468, 0.4326, 1.1306, 0.5248],\n",
      "        [0.0000, 0.4106, 0.0000, 0.4172, 1.3233]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1584537029266357\n",
      "Batch 139, Loss: 3.1584537029266357\n",
      "Processing Batch 140/157\n",
      "Batch 140 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012658117339015007, Std: 0.08536363393068314\n",
      "z_j Mean: 0.012675782665610313, Std: 0.08536101132631302\n",
      "Similarities: tensor([[1.3405, 0.7133, 1.0092, 0.0000, 0.0000],\n",
      "        [0.5213, 1.3981, 0.8531, 0.0000, 0.0000],\n",
      "        [0.8326, 0.7133, 1.4264, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4038, 1.0913],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0067, 1.4197]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.156115770339966\n",
      "Batch 140, Loss: 3.156115770339966\n",
      "Processing Batch 141/157\n",
      "Batch 141 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012282698415219784, Std: 0.0854184702038765\n",
      "z_j Mean: 0.01249435544013977, Std: 0.08538775891065598\n",
      "Similarities: tensor([[1.4012, 0.0000, 0.0000, 0.0000, 0.5578],\n",
      "        [0.0000, 1.4186, 0.4952, 0.2656, 0.4555],\n",
      "        [0.0383, 0.5233, 1.4130, 0.0000, 0.2525],\n",
      "        [0.0000, 0.1456, 0.0000, 1.4193, 0.3564],\n",
      "        [0.4413, 0.3277, 0.0370, 0.4057, 1.3703]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.149082660675049\n",
      "Batch 141, Loss: 3.149082660675049\n",
      "Processing Batch 142/157\n",
      "Batch 142 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013224305585026741, Std: 0.08599057793617249\n",
      "z_j Mean: 0.013236163184046745, Std: 0.08598875254392624\n",
      "Similarities: tensor([[1.2262e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.9565e-03],\n",
      "        [6.9857e-03, 1.4200e+00, 0.0000e+00, 0.0000e+00, 3.5520e-05],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 1.3912e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.4212e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.210784435272217\n",
      "Batch 142, Loss: 3.210784435272217\n",
      "Processing Batch 143/157\n",
      "Batch 143 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013978236354887486, Std: 0.08657918125391006\n",
      "z_j Mean: 0.014524520374834538, Std: 0.0871921256184578\n",
      "Similarities: tensor([[1.4063, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.2924, 0.0407, 0.7193, 1.3677],\n",
      "        [0.0000, 0.0000, 1.4254, 0.6543, 0.0000],\n",
      "        [0.0000, 0.3898, 0.5446, 1.3596, 0.4939],\n",
      "        [0.0000, 1.2550, 0.0000, 0.5776, 1.3873]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1630184650421143\n",
      "Batch 143, Loss: 3.1630184650421143\n",
      "Processing Batch 144/157\n",
      "Batch 144 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01483912579715252, Std: 0.08572664111852646\n",
      "z_j Mean: 0.014803175814449787, Std: 0.0857328549027443\n",
      "Similarities: tensor([[1.4133, 0.0000, 1.0438, 0.0418, 0.0000],\n",
      "        [0.0000, 1.3297, 0.3018, 0.3511, 0.0000],\n",
      "        [0.9703, 0.3304, 1.1973, 0.2740, 0.0000],\n",
      "        [0.0165, 0.2277, 0.4194, 1.3791, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3919]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1736433506011963\n",
      "Batch 144, Loss: 3.1736433506011963\n",
      "Processing Batch 145/157\n",
      "Batch 145 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013715347275137901, Std: 0.08520014584064484\n",
      "z_j Mean: 0.013793077319860458, Std: 0.08518758416175842\n",
      "Similarities: tensor([[1.4191, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4001, 0.4562, 0.0000, 0.4289],\n",
      "        [0.0000, 0.2286, 0.8725, 0.0000, 1.0794],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.4986, 1.4247, 0.0000, 1.4172]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.204986095428467\n",
      "Batch 145, Loss: 3.204986095428467\n",
      "Processing Batch 146/157\n",
      "Batch 146 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013281108811497688, Std: 0.08668886870145798\n",
      "z_j Mean: 0.013542188331484795, Std: 0.08735010027885437\n",
      "Similarities: tensor([[1.4087e+00, 2.9153e-01, 1.5584e-04, 5.7391e-01, 0.0000e+00],\n",
      "        [1.7878e-01, 1.4044e+00, 1.1890e-01, 0.0000e+00, 4.6750e-01],\n",
      "        [0.0000e+00, 1.5886e-01, 1.3701e+00, 0.0000e+00, 9.9723e-01],\n",
      "        [3.8644e-01, 5.7304e-03, 0.0000e+00, 1.4262e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 2.9772e-01, 1.0180e+00, 0.0000e+00, 1.3683e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1026558876037598\n",
      "Batch 146, Loss: 3.1026558876037598\n",
      "Processing Batch 147/157\n",
      "Batch 147 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013916239142417908, Std: 0.0858813002705574\n",
      "z_j Mean: 0.01349471602588892, Std: 0.08451617509126663\n",
      "Similarities: tensor([[1.3212, 0.8823, 0.0000, 0.0000, 0.0000],\n",
      "        [0.6152, 1.4079, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3938, 0.0917, 1.1357],\n",
      "        [0.1554, 0.1632, 0.0218, 1.2341, 0.0280],\n",
      "        [0.0000, 0.0000, 1.0262, 0.1097, 1.4001]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1619157791137695\n",
      "Batch 147, Loss: 3.1619157791137695\n",
      "Processing Batch 148/157\n",
      "Batch 148 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013213222846388817, Std: 0.08740047365427017\n",
      "z_j Mean: 0.013264819979667664, Std: 0.08669135719537735\n",
      "Similarities: tensor([[1.3880, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.7873, 1.2384, 0.7066, 0.0000],\n",
      "        [0.0000, 0.7330, 1.4178, 0.9136, 0.0000],\n",
      "        [0.0000, 0.1806, 0.5711, 1.3458, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4096]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.118020534515381\n",
      "Batch 148, Loss: 3.118020534515381\n",
      "Processing Batch 149/157\n",
      "Batch 149 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012257975526154041, Std: 0.08398066461086273\n",
      "z_j Mean: 0.012149822898209095, Std: 0.08253012597560883\n",
      "Similarities: tensor([[1.3177, 0.0000, 0.3313, 1.1214, 0.0000],\n",
      "        [0.0000, 1.3553, 0.5308, 0.0000, 0.0123],\n",
      "        [0.2951, 0.4322, 1.4199, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4215]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1856439113616943\n",
      "Batch 149, Loss: 3.1856439113616943\n",
      "Processing Batch 150/157\n",
      "Batch 150 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013464318588376045, Std: 0.08666059374809265\n",
      "z_j Mean: 0.013419660739600658, Std: 0.08736901730298996\n",
      "Similarities: tensor([[1.4143, 0.0000, 0.0000, 0.0000, 1.2513],\n",
      "        [0.0000, 1.2002, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.4708, 1.0308, 0.0000],\n",
      "        [0.0000, 0.0000, 0.9813, 1.4155, 0.0000],\n",
      "        [1.1864, 0.0000, 0.0000, 0.0000, 1.4174]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1365621089935303\n",
      "Batch 150, Loss: 3.1365621089935303\n",
      "Processing Batch 151/157\n",
      "Batch 151 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012456297874450684, Std: 0.08610519766807556\n",
      "z_j Mean: 0.012766115367412567, Std: 0.0860598012804985\n",
      "Similarities: tensor([[1.4230, 0.0000, 0.0000, 0.0000, 0.0642],\n",
      "        [0.0000, 0.5414, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4026, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0117, 0.0000, 1.4222, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4107]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.157449245452881\n",
      "Batch 151, Loss: 3.157449245452881\n",
      "Processing Batch 152/157\n",
      "Batch 152 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012396594509482384, Std: 0.08540201187133789\n",
      "z_j Mean: 0.012666753493249416, Std: 0.08607448637485504\n",
      "Similarities: tensor([[1.3764, 0.0000, 0.0000, 0.0000, 0.1498],\n",
      "        [0.0000, 1.4249, 0.0000, 0.0000, 0.5594],\n",
      "        [0.0000, 0.0000, 1.4286, 0.0000, 0.0000],\n",
      "        [0.0000, 0.5977, 0.0000, 0.0000, 0.0364],\n",
      "        [0.1348, 0.5944, 0.0000, 0.0000, 1.4141]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.134536027908325\n",
      "Batch 152, Loss: 3.134536027908325\n",
      "Processing Batch 153/157\n",
      "Batch 153 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012373490259051323, Std: 0.08682307600975037\n",
      "z_j Mean: 0.012058688327670097, Std: 0.0861617848277092\n",
      "Similarities: tensor([[1.4023, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.2927, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3959, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.2260, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3767]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1706607341766357\n",
      "Batch 153, Loss: 3.1706607341766357\n",
      "Processing Batch 154/157\n",
      "Batch 154 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012287705205380917, Std: 0.08612941950559616\n",
      "z_j Mean: 0.012461328878998756, Std: 0.0875108614563942\n",
      "Similarities: tensor([[1.3977, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.5988, 1.2752, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4223, 0.5555],\n",
      "        [0.0000, 0.0000, 0.1509, 0.0000, 1.0706]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.180371046066284\n",
      "Batch 154, Loss: 3.180371046066284\n",
      "Processing Batch 155/157\n",
      "Batch 155 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012612413614988327, Std: 0.08748921006917953\n",
      "z_j Mean: 0.01276296004652977, Std: 0.08746737241744995\n",
      "Similarities: tensor([[1.3862, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.1321, 0.0000, 0.0969, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4000, 1.1284, 0.0000],\n",
      "        [0.0000, 0.0820, 1.1491, 1.4140, 0.0000],\n",
      "        [0.1290, 0.0000, 0.0000, 0.0000, 1.3906]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.079862356185913\n",
      "Batch 155, Loss: 3.079862356185913\n",
      "Processing Batch 156/157\n",
      "Batch 156 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011911915615200996, Std: 0.08688761293888092\n",
      "z_j Mean: 0.011968422681093216, Std: 0.086174376308918\n",
      "Similarities: tensor([[1.4235, 0.0000, 0.0000, 0.2545, 0.0000],\n",
      "        [0.0000, 0.8939, 0.0000, 0.0000, 0.1704],\n",
      "        [0.0000, 0.0000, 1.1344, 0.0000, 0.0000],\n",
      "        [0.0966, 0.0000, 0.0000, 1.4020, 0.0000],\n",
      "        [0.0000, 0.3953, 0.0000, 0.0000, 1.4097]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1244559288024902\n",
      "Batch 156, Loss: 3.1244559288024902\n",
      "Processing Batch 157/157\n",
      "Batch 157 loaded: x.shape=torch.Size([16, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([16, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([16, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([16, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([16, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([16, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012483673170208931, Std: 0.0875236988067627\n",
      "z_j Mean: 0.012375453487038612, Std: 0.08470283448696136\n",
      "Similarities: tensor([[1.3565, 0.0000, 0.4110, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [1.1998, 0.0000, 1.1195, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.1772, 1.4248, 0.0000],\n",
      "        [0.1904, 0.0000, 0.0000, 0.0000, 1.4286]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 1.928798794746399\n",
      "Batch 157, Loss: 1.928798794746399\n",
      "Epoch 4/10, Loss: 3.1831\n",
      "Entered Epoch 5\n",
      "Starting Training Loop with 157 batches.\n",
      "Processing Batch 1/157\n",
      "Batch 1 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01199133787304163, Std: 0.0875765010714531\n",
      "z_j Mean: 0.011795032769441605, Std: 0.08690355718135834\n",
      "Similarities: tensor([[1.3967, 0.7933, 0.0000, 0.0000, 0.0000],\n",
      "        [0.6808, 1.3951, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4285, 0.2372, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3792, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0792794227600098\n",
      "Batch 1, Loss: 3.0792794227600098\n",
      "Processing Batch 2/157\n",
      "Batch 2 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011633935384452343, Std: 0.08692527562379837\n",
      "z_j Mean: 0.01144474372267723, Std: 0.0869503915309906\n",
      "Similarities: tensor([[1.4014, 0.0000, 0.0000, 0.2273, 0.0000],\n",
      "        [0.0000, 0.8378, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4286, 0.0000, 1.4286],\n",
      "        [0.1663, 0.0000, 0.0000, 1.4130, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4280, 0.0000, 1.4280]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1386239528656006\n",
      "Batch 2, Loss: 3.1386239528656006\n",
      "Processing Batch 3/157\n",
      "Batch 3 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012570097111165524, Std: 0.08608865737915039\n",
      "z_j Mean: 0.012582730501890182, Std: 0.08608680963516235\n",
      "Similarities: tensor([[1.4144, 1.3606, 0.0067, 0.0177, 0.0000],\n",
      "        [1.3826, 1.3471, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1066, 0.1128, 1.3638, 0.9083, 0.0000],\n",
      "        [0.1069, 0.1131, 1.1201, 0.9958, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4286]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1926167011260986\n",
      "Batch 3, Loss: 3.1926167011260986\n",
      "Processing Batch 4/157\n",
      "Batch 4 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012507119216024876, Std: 0.0875043198466301\n",
      "z_j Mean: 0.012304368428885937, Std: 0.08753307163715363\n",
      "Similarities: tensor([[1.3745, 1.0318, 0.0000, 0.0000, 0.0000],\n",
      "        [1.2033, 1.3726, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3608, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4140, 0.8561],\n",
      "        [0.0000, 0.0000, 0.0000, 0.8721, 1.4235]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0790209770202637\n",
      "Batch 4, Loss: 3.0790209770202637\n",
      "Processing Batch 5/157\n",
      "Batch 5 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012481193989515305, Std: 0.08610159158706665\n",
      "z_j Mean: 0.012534307315945625, Std: 0.0868000015616417\n",
      "Similarities: tensor([[1.3947e+00, 0.0000e+00, 0.0000e+00, 5.2312e-01, 3.2576e-01],\n",
      "        [1.6085e-02, 1.3948e+00, 1.3607e+00, 7.7187e-01, 4.3385e-04],\n",
      "        [0.0000e+00, 1.2270e+00, 1.3673e+00, 5.2147e-01, 0.0000e+00],\n",
      "        [9.2501e-01, 9.7998e-02, 9.4166e-02, 1.2153e+00, 4.4918e-03],\n",
      "        [2.1424e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.4168e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.138413190841675\n",
      "Batch 5, Loss: 3.138413190841675\n",
      "Processing Batch 6/157\n",
      "Batch 6 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012268099002540112, Std: 0.08753816038370132\n",
      "z_j Mean: 0.012289091013371944, Std: 0.08683505654335022\n",
      "Similarities: tensor([[0.0000, 0.0000, 0.3745, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.9056],\n",
      "        [0.2205, 0.0000, 1.2739, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.1741, 1.3468, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.3830]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.136638879776001\n",
      "Batch 6, Loss: 3.136638879776001\n",
      "Processing Batch 7/157\n",
      "Batch 7 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012624511495232582, Std: 0.08678692579269409\n",
      "z_j Mean: 0.012921725399792194, Std: 0.08674316108226776\n",
      "Similarities: tensor([[1.3047e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.2354e-01],\n",
      "        [0.0000e+00, 1.3784e+00, 0.0000e+00, 0.0000e+00, 9.4431e-02],\n",
      "        [0.0000e+00, 0.0000e+00, 1.4157e+00, 1.9864e-01, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 1.7654e-01, 1.4205e+00, 0.0000e+00],\n",
      "        [9.9097e-01, 3.0135e-04, 0.0000e+00, 0.0000e+00, 1.0687e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0965323448181152\n",
      "Batch 7, Loss: 3.0965323448181152\n",
      "Processing Batch 8/157\n",
      "Batch 8 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01238030381500721, Std: 0.08752235770225525\n",
      "z_j Mean: 0.012173827737569809, Std: 0.08755131810903549\n",
      "Similarities: tensor([[1.3882, 0.0000, 0.0000, 0.0302, 0.0025],\n",
      "        [0.0000, 1.3641, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4286, 0.0000, 0.0000],\n",
      "        [0.1126, 0.0000, 0.0000, 1.4024, 0.1074],\n",
      "        [0.3665, 0.0038, 0.0000, 0.0021, 0.9987]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1625444889068604\n",
      "Batch 8, Loss: 3.1625444889068604\n",
      "Processing Batch 9/157\n",
      "Batch 9 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013162363320589066, Std: 0.0874081552028656\n",
      "z_j Mean: 0.0130060575902462, Std: 0.08743155002593994\n",
      "Similarities: tensor([[1.4250, 0.4176, 0.0000, 0.2743, 0.0000],\n",
      "        [0.3952, 1.4286, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3194, 0.0000, 0.0000],\n",
      "        [0.2349, 0.0000, 0.0000, 1.3770, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0270, 0.0000, 1.1965]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.121354579925537\n",
      "Batch 9, Loss: 3.121354579925537\n",
      "Processing Batch 10/157\n",
      "Batch 10 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012059303931891918, Std: 0.08686728030443192\n",
      "z_j Mean: 0.012029118835926056, Std: 0.08757131546735764\n",
      "Similarities: tensor([[1.4163, 0.0000, 0.0000, 0.0000, 0.5525],\n",
      "        [0.0000, 1.4219, 0.2650, 0.0000, 0.0000],\n",
      "        [0.0000, 0.5521, 1.3901, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4026, 0.0000],\n",
      "        [0.2831, 0.0000, 0.0000, 0.4522, 1.2012]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.128190755844116\n",
      "Batch 10, Loss: 3.128190755844116\n",
      "Processing Batch 11/157\n",
      "Batch 11 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013164818286895752, Std: 0.08740778267383575\n",
      "z_j Mean: 0.01296159252524376, Std: 0.08743815869092941\n",
      "Similarities: tensor([[1.3792, 0.0000, 0.1152, 0.0000, 0.3325],\n",
      "        [0.0000, 1.4059, 0.2432, 0.0000, 0.0000],\n",
      "        [0.0000, 0.4836, 1.3664, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4086, 0.4033],\n",
      "        [0.4366, 0.0000, 0.0593, 0.3036, 1.4019]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0922462940216064\n",
      "Batch 11, Loss: 3.0922462940216064\n",
      "Processing Batch 12/157\n",
      "Batch 12 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013026311062276363, Std: 0.08742853254079819\n",
      "z_j Mean: 0.013006840832531452, Std: 0.08743143081665039\n",
      "Similarities: tensor([[1.4213e+00, 7.0598e-01, 0.0000e+00, 8.5775e-01, 0.0000e+00],\n",
      "        [7.4894e-01, 1.4271e+00, 0.0000e+00, 1.5002e-01, 0.0000e+00],\n",
      "        [0.0000e+00, 4.9600e-04, 1.2480e+00, 0.0000e+00, 6.1563e-01],\n",
      "        [7.6866e-01, 1.4542e-01, 0.0000e+00, 1.4249e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 6.2621e-02, 0.0000e+00, 1.2698e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1625380516052246\n",
      "Batch 12, Loss: 3.1625380516052246\n",
      "Processing Batch 13/157\n",
      "Batch 13 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013333273120224476, Std: 0.0873822420835495\n",
      "z_j Mean: 0.013447370380163193, Std: 0.08736475557088852\n",
      "Similarities: tensor([[1.4224, 0.3082, 0.0000, 0.0000, 0.3224],\n",
      "        [0.0828, 1.3573, 0.0000, 0.0000, 0.1859],\n",
      "        [0.0000, 0.0000, 1.3557, 0.2605, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0930, 1.4014, 0.0000],\n",
      "        [0.1868, 0.0932, 0.0000, 0.0000, 1.4058]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.150115489959717\n",
      "Batch 13, Loss: 3.150115489959717\n",
      "Processing Batch 14/157\n",
      "Batch 14 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012715278193354607, Std: 0.08747431635856628\n",
      "z_j Mean: 0.013069786131381989, Std: 0.08742203563451767\n",
      "Similarities: tensor([[1.1770, 0.0000, 0.5241, 0.0042, 0.0000],\n",
      "        [0.0000, 1.4286, 0.0000, 0.0000, 0.5893],\n",
      "        [0.8439, 0.0000, 1.3072, 0.0033, 0.0601],\n",
      "        [0.1159, 0.0000, 0.0098, 1.4181, 0.0000],\n",
      "        [0.0000, 0.3447, 0.0000, 0.0000, 1.3072]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.152895927429199\n",
      "Batch 14, Loss: 3.152895927429199\n",
      "Processing Batch 15/157\n",
      "Batch 15 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01281860563904047, Std: 0.08745922893285751\n",
      "z_j Mean: 0.012902321293950081, Std: 0.0874469205737114\n",
      "Similarities: tensor([[1.4270, 0.0000, 0.5612, 0.0000, 0.0000],\n",
      "        [0.0097, 1.3895, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4114, 0.0000, 1.4028, 0.7568, 1.2182],\n",
      "        [0.0000, 0.0000, 0.9094, 1.3713, 1.1790],\n",
      "        [0.0000, 0.0000, 1.0476, 1.1874, 1.3779]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1842029094696045\n",
      "Batch 15, Loss: 3.1842029094696045\n",
      "Processing Batch 16/157\n",
      "Batch 16 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012703453190624714, Std: 0.08747602999210358\n",
      "z_j Mean: 0.012963391840457916, Std: 0.08743788301944733\n",
      "Similarities: tensor([[1.4177, 0.0000, 0.0412, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3334, 0.0000, 1.4159, 0.4892],\n",
      "        [0.0104, 0.0000, 1.4036, 0.0000, 0.0000],\n",
      "        [0.0000, 1.2192, 0.0000, 1.3793, 0.8163],\n",
      "        [0.0000, 0.5579, 0.0000, 0.6416, 1.3795]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.190239667892456\n",
      "Batch 16, Loss: 3.190239667892456\n",
      "Processing Batch 17/157\n",
      "Batch 17 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012574444524943829, Std: 0.08749467134475708\n",
      "z_j Mean: 0.012449368834495544, Std: 0.0875125601887703\n",
      "Similarities: tensor([[1.4229, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3863, 0.9339, 0.2810, 1.3298],\n",
      "        [0.0000, 0.8223, 1.3410, 0.7511, 1.1087],\n",
      "        [0.0000, 0.2000, 0.4691, 1.4153, 0.4104],\n",
      "        [0.0000, 1.0671, 1.0373, 0.4396, 1.3488]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2274856567382812\n",
      "Batch 17, Loss: 3.2274856567382812\n",
      "Processing Batch 18/157\n",
      "Batch 18 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013187327422201633, Std: 0.08740439265966415\n",
      "z_j Mean: 0.013085571117699146, Std: 0.08741968125104904\n",
      "Similarities: tensor([[1.4076, 0.8544, 0.0000, 0.0000, 0.0000],\n",
      "        [0.6772, 1.4223, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3893, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3702, 0.3899],\n",
      "        [0.0000, 0.0000, 0.0071, 0.4369, 1.4095]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1360621452331543\n",
      "Batch 18, Loss: 3.1360621452331543\n",
      "Processing Batch 19/157\n",
      "Batch 19 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013597245328128338, Std: 0.0873415544629097\n",
      "z_j Mean: 0.013768522068858147, Std: 0.08731471002101898\n",
      "Similarities: tensor([[1.4286, 0.0000, 0.6679, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3607, 0.0000, 0.0000, 0.2695],\n",
      "        [0.5687, 0.0000, 1.4125, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.2826, 0.0000],\n",
      "        [0.0000, 0.5984, 0.0000, 0.0000, 0.9577]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.108593702316284\n",
      "Batch 19, Loss: 3.108593702316284\n",
      "Processing Batch 20/157\n",
      "Batch 20 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013822915963828564, Std: 0.08730611205101013\n",
      "z_j Mean: 0.014038098976016045, Std: 0.08727177232503891\n",
      "Similarities: tensor([[1.2727, 0.0000, 0.0000, 0.0000, 0.0014],\n",
      "        [0.0785, 1.4179, 0.2291, 0.0000, 0.1116],\n",
      "        [0.0364, 0.3153, 1.4180, 0.0000, 1.3773],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3924, 0.0000],\n",
      "        [0.0727, 0.1324, 1.3007, 0.0000, 1.4094]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1298866271972656\n",
      "Batch 20, Loss: 3.1298866271972656\n",
      "Processing Batch 21/157\n",
      "Batch 21 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013798398897051811, Std: 0.08730998635292053\n",
      "z_j Mean: 0.013649603351950645, Std: 0.08733338117599487\n",
      "Similarities: tensor([[1.4286, 0.0000, 0.0000, 0.5910, 0.0000],\n",
      "        [0.0000, 1.1823, 0.0404, 0.2362, 1.3501],\n",
      "        [0.0000, 0.0000, 1.4273, 0.1610, 0.0000],\n",
      "        [0.3345, 0.0000, 0.0961, 1.3493, 0.0000],\n",
      "        [0.0000, 1.4006, 0.0000, 0.0000, 1.3576]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.128676652908325\n",
      "Batch 21, Loss: 3.128676652908325\n",
      "Processing Batch 22/157\n",
      "Batch 22 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.014236674644052982, Std: 0.0872395858168602\n",
      "z_j Mean: 0.014442052692174911, Std: 0.08720581978559494\n",
      "Similarities: tensor([[1.4079, 0.8145, 0.5655, 0.0415, 1.1158],\n",
      "        [0.6620, 1.0104, 0.4082, 0.1873, 0.2061],\n",
      "        [0.5347, 1.0541, 1.3201, 0.9618, 0.6540],\n",
      "        [0.0565, 0.1111, 1.1791, 1.4285, 0.0957],\n",
      "        [0.8895, 0.9163, 0.6090, 0.0939, 1.2442]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1062018871307373\n",
      "Batch 22, Loss: 3.1062018871307373\n",
      "Processing Batch 23/157\n",
      "Batch 23 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013375403359532356, Std: 0.08737579733133316\n",
      "z_j Mean: 0.013623857870697975, Std: 0.08733739703893661\n",
      "Similarities: tensor([[1.3979, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4054, 0.0000, 0.9612, 1.2486],\n",
      "        [0.0182, 0.0000, 1.4247, 0.0000, 0.0000],\n",
      "        [0.0000, 0.8676, 0.0000, 1.4259, 1.2183],\n",
      "        [0.0000, 1.2329, 0.0000, 1.1662, 1.4233]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1188948154449463\n",
      "Batch 23, Loss: 3.1188948154449463\n",
      "Processing Batch 24/157\n",
      "Batch 24 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013259867206215858, Std: 0.08739341050386429\n",
      "z_j Mean: 0.013262620195746422, Std: 0.08739299327135086\n",
      "Similarities: tensor([[1.4274, 0.0338, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4093, 0.0000, 0.0000, 0.0223],\n",
      "        [0.0000, 0.0000, 0.8881, 0.6958, 0.3848],\n",
      "        [0.0000, 0.0000, 1.0748, 1.4267, 0.0000],\n",
      "        [0.0000, 0.0000, 0.1798, 0.0000, 1.3641]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0944480895996094\n",
      "Batch 24, Loss: 3.0944480895996094\n",
      "Processing Batch 25/157\n",
      "Batch 25 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013451253063976765, Std: 0.08736415207386017\n",
      "z_j Mean: 0.013824820518493652, Std: 0.08730581402778625\n",
      "Similarities: tensor([[1.1414, 0.0000, 0.1554, 0.0000, 1.3988],\n",
      "        [0.0000, 1.3587, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5264, 0.0000, 1.3572, 0.0000, 0.1998],\n",
      "        [0.0000, 0.0000, 0.2096, 1.4286, 0.0000],\n",
      "        [1.3202, 0.0000, 0.2315, 0.0000, 1.3682]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0940706729888916\n",
      "Batch 25, Loss: 3.0940706729888916\n",
      "Processing Batch 26/157\n",
      "Batch 26 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012898892164230347, Std: 0.08744743466377258\n",
      "z_j Mean: 0.012870809063315392, Std: 0.08745156228542328\n",
      "Similarities: tensor([[1.4191, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.2866, 0.0653, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.1202, 0.0000, 0.2629],\n",
      "        [0.0000, 0.4287, 0.0000, 0.8499, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0166, 0.0000, 1.4214]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0840659141540527\n",
      "Batch 26, Loss: 3.0840659141540527\n",
      "Processing Batch 27/157\n",
      "Batch 27 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013229286298155785, Std: 0.08739804476499557\n",
      "z_j Mean: 0.013574553653597832, Std: 0.08734507858753204\n",
      "Similarities: tensor([[1.4215, 0.0000, 0.0000, 0.4087, 0.0000],\n",
      "        [0.0000, 1.4027, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4194, 0.0000, 0.7588],\n",
      "        [0.4201, 0.0000, 0.0000, 1.3821, 0.0000],\n",
      "        [0.0000, 0.0000, 0.6201, 0.0000, 1.4286]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1038975715637207\n",
      "Batch 27, Loss: 3.1038975715637207\n",
      "Processing Batch 28/157\n",
      "Batch 28 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01365560945123434, Std: 0.08733244240283966\n",
      "z_j Mean: 0.01379915326833725, Std: 0.08730987459421158\n",
      "Similarities: tensor([[1.3757, 0.1642, 0.0000, 0.0000, 0.8938],\n",
      "        [0.1610, 1.4171, 0.0000, 1.1195, 0.4177],\n",
      "        [0.0000, 0.0000, 1.4160, 0.1386, 0.0000],\n",
      "        [0.0000, 0.8865, 0.2756, 1.3966, 0.0000],\n",
      "        [0.9189, 0.6849, 0.0000, 0.1309, 1.3953]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.114004135131836\n",
      "Batch 28, Loss: 3.114004135131836\n",
      "Processing Batch 29/157\n",
      "Batch 29 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013048314489424229, Std: 0.08742524683475494\n",
      "z_j Mean: 0.013134347274899483, Std: 0.08671122044324875\n",
      "Similarities: tensor([[1.4182, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4089, 0.0000, 0.0000, 0.8992],\n",
      "        [0.0000, 0.0000, 1.4180, 0.1786, 0.0000],\n",
      "        [0.0000, 0.0000, 0.3457, 1.4071, 0.0000],\n",
      "        [0.0000, 0.9132, 0.0000, 0.0000, 1.3911]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.069798469543457\n",
      "Batch 29, Loss: 3.069798469543457\n",
      "Processing Batch 30/157\n",
      "Batch 30 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013136260211467743, Std: 0.08671092987060547\n",
      "z_j Mean: 0.013316717930138111, Std: 0.08668340742588043\n",
      "Similarities: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4101, 0.0000, 0.0000, 1.2799],\n",
      "        [0.0000, 0.0000, 1.4244, 0.7406, 0.0000],\n",
      "        [0.0000, 0.0000, 1.0082, 1.3559, 0.0000],\n",
      "        [0.0000, 1.2726, 0.0000, 0.0000, 1.4047]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.129242420196533\n",
      "Batch 30, Loss: 3.129242420196533\n",
      "Processing Batch 31/157\n",
      "Batch 31 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012340288609266281, Std: 0.08682779967784882\n",
      "z_j Mean: 0.012281865812838078, Std: 0.08753623068332672\n",
      "Similarities: tensor([[1.3716, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.2226, 0.0000, 0.0050, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3185, 0.0000, 0.7822],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4157, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.1062]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0964596271514893\n",
      "Batch 31, Loss: 3.0964596271514893\n",
      "Processing Batch 32/157\n",
      "Batch 32 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012788224965333939, Std: 0.08676294982433319\n",
      "z_j Mean: 0.012888319790363312, Std: 0.08744898438453674\n",
      "Similarities: tensor([[1.2643e+00, 8.0199e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [4.2536e-01, 1.3988e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 1.4286e+00, 0.0000e+00, 7.1140e-02],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 1.4116e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 1.1066e-02, 0.0000e+00, 1.3866e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.05584454536438\n",
      "Batch 32, Loss: 3.05584454536438\n",
      "Processing Batch 33/157\n",
      "Batch 33 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012243632227182388, Std: 0.08613570034503937\n",
      "z_j Mean: 0.012275617569684982, Std: 0.08613114058971405\n",
      "Similarities: tensor([[1.3235, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3662, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.2827, 0.0000, 0.6837],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3951, 0.0000],\n",
      "        [0.0000, 0.0000, 0.1632, 0.0000, 1.4201]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.129321336746216\n",
      "Batch 33, Loss: 3.129321336746216\n",
      "Processing Batch 34/157\n",
      "Batch 34 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012938297353684902, Std: 0.08744160085916519\n",
      "z_j Mean: 0.012660572305321693, Std: 0.08678167313337326\n",
      "Similarities: tensor([[1.4140, 1.2055, 0.1992, 0.0000, 0.0000],\n",
      "        [1.0180, 1.3572, 0.7545, 0.0000, 0.0000],\n",
      "        [0.1995, 0.3484, 1.3816, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3534, 1.3079],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0632, 1.3921]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.108989715576172\n",
      "Batch 34, Loss: 3.108989715576172\n",
      "Processing Batch 35/157\n",
      "Batch 35 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012118080630898476, Std: 0.08615345507860184\n",
      "z_j Mean: 0.012208212167024612, Std: 0.08542915433645248\n",
      "Similarities: tensor([[1.4226, 0.0000, 0.0000, 1.1383, 0.0000],\n",
      "        [0.0000, 1.3899, 0.3922, 0.0000, 0.6325],\n",
      "        [0.0000, 1.2570, 0.9855, 0.0000, 1.0124],\n",
      "        [0.7707, 0.0000, 0.0000, 1.2819, 0.0000],\n",
      "        [0.0000, 0.6228, 1.3821, 0.0000, 1.1473]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.152892589569092\n",
      "Batch 35, Loss: 3.152892589569092\n",
      "Processing Batch 36/157\n",
      "Batch 36 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011870840564370155, Std: 0.08618786931037903\n",
      "z_j Mean: 0.011860713362693787, Std: 0.08547808974981308\n",
      "Similarities: tensor([[1.3966, 0.0000, 0.0000, 0.4794, 0.0000],\n",
      "        [0.0000, 1.4247, 0.0227, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0180, 1.0065, 0.0000, 0.0000],\n",
      "        [0.3010, 0.0000, 0.7785, 0.0000, 1.1088],\n",
      "        [0.0894, 0.0000, 0.9852, 0.0000, 1.4033]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.108807325363159\n",
      "Batch 36, Loss: 3.108807325363159\n",
      "Processing Batch 37/157\n",
      "Batch 37 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012252558954060078, Std: 0.08754033595323563\n",
      "z_j Mean: 0.012028338387608528, Std: 0.08616603165864944\n",
      "Similarities: tensor([[1.4011, 0.0000, 0.9002, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4068, 0.0000, 0.0000, 0.0000],\n",
      "        [0.8024, 0.0000, 1.4286, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4154, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3348]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.103541374206543\n",
      "Batch 37, Loss: 3.103541374206543\n",
      "Processing Batch 38/157\n",
      "Batch 38 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01249338686466217, Std: 0.08538790047168732\n",
      "z_j Mean: 0.01224260963499546, Std: 0.08542422950267792\n",
      "Similarities: tensor([[1.4138, 0.0000, 0.0000, 0.0000, 1.0986],\n",
      "        [0.0000, 1.3826, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4282, 0.0000, 0.0000],\n",
      "        [0.0058, 0.0000, 0.0000, 1.3989, 0.0000],\n",
      "        [1.0509, 0.0000, 0.0000, 0.0000, 1.3931]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1179311275482178\n",
      "Batch 38, Loss: 3.1179311275482178\n",
      "Processing Batch 39/157\n",
      "Batch 39 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013535749167203903, Std: 0.08735109865665436\n",
      "z_j Mean: 0.013113537803292274, Std: 0.08600754290819168\n",
      "Similarities: tensor([[1.3717, 0.0000, 0.0000, 0.0592, 0.0000],\n",
      "        [0.0000, 1.2551, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3889, 0.0000, 0.5317],\n",
      "        [0.0977, 0.0000, 0.0000, 1.3027, 0.0000],\n",
      "        [0.0000, 0.0000, 0.2762, 0.0000, 1.2515]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1014206409454346\n",
      "Batch 39, Loss: 3.1014206409454346\n",
      "Processing Batch 40/157\n",
      "Batch 40 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011799300089478493, Std: 0.08476952463388443\n",
      "z_j Mean: 0.012103966437280178, Std: 0.08544398844242096\n",
      "Similarities: tensor([[1.2295, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4068, 0.0000, 0.6932, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.5873, 0.0000, 1.4203, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.144656181335449\n",
      "Batch 40, Loss: 3.144656181335449\n",
      "Processing Batch 41/157\n",
      "Batch 41 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012301196344196796, Std: 0.08612749725580215\n",
      "z_j Mean: 0.012053355574607849, Std: 0.08545113354921341\n",
      "Similarities: tensor([[1.3252, 0.0000, 0.6384, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4078, 0.0178, 0.0000, 1.0864],\n",
      "        [0.3593, 0.0102, 1.4232, 0.0000, 0.0679],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4255, 0.0000],\n",
      "        [0.0000, 0.9439, 0.0647, 0.0000, 1.4227]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1138830184936523\n",
      "Batch 41, Loss: 3.1138830184936523\n",
      "Processing Batch 42/157\n",
      "Batch 42 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011159307323396206, Std: 0.08413371443748474\n",
      "z_j Mean: 0.01090927142649889, Std: 0.08343809098005295\n",
      "Similarities: tensor([[1.4212, 0.0000, 0.0172, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4029, 0.6410, 0.0294, 0.0000],\n",
      "        [0.0168, 1.1376, 1.3223, 0.0173, 0.0000],\n",
      "        [0.0000, 0.7086, 0.2268, 1.1052, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.1729]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.2119534015655518\n",
      "Batch 42, Loss: 3.2119534015655518\n",
      "Processing Batch 43/157\n",
      "Batch 43 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011440832167863846, Std: 0.08624602109193802\n",
      "z_j Mean: 0.011242368258535862, Std: 0.08412265032529831\n",
      "Similarities: tensor([[1.4225, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4272, 0.0000, 0.7798, 0.0070],\n",
      "        [0.0000, 0.0000, 1.2982, 0.0000, 0.0000],\n",
      "        [0.0000, 1.0113, 0.0000, 1.3933, 0.3551],\n",
      "        [0.0000, 0.0000, 0.0000, 0.6563, 1.3605]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.169893503189087\n",
      "Batch 43, Loss: 3.169893503189087\n",
      "Processing Batch 44/157\n",
      "Batch 44 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012427985668182373, Std: 0.08681529015302658\n",
      "z_j Mean: 0.01272722240537405, Std: 0.0874725729227066\n",
      "Similarities: tensor([[1.3714, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3151, 0.2810, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2405, 1.3862, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4286, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.078352928161621\n",
      "Batch 44, Loss: 3.078352928161621\n",
      "Processing Batch 45/157\n",
      "Batch 45 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013076623901724815, Std: 0.08742102235555649\n",
      "z_j Mean: 0.012639910914003849, Std: 0.08607842773199081\n",
      "Similarities: tensor([[1.4165, 0.0000, 0.0000, 0.1391, 1.2484],\n",
      "        [0.0000, 1.4280, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4136, 0.0000],\n",
      "        [0.6354, 0.0000, 0.0000, 0.0186, 1.1257]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0935144424438477\n",
      "Batch 45, Loss: 3.0935144424438477\n",
      "Processing Batch 46/157\n",
      "Batch 46 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012302800081670284, Std: 0.08541557937860489\n",
      "z_j Mean: 0.01233040913939476, Std: 0.08612331748008728\n",
      "Similarities: tensor([[1.0599, 0.0176, 0.0000, 0.0621, 0.0000],\n",
      "        [0.0737, 1.4146, 0.0000, 1.3254, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4286, 0.0000, 0.0000],\n",
      "        [0.3338, 1.3732, 0.0000, 1.4148, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3367]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1229405403137207\n",
      "Batch 46, Loss: 3.1229405403137207\n",
      "Processing Batch 47/157\n",
      "Batch 47 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012212569825351238, Std: 0.08754592388868332\n",
      "z_j Mean: 0.012510628439486027, Std: 0.0875038206577301\n",
      "Similarities: tensor([[1.3246, 0.0000, 0.1171, 0.0000, 0.6841],\n",
      "        [0.0000, 1.4257, 0.1240, 0.0000, 0.0000],\n",
      "        [0.0136, 0.1618, 1.3958, 0.7708, 0.3512],\n",
      "        [0.0000, 0.0000, 0.6873, 1.3988, 0.7420],\n",
      "        [0.5616, 0.0000, 0.0000, 0.4136, 1.2632]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0747618675231934\n",
      "Batch 47, Loss: 3.0747618675231934\n",
      "Processing Batch 48/157\n",
      "Batch 48 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012527750805020332, Std: 0.0868009477853775\n",
      "z_j Mean: 0.012575081549584866, Std: 0.08679410815238953\n",
      "Similarities: tensor([[1.4255, 0.0000, 0.0000, 0.0000, 0.9153],\n",
      "        [0.0000, 1.4196, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4156, 0.8409, 0.6091],\n",
      "        [0.0000, 0.0000, 0.7441, 1.3511, 0.8306],\n",
      "        [0.3020, 0.0000, 0.8208, 1.3129, 1.2023]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285629987716675]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.110759973526001\n",
      "Batch 48, Loss: 3.110759973526001\n",
      "Processing Batch 49/157\n",
      "Batch 49 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.014619840309023857, Std: 0.08717618882656097\n",
      "z_j Mean: 0.014772999100387096, Std: 0.08715036511421204\n",
      "Similarities: tensor([[1.3609, 0.0039, 0.0068, 0.1524, 0.4086],\n",
      "        [0.0035, 1.4046, 0.0137, 0.0070, 0.0082],\n",
      "        [0.0064, 0.0145, 1.4281, 0.0128, 0.0150],\n",
      "        [0.2272, 0.0085, 0.0147, 1.4122, 1.1339],\n",
      "        [0.6228, 0.0078, 0.0135, 1.2984, 1.3532]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0013416212750598788, 1.4282917976379395]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1119253635406494\n",
      "Batch 49, Loss: 3.1119253635406494\n",
      "Processing Batch 50/157\n",
      "Batch 50 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013780657202005386, Std: 0.08731279522180557\n",
      "z_j Mean: 0.014076454564929008, Std: 0.08726558834314346\n",
      "Similarities: tensor([[1.3749, 0.0114, 0.0000, 0.0935, 0.0507],\n",
      "        [0.0067, 1.4236, 0.4375, 0.0368, 0.0094],\n",
      "        [0.0000, 0.4337, 1.4180, 0.0000, 0.0000],\n",
      "        [0.0478, 0.0323, 0.0000, 1.3359, 0.9458],\n",
      "        [0.0346, 0.0078, 0.0000, 1.2300, 1.4010]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4280998706817627]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1322762966156006\n",
      "Batch 50, Loss: 3.1322762966156006\n",
      "Processing Batch 51/157\n",
      "Batch 51 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013706552796065807, Std: 0.08732446283102036\n",
      "z_j Mean: 0.01371241919696331, Std: 0.08732353895902634\n",
      "Similarities: tensor([[1.3559e+00, 0.0000e+00, 3.8082e-01, 4.9457e-03, 4.4615e-03],\n",
      "        [0.0000e+00, 1.4232e+00, 4.3227e-02, 0.0000e+00, 0.0000e+00],\n",
      "        [7.2798e-02, 1.5316e-01, 1.2095e+00, 5.2541e-03, 1.7707e-03],\n",
      "        [3.8733e-03, 0.0000e+00, 1.1918e-03, 1.3634e+00, 3.6929e-01],\n",
      "        [5.3646e-02, 0.0000e+00, 3.8734e-04, 3.3272e-01, 1.4173e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4283400774002075]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.100351095199585\n",
      "Batch 51, Loss: 3.100351095199585\n",
      "Processing Batch 52/157\n",
      "Batch 52 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013304178602993488, Std: 0.08668532967567444\n",
      "z_j Mean: 0.013535418547689915, Std: 0.08664950728416443\n",
      "Similarities: tensor([[1.4124, 0.4848, 0.0000, 0.1302, 0.5665],\n",
      "        [1.0716, 1.2853, 0.0000, 0.0000, 0.2317],\n",
      "        [0.0000, 0.0000, 1.4276, 0.7106, 0.8383],\n",
      "        [0.0548, 0.0000, 0.7145, 1.4226, 1.1965],\n",
      "        [0.5709, 0.0160, 0.7241, 1.1427, 1.4124]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1272671222686768\n",
      "Batch 52, Loss: 3.1272671222686768\n",
      "Processing Batch 53/157\n",
      "Batch 53 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01308373361825943, Std: 0.08741995692253113\n",
      "z_j Mean: 0.013168204575777054, Std: 0.08740726858377457\n",
      "Similarities: tensor([[1.3724, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4033, 0.0000, 0.0000, 0.8373],\n",
      "        [0.0000, 0.0000, 1.1546, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.2867, 0.0000],\n",
      "        [0.0000, 0.5965, 0.0000, 0.0000, 1.2321]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1132843494415283\n",
      "Batch 53, Loss: 3.1132843494415283\n",
      "Processing Batch 54/157\n",
      "Batch 54 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013281118124723434, Std: 0.08739018440246582\n",
      "z_j Mean: 0.012951217591762543, Std: 0.08673876523971558\n",
      "Similarities: tensor([[1.4025, 0.0000, 0.9853, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4067, 0.0000, 0.1310, 0.7742],\n",
      "        [0.8780, 0.0000, 1.4226, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0714, 0.0000, 1.4264, 0.9421],\n",
      "        [0.0000, 0.7426, 0.0000, 0.8856, 1.4130]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0789597034454346\n",
      "Batch 54, Loss: 3.0789597034454346\n",
      "Processing Batch 55/157\n",
      "Batch 55 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013008181937038898, Std: 0.08673024922609329\n",
      "z_j Mean: 0.012849713675677776, Std: 0.0874546617269516\n",
      "Similarities: tensor([[1.2975, 0.0000, 0.9790, 0.0000, 0.8994],\n",
      "        [0.0000, 1.4032, 0.0000, 0.3296, 0.0000],\n",
      "        [0.5508, 0.0000, 1.4194, 0.0000, 0.7874],\n",
      "        [0.0000, 0.5140, 0.0000, 1.4132, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1416804790496826\n",
      "Batch 55, Loss: 3.1416804790496826\n",
      "Processing Batch 56/157\n",
      "Batch 56 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013437621295452118, Std: 0.0873662531375885\n",
      "z_j Mean: 0.013280952349305153, Std: 0.08739020675420761\n",
      "Similarities: tensor([[1.4259, 0.0000, 0.0000, 1.2504, 0.0000],\n",
      "        [0.0000, 1.4285, 0.2379, 0.1571, 0.0000],\n",
      "        [0.0000, 0.3665, 1.3912, 0.4548, 0.0000],\n",
      "        [1.0451, 0.3711, 0.5608, 1.3400, 0.0000],\n",
      "        [0.0784, 0.0000, 0.0000, 0.0403, 1.2823]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1310064792633057\n",
      "Batch 56, Loss: 3.1310064792633057\n",
      "Processing Batch 57/157\n",
      "Batch 57 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013377996161580086, Std: 0.08737540245056152\n",
      "z_j Mean: 0.013620253652334213, Std: 0.08733796328306198\n",
      "Similarities: tensor([[1.4220, 0.7250, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5922, 0.9604, 0.0000, 0.0000, 0.1932],\n",
      "        [0.0000, 0.0000, 1.4286, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4055, 0.0000],\n",
      "        [0.0000, 0.1734, 0.0000, 0.0000, 1.3227]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.112964630126953\n",
      "Batch 57, Loss: 3.112964630126953\n",
      "Processing Batch 58/157\n",
      "Batch 58 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013856451027095318, Std: 0.08730079978704453\n",
      "z_j Mean: 0.013743043877184391, Std: 0.08731872588396072\n",
      "Similarities: tensor([[1.3970, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3790, 0.7772, 0.0000, 0.3566],\n",
      "        [0.0000, 0.6330, 1.3935, 0.0315, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3884, 0.0000],\n",
      "        [0.0983, 0.4650, 0.0000, 0.0000, 1.3323]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1079800128936768\n",
      "Batch 58, Loss: 3.1079800128936768\n",
      "Processing Batch 59/157\n",
      "Batch 59 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012813406065106392, Std: 0.0874599888920784\n",
      "z_j Mean: 0.01281031221151352, Std: 0.08675968647003174\n",
      "Similarities: tensor([[0.9746, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.1842, 0.8722, 0.0000, 0.0000],\n",
      "        [0.0000, 0.7029, 1.3719, 0.3885, 0.0000],\n",
      "        [0.0759, 0.0907, 0.6230, 1.2249, 0.0000],\n",
      "        [0.9218, 0.0000, 0.0000, 0.0000, 0.0000]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1432151794433594\n",
      "Batch 59, Loss: 3.1432151794433594\n",
      "Processing Batch 60/157\n",
      "Batch 60 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012672798708081245, Std: 0.08748047798871994\n",
      "z_j Mean: 0.012154409661889076, Std: 0.08614833652973175\n",
      "Similarities: tensor([[1.3577e+00, 0.0000e+00, 0.0000e+00, 9.6363e-02, 3.2018e-01],\n",
      "        [0.0000e+00, 1.4286e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [2.6163e-02, 0.0000e+00, 0.0000e+00, 1.3818e+00, 9.3191e-05],\n",
      "        [4.1294e-01, 0.0000e+00, 0.0000e+00, 3.2503e-02, 1.3889e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1423511505126953\n",
      "Batch 60, Loss: 3.1423511505126953\n",
      "Processing Batch 61/157\n",
      "Batch 61 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012547912076115608, Std: 0.08749847859144211\n",
      "z_j Mean: 0.01284322515130043, Std: 0.087455615401268\n",
      "Similarities: tensor([[1.4085, 0.0383, 0.0000, 0.7053, 0.0000],\n",
      "        [0.0000, 1.3082, 0.7502, 0.0000, 0.0000],\n",
      "        [0.0171, 1.1180, 1.4118, 0.0384, 0.0000],\n",
      "        [0.6381, 0.0618, 0.0000, 1.4181, 0.7095],\n",
      "        [0.0000, 0.4239, 0.1086, 0.5602, 1.1943]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0890095233917236\n",
      "Batch 61, Loss: 3.0890095233917236\n",
      "Processing Batch 62/157\n",
      "Batch 62 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012644987553358078, Std: 0.08748450130224228\n",
      "z_j Mean: 0.012514540925621986, Std: 0.08680285513401031\n",
      "Similarities: tensor([[1.4043, 0.0000, 0.0000, 0.0000, 0.3615],\n",
      "        [0.0000, 1.4123, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4010, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.1881, 1.4174, 0.0000],\n",
      "        [0.5093, 0.0000, 0.0000, 0.0000, 1.2634]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1002490520477295\n",
      "Batch 62, Loss: 3.1002490520477295\n",
      "Processing Batch 63/157\n",
      "Batch 63 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01247129961848259, Std: 0.08750943839550018\n",
      "z_j Mean: 0.012545093894004822, Std: 0.08749888837337494\n",
      "Similarities: tensor([[1.4285, 0.1920, 0.0071, 0.0000, 0.0000],\n",
      "        [0.0648, 1.4224, 0.0000, 0.0000, 0.0582],\n",
      "        [0.0000, 0.0000, 1.2958, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4216, 0.0000],\n",
      "        [0.0000, 0.0472, 0.0000, 0.0000, 1.4278]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0588364601135254\n",
      "Batch 63, Loss: 3.0588364601135254\n",
      "Processing Batch 64/157\n",
      "Batch 64 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012561222538352013, Std: 0.0874965712428093\n",
      "z_j Mean: 0.012726977467536926, Std: 0.08747261762619019\n",
      "Similarities: tensor([[1.2301, 0.1257, 0.0000, 0.0000, 0.6743],\n",
      "        [0.0000, 1.2748, 0.0000, 0.2129, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4210, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.1550, 0.0000],\n",
      "        [0.5410, 0.1867, 0.0000, 0.0000, 1.3686]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0716302394866943\n",
      "Batch 64, Loss: 3.0716302394866943\n",
      "Processing Batch 65/157\n",
      "Batch 65 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012169159017503262, Std: 0.08755196630954742\n",
      "z_j Mean: 0.011870542541146278, Std: 0.0875929594039917\n",
      "Similarities: tensor([[1.4032, 0.7188, 0.0000, 0.0000, 0.0000],\n",
      "        [0.8343, 1.3810, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.2262, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4286, 0.9729],\n",
      "        [0.0000, 0.0000, 0.0000, 0.4059, 1.2699]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0640177726745605\n",
      "Batch 65, Loss: 3.0640177726745605\n",
      "Processing Batch 66/157\n",
      "Batch 66 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011916782706975937, Std: 0.0868869498372078\n",
      "z_j Mean: 0.012047266587615013, Std: 0.08686894923448563\n",
      "Similarities: tensor([[1.3281, 0.2133, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3170, 1.3101, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4282, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3578, 0.0000],\n",
      "        [0.0104, 0.0000, 0.0000, 0.0000, 1.0164]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1113011837005615\n",
      "Batch 66, Loss: 3.1113011837005615\n",
      "Processing Batch 67/157\n",
      "Batch 67 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01122648548334837, Std: 0.08697884529829025\n",
      "z_j Mean: 0.011222314089536667, Std: 0.08627472817897797\n",
      "Similarities: tensor([[1.4271e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [1.1560e-03, 1.4283e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 1.3968e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 1.0567e-01, 1.3742e+00, 5.6029e-01],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 8.3532e-01, 1.4272e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1253507137298584\n",
      "Batch 67, Loss: 3.1253507137298584\n",
      "Processing Batch 68/157\n",
      "Batch 68 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012641966342926025, Std: 0.0874849408864975\n",
      "z_j Mean: 0.013046300038695335, Std: 0.08742555230855942\n",
      "Similarities: tensor([[1.4092, 1.2547, 0.0073, 0.0000, 0.0000],\n",
      "        [1.0746, 1.3915, 0.0048, 0.0000, 0.0318],\n",
      "        [0.0000, 0.0111, 1.3289, 0.1345, 0.2235],\n",
      "        [0.0000, 0.0000, 0.0105, 1.4176, 1.1476],\n",
      "        [0.0000, 0.0367, 0.1285, 0.9414, 1.3787]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.090794563293457\n",
      "Batch 68, Loss: 3.090794563293457\n",
      "Processing Batch 69/157\n",
      "Batch 69 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011582259088754654, Std: 0.08763155341148376\n",
      "z_j Mean: 0.011308454908430576, Std: 0.0876673087477684\n",
      "Similarities: tensor([[1.2520, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4195, 0.0000, 0.7402, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4233, 0.0000, 0.0000],\n",
      "        [0.0000, 0.7362, 0.0000, 1.3907, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4222]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0877599716186523\n",
      "Batch 69, Loss: 3.0877599716186523\n",
      "Processing Batch 70/157\n",
      "Batch 70 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012333723716437817, Std: 0.08752893656492233\n",
      "z_j Mean: 0.012048414908349514, Std: 0.08756866306066513\n",
      "Similarities: tensor([[1.4267, 0.0000, 0.0810, 0.1015, 0.0000],\n",
      "        [0.0000, 1.2586, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0194, 0.0000, 1.3203, 0.0000, 0.0000],\n",
      "        [0.4569, 0.3404, 0.0000, 0.8335, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4186]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.048771858215332\n",
      "Batch 70, Loss: 3.048771858215332\n",
      "Processing Batch 71/157\n",
      "Batch 71 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011979237198829651, Std: 0.08757816255092621\n",
      "z_j Mean: 0.011409789323806763, Std: 0.0869549885392189\n",
      "Similarities: tensor([[1.4218, 0.1287, 0.6798, 0.0000, 1.0325],\n",
      "        [0.1443, 0.6767, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5117, 0.0000, 0.0000, 0.0000, 1.1104],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3815, 0.0000],\n",
      "        [1.0102, 0.0000, 0.3420, 0.0000, 1.4100]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1527388095855713\n",
      "Batch 71, Loss: 3.1527388095855713\n",
      "Processing Batch 72/157\n",
      "Batch 72 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011542873457074165, Std: 0.08763673901557922\n",
      "z_j Mean: 0.012082510627806187, Std: 0.08756396919488907\n",
      "Similarities: tensor([[1.4138, 0.0000, 0.5992, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4032, 0.0000, 0.0260, 0.0000],\n",
      "        [0.4719, 0.0000, 1.4286, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4267, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.2918]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.077324867248535\n",
      "Batch 72, Loss: 3.077324867248535\n",
      "Processing Batch 73/157\n",
      "Batch 73 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011529453098773956, Std: 0.08693920075893402\n",
      "z_j Mean: 0.01154464203864336, Std: 0.08693718165159225\n",
      "Similarities: tensor([[1.3974, 0.0000, 0.0000, 0.2733, 0.0000],\n",
      "        [0.0000, 1.4286, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.0698, 0.0000, 1.1246],\n",
      "        [0.5104, 0.0000, 0.0000, 1.3982, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3295, 0.0000, 1.2505]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.109600305557251\n",
      "Batch 73, Loss: 3.109600305557251\n",
      "Processing Batch 74/157\n",
      "Batch 74 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012306524440646172, Std: 0.08753276616334915\n",
      "z_j Mean: 0.01208893209695816, Std: 0.08615755289793015\n",
      "Similarities: tensor([[1.4209, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4185, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4058, 0.7980, 0.0000],\n",
      "        [0.0000, 0.0000, 0.8879, 1.3644, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4064]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1113626956939697\n",
      "Batch 74, Loss: 3.1113626956939697\n",
      "Processing Batch 75/157\n",
      "Batch 75 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012244457378983498, Std: 0.08684136718511581\n",
      "z_j Mean: 0.012366963550448418, Std: 0.08682400733232498\n",
      "Similarities: tensor([[1.0047, 0.1491, 0.0000, 0.7709, 0.0000],\n",
      "        [0.0000, 1.3820, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4160, 0.0000, 0.0000],\n",
      "        [0.8593, 0.0000, 0.0000, 1.2727, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4148]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1765575408935547\n",
      "Batch 75, Loss: 3.1765575408935547\n",
      "Processing Batch 76/157\n",
      "Batch 76 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012557792477309704, Std: 0.08537846058607101\n",
      "z_j Mean: 0.012611563317477703, Std: 0.08537052571773529\n",
      "Similarities: tensor([[1.4091, 0.0000, 0.0124, 0.0000, 0.2350],\n",
      "        [0.0000, 1.4286, 0.3444, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2853, 1.4246, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4077, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4050]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1522812843322754\n",
      "Batch 76, Loss: 3.1522812843322754\n",
      "Processing Batch 77/157\n",
      "Batch 77 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01223963126540184, Std: 0.08754214644432068\n",
      "z_j Mean: 0.012228047475218773, Std: 0.08754375576972961\n",
      "Similarities: tensor([[1.4007, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4198, 0.9512, 0.0000, 0.0000],\n",
      "        [0.0000, 0.8567, 1.4013, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.2820, 0.8532],\n",
      "        [0.0000, 0.0000, 0.0000, 0.3726, 1.4169]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.087886333465576\n",
      "Batch 77, Loss: 3.087886333465576\n",
      "Processing Batch 78/157\n",
      "Batch 78 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012894188985228539, Std: 0.08674726635217667\n",
      "z_j Mean: 0.013100012205541134, Std: 0.0867164134979248\n",
      "Similarities: tensor([[1.4221, 0.0000, 0.0000, 0.0465, 0.0000],\n",
      "        [0.0000, 1.3866, 0.0000, 0.1681, 0.4028],\n",
      "        [0.0000, 0.0000, 1.2566, 0.0000, 0.0000],\n",
      "        [0.1530, 0.0061, 0.0000, 1.3181, 0.0000],\n",
      "        [0.0000, 0.2651, 0.0000, 0.0000, 1.4082]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0958893299102783\n",
      "Batch 78, Loss: 3.0958893299102783\n",
      "Processing Batch 79/157\n",
      "Batch 79 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012261132709681988, Std: 0.08753913640975952\n",
      "z_j Mean: 0.012268345803022385, Std: 0.086837999522686\n",
      "Similarities: tensor([[1.4240, 0.0000, 0.0000, 1.0463, 0.0000],\n",
      "        [0.0000, 1.3292, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4275, 0.0000, 0.0000],\n",
      "        [1.2392, 0.0000, 0.0000, 1.4084, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3659]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0789244174957275\n",
      "Batch 79, Loss: 3.0789244174957275\n",
      "Processing Batch 80/157\n",
      "Batch 80 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013021519407629967, Std: 0.08672823756933212\n",
      "z_j Mean: 0.013089604675769806, Std: 0.08741907775402069\n",
      "Similarities: tensor([[1.4193, 0.0000, 1.2954, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [1.2836, 0.6097, 1.4285, 0.0737, 0.0000],\n",
      "        [0.0000, 0.2820, 0.1189, 0.9688, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3929]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.07645583152771\n",
      "Batch 80, Loss: 3.07645583152771\n",
      "Processing Batch 81/157\n",
      "Batch 81 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012542637065052986, Std: 0.08679880201816559\n",
      "z_j Mean: 0.012893131002783775, Std: 0.08744827657938004\n",
      "Similarities: tensor([[1.3362, 0.0000, 0.0000, 1.0284, 0.0000],\n",
      "        [0.0000, 1.4016, 0.0000, 0.0000, 0.9046],\n",
      "        [0.0000, 0.0000, 1.3157, 0.0000, 0.0000],\n",
      "        [1.2575, 0.0000, 0.0000, 1.4145, 0.1117],\n",
      "        [0.0000, 1.1277, 0.0000, 0.1024, 1.3899]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.121605157852173\n",
      "Batch 81, Loss: 3.121605157852173\n",
      "Processing Batch 82/157\n",
      "Batch 82 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012978486716747284, Std: 0.08743564039468765\n",
      "z_j Mean: 0.012835470959544182, Std: 0.08745675534009933\n",
      "Similarities: tensor([[1.0013, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3796, 0.0000, 0.2825, 0.0000],\n",
      "        [0.0000, 0.0000, 1.1265, 0.3333, 0.0000],\n",
      "        [0.0000, 0.2387, 0.0806, 1.4178, 0.1722],\n",
      "        [0.0000, 0.0000, 0.0000, 0.2857, 1.4072]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.061650514602661\n",
      "Batch 82, Loss: 3.061650514602661\n",
      "Processing Batch 83/157\n",
      "Batch 83 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012927388772368431, Std: 0.08674231916666031\n",
      "z_j Mean: 0.013198266737163067, Std: 0.08740273863077164\n",
      "Similarities: tensor([[1.1236, 0.0295, 0.0000, 0.2515, 0.3283],\n",
      "        [0.0000, 1.4245, 0.0000, 0.0000, 0.2362],\n",
      "        [0.0000, 0.0000, 1.4014, 0.0000, 0.0000],\n",
      "        [0.4641, 0.0000, 0.0027, 1.2066, 0.4441],\n",
      "        [0.6711, 0.3397, 0.0000, 0.2875, 1.3878]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.095545530319214\n",
      "Batch 83, Loss: 3.095545530319214\n",
      "Processing Batch 84/157\n",
      "Batch 84 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012106215581297874, Std: 0.08686075359582901\n",
      "z_j Mean: 0.012182140722870827, Std: 0.0868501365184784\n",
      "Similarities: tensor([[1.3586, 0.0000, 0.0000, 0.3447, 0.0000],\n",
      "        [0.0000, 1.3882, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4150, 1.3006, 0.0000],\n",
      "        [0.2126, 0.0000, 1.3780, 1.3896, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4096]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0940027236938477\n",
      "Batch 84, Loss: 3.0940027236938477\n",
      "Processing Batch 85/157\n",
      "Batch 85 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012128205969929695, Std: 0.08685768395662308\n",
      "z_j Mean: 0.011820057407021523, Std: 0.08690015971660614\n",
      "Similarities: tensor([[1.4122e+00, 1.1789e-02, 0.0000e+00, 1.8101e-01, 0.0000e+00],\n",
      "        [4.8215e-02, 1.3288e+00, 0.0000e+00, 0.0000e+00, 3.3802e-02],\n",
      "        [0.0000e+00, 0.0000e+00, 1.3490e+00, 0.0000e+00, 5.7251e-01],\n",
      "        [1.7443e-01, 0.0000e+00, 0.0000e+00, 1.3769e+00, 0.0000e+00],\n",
      "        [2.7807e-04, 2.0043e-03, 3.2950e-01, 0.0000e+00, 1.3013e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.08833646774292\n",
      "Batch 85, Loss: 3.08833646774292\n",
      "Processing Batch 86/157\n",
      "Batch 86 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013134271837770939, Std: 0.08741237223148346\n",
      "z_j Mean: 0.013073615729808807, Std: 0.0874214693903923\n",
      "Similarities: tensor([[1.3776, 0.0000, 0.0000, 0.0000, 1.1029],\n",
      "        [0.0591, 1.2846, 0.0812, 0.8002, 0.0000],\n",
      "        [0.0000, 0.3459, 1.2722, 0.4395, 0.0000],\n",
      "        [0.0000, 0.8262, 0.2678, 1.2537, 0.0000],\n",
      "        [1.2670, 0.0000, 0.0000, 0.0000, 1.4016]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.063568592071533\n",
      "Batch 86, Loss: 3.063568592071533\n",
      "Processing Batch 87/157\n",
      "Batch 87 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012824508361518383, Std: 0.08745837211608887\n",
      "z_j Mean: 0.01258562970906496, Std: 0.08749306946992874\n",
      "Similarities: tensor([[1.2721, 0.0000, 0.0000, 0.3108, 0.0000],\n",
      "        [0.0000, 1.4219, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0267, 0.0000, 1.0611, 0.0468, 0.0000],\n",
      "        [0.4255, 0.0000, 0.0000, 1.2118, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.1674, 1.3315]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.049736738204956\n",
      "Batch 87, Loss: 3.049736738204956\n",
      "Processing Batch 88/157\n",
      "Batch 88 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012780164368450642, Std: 0.08746486157178879\n",
      "z_j Mean: 0.012704807333648205, Std: 0.08747583627700806\n",
      "Similarities: tensor([[1.3718, 0.0000, 0.0000, 1.3591, 0.0000],\n",
      "        [0.0000, 1.3891, 0.3827, 0.0000, 0.1560],\n",
      "        [0.0000, 0.5986, 1.4036, 0.0000, 0.0000],\n",
      "        [0.3768, 0.1723, 0.0719, 0.4161, 0.0000],\n",
      "        [0.0000, 0.0961, 0.0000, 0.0000, 1.4205]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1532845497131348\n",
      "Batch 88, Loss: 3.1532845497131348\n",
      "Processing Batch 89/157\n",
      "Batch 89 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011928047053515911, Std: 0.08688540011644363\n",
      "z_j Mean: 0.011809557676315308, Std: 0.08619628846645355\n",
      "Similarities: tensor([[1.3837, 0.0000, 0.0042, 0.0000, 1.1913],\n",
      "        [0.0000, 1.4110, 0.2298, 0.0000, 0.0025],\n",
      "        [0.0000, 0.4909, 1.3970, 0.0221, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0239, 1.3453, 0.0000],\n",
      "        [0.9787, 0.1198, 0.0000, 0.0000, 1.3790]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1179096698760986\n",
      "Batch 89, Loss: 3.1179096698760986\n",
      "Processing Batch 90/157\n",
      "Batch 90 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012685578316450119, Std: 0.0874786227941513\n",
      "z_j Mean: 0.012619415298104286, Std: 0.08748819679021835\n",
      "Similarities: tensor([[1.4201, 0.0000, 0.8189, 0.0000, 0.8220],\n",
      "        [0.0000, 1.4240, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.8271, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4104, 0.0000],\n",
      "        [1.0767, 0.0000, 0.0000, 0.0000, 1.2667]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0978050231933594\n",
      "Batch 90, Loss: 3.0978050231933594\n",
      "Processing Batch 91/157\n",
      "Batch 91 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.010883037000894547, Std: 0.08631818741559982\n",
      "z_j Mean: 0.011167519725859165, Std: 0.08698643743991852\n",
      "Similarities: tensor([[1.4273, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4069, 0.0000, 1.3094, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4286, 0.0000, 0.0000],\n",
      "        [0.0000, 1.2969, 0.0000, 1.4286, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.1478]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.120612382888794\n",
      "Batch 91, Loss: 3.120612382888794\n",
      "Processing Batch 92/157\n",
      "Batch 92 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011393660679459572, Std: 0.08765627443790436\n",
      "z_j Mean: 0.011258816346526146, Std: 0.08626996725797653\n",
      "Similarities: tensor([[1.4271, 1.2821, 0.0000, 0.0000, 0.0000],\n",
      "        [1.0820, 1.3357, 0.0000, 0.2862, 0.0000],\n",
      "        [0.0000, 0.0000, 1.1627, 0.7961, 0.0000],\n",
      "        [0.0000, 0.1338, 0.4645, 1.4123, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4097]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.132709264755249\n",
      "Batch 92, Loss: 3.132709264755249\n",
      "Processing Batch 93/157\n",
      "Batch 93 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011521166190505028, Std: 0.08763960748910904\n",
      "z_j Mean: 0.011220836080610752, Std: 0.08697957545518875\n",
      "Similarities: tensor([[1.4286, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3902, 0.3178, 0.9299, 0.0000],\n",
      "        [0.0000, 0.0675, 1.4286, 1.2641, 0.0000],\n",
      "        [0.0000, 0.4023, 1.2811, 1.3269, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3928]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.156026601791382\n",
      "Batch 93, Loss: 3.156026601791382\n",
      "Processing Batch 94/157\n",
      "Batch 94 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011759497225284576, Std: 0.08760794252157211\n",
      "z_j Mean: 0.01151218730956316, Std: 0.08694148063659668\n",
      "Similarities: tensor([[1.4286, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4014, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4286, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3062, 1.2944],\n",
      "        [0.0000, 0.0000, 0.0000, 1.2555, 1.4001]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0595970153808594\n",
      "Batch 94, Loss: 3.0595970153808594\n",
      "Processing Batch 95/157\n",
      "Batch 95 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01156069990247488, Std: 0.08693504333496094\n",
      "z_j Mean: 0.011821707710623741, Std: 0.08689993619918823\n",
      "Similarities: tensor([[1.4129, 0.5014, 0.0000, 0.0000, 1.3787],\n",
      "        [0.6613, 1.4034, 0.0000, 0.0000, 0.7129],\n",
      "        [0.0000, 0.0000, 1.4270, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4281, 0.0000],\n",
      "        [1.4193, 0.6722, 0.0000, 0.0000, 1.4247]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0780982971191406\n",
      "Batch 95, Loss: 3.0780982971191406\n",
      "Processing Batch 96/157\n",
      "Batch 96 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012067068368196487, Std: 0.08756610006093979\n",
      "z_j Mean: 0.011863773688673973, Std: 0.08759388327598572\n",
      "Similarities: tensor([[1.4171, 0.0000, 0.0000, 0.5247, 0.0000],\n",
      "        [0.0000, 1.4251, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4095, 0.0000, 1.2488],\n",
      "        [0.6798, 0.0000, 0.0000, 1.4136, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3620, 0.0000, 1.2493]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.076014757156372\n",
      "Batch 96, Loss: 3.076014757156372\n",
      "Processing Batch 97/157\n",
      "Batch 97 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011631357483565807, Std: 0.08762504905462265\n",
      "z_j Mean: 0.011542019434273243, Std: 0.08693752437829971\n",
      "Similarities: tensor([[1.3584, 0.0000, 0.0765, 0.1887, 0.0000],\n",
      "        [0.0000, 1.4181, 0.4562, 0.0000, 0.0000],\n",
      "        [0.3946, 0.0000, 1.3012, 0.0000, 0.0000],\n",
      "        [0.3044, 0.0000, 0.0000, 1.3628, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4075]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0985257625579834\n",
      "Batch 97, Loss: 3.0985257625579834\n",
      "Processing Batch 98/157\n",
      "Batch 98 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011634059250354767, Std: 0.087624691426754\n",
      "z_j Mean: 0.011353570967912674, Std: 0.08696234226226807\n",
      "Similarities: tensor([[1.1867, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3577, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4271, 0.0019, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0054, 1.4285, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4204]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0978047847747803\n",
      "Batch 98, Loss: 3.0978047847747803\n",
      "Processing Batch 99/157\n",
      "Batch 99 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011720177717506886, Std: 0.08761321008205414\n",
      "z_j Mean: 0.011869313195347786, Std: 0.08759312331676483\n",
      "Similarities: tensor([[1.4196, 0.0000, 1.1952, 1.0647, 0.0000],\n",
      "        [0.0000, 1.3996, 0.0000, 0.0000, 0.0000],\n",
      "        [0.8543, 0.0000, 1.3662, 1.1295, 0.0000],\n",
      "        [0.8498, 0.0000, 1.1017, 1.4092, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.1979]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.102882146835327\n",
      "Batch 99, Loss: 3.102882146835327\n",
      "Processing Batch 100/157\n",
      "Batch 100 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01242163497954607, Std: 0.08751650154590607\n",
      "z_j Mean: 0.012633394449949265, Std: 0.08748617768287659\n",
      "Similarities: tensor([[1.4032, 0.0512, 0.0000, 1.0751, 0.9153],\n",
      "        [0.0000, 1.3627, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4195, 0.0000, 0.0000],\n",
      "        [0.9000, 0.0000, 0.0000, 1.4079, 1.3195],\n",
      "        [0.9731, 0.0000, 0.0000, 1.0682, 1.1489]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0840790271759033\n",
      "Batch 100, Loss: 3.0840790271759033\n",
      "Processing Batch 101/157\n",
      "Batch 101 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011943316087126732, Std: 0.08758306503295898\n",
      "z_j Mean: 0.01232847198843956, Std: 0.08752968162298203\n",
      "Similarities: tensor([[1.1828, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.2301, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1517, 1.3496, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.1523, 0.0000],\n",
      "        [0.4004, 0.0000, 0.0000, 0.0000, 1.4066]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1086840629577637\n",
      "Batch 101, Loss: 3.1086840629577637\n",
      "Processing Batch 102/157\n",
      "Batch 102 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011891261674463749, Std: 0.08759015053510666\n",
      "z_j Mean: 0.011892301961779594, Std: 0.08759000897407532\n",
      "Similarities: tensor([[1.4286, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4143, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4286, 1.1455, 0.0000],\n",
      "        [0.0000, 0.0000, 1.1423, 1.4128, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4102]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1212620735168457\n",
      "Batch 102, Loss: 3.1212620735168457\n",
      "Processing Batch 103/157\n",
      "Batch 103 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012364957481622696, Std: 0.08752452582120895\n",
      "z_j Mean: 0.012897681444883347, Std: 0.08744760602712631\n",
      "Similarities: tensor([[1.4237, 0.0000, 1.3459, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3804, 0.0000, 0.0000, 1.2414],\n",
      "        [1.3886, 0.0000, 1.3880, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0334, 0.0000, 1.4141, 0.0000],\n",
      "        [0.0000, 1.1459, 0.0000, 0.0000, 1.3735]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.084853410720825\n",
      "Batch 103, Loss: 3.084853410720825\n",
      "Processing Batch 104/157\n",
      "Batch 104 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01214347593486309, Std: 0.08755553513765335\n",
      "z_j Mean: 0.012323584407567978, Std: 0.08683017641305923\n",
      "Similarities: tensor([[1.3976, 0.0027, 0.6049, 0.4375, 0.0040],\n",
      "        [0.0190, 1.4033, 0.0000, 0.1696, 1.3364],\n",
      "        [0.3965, 0.0000, 1.3733, 0.0000, 0.0000],\n",
      "        [0.4352, 0.0999, 0.0000, 1.4000, 0.1502],\n",
      "        [0.0024, 1.3949, 0.0000, 0.0213, 1.4007]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.118283987045288\n",
      "Batch 104, Loss: 3.118283987045288\n",
      "Processing Batch 105/157\n",
      "Batch 105 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01283390261232853, Std: 0.08745698630809784\n",
      "z_j Mean: 0.013091979548335075, Std: 0.08741872012615204\n",
      "Similarities: tensor([[1.4178, 0.0000, 0.3911, 0.1607, 0.0959],\n",
      "        [0.0000, 1.4107, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4187, 0.0000, 1.4037, 0.0000, 0.0000],\n",
      "        [0.1494, 0.0000, 0.0000, 1.3339, 0.9801],\n",
      "        [0.0769, 0.0000, 0.0000, 0.7636, 1.4225]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0993874073028564\n",
      "Batch 105, Loss: 3.0993874073028564\n",
      "Processing Batch 106/157\n",
      "Batch 106 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01229513343423605, Std: 0.08753436803817749\n",
      "z_j Mean: 0.012623443268239498, Std: 0.0874876156449318\n",
      "Similarities: tensor([[1.4197, 0.0000, 0.0358, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4169, 0.0000, 0.0000, 0.4414],\n",
      "        [0.0000, 0.0000, 1.3600, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.2590, 0.0000],\n",
      "        [0.0000, 0.6173, 0.0000, 0.0000, 1.3363]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.07177734375\n",
      "Batch 106, Loss: 3.07177734375\n",
      "Processing Batch 107/157\n",
      "Batch 107 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012402892112731934, Std: 0.08751915395259857\n",
      "z_j Mean: 0.012724742293357849, Std: 0.08747293800115585\n",
      "Similarities: tensor([[1.4027, 1.0276, 0.1714, 0.0000, 0.4725],\n",
      "        [0.6061, 1.3716, 0.3493, 0.0000, 1.3680],\n",
      "        [0.1387, 0.4153, 1.4112, 0.0000, 0.3722],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4265, 0.0000],\n",
      "        [0.1220, 1.0721, 0.2945, 0.0000, 1.3826]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.077855348587036\n",
      "Batch 107, Loss: 3.077855348587036\n",
      "Processing Batch 108/157\n",
      "Batch 108 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01264684833586216, Std: 0.08748423308134079\n",
      "z_j Mean: 0.012359743937849998, Std: 0.08752526342868805\n",
      "Similarities: tensor([[1.3081, 1.3514, 0.4047, 0.0000, 0.0000],\n",
      "        [1.3511, 1.3240, 0.3976, 0.0000, 0.0000],\n",
      "        [0.3223, 0.4078, 1.3949, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4240, 0.9274],\n",
      "        [0.0000, 0.0000, 0.0000, 0.6718, 1.3465]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0671207904815674\n",
      "Batch 108, Loss: 3.0671207904815674\n",
      "Processing Batch 109/157\n",
      "Batch 109 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012591112405061722, Std: 0.08749227970838547\n",
      "z_j Mean: 0.01231144554913044, Std: 0.08753207325935364\n",
      "Similarities: tensor([[0.8707, 0.5842, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4242, 1.3486, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3991, 0.0327, 1.0346],\n",
      "        [0.0000, 0.0000, 0.1186, 1.3995, 0.0000],\n",
      "        [0.0000, 0.0000, 0.9348, 0.0000, 1.3771]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0633010864257812\n",
      "Batch 109, Loss: 3.0633010864257812\n",
      "Processing Batch 110/157\n",
      "Batch 110 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01252983883023262, Std: 0.08750107884407043\n",
      "z_j Mean: 0.012256763875484467, Std: 0.08753974735736847\n",
      "Similarities: tensor([[1.3585e+00, 2.5951e-01, 0.0000e+00, 1.1147e-01, 0.0000e+00],\n",
      "        [4.0856e-01, 1.3419e+00, 0.0000e+00, 3.9539e-01, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 1.4168e+00, 0.0000e+00, 1.3460e+00],\n",
      "        [1.3224e-03, 2.2506e-01, 0.0000e+00, 1.3837e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 1.3410e+00, 0.0000e+00, 1.4078e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1334333419799805\n",
      "Batch 110, Loss: 3.1334333419799805\n",
      "Processing Batch 111/157\n",
      "Batch 111 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01265050657093525, Std: 0.08748370409011841\n",
      "z_j Mean: 0.012517166323959827, Std: 0.08750288188457489\n",
      "Similarities: tensor([[1.3302, 0.0000, 0.0000, 0.8177, 0.6852],\n",
      "        [0.0000, 1.3801, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4280, 0.9271, 0.0000],\n",
      "        [0.7580, 0.0000, 1.1542, 1.3893, 0.0000],\n",
      "        [0.1920, 0.0000, 0.0000, 0.0000, 1.2423]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0882434844970703\n",
      "Batch 111, Loss: 3.0882434844970703\n",
      "Processing Batch 112/157\n",
      "Batch 112 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012404677458107471, Std: 0.08751890808343887\n",
      "z_j Mean: 0.012796049937605858, Std: 0.08746253699064255\n",
      "Similarities: tensor([[1.4097, 0.0000, 0.2273, 0.5542, 0.0000],\n",
      "        [0.0000, 1.4213, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0435, 0.0000, 1.4221, 1.0056, 0.0000],\n",
      "        [0.2061, 0.0000, 1.1085, 1.4227, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4182]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.070802927017212\n",
      "Batch 112, Loss: 3.070802927017212\n",
      "Processing Batch 113/157\n",
      "Batch 113 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012671323493123055, Std: 0.08748069405555725\n",
      "z_j Mean: 0.01242670975625515, Std: 0.08751577883958817\n",
      "Similarities: tensor([[1.4050e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.4050e+00],\n",
      "        [0.0000e+00, 1.4026e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 7.7733e-04, 1.4221e+00, 1.4047e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 1.9775e-03, 1.3514e+00, 1.4016e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1289758682250977\n",
      "Batch 113, Loss: 3.1289758682250977\n",
      "Processing Batch 114/157\n",
      "Batch 114 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01234027836471796, Std: 0.08612190186977386\n",
      "z_j Mean: 0.012740787118673325, Std: 0.08606355637311935\n",
      "Similarities: tensor([[1.3934, 0.0000, 0.4767, 0.1714, 0.0000],\n",
      "        [0.0000, 1.4170, 0.0000, 0.0000, 0.9681],\n",
      "        [0.7825, 0.0000, 1.2531, 0.0000, 0.0666],\n",
      "        [0.4347, 0.0000, 0.0000, 1.3415, 0.0000],\n",
      "        [0.0000, 0.8023, 0.1086, 0.0000, 1.4103]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0930287837982178\n",
      "Batch 114, Loss: 3.0930287837982178\n",
      "Processing Batch 115/157\n",
      "Batch 115 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012678947299718857, Std: 0.08747958391904831\n",
      "z_j Mean: 0.012716377153992653, Std: 0.08747415244579315\n",
      "Similarities: tensor([[1.3704, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4199, 0.0424, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0087, 1.4016, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.2799, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3225]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.098250389099121\n",
      "Batch 115, Loss: 3.098250389099121\n",
      "Processing Batch 116/157\n",
      "Batch 116 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012576359324157238, Std: 0.087494395673275\n",
      "z_j Mean: 0.012708714231848717, Std: 0.08747527003288269\n",
      "Similarities: tensor([[1.4234, 0.0000, 0.0000, 0.0862, 0.5608],\n",
      "        [0.0000, 1.4187, 0.0000, 0.6072, 0.5580],\n",
      "        [0.0000, 0.0000, 1.4255, 0.0000, 0.0000],\n",
      "        [0.0000, 0.4529, 0.0000, 1.4254, 1.3097],\n",
      "        [0.5195, 0.4183, 0.0000, 1.3523, 1.4276]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0791850090026855\n",
      "Batch 116, Loss: 3.0791850090026855\n",
      "Processing Batch 117/157\n",
      "Batch 117 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012368746101856232, Std: 0.08752399682998657\n",
      "z_j Mean: 0.012435831129550934, Std: 0.0875144824385643\n",
      "Similarities: tensor([[1.2869, 0.0000, 0.0000, 0.0000, 0.9692],\n",
      "        [0.0000, 1.4153, 0.0000, 0.1161, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3599, 0.0000, 0.0000],\n",
      "        [0.0225, 0.0288, 0.0000, 1.3907, 0.2435],\n",
      "        [0.6344, 0.0000, 0.0000, 0.0594, 1.4003]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0718376636505127\n",
      "Batch 117, Loss: 3.0718376636505127\n",
      "Processing Batch 118/157\n",
      "Batch 118 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013346342369914055, Std: 0.08738025277853012\n",
      "z_j Mean: 0.013491456396877766, Std: 0.08735796064138412\n",
      "Similarities: tensor([[1.2901, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0962, 1.4249, 0.7428, 0.0000, 0.0000],\n",
      "        [0.2867, 0.3696, 1.2691, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.1782, 0.5661],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0599, 1.4078]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0629870891571045\n",
      "Batch 118, Loss: 3.0629870891571045\n",
      "Processing Batch 119/157\n",
      "Batch 119 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011920146644115448, Std: 0.08758621662855148\n",
      "z_j Mean: 0.01185806654393673, Std: 0.0875946506857872\n",
      "Similarities: tensor([[1.3998, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4223, 0.5849, 0.5743, 0.0000],\n",
      "        [0.0000, 0.7546, 1.2523, 0.1852, 0.4634],\n",
      "        [0.0000, 0.5539, 0.2078, 1.4089, 0.0000],\n",
      "        [0.0000, 0.0000, 0.8979, 0.0000, 1.1348]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.061706066131592\n",
      "Batch 119, Loss: 3.061706066131592\n",
      "Processing Batch 120/157\n",
      "Batch 120 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012397712096571922, Std: 0.08751989901065826\n",
      "z_j Mean: 0.012410180643200874, Std: 0.08681783825159073\n",
      "Similarities: tensor([[1.3872, 0.1561, 0.0000, 1.0732, 0.1944],\n",
      "        [0.1710, 1.4043, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4286, 0.0000, 0.0000],\n",
      "        [1.2315, 0.0000, 0.0000, 1.4265, 0.5424],\n",
      "        [0.0000, 0.0000, 0.0000, 0.3563, 1.4073]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0758328437805176\n",
      "Batch 120, Loss: 3.0758328437805176\n",
      "Processing Batch 121/157\n",
      "Batch 121 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012408165261149406, Std: 0.08751840889453888\n",
      "z_j Mean: 0.01256219670176506, Std: 0.08749643713235855\n",
      "Similarities: tensor([[1.3835, 0.0000, 0.0000, 0.8507, 0.0509],\n",
      "        [0.0000, 1.4112, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4285, 0.0000, 0.0000],\n",
      "        [0.7813, 0.0000, 0.0000, 1.3752, 0.6440],\n",
      "        [0.1040, 0.0000, 0.0000, 0.9453, 1.3984]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0323071479797363\n",
      "Batch 121, Loss: 3.0323071479797363\n",
      "Processing Batch 122/157\n",
      "Batch 122 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011990339495241642, Std: 0.08757664263248444\n",
      "z_j Mean: 0.011828133836388588, Std: 0.0868990570306778\n",
      "Similarities: tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.2999e+00],\n",
      "        [0.0000e+00, 1.3753e+00, 0.0000e+00, 2.3890e-01, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 1.4224e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 9.7419e-04, 1.4146e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.2283e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0777807235717773\n",
      "Batch 122, Loss: 3.0777807235717773\n",
      "Processing Batch 123/157\n",
      "Batch 123 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012300604954361916, Std: 0.08753360062837601\n",
      "z_j Mean: 0.012208615429699421, Std: 0.08684641867876053\n",
      "Similarities: tensor([[1.4277, 0.0000, 0.1684, 0.0000, 0.0000],\n",
      "        [0.0034, 1.1823, 0.6518, 0.0000, 0.0000],\n",
      "        [0.2391, 0.0000, 0.9576, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3691, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3356]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.130131483078003\n",
      "Batch 123, Loss: 3.130131483078003\n",
      "Processing Batch 124/157\n",
      "Batch 124 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012568894773721695, Std: 0.08749546855688095\n",
      "z_j Mean: 0.012437599711120129, Std: 0.0875142365694046\n",
      "Similarities: tensor([[1.4258, 0.0000, 0.0070, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4286, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0184, 0.0000, 1.4067, 0.8457, 0.9988],\n",
      "        [0.0000, 0.0000, 0.3684, 1.2463, 0.1108],\n",
      "        [0.0000, 0.0000, 1.1912, 0.5651, 1.3885]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.039961814880371\n",
      "Batch 124, Loss: 3.039961814880371\n",
      "Processing Batch 125/157\n",
      "Batch 125 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01237381249666214, Std: 0.08752328157424927\n",
      "z_j Mean: 0.012179110199213028, Std: 0.087550587952137\n",
      "Similarities: tensor([[1.4246, 0.1996, 0.3750, 0.0000, 1.3856],\n",
      "        [0.3068, 1.2277, 1.0310, 0.0000, 0.4110],\n",
      "        [0.3334, 0.5964, 1.4127, 0.0000, 0.4468],\n",
      "        [0.0000, 0.0000, 0.0619, 1.4286, 0.0000],\n",
      "        [1.3922, 0.2184, 0.4103, 0.0000, 1.4272]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.094111204147339\n",
      "Batch 125, Loss: 3.094111204147339\n",
      "Processing Batch 126/157\n",
      "Batch 126 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011704799719154835, Std: 0.0869157612323761\n",
      "z_j Mean: 0.012215390801429749, Std: 0.08684546500444412\n",
      "Similarities: tensor([[1.4283, 0.1352, 0.6069, 0.0000, 0.2109],\n",
      "        [0.1053, 1.3957, 0.0000, 0.0000, 0.4732],\n",
      "        [0.6360, 0.1469, 1.2967, 0.0000, 0.2300],\n",
      "        [0.0000, 0.0000, 0.0038, 1.4284, 0.0000],\n",
      "        [0.2600, 0.7491, 0.0168, 0.0000, 1.3809]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.067894220352173\n",
      "Batch 126, Loss: 3.067894220352173\n",
      "Processing Batch 127/157\n",
      "Batch 127 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012304354459047318, Std: 0.08683290332555771\n",
      "z_j Mean: 0.012501036748290062, Std: 0.08680479973554611\n",
      "Similarities: tensor([[1.3986, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4286, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4030, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4286]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.108976125717163\n",
      "Batch 127, Loss: 3.108976125717163\n",
      "Processing Batch 128/157\n",
      "Batch 128 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012032680213451385, Std: 0.08757083117961884\n",
      "z_j Mean: 0.011826487258076668, Std: 0.0868992879986763\n",
      "Similarities: tensor([[1.4248, 0.0000, 0.9676, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4233, 0.0000, 0.0071, 0.0000],\n",
      "        [0.9975, 0.0000, 1.4253, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4160, 0.1326],\n",
      "        [0.0000, 0.0000, 0.0000, 0.1609, 1.4034]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1054396629333496\n",
      "Batch 128, Loss: 3.1054396629333496\n",
      "Processing Batch 129/157\n",
      "Batch 129 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012403794564306736, Std: 0.08751903474330902\n",
      "z_j Mean: 0.012813219800591469, Std: 0.08746001869440079\n",
      "Similarities: tensor([[1.3467, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.2753, 0.0000, 0.0562, 0.3376],\n",
      "        [0.0000, 0.0000, 1.4120, 1.0073, 0.0000],\n",
      "        [0.0000, 0.0000, 0.8433, 1.2352, 0.0000],\n",
      "        [0.0000, 0.6512, 0.0000, 0.0000, 1.4128]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.080634832382202\n",
      "Batch 129, Loss: 3.080634832382202\n",
      "Processing Batch 130/157\n",
      "Batch 130 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013228319585323334, Std: 0.08669693022966385\n",
      "z_j Mean: 0.01292036660015583, Std: 0.08674336224794388\n",
      "Similarities: tensor([[0.8583, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4209, 0.0000, 0.0000, 0.0000],\n",
      "        [0.9019, 0.0000, 0.6644, 0.8647, 0.0000],\n",
      "        [0.0000, 0.0000, 0.1161, 1.3182, 0.4632],\n",
      "        [0.0000, 0.0000, 0.0438, 0.0832, 1.3302]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0904011726379395\n",
      "Batch 130, Loss: 3.0904011726379395\n",
      "Processing Batch 131/157\n",
      "Batch 131 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011898723430931568, Std: 0.08547280728816986\n",
      "z_j Mean: 0.012076733633875847, Std: 0.08615925908088684\n",
      "Similarities: tensor([[1.3860, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.9736, 0.0000, 0.6628, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3959, 0.0000, 0.7132],\n",
      "        [0.0000, 0.5257, 0.0000, 1.3309, 0.0000],\n",
      "        [0.0000, 0.0000, 1.0312, 0.0000, 1.2362]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0998737812042236\n",
      "Batch 131, Loss: 3.0998737812042236\n",
      "Processing Batch 132/157\n",
      "Batch 132 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01171059999614954, Std: 0.08549878746271133\n",
      "z_j Mean: 0.011534903198480606, Std: 0.08552267402410507\n",
      "Similarities: tensor([[1.4166, 0.0000, 0.0863, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0889, 0.0000, 1.3618, 0.0418, 0.0539],\n",
      "        [0.0000, 0.0000, 0.1630, 1.3380, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4111]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1876351833343506\n",
      "Batch 132, Loss: 3.1876351833343506\n",
      "Processing Batch 133/157\n",
      "Batch 133 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012344117276370525, Std: 0.08682725578546524\n",
      "z_j Mean: 0.012478518299758434, Std: 0.08680804073810577\n",
      "Similarities: tensor([[1.4108, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3872, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4261, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4255, 0.0000],\n",
      "        [0.0000, 0.0510, 0.0000, 0.0000, 1.3370]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.114973306655884\n",
      "Batch 133, Loss: 3.114973306655884\n",
      "Processing Batch 134/157\n",
      "Batch 134 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011785060167312622, Std: 0.08760450780391693\n",
      "z_j Mean: 0.011635811999440193, Std: 0.08692502230405807\n",
      "Similarities: tensor([[1.3853, 0.1576, 0.0000, 0.0952, 0.0000],\n",
      "        [0.5440, 1.4278, 0.0000, 0.8625, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4019, 0.0000, 0.0000],\n",
      "        [0.7495, 0.3771, 0.0000, 1.1859, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.2094, 1.4037]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.142221450805664\n",
      "Batch 134, Loss: 3.142221450805664\n",
      "Processing Batch 135/157\n",
      "Batch 135 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012794576585292816, Std: 0.08746274560689926\n",
      "z_j Mean: 0.013020110316574574, Std: 0.0874294638633728\n",
      "Similarities: tensor([[1.1864, 0.0000, 0.0000, 0.0000, 0.4290],\n",
      "        [0.0000, 1.4010, 0.0000, 1.3962, 0.0000],\n",
      "        [0.0000, 0.0625, 1.4033, 0.0336, 0.0000],\n",
      "        [0.0000, 1.4254, 0.0481, 1.4274, 0.0000],\n",
      "        [0.6699, 0.0000, 0.0000, 0.0000, 1.1673]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.082228422164917\n",
      "Batch 135, Loss: 3.082228422164917\n",
      "Processing Batch 136/157\n",
      "Batch 136 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01261047087609768, Std: 0.08748949319124222\n",
      "z_j Mean: 0.012485504150390625, Std: 0.08750741183757782\n",
      "Similarities: tensor([[1.3871, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3345, 0.8447, 0.0000, 0.0000],\n",
      "        [0.0000, 0.8228, 1.0376, 0.0000, 0.5175],\n",
      "        [0.0000, 0.0421, 0.1726, 1.3007, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4057]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0451061725616455\n",
      "Batch 136, Loss: 3.0451061725616455\n",
      "Processing Batch 137/157\n",
      "Batch 137 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012668903917074203, Std: 0.0874810442328453\n",
      "z_j Mean: 0.012476431205868721, Std: 0.08750870823860168\n",
      "Similarities: tensor([[1.4119e+00, 0.0000e+00, 6.0798e-04, 2.4750e-01, 1.1087e+00],\n",
      "        [0.0000e+00, 1.3971e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 9.5515e-01, 9.6508e-01, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 2.3221e-02, 1.3427e+00, 0.0000e+00],\n",
      "        [1.2542e+00, 0.0000e+00, 0.0000e+00, 1.9053e-01, 1.4008e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0939836502075195\n",
      "Batch 137, Loss: 3.0939836502075195\n",
      "Processing Batch 138/157\n",
      "Batch 138 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012237343937158585, Std: 0.08754245936870575\n",
      "z_j Mean: 0.012326065450906754, Std: 0.08753001689910889\n",
      "Similarities: tensor([[1.0316, 0.7185, 0.8979, 0.5593, 0.0000],\n",
      "        [0.4948, 0.1814, 0.4202, 0.0000, 0.0000],\n",
      "        [0.1964, 0.5114, 1.4187, 0.0000, 0.0000],\n",
      "        [0.6423, 0.9030, 0.0000, 1.4286, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.2616]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.095674991607666\n",
      "Batch 138, Loss: 3.095674991607666\n",
      "Processing Batch 139/157\n",
      "Batch 139 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013323100283741951, Std: 0.08738379180431366\n",
      "z_j Mean: 0.013441511429846287, Std: 0.08736565709114075\n",
      "Similarities: tensor([[1.4093, 0.0796, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1256, 1.3005, 1.0547, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3056, 1.4267, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.2516, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3098]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0532870292663574\n",
      "Batch 139, Loss: 3.0532870292663574\n",
      "Processing Batch 140/157\n",
      "Batch 140 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012678822502493858, Std: 0.0874796062707901\n",
      "z_j Mean: 0.012963796965777874, Std: 0.08673689514398575\n",
      "Similarities: tensor([[1.3988, 0.7618, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5986, 1.4245, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4217, 0.0000, 0.0000],\n",
      "        [0.0173, 0.0677, 0.0000, 1.4106, 0.0364],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.6894]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0598666667938232\n",
      "Batch 140, Loss: 3.0598666667938232\n",
      "Processing Batch 141/157\n",
      "Batch 141 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01267351396381855, Std: 0.08748037368059158\n",
      "z_j Mean: 0.012722335755825043, Std: 0.08747328817844391\n",
      "Similarities: tensor([[1.3181e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.1317e+00],\n",
      "        [0.0000e+00, 1.4082e+00, 1.2552e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 1.2343e+00, 1.2361e+00, 9.3178e-04, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 9.7480e-01, 0.0000e+00],\n",
      "        [7.3371e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.4062e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0808191299438477\n",
      "Batch 141, Loss: 3.0808191299438477\n",
      "Processing Batch 142/157\n",
      "Batch 142 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013336289674043655, Std: 0.08738178759813309\n",
      "z_j Mean: 0.013420458883047104, Std: 0.08736889064311981\n",
      "Similarities: tensor([[1.4221, 0.0000, 0.0000, 0.5304, 0.0000],\n",
      "        [0.0000, 1.3965, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4286, 0.0000, 0.0000],\n",
      "        [0.5921, 0.0000, 0.0000, 1.4197, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4285]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.428554892539978]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.044343948364258\n",
      "Batch 142, Loss: 3.044343948364258\n",
      "Processing Batch 143/157\n",
      "Batch 143 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012515353038907051, Std: 0.08750314265489578\n",
      "z_j Mean: 0.012110693380236626, Std: 0.08686012774705887\n",
      "Similarities: tensor([[1.4272, 0.1999, 0.0000, 0.0000, 0.1999],\n",
      "        [0.2614, 1.4286, 0.0000, 0.0000, 1.4286],\n",
      "        [0.0000, 0.0000, 1.3761, 0.1222, 0.0000],\n",
      "        [0.0000, 0.0000, 0.5266, 1.0761, 0.0000],\n",
      "        [0.2614, 1.4286, 0.0000, 0.0000, 1.4286]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.083631992340088\n",
      "Batch 143, Loss: 3.083631992340088\n",
      "Processing Batch 144/157\n",
      "Batch 144 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012622552923858166, Std: 0.08748774230480194\n",
      "z_j Mean: 0.012212531641125679, Std: 0.08754593133926392\n",
      "Similarities: tensor([[1.4132, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3237, 0.0000, 0.0000, 0.1429],\n",
      "        [0.0039, 0.0000, 1.2220, 0.0000, 0.5568],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3977, 0.0000],\n",
      "        [0.0021, 0.1970, 0.3397, 0.0000, 1.3612]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1007068157196045\n",
      "Batch 144, Loss: 3.1007068157196045\n",
      "Processing Batch 145/157\n",
      "Batch 145 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013055584393441677, Std: 0.08742416650056839\n",
      "z_j Mean: 0.012585011310875416, Std: 0.08749315142631531\n",
      "Similarities: tensor([[1.3774, 0.2611, 0.7230, 0.0000, 0.0000],\n",
      "        [0.3296, 1.2580, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5952, 0.0000, 1.3977, 0.0000, 0.5256],\n",
      "        [0.0000, 0.0000, 0.0000, 0.9796, 0.0000],\n",
      "        [0.0220, 0.0000, 0.2719, 0.0000, 1.2508]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.092466115951538\n",
      "Batch 145, Loss: 3.092466115951538\n",
      "Processing Batch 146/157\n",
      "Batch 146 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013053962029516697, Std: 0.08672336488962173\n",
      "z_j Mean: 0.012855978682637215, Std: 0.08604642748832703\n",
      "Similarities: tensor([[1.3190, 0.1983, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1687, 1.4070, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2421, 0.0000, 1.2504, 0.6766, 0.0000],\n",
      "        [0.0000, 0.0000, 0.9591, 1.4276, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3371]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.116389274597168\n",
      "Batch 146, Loss: 3.116389274597168\n",
      "Processing Batch 147/157\n",
      "Batch 147 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012191067449748516, Std: 0.08614315092563629\n",
      "z_j Mean: 0.012385379523038864, Std: 0.08540364354848862\n",
      "Similarities: tensor([[1.0582, 0.3711, 0.0000, 0.0000, 0.0000],\n",
      "        [0.8382, 1.3008, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.2961, 0.0000, 0.5963],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3807, 0.0000],\n",
      "        [0.0000, 0.0000, 0.8048, 0.0000, 1.4263]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1013665199279785\n",
      "Batch 147, Loss: 3.1013665199279785\n",
      "Processing Batch 148/157\n",
      "Batch 148 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012020111083984375, Std: 0.08757255971431732\n",
      "z_j Mean: 0.012828003615140915, Std: 0.08745785057544708\n",
      "Similarities: tensor([[1.4036e+00, 0.0000e+00, 6.1291e-02, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 1.4283e+00, 5.0007e-03, 1.0916e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 9.2696e-04, 1.3372e+00, 2.0989e-02, 0.0000e+00],\n",
      "        [0.0000e+00, 1.1910e+00, 0.0000e+00, 1.4112e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.3080e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.081010580062866\n",
      "Batch 148, Loss: 3.081010580062866\n",
      "Processing Batch 149/157\n",
      "Batch 149 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012454263865947723, Std: 0.08751186728477478\n",
      "z_j Mean: 0.0119972825050354, Std: 0.08757568895816803\n",
      "Similarities: tensor([[1.4095, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4256, 0.0000, 0.0000, 0.0557],\n",
      "        [0.0000, 0.0000, 0.8660, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3335, 0.0000],\n",
      "        [0.0000, 0.0125, 0.0000, 0.0000, 1.4242]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.09734845161438\n",
      "Batch 149, Loss: 3.09734845161438\n",
      "Processing Batch 150/157\n",
      "Batch 150 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01186138391494751, Std: 0.08689452707767487\n",
      "z_j Mean: 0.012065865099430084, Std: 0.08756626397371292\n",
      "Similarities: tensor([[1.4160, 0.7466, 0.0000, 0.0000, 0.0000],\n",
      "        [0.8026, 1.3697, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4138, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0805, 0.0000, 1.4098, 0.9171],\n",
      "        [0.0000, 0.0241, 0.0000, 0.9957, 1.3892]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1044533252716064\n",
      "Batch 150, Loss: 3.1044533252716064\n",
      "Processing Batch 151/157\n",
      "Batch 151 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012475527822971344, Std: 0.08750883489847183\n",
      "z_j Mean: 0.012222345918416977, Std: 0.08754456043243408\n",
      "Similarities: tensor([[1.1189, 0.0000, 0.0000, 1.0692, 0.5238],\n",
      "        [0.0000, 1.3944, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.5387, 1.2620, 0.0000, 0.0000],\n",
      "        [1.0638, 0.0000, 0.0000, 1.4281, 0.7907],\n",
      "        [0.5913, 0.0000, 0.0000, 1.0803, 1.3718]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.06840181350708\n",
      "Batch 151, Loss: 3.06840181350708\n",
      "Processing Batch 152/157\n",
      "Batch 152 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01230749860405922, Std: 0.0868324562907219\n",
      "z_j Mean: 0.012185391038656235, Std: 0.08684967458248138\n",
      "Similarities: tensor([[1.4263, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3709, 0.0000, 0.0000, 1.1620],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.0121, 0.0000, 0.0000, 1.3942]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.116304636001587\n",
      "Batch 152, Loss: 3.116304636001587\n",
      "Processing Batch 153/157\n",
      "Batch 153 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012140635401010513, Std: 0.08755593001842499\n",
      "z_j Mean: 0.011895989999175072, Std: 0.08688979595899582\n",
      "Similarities: tensor([[1.0841, 0.3236, 0.0000, 0.0417, 0.0000],\n",
      "        [0.3167, 1.3128, 0.0000, 0.4162, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4269, 0.0254, 0.0000],\n",
      "        [0.0000, 0.3350, 0.0000, 1.2779, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4205]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1104023456573486\n",
      "Batch 153, Loss: 3.1104023456573486\n",
      "Processing Batch 154/157\n",
      "Batch 154 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01197061501443386, Std: 0.08757933974266052\n",
      "z_j Mean: 0.011800216510891914, Std: 0.08690285682678223\n",
      "Similarities: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.1208, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.2083, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4199, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4230]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1048424243927\n",
      "Batch 154, Loss: 3.1048424243927\n",
      "Processing Batch 155/157\n",
      "Batch 155 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011569822207093239, Std: 0.08693382889032364\n",
      "z_j Mean: 0.012002924457192421, Std: 0.08757491409778595\n",
      "Similarities: tensor([[1.4286, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3152, 0.0000, 0.0000, 0.6296],\n",
      "        [0.0000, 0.0000, 1.4145, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4279, 0.0000],\n",
      "        [0.0000, 0.4873, 0.0000, 0.0000, 1.3742]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0599794387817383\n",
      "Batch 155, Loss: 3.0599794387817383\n",
      "Processing Batch 156/157\n",
      "Batch 156 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011549971997737885, Std: 0.08693647384643555\n",
      "z_j Mean: 0.011792624369263649, Std: 0.0869038850069046\n",
      "Similarities: tensor([[1.3766, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4275, 0.0028, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.0920, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4286, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.1008]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0800812244415283\n",
      "Batch 156, Loss: 3.0800812244415283\n",
      "Processing Batch 157/157\n",
      "Batch 157 loaded: x.shape=torch.Size([16, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([16, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([16, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([16, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([16, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([16, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.0120773296803236, Std: 0.08758073300123215\n",
      "z_j Mean: 0.012517929077148438, Std: 0.08751880377531052\n",
      "Similarities: tensor([[0.7088, 0.2948, 0.2359, 0.0000, 0.0000],\n",
      "        [0.0000, 1.1478, 0.0000, 0.0000, 0.5484],\n",
      "        [0.3142, 0.0000, 1.2857, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4239, 0.5341],\n",
      "        [0.0000, 0.2923, 0.0000, 0.3883, 1.4258]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285084009170532]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 1.8507068157196045\n",
      "Batch 157, Loss: 1.8507068157196045\n",
      "Epoch 5/10, Loss: 3.0967\n",
      "Entered Epoch 6\n",
      "Starting Training Loop with 157 batches.\n",
      "Processing Batch 1/157\n",
      "Batch 1 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011629872024059296, Std: 0.08762524276971817\n",
      "z_j Mean: 0.011087669059634209, Std: 0.08629213273525238\n",
      "Similarities: tensor([[1.0069, 0.0000, 0.0000, 0.3972, 0.0000],\n",
      "        [0.0000, 1.2713, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4286, 1.2508, 0.0000],\n",
      "        [0.3945, 0.0000, 1.3018, 1.4240, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.1843]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.114389419555664\n",
      "Batch 1, Loss: 3.114389419555664\n",
      "Processing Batch 2/157\n",
      "Batch 2 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011960741132497787, Std: 0.08688090741634369\n",
      "z_j Mean: 0.011875059455633163, Std: 0.08689265698194504\n",
      "Similarities: tensor([[1.3737, 0.0000, 1.1937, 0.0000, 0.0000],\n",
      "        [0.0000, 0.9388, 0.0000, 0.0000, 0.0000],\n",
      "        [1.0085, 0.0000, 1.4266, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3938, 0.0036],\n",
      "        [0.0000, 0.3793, 0.0000, 0.5506, 1.3409]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1158998012542725\n",
      "Batch 2, Loss: 3.1158998012542725\n",
      "Processing Batch 3/157\n",
      "Batch 3 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013132264837622643, Std: 0.08741267770528793\n",
      "z_j Mean: 0.013379158452153206, Std: 0.0866737812757492\n",
      "Similarities: tensor([[0.9796, 0.0000, 0.3436, 0.0000, 0.7809],\n",
      "        [0.0000, 1.3883, 0.0000, 1.0123, 0.0000],\n",
      "        [0.0000, 0.0000, 1.1307, 0.0000, 0.1940],\n",
      "        [0.0000, 0.6084, 0.0000, 1.3837, 0.0000],\n",
      "        [0.1193, 0.0000, 0.7012, 0.0000, 1.3458]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1146085262298584\n",
      "Batch 3, Loss: 3.1146085262298584\n",
      "Processing Batch 4/157\n",
      "Batch 4 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011682139709591866, Std: 0.08691880851984024\n",
      "z_j Mean: 0.012194201350212097, Std: 0.08754847943782806\n",
      "Similarities: tensor([[0.3777, 0.0000, 0.0000, 1.3791, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4286, 0.0000, 0.0000],\n",
      "        [0.3730, 0.0000, 0.0000, 1.4246, 0.0000],\n",
      "        [0.0486, 0.0000, 0.0000, 0.0000, 1.4040]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.169168710708618\n",
      "Batch 4, Loss: 3.169168710708618\n",
      "Processing Batch 5/157\n",
      "Batch 5 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012117093428969383, Std: 0.08755918592214584\n",
      "z_j Mean: 0.012103196233510971, Std: 0.08756111562252045\n",
      "Similarities: tensor([[1.4207, 0.0000, 0.6123, 0.0000, 0.4107],\n",
      "        [0.0000, 1.4143, 0.0000, 0.2691, 0.0000],\n",
      "        [0.6184, 0.0000, 1.4093, 0.0000, 1.3199],\n",
      "        [0.0000, 0.2301, 0.0000, 1.3302, 0.0000],\n",
      "        [0.4270, 0.0000, 1.3460, 0.0000, 1.4267]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0424466133117676\n",
      "Batch 5, Loss: 3.0424466133117676\n",
      "Processing Batch 6/157\n",
      "Batch 6 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012863624840974808, Std: 0.08745262771844864\n",
      "z_j Mean: 0.012718377634882927, Std: 0.08747386187314987\n",
      "Similarities: tensor([[1.0260, 0.0726, 0.0464, 0.4925, 1.0810],\n",
      "        [0.6223, 1.4268, 0.7134, 0.0000, 0.0107],\n",
      "        [0.3764, 0.7518, 1.3887, 0.0000, 0.0065],\n",
      "        [0.5890, 0.0000, 0.0000, 1.3087, 0.1812],\n",
      "        [0.2481, 0.0631, 0.0403, 0.1030, 1.4010]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1077423095703125\n",
      "Batch 6, Loss: 3.1077423095703125\n",
      "Processing Batch 7/157\n",
      "Batch 7 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012637995183467865, Std: 0.08748551458120346\n",
      "z_j Mean: 0.01249698270112276, Std: 0.0875057727098465\n",
      "Similarities: tensor([[1.3885, 0.0000, 0.0656, 0.0000, 0.0000],\n",
      "        [0.8200, 0.1333, 0.1103, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3374, 0.6102, 0.6017],\n",
      "        [0.0000, 0.0000, 0.9140, 1.2050, 1.0374],\n",
      "        [0.0000, 0.0000, 0.2031, 0.6480, 1.1444]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0800888538360596\n",
      "Batch 7, Loss: 3.0800888538360596\n",
      "Processing Batch 8/157\n",
      "Batch 8 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013934211805462837, Std: 0.08728841692209244\n",
      "z_j Mean: 0.013363366946578026, Std: 0.0873776376247406\n",
      "Similarities: tensor([[1.3701, 0.0000, 0.3090, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4265, 0.0000, 0.9893, 0.0000],\n",
      "        [0.7184, 0.0000, 1.4194, 0.0000, 0.0000],\n",
      "        [0.0000, 0.8289, 0.0000, 1.3275, 0.0000],\n",
      "        [0.0000, 0.0294, 0.0000, 0.0000, 1.1419]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0564990043640137\n",
      "Batch 8, Loss: 3.0564990043640137\n",
      "Processing Batch 9/157\n",
      "Batch 9 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012898917309939861, Std: 0.08674656599760056\n",
      "z_j Mean: 0.01248667947947979, Std: 0.08610079437494278\n",
      "Similarities: tensor([[1.4286, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.2882, 0.0000, 0.0716, 0.1565],\n",
      "        [0.0000, 0.0000, 1.4115, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.2415, 0.0000],\n",
      "        [0.0000, 0.1454, 0.0000, 0.0000, 1.4054]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.08573842048645\n",
      "Batch 9, Loss: 3.08573842048645\n",
      "Processing Batch 10/157\n",
      "Batch 10 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012123672291636467, Std: 0.08685831725597382\n",
      "z_j Mean: 0.01242818683385849, Std: 0.08751557022333145\n",
      "Similarities: tensor([[1.3912, 0.0000, 0.0000, 0.0000, 0.7901],\n",
      "        [0.0000, 1.3676, 0.0000, 0.0000, 0.3533],\n",
      "        [0.0000, 0.0000, 1.3880, 0.7180, 0.0000],\n",
      "        [0.0000, 0.0000, 0.7014, 1.4216, 0.0000],\n",
      "        [1.0458, 0.0399, 0.0000, 0.0000, 1.3952]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1291394233703613\n",
      "Batch 10, Loss: 3.1291394233703613\n",
      "Processing Batch 11/157\n",
      "Batch 11 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011832213029265404, Std: 0.08619318902492523\n",
      "z_j Mean: 0.0123331593349576, Std: 0.0875290185213089\n",
      "Similarities: tensor([[1.4160, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4286, 0.0000, 1.3668, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3141, 0.2756, 1.0538],\n",
      "        [0.0000, 1.3788, 0.1612, 1.3784, 0.1967],\n",
      "        [0.0000, 0.0000, 1.1784, 0.3576, 1.3705]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0916295051574707\n",
      "Batch 11, Loss: 3.0916295051574707\n",
      "Processing Batch 12/157\n",
      "Batch 12 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01259604562073946, Std: 0.08679106086492538\n",
      "z_j Mean: 0.012865988537669182, Std: 0.08745227754116058\n",
      "Similarities: tensor([[1.3964, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4268, 0.0000, 0.5520, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.6421, 0.0000, 1.4212, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4018]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.093675136566162\n",
      "Batch 12, Loss: 3.093675136566162\n",
      "Processing Batch 13/157\n",
      "Batch 13 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012164450250566006, Std: 0.08755261451005936\n",
      "z_j Mean: 0.012356393970549107, Std: 0.08682551234960556\n",
      "Similarities: tensor([[1.3850, 0.0324, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1355, 1.4086, 0.0000, 0.0000, 0.7963],\n",
      "        [0.0000, 0.0000, 1.4013, 0.0000, 0.0034],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4151, 0.0000],\n",
      "        [0.0000, 0.4595, 0.1064, 0.0000, 1.1811]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.103498697280884\n",
      "Batch 13, Loss: 3.103498697280884\n",
      "Processing Batch 14/157\n",
      "Batch 14 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012167079374194145, Std: 0.0875522643327713\n",
      "z_j Mean: 0.01192007027566433, Std: 0.08758623152971268\n",
      "Similarities: tensor([[1.3381, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.8137, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4281, 0.7458, 0.0057],\n",
      "        [0.0000, 0.0000, 0.4914, 1.3808, 0.3470],\n",
      "        [0.0000, 0.0000, 0.0264, 0.1995, 1.3967]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.043996572494507\n",
      "Batch 14, Loss: 3.043996572494507\n",
      "Processing Batch 15/157\n",
      "Batch 15 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01199587807059288, Std: 0.08757588267326355\n",
      "z_j Mean: 0.012068143114447594, Std: 0.08756595104932785\n",
      "Similarities: tensor([[0.6738, 1.4033, 0.0000, 0.5467, 0.0000],\n",
      "        [0.6805, 1.4281, 0.0000, 0.5522, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3055, 0.0000, 0.5704],\n",
      "        [0.9935, 1.1457, 0.0000, 1.2070, 0.0000],\n",
      "        [0.0000, 0.0000, 0.2600, 0.0000, 1.4285]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1209545135498047\n",
      "Batch 15, Loss: 3.1209545135498047\n",
      "Processing Batch 16/157\n",
      "Batch 16 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011951079592108727, Std: 0.08688223361968994\n",
      "z_j Mean: 0.011576705612242222, Std: 0.08622787147760391\n",
      "Similarities: tensor([[1.4214, 0.0000, 0.0000, 1.2748, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4120, 0.0000, 0.0000],\n",
      "        [1.2600, 0.0000, 0.0000, 1.4286, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4177]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0789573192596436\n",
      "Batch 16, Loss: 3.0789573192596436\n",
      "Processing Batch 17/157\n",
      "Batch 17 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013105316087603569, Std: 0.08741671591997147\n",
      "z_j Mean: 0.01298532821238041, Std: 0.08743463456630707\n",
      "Similarities: tensor([[1.2113, 0.0000, 0.0000, 0.0000, 0.8193],\n",
      "        [0.0000, 1.4286, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3550, 0.0000, 0.0000],\n",
      "        [0.0525, 0.0000, 0.0000, 1.3593, 0.0000],\n",
      "        [0.3912, 0.0000, 0.0000, 0.0000, 1.4242]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.075193405151367\n",
      "Batch 17, Loss: 3.075193405151367\n",
      "Processing Batch 18/157\n",
      "Batch 18 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01209464855492115, Std: 0.08686236292123795\n",
      "z_j Mean: 0.012243895791471004, Std: 0.08613566309213638\n",
      "Similarities: tensor([[1.3950, 0.0000, 0.2982, 0.5442, 0.0000],\n",
      "        [0.0000, 1.3006, 0.4783, 0.0000, 0.8675],\n",
      "        [0.0712, 0.6209, 1.3577, 0.9939, 0.1426],\n",
      "        [1.1187, 0.0000, 0.6745, 1.0339, 0.0000],\n",
      "        [0.0000, 1.3559, 0.7041, 0.0000, 1.0714]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0851781368255615\n",
      "Batch 18, Loss: 3.0851781368255615\n",
      "Processing Batch 19/157\n",
      "Batch 19 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012601766735315323, Std: 0.08679024130105972\n",
      "z_j Mean: 0.012937075458467007, Std: 0.0867408812046051\n",
      "Similarities: tensor([[1.3931, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3942, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4165, 0.0520, 0.0898],\n",
      "        [0.0000, 0.0000, 0.0274, 1.4257, 0.0000],\n",
      "        [0.0000, 0.0000, 0.2996, 0.0000, 1.2329]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0885426998138428\n",
      "Batch 19, Loss: 3.0885426998138428\n",
      "Processing Batch 20/157\n",
      "Batch 20 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01167672872543335, Std: 0.08761901408433914\n",
      "z_j Mean: 0.011768929660320282, Std: 0.08760667592287064\n",
      "Similarities: tensor([[1.4274, 0.0484, 0.2747, 0.0000, 0.0000],\n",
      "        [0.0216, 1.4083, 1.3397, 0.0000, 0.0000],\n",
      "        [0.0025, 1.2441, 1.3311, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4286, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.7165]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.054210662841797\n",
      "Batch 20, Loss: 3.054210662841797\n",
      "Processing Batch 21/157\n",
      "Batch 21 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012073234654963017, Std: 0.08686534315347672\n",
      "z_j Mean: 0.012725534848868847, Std: 0.0874728262424469\n",
      "Similarities: tensor([[1.2467, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.2577, 0.0000, 0.0000, 0.8238],\n",
      "        [0.0000, 0.0000, 1.2025, 0.5708, 0.0000],\n",
      "        [0.0000, 0.0000, 0.3666, 1.4248, 0.0000],\n",
      "        [0.0000, 0.4222, 0.0000, 0.0000, 1.1818]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.054962396621704\n",
      "Batch 21, Loss: 3.054962396621704\n",
      "Processing Batch 22/157\n",
      "Batch 22 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011237401515245438, Std: 0.08556227385997772\n",
      "z_j Mean: 0.01131877675652504, Std: 0.08555155247449875\n",
      "Similarities: tensor([[1.4247, 0.3353, 0.4886, 0.0000, 0.0810],\n",
      "        [0.2566, 1.3788, 0.2808, 0.0000, 0.5191],\n",
      "        [0.3974, 0.1475, 1.4068, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4277, 0.0000],\n",
      "        [0.4008, 0.8016, 0.0000, 0.0000, 1.2916]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.143866777420044\n",
      "Batch 22, Loss: 3.143866777420044\n",
      "Processing Batch 23/157\n",
      "Batch 23 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011816238984465599, Std: 0.08690068125724792\n",
      "z_j Mean: 0.011442828923463821, Std: 0.0869506448507309\n",
      "Similarities: tensor([[1.4132, 0.9998, 0.0000, 0.0000, 0.0000],\n",
      "        [0.7945, 1.3261, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3876, 0.8843, 0.0000],\n",
      "        [0.0000, 0.0000, 0.9634, 1.4284, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.6355]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.143420934677124\n",
      "Batch 23, Loss: 3.143420934677124\n",
      "Processing Batch 24/157\n",
      "Batch 24 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01186412200331688, Std: 0.08689415454864502\n",
      "z_j Mean: 0.011666186153888702, Std: 0.08692095428705215\n",
      "Similarities: tensor([[1.4085, 0.0000, 0.0000, 1.1582, 0.1792],\n",
      "        [0.0000, 1.4284, 0.0000, 0.2459, 0.0000],\n",
      "        [0.0000, 0.0000, 1.0760, 0.0000, 0.0000],\n",
      "        [1.0009, 0.5068, 0.0000, 1.3619, 0.0000],\n",
      "        [0.1227, 0.0000, 0.0000, 0.0000, 1.4143]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1345934867858887\n",
      "Batch 24, Loss: 3.1345934867858887\n",
      "Processing Batch 25/157\n",
      "Batch 25 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011885386891663074, Std: 0.08759094774723053\n",
      "z_j Mean: 0.012281088158488274, Std: 0.08753633499145508\n",
      "Similarities: tensor([[1.4270, 0.2277, 0.0000, 1.2672, 0.6599],\n",
      "        [0.2208, 1.3685, 0.0000, 0.1530, 1.1839],\n",
      "        [0.0000, 0.0000, 1.3684, 0.0000, 0.0000],\n",
      "        [1.2278, 0.1559, 0.0000, 1.4207, 0.4518],\n",
      "        [0.6912, 1.1375, 0.0000, 0.4789, 1.3931]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1151983737945557\n",
      "Batch 25, Loss: 3.1151983737945557\n",
      "Processing Batch 26/157\n",
      "Batch 26 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012011408805847168, Std: 0.08687391132116318\n",
      "z_j Mean: 0.011695663444697857, Std: 0.08550084382295609\n",
      "Similarities: tensor([[1.3417, 1.3147, 0.0000, 0.0000, 0.6749],\n",
      "        [0.7695, 1.3610, 0.0000, 0.0000, 0.8431],\n",
      "        [0.0000, 0.0000, 1.4147, 0.9617, 0.0000],\n",
      "        [0.0000, 0.0000, 0.8660, 1.4144, 0.0000],\n",
      "        [0.4552, 0.7558, 0.0000, 0.0000, 1.4209]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1074671745300293\n",
      "Batch 26, Loss: 3.1074671745300293\n",
      "Processing Batch 27/157\n",
      "Batch 27 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012156999669969082, Std: 0.08755365759134293\n",
      "z_j Mean: 0.011833585798740387, Std: 0.0868983194231987\n",
      "Similarities: tensor([[1.3462, 0.3759, 0.0000, 0.0000, 0.5741],\n",
      "        [0.3881, 1.3913, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4143, 0.0000, 0.4211],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4286, 0.0000],\n",
      "        [0.0000, 0.0000, 0.2885, 0.0000, 0.7072]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.116757869720459\n",
      "Batch 27, Loss: 3.116757869720459\n",
      "Processing Batch 28/157\n",
      "Batch 28 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012073962017893791, Std: 0.08686524629592896\n",
      "z_j Mean: 0.012037953361868858, Std: 0.0868702381849289\n",
      "Similarities: tensor([[1.3103e+00, 0.0000e+00, 1.0592e-03, 0.0000e+00, 4.7309e-02],\n",
      "        [0.0000e+00, 1.4083e+00, 0.0000e+00, 0.0000e+00, 3.1025e-01],\n",
      "        [0.0000e+00, 0.0000e+00, 1.3994e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 3.9400e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 4.8334e-01, 0.0000e+00, 0.0000e+00, 1.3984e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1406280994415283\n",
      "Batch 28, Loss: 3.1406280994415283\n",
      "Processing Batch 29/157\n",
      "Batch 29 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012371771968901157, Std: 0.08682332187891006\n",
      "z_j Mean: 0.012511092238128185, Std: 0.08609725534915924\n",
      "Similarities: tensor([[1.4005, 0.0000, 0.0000, 0.0000, 0.4982],\n",
      "        [0.0000, 1.4233, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4286, 0.0000, 0.0000],\n",
      "        [0.0337, 0.0000, 0.0000, 1.4166, 0.0000],\n",
      "        [0.2940, 0.0000, 0.0000, 0.0000, 1.3966]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.083894729614258\n",
      "Batch 29, Loss: 3.083894729614258\n",
      "Processing Batch 30/157\n",
      "Batch 30 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013296732679009438, Std: 0.0873878076672554\n",
      "z_j Mean: 0.013208741322159767, Std: 0.08740115910768509\n",
      "Similarities: tensor([[1.3969, 0.1596, 0.0000, 0.5088, 0.0000],\n",
      "        [0.1190, 1.4237, 0.2771, 0.1445, 0.0000],\n",
      "        [0.0585, 0.3207, 1.3939, 0.0000, 0.0000],\n",
      "        [0.1519, 0.4012, 0.0000, 1.2335, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4279]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0650315284729004\n",
      "Batch 30, Loss: 3.0650315284729004\n",
      "Processing Batch 31/157\n",
      "Batch 31 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012924707494676113, Std: 0.08674272149801254\n",
      "z_j Mean: 0.012998745776712894, Std: 0.08673165738582611\n",
      "Similarities: tensor([[1.3806, 0.7415, 0.0157, 0.0000, 0.0000],\n",
      "        [1.0994, 1.3470, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4172, 0.0000, 0.0193],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3569, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4111]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.428282380104065]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.080620288848877\n",
      "Batch 31, Loss: 3.080620288848877\n",
      "Processing Batch 32/157\n",
      "Batch 32 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013466568663716316, Std: 0.08736179769039154\n",
      "z_j Mean: 0.013183653354644775, Std: 0.08740493655204773\n",
      "Similarities: tensor([[1.4286, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3584, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.0361, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.2801, 0.0000],\n",
      "        [0.0000, 0.0201, 0.0000, 0.0000, 1.3972]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0685877799987793\n",
      "Batch 32, Loss: 3.0685877799987793\n",
      "Processing Batch 33/157\n",
      "Batch 33 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01432223990559578, Std: 0.08652292937040329\n",
      "z_j Mean: 0.014794080518186092, Std: 0.08714678883552551\n",
      "Similarities: tensor([[1.3522, 0.0000, 0.7586, 0.0000, 1.2228],\n",
      "        [0.0000, 1.2746, 0.0000, 0.0000, 0.0000],\n",
      "        [0.6529, 0.0000, 1.3814, 0.0000, 0.7509],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3660, 0.0000],\n",
      "        [1.4215, 0.0000, 0.7598, 0.0000, 1.3749]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285701513290405]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0960917472839355\n",
      "Batch 33, Loss: 3.0960917472839355\n",
      "Processing Batch 34/157\n",
      "Batch 34 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013548989780247211, Std: 0.0866473913192749\n",
      "z_j Mean: 0.013354217633605003, Std: 0.08737904578447342\n",
      "Similarities: tensor([[1.3238, 0.0000, 1.3032, 0.0036, 0.6822],\n",
      "        [0.0000, 1.4246, 0.0000, 1.2556, 0.0000],\n",
      "        [1.0005, 0.0000, 1.4159, 0.0000, 0.9641],\n",
      "        [0.0000, 1.2067, 0.0000, 1.3895, 0.0000],\n",
      "        [0.3300, 0.0000, 0.5213, 0.0000, 1.3896]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1141397953033447\n",
      "Batch 34, Loss: 3.1141397953033447\n",
      "Processing Batch 35/157\n",
      "Batch 35 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012941068038344383, Std: 0.08674028515815735\n",
      "z_j Mean: 0.013116796500980854, Std: 0.08741500228643417\n",
      "Similarities: tensor([[1.3830, 0.0000, 0.0000, 0.0000, 0.8821],\n",
      "        [0.0000, 1.2122, 0.2899, 0.6102, 0.0000],\n",
      "        [0.0000, 0.3349, 1.3737, 1.0378, 0.0000],\n",
      "        [0.0000, 0.8498, 0.7596, 1.4018, 0.0000],\n",
      "        [0.6289, 0.0000, 0.0000, 0.0000, 1.3859]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.078148126602173\n",
      "Batch 35, Loss: 3.078148126602173\n",
      "Processing Batch 36/157\n",
      "Batch 36 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013081362470984459, Std: 0.08601243793964386\n",
      "z_j Mean: 0.013439888134598732, Std: 0.08666437864303589\n",
      "Similarities: tensor([[1.3877, 0.0000, 0.0000, 0.0206, 0.0120],\n",
      "        [0.0000, 1.3397, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3924, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4046, 0.0000],\n",
      "        [0.0085, 0.0000, 0.0000, 0.0000, 1.4252]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.122507333755493\n",
      "Batch 36, Loss: 3.122507333755493\n",
      "Processing Batch 37/157\n",
      "Batch 37 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013013330288231373, Std: 0.08672946691513062\n",
      "z_j Mean: 0.013138843700289726, Std: 0.08671054989099503\n",
      "Similarities: tensor([[1.4225, 0.3278, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4093, 1.4215, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4005, 0.0000, 0.4527],\n",
      "        [0.0000, 0.0000, 0.0000, 1.1308, 0.0000],\n",
      "        [0.0000, 0.0000, 0.4369, 0.0000, 1.3966]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.058206796646118\n",
      "Batch 37, Loss: 3.058206796646118\n",
      "Processing Batch 38/157\n",
      "Batch 38 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012889094650745392, Std: 0.08744887262582779\n",
      "z_j Mean: 0.013022291474044323, Std: 0.08742913603782654\n",
      "Similarities: tensor([[1.4050, 0.0000, 0.0000, 1.4062, 0.0000],\n",
      "        [0.0000, 1.4097, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4253, 0.0000, 0.0000],\n",
      "        [1.3049, 0.0000, 0.0000, 1.3730, 0.0067],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0098, 1.4140]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0858333110809326\n",
      "Batch 38, Loss: 3.0858333110809326\n",
      "Processing Batch 39/157\n",
      "Batch 39 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012937108986079693, Std: 0.08744177967309952\n",
      "z_j Mean: 0.013426043093204498, Std: 0.08736803382635117\n",
      "Similarities: tensor([[1.4286, 0.3966, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1571, 1.3548, 0.0000, 0.0000, 0.0854],\n",
      "        [0.0000, 0.0000, 1.4140, 0.6163, 0.0000],\n",
      "        [0.0000, 0.0000, 0.2608, 1.2733, 0.0000],\n",
      "        [0.0000, 0.3352, 0.0000, 0.0000, 1.4256]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0651469230651855\n",
      "Batch 39, Loss: 3.0651469230651855\n",
      "Processing Batch 40/157\n",
      "Batch 40 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012769175693392754, Std: 0.08746646344661713\n",
      "z_j Mean: 0.012374447658658028, Std: 0.08682294189929962\n",
      "Similarities: tensor([[1.3107, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.1608, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0089, 0.0000, 1.4264, 0.0000, 0.2873],\n",
      "        [0.2362, 0.0000, 0.0000, 1.1527, 0.0000],\n",
      "        [0.0000, 0.0000, 0.1111, 0.0000, 1.4174]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0527212619781494\n",
      "Batch 40, Loss: 3.0527212619781494\n",
      "Processing Batch 41/157\n",
      "Batch 41 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012414946220815182, Std: 0.08681716024875641\n",
      "z_j Mean: 0.012754114344716072, Std: 0.08676797151565552\n",
      "Similarities: tensor([[1.3972, 1.1762, 0.0000, 0.0000, 0.0000],\n",
      "        [1.2927, 1.4271, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4126, 0.0000, 0.2661],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4200, 0.0000],\n",
      "        [0.0000, 0.0000, 0.4685, 0.0931, 1.4132]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.087402105331421\n",
      "Batch 41, Loss: 3.087402105331421\n",
      "Processing Batch 42/157\n",
      "Batch 42 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012805303558707237, Std: 0.08746118098497391\n",
      "z_j Mean: 0.012620899826288223, Std: 0.08748798072338104\n",
      "Similarities: tensor([[1.2085, 1.2381, 0.7787, 1.2557, 0.2762],\n",
      "        [0.8233, 1.3403, 0.8924, 1.3897, 0.2827],\n",
      "        [0.5942, 0.5372, 1.4235, 0.6959, 0.0951],\n",
      "        [0.7482, 1.3201, 0.9225, 1.3926, 0.1992],\n",
      "        [0.1197, 0.2321, 0.0077, 0.1399, 1.0292]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0912065505981445\n",
      "Batch 42, Loss: 3.0912065505981445\n",
      "Processing Batch 43/157\n",
      "Batch 43 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01270241104066372, Std: 0.08747617900371552\n",
      "z_j Mean: 0.012968819588422775, Std: 0.08743707835674286\n",
      "Similarities: tensor([[1.3894, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0110, 1.3971, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3720, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4222, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4162]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.069613218307495\n",
      "Batch 43, Loss: 3.069613218307495\n",
      "Processing Batch 44/157\n",
      "Batch 44 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012623637914657593, Std: 0.0874875858426094\n",
      "z_j Mean: 0.01266326755285263, Std: 0.08748185634613037\n",
      "Similarities: tensor([[1.2546, 0.0000, 0.0000, 0.0000, 0.9498],\n",
      "        [0.0000, 1.4278, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4274, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0938, 0.0000, 1.3570, 0.0000],\n",
      "        [1.0821, 0.0000, 0.0000, 0.0000, 1.4197]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.078672170639038\n",
      "Batch 44, Loss: 3.078672170639038\n",
      "Processing Batch 45/157\n",
      "Batch 45 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01254111248999834, Std: 0.0874994546175003\n",
      "z_j Mean: 0.012687400914728642, Std: 0.08677776157855988\n",
      "Similarities: tensor([[1.4286, 0.0000, 0.0000, 0.9733, 0.0000],\n",
      "        [0.0000, 1.3603, 0.1093, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0824, 1.3277, 0.0000, 0.2459],\n",
      "        [0.9564, 0.0000, 0.0000, 1.4110, 0.0000],\n",
      "        [0.0000, 0.0000, 0.6273, 0.0000, 1.4032]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0829243659973145\n",
      "Batch 45, Loss: 3.0829243659973145\n",
      "Processing Batch 46/157\n",
      "Batch 46 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01332184486091137, Std: 0.08738398551940918\n",
      "z_j Mean: 0.01367967389523983, Std: 0.08732867240905762\n",
      "Similarities: tensor([[1.3916, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0508, 1.4074, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3828, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3753, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.1473, 1.0686]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285311698913574]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.06365966796875\n",
      "Batch 46, Loss: 3.06365966796875\n",
      "Processing Batch 47/157\n",
      "Batch 47 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012776138260960579, Std: 0.08746544271707535\n",
      "z_j Mean: 0.012425648048520088, Std: 0.0875159278512001\n",
      "Similarities: tensor([[1.3934, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3681, 0.3797, 0.0024, 0.0782],\n",
      "        [0.0000, 0.0134, 0.8147, 0.3817, 0.9429],\n",
      "        [0.0000, 0.0000, 0.3569, 1.4264, 0.8667],\n",
      "        [0.0000, 0.0211, 1.0316, 0.9675, 1.2309]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285528659820557]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.067993640899658\n",
      "Batch 47, Loss: 3.067993640899658\n",
      "Processing Batch 48/157\n",
      "Batch 48 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013361130841076374, Std: 0.08737798035144806\n",
      "z_j Mean: 0.013519668951630592, Std: 0.08735359460115433\n",
      "Similarities: tensor([[1.4150, 0.0000, 0.5360, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3907, 0.0000, 0.3275, 0.0000],\n",
      "        [0.2502, 0.0000, 1.1303, 0.0000, 0.0000],\n",
      "        [0.0000, 0.4537, 0.0000, 1.3998, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4211]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.042297840118408\n",
      "Batch 48, Loss: 3.042297840118408\n",
      "Processing Batch 49/157\n",
      "Batch 49 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01294812560081482, Std: 0.08744014799594879\n",
      "z_j Mean: 0.012989135459065437, Std: 0.0874340608716011\n",
      "Similarities: tensor([[1.4110, 0.0000, 0.0081, 0.4022, 0.5054],\n",
      "        [0.0000, 1.3285, 0.0000, 0.0000, 0.0223],\n",
      "        [0.0114, 0.0000, 1.4140, 0.0000, 0.0000],\n",
      "        [0.5577, 0.0000, 0.0000, 1.4193, 0.7989],\n",
      "        [0.3899, 0.1938, 0.0000, 0.4798, 1.3116]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0505549907684326\n",
      "Batch 49, Loss: 3.0505549907684326\n",
      "Processing Batch 50/157\n",
      "Batch 50 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012689965777099133, Std: 0.08747798949480057\n",
      "z_j Mean: 0.012861122377216816, Std: 0.08745298534631729\n",
      "Similarities: tensor([[1.0621, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4113, 0.0000, 0.0000, 0.1296],\n",
      "        [0.0000, 0.0000, 1.4267, 0.0000, 0.8588],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4262, 0.0000],\n",
      "        [0.0000, 0.2020, 0.5925, 0.0000, 1.3022]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4283665418624878]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0429930686950684\n",
      "Batch 50, Loss: 3.0429930686950684\n",
      "Processing Batch 51/157\n",
      "Batch 51 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013544722460210323, Std: 0.08734970539808273\n",
      "z_j Mean: 0.013532409444451332, Std: 0.08735162019729614\n",
      "Similarities: tensor([[1.3934, 0.0000, 0.8913, 0.5179, 0.3026],\n",
      "        [0.0000, 1.3737, 0.0000, 0.0000, 0.0000],\n",
      "        [0.6631, 0.0000, 1.2764, 0.6466, 0.0000],\n",
      "        [0.2726, 0.0000, 0.5119, 1.3085, 0.0000],\n",
      "        [0.4489, 0.0000, 0.0000, 0.0000, 1.2956]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.093038558959961\n",
      "Batch 51, Loss: 3.093038558959961\n",
      "Processing Batch 52/157\n",
      "Batch 52 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012954884208738804, Std: 0.0874391421675682\n",
      "z_j Mean: 0.013350591994822025, Std: 0.08737959712743759\n",
      "Similarities: tensor([[1.3536, 0.0000, 0.0000, 0.1141, 0.0064],\n",
      "        [0.0000, 1.3395, 0.0000, 0.0287, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4281, 0.0000, 0.0000],\n",
      "        [0.0696, 0.4173, 0.0000, 1.2142, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3657]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0568599700927734\n",
      "Batch 52, Loss: 3.0568599700927734\n",
      "Processing Batch 53/157\n",
      "Batch 53 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012846850790083408, Std: 0.08745508641004562\n",
      "z_j Mean: 0.012911229394376278, Std: 0.08744560927152634\n",
      "Similarities: tensor([[1.4036, 0.0000, 0.9542, 0.6625, 0.0000],\n",
      "        [0.0000, 1.4276, 0.0231, 0.0725, 0.0000],\n",
      "        [0.9532, 0.0160, 1.4264, 0.8937, 0.0000],\n",
      "        [0.6777, 0.0759, 0.8496, 1.3840, 0.0187],\n",
      "        [0.0834, 0.0000, 0.0000, 0.0423, 1.0412]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0575551986694336\n",
      "Batch 53, Loss: 3.0575551986694336\n",
      "Processing Batch 54/157\n",
      "Batch 54 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012875089421868324, Std: 0.08745093643665314\n",
      "z_j Mean: 0.013077547773718834, Std: 0.08742088079452515\n",
      "Similarities: tensor([[1.4262, 0.0000, 0.0193, 0.0000, 1.1682],\n",
      "        [0.0000, 1.4190, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0326, 0.0000, 1.4156, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.3607, 1.3630, 0.0000],\n",
      "        [0.9493, 0.0000, 0.0000, 0.0000, 1.3505]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285483360290527]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0684525966644287\n",
      "Batch 54, Loss: 3.0684525966644287\n",
      "Processing Batch 55/157\n",
      "Batch 55 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012614916078746319, Std: 0.08748884499073029\n",
      "z_j Mean: 0.012410514056682587, Std: 0.08681779354810715\n",
      "Similarities: tensor([[1.3242, 0.9816, 1.0906, 0.6200, 0.0000],\n",
      "        [0.9453, 1.3799, 1.2564, 0.1382, 0.0000],\n",
      "        [0.7679, 1.2259, 1.3917, 0.3576, 0.0000],\n",
      "        [0.2444, 0.4391, 0.4914, 1.3141, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.2891]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0921897888183594\n",
      "Batch 55, Loss: 3.0921897888183594\n",
      "Processing Batch 56/157\n",
      "Batch 56 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011966084130108356, Std: 0.08757995814085007\n",
      "z_j Mean: 0.011898744851350784, Std: 0.08688942342996597\n",
      "Similarities: tensor([[1.4097, 0.1520, 0.0000, 0.0000, 1.0430],\n",
      "        [0.0120, 1.3555, 0.0077, 0.0000, 0.0265],\n",
      "        [0.0000, 0.0709, 1.3091, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4286, 0.0000],\n",
      "        [0.8950, 0.3123, 0.0000, 0.0000, 1.4189]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.092210292816162\n",
      "Batch 56, Loss: 3.092210292816162\n",
      "Processing Batch 57/157\n",
      "Batch 57 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01324496790766716, Std: 0.08739566802978516\n",
      "z_j Mean: 0.013241779059171677, Std: 0.08739615231752396\n",
      "Similarities: tensor([[1.4109, 0.0000, 0.9011, 0.0248, 0.0000],\n",
      "        [0.0000, 1.3224, 0.0723, 0.0000, 0.0000],\n",
      "        [0.8871, 0.0000, 1.4241, 0.0000, 0.0000],\n",
      "        [0.0147, 0.0000, 0.0000, 1.4020, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3927]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0565237998962402\n",
      "Batch 57, Loss: 3.0565237998962402\n",
      "Processing Batch 58/157\n",
      "Batch 58 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012298157438635826, Std: 0.08683378249406815\n",
      "z_j Mean: 0.01208105031400919, Std: 0.08686425536870956\n",
      "Similarities: tensor([[1.4169, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.2206, 0.0000, 1.1153, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4227, 0.0000, 0.0000],\n",
      "        [0.0000, 1.0040, 0.0499, 1.3865, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3970]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.428147792816162]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0853757858276367\n",
      "Batch 58, Loss: 3.0853757858276367\n",
      "Processing Batch 59/157\n",
      "Batch 59 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011680935509502888, Std: 0.08761844784021378\n",
      "z_j Mean: 0.011763013899326324, Std: 0.08760746568441391\n",
      "Similarities: tensor([[1.3129, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.2264, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3796, 0.3400, 0.0000],\n",
      "        [0.0000, 0.0000, 0.4548, 1.3871, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3541]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.024618625640869\n",
      "Batch 59, Loss: 3.024618625640869\n",
      "Processing Batch 60/157\n",
      "Batch 60 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012075044214725494, Std: 0.08756500482559204\n",
      "z_j Mean: 0.012179910205304623, Std: 0.08755047619342804\n",
      "Similarities: tensor([[1.4138, 0.7055, 0.0364, 0.0000, 0.0000],\n",
      "        [0.1872, 1.3311, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.7864, 0.7619, 0.4786],\n",
      "        [0.0000, 0.0000, 0.0517, 1.3878, 0.9226],\n",
      "        [0.0000, 0.0000, 0.0366, 1.0223, 1.3339]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0572073459625244\n",
      "Batch 60, Loss: 3.0572073459625244\n",
      "Processing Batch 61/157\n",
      "Batch 61 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011714139953255653, Std: 0.0876140147447586\n",
      "z_j Mean: 0.011718017980456352, Std: 0.08761350065469742\n",
      "Similarities: tensor([[1.3413, 0.0000, 0.0000, 0.2392, 0.0879],\n",
      "        [0.0000, 1.4200, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3564, 0.0000, 0.8264],\n",
      "        [0.1233, 0.0000, 0.0000, 1.3876, 0.0399],\n",
      "        [0.2044, 0.0000, 0.4498, 0.0740, 1.3977]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0445423126220703\n",
      "Batch 61, Loss: 3.0445423126220703\n",
      "Processing Batch 62/157\n",
      "Batch 62 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012496600858867168, Std: 0.08750582486391068\n",
      "z_j Mean: 0.012553204782307148, Std: 0.08749771863222122\n",
      "Similarities: tensor([[1.3915, 0.1187, 0.0322, 0.0000, 0.2782],\n",
      "        [0.1479, 1.3646, 0.0000, 0.0000, 0.1083],\n",
      "        [0.0059, 0.0000, 1.4000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4126, 0.0000],\n",
      "        [0.1845, 0.5084, 0.0000, 0.0000, 1.4105]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.06347918510437\n",
      "Batch 62, Loss: 3.06347918510437\n",
      "Processing Batch 63/157\n",
      "Batch 63 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011704539880156517, Std: 0.08761529624462128\n",
      "z_j Mean: 0.011651454493403435, Std: 0.08762238174676895\n",
      "Similarities: tensor([[1.4277, 0.0000, 0.0112, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4251, 0.0000, 0.0135, 0.8615],\n",
      "        [0.0000, 0.0000, 1.4280, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0188, 0.0000, 1.1852, 0.0000],\n",
      "        [0.0000, 0.7268, 0.0000, 0.0000, 1.4216]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0351760387420654\n",
      "Batch 63, Loss: 3.0351760387420654\n",
      "Processing Batch 64/157\n",
      "Batch 64 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011798903346061707, Std: 0.0876026451587677\n",
      "z_j Mean: 0.011704105883836746, Std: 0.08691585063934326\n",
      "Similarities: tensor([[1.4115, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.9212, 0.0000, 0.0000, 0.1747],\n",
      "        [0.0000, 0.0000, 1.4133, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.2845, 0.0000],\n",
      "        [0.0000, 1.1963, 0.0000, 0.0000, 1.4286]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.057399034500122\n",
      "Batch 64, Loss: 3.057399034500122\n",
      "Processing Batch 65/157\n",
      "Batch 65 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012161990627646446, Std: 0.08755296468734741\n",
      "z_j Mean: 0.012104572728276253, Std: 0.08756092190742493\n",
      "Similarities: tensor([[1.4283, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4198, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4244, 0.0000, 0.7720],\n",
      "        [0.0000, 0.0000, 0.0000, 0.8664, 0.1466],\n",
      "        [0.0000, 0.0000, 0.6875, 0.1598, 1.4158]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.05318284034729\n",
      "Batch 65, Loss: 3.05318284034729\n",
      "Processing Batch 66/157\n",
      "Batch 66 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012095311656594276, Std: 0.0875622034072876\n",
      "z_j Mean: 0.012218872085213661, Std: 0.08684497326612473\n",
      "Similarities: tensor([[1.3425, 0.0000, 0.0000, 0.5126, 1.0238],\n",
      "        [0.0000, 1.3487, 0.6951, 0.0690, 0.0000],\n",
      "        [0.0000, 0.2616, 1.2805, 0.0000, 0.0000],\n",
      "        [0.2876, 0.0000, 0.0000, 1.3666, 0.0000],\n",
      "        [0.9676, 0.0000, 0.0000, 0.0000, 1.3693]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.096409797668457\n",
      "Batch 66, Loss: 3.096409797668457\n",
      "Processing Batch 67/157\n",
      "Batch 67 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012044357135891914, Std: 0.0875692293047905\n",
      "z_j Mean: 0.011864928528666496, Std: 0.08689404278993607\n",
      "Similarities: tensor([[1.3929, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.1990, 0.3814, 1.3109, 0.0044],\n",
      "        [0.0000, 0.0703, 1.3550, 0.3091, 1.1011],\n",
      "        [0.0000, 1.2794, 0.2671, 1.4139, 0.0000],\n",
      "        [0.0000, 0.0913, 1.3204, 0.0449, 1.4286]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0721559524536133\n",
      "Batch 67, Loss: 3.0721559524536133\n",
      "Processing Batch 68/157\n",
      "Batch 68 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012535424903035164, Std: 0.08750026673078537\n",
      "z_j Mean: 0.012297045439481735, Std: 0.08683393895626068\n",
      "Similarities: tensor([[1.3748, 0.0000, 0.1692, 0.2084, 0.1207],\n",
      "        [0.0000, 1.4205, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1518, 0.0000, 1.4159, 0.6781, 1.4271],\n",
      "        [0.1970, 0.0000, 0.6955, 1.4172, 0.5003],\n",
      "        [0.1181, 0.0000, 1.4032, 0.6328, 1.4280]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.428565263748169]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.06264591217041\n",
      "Batch 68, Loss: 3.06264591217041\n",
      "Processing Batch 69/157\n",
      "Batch 69 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012272967025637627, Std: 0.08753747493028641\n",
      "z_j Mean: 0.012304889038205147, Std: 0.08683282881975174\n",
      "Similarities: tensor([[1.3876, 0.5424, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3485, 1.3977, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.8398, 1.2978, 0.0205],\n",
      "        [0.0000, 0.0000, 0.5457, 1.3088, 0.0765],\n",
      "        [0.0000, 0.0000, 0.4184, 0.0000, 1.3923]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1038105487823486\n",
      "Batch 69, Loss: 3.1038105487823486\n",
      "Processing Batch 70/157\n",
      "Batch 70 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012487109750509262, Std: 0.08750718086957932\n",
      "z_j Mean: 0.012517407536506653, Std: 0.0875028520822525\n",
      "Similarities: tensor([[1.3754, 0.0000, 0.0000, 0.0000, 0.9832],\n",
      "        [0.0000, 1.4176, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4186, 0.8054, 0.0000],\n",
      "        [0.0000, 0.0000, 0.7934, 1.4226, 0.0000],\n",
      "        [1.0016, 0.0000, 0.0000, 0.0000, 1.4219]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0703623294830322\n",
      "Batch 70, Loss: 3.0703623294830322\n",
      "Processing Batch 71/157\n",
      "Batch 71 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012148884125053883, Std: 0.08755479007959366\n",
      "z_j Mean: 0.012395435944199562, Std: 0.08752021938562393\n",
      "Similarities: tensor([[1.2417, 0.2384, 0.0000, 0.9090, 0.2718],\n",
      "        [0.1768, 1.3270, 0.0000, 0.3063, 0.0916],\n",
      "        [0.0000, 0.0000, 1.4215, 0.0000, 0.6173],\n",
      "        [0.4427, 0.2012, 0.0000, 1.3428, 0.2294],\n",
      "        [0.1358, 0.0617, 0.5356, 0.2407, 1.0681]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.069340705871582\n",
      "Batch 71, Loss: 3.069340705871582\n",
      "Processing Batch 72/157\n",
      "Batch 72 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011858721263706684, Std: 0.08759455382823944\n",
      "z_j Mean: 0.011811555363237858, Std: 0.08690131455659866\n",
      "Similarities: tensor([[1.3873, 0.4473, 0.0000, 0.0000, 1.2185],\n",
      "        [0.5716, 1.4268, 0.0000, 0.0000, 0.0488],\n",
      "        [0.0000, 0.0000, 1.3705, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0479, 1.3948, 0.0000],\n",
      "        [1.0499, 0.0448, 0.0000, 0.0000, 1.4273]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0721025466918945\n",
      "Batch 72, Loss: 3.0721025466918945\n",
      "Processing Batch 73/157\n",
      "Batch 73 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01201095711439848, Std: 0.0875738114118576\n",
      "z_j Mean: 0.012226590886712074, Std: 0.08684389293193817\n",
      "Similarities: tensor([[1.3635, 0.0000, 0.4891, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3843, 0.2246, 0.0000, 0.0000],\n",
      "        [0.7116, 0.3964, 1.2443, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4286, 0.0000],\n",
      "        [0.8482, 0.0000, 0.1026, 0.0000, 0.0000]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0597097873687744\n",
      "Batch 73, Loss: 3.0597097873687744\n",
      "Processing Batch 74/157\n",
      "Batch 74 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012396357953548431, Std: 0.08752008527517319\n",
      "z_j Mean: 0.012166544795036316, Std: 0.08755233138799667\n",
      "Similarities: tensor([[1.4170, 0.0000, 0.0000, 0.0000, 1.4070],\n",
      "        [0.0149, 1.4009, 0.0000, 0.0301, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3568, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0496, 0.0000, 1.2783, 0.0000],\n",
      "        [1.3961, 0.0000, 0.0000, 0.0000, 1.4257]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0556256771087646\n",
      "Batch 74, Loss: 3.0556256771087646\n",
      "Processing Batch 75/157\n",
      "Batch 75 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011943237856030464, Std: 0.08758307248353958\n",
      "z_j Mean: 0.011775888502597809, Std: 0.08760573714971542\n",
      "Similarities: tensor([[1.4035, 0.0000, 0.3167, 0.3167, 0.1861],\n",
      "        [0.0000, 1.2570, 0.0000, 0.0000, 0.4493],\n",
      "        [0.0823, 0.0000, 1.4286, 1.4285, 0.0000],\n",
      "        [0.0823, 0.0000, 1.4282, 1.4284, 0.0000],\n",
      "        [0.1704, 0.0841, 0.0000, 0.0000, 1.4224]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.069692373275757\n",
      "Batch 75, Loss: 3.069692373275757\n",
      "Processing Batch 76/157\n",
      "Batch 76 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012536984868347645, Std: 0.08750005066394806\n",
      "z_j Mean: 0.012345279566943645, Std: 0.0875273048877716\n",
      "Similarities: tensor([[1.4118, 0.5388, 0.0000, 1.3890, 0.0000],\n",
      "        [0.4914, 1.4252, 0.0000, 0.5573, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4160, 0.0000, 0.0069],\n",
      "        [1.1671, 0.5207, 0.0000, 1.3237, 0.0000],\n",
      "        [0.0112, 0.0000, 0.0437, 0.0000, 1.3820]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.060102701187134\n",
      "Batch 76, Loss: 3.060102701187134\n",
      "Processing Batch 77/157\n",
      "Batch 77 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012096447870135307, Std: 0.08756204694509506\n",
      "z_j Mean: 0.012281393632292747, Std: 0.0875362977385521\n",
      "Similarities: tensor([[1.4254, 0.0000, 0.0413, 0.3171, 0.2967],\n",
      "        [0.0000, 1.4181, 0.2435, 0.0000, 0.0000],\n",
      "        [0.0277, 0.4049, 1.4071, 0.1117, 0.0000],\n",
      "        [0.3226, 0.0000, 0.1885, 1.3903, 0.0000],\n",
      "        [0.3451, 0.0000, 0.0000, 0.0000, 1.3861]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0535805225372314\n",
      "Batch 77, Loss: 3.0535805225372314\n",
      "Processing Batch 78/157\n",
      "Batch 78 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012204939499497414, Std: 0.08754698932170868\n",
      "z_j Mean: 0.012266786769032478, Std: 0.08753833919763565\n",
      "Similarities: tensor([[1.4278, 0.0000, 0.0000, 0.0000, 0.1400],\n",
      "        [0.0467, 1.4286, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.2932, 0.0873, 0.0000],\n",
      "        [0.0000, 0.0000, 0.1542, 1.4282, 0.0000],\n",
      "        [0.0236, 0.0000, 0.0000, 0.0000, 1.4161]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1093392372131348\n",
      "Batch 78, Loss: 3.1093392372131348\n",
      "Processing Batch 79/157\n",
      "Batch 79 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01228303462266922, Std: 0.08753605931997299\n",
      "z_j Mean: 0.011871042661368847, Std: 0.08618784695863724\n",
      "Similarities: tensor([[1.3822, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3333, 0.0000, 0.0000, 0.0931],\n",
      "        [0.0000, 0.0000, 1.4284, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3826, 0.3346],\n",
      "        [0.0653, 0.0000, 0.0000, 0.3282, 1.3404]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.130652904510498\n",
      "Batch 79, Loss: 3.130652904510498\n",
      "Processing Batch 80/157\n",
      "Batch 80 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012927159667015076, Std: 0.0867423564195633\n",
      "z_j Mean: 0.01299228984862566, Std: 0.0874335914850235\n",
      "Similarities: tensor([[1.1849, 0.0000, 0.0000, 0.2382, 0.0000],\n",
      "        [0.0000, 1.4053, 0.6780, 0.0000, 0.6046],\n",
      "        [0.0000, 0.4234, 1.3837, 0.0000, 0.2309],\n",
      "        [0.0000, 0.0000, 0.1103, 1.3708, 0.0000],\n",
      "        [0.0000, 0.5614, 0.3855, 0.0000, 1.4212]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1036853790283203\n",
      "Batch 80, Loss: 3.1036853790283203\n",
      "Processing Batch 81/157\n",
      "Batch 81 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01243848167359829, Std: 0.08751410990953445\n",
      "z_j Mean: 0.01266665942966938, Std: 0.08748137205839157\n",
      "Similarities: tensor([[1.3594, 0.0000, 0.0000, 0.2928, 0.0000],\n",
      "        [0.1246, 1.3810, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4237, 0.0000, 0.0000],\n",
      "        [0.1873, 0.0000, 0.0000, 1.3826, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3717]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.09066104888916\n",
      "Batch 81, Loss: 3.09066104888916\n",
      "Processing Batch 82/157\n",
      "Batch 82 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012060667388141155, Std: 0.0868670865893364\n",
      "z_j Mean: 0.0122365178540349, Std: 0.08613670617341995\n",
      "Similarities: tensor([[1.4161, 0.4481, 0.0000, 0.6834, 1.1380],\n",
      "        [0.3287, 1.4049, 0.4274, 0.6415, 0.0000],\n",
      "        [0.0000, 0.3520, 1.4282, 0.2131, 0.0000],\n",
      "        [0.7382, 0.7707, 0.2879, 1.4013, 0.4637],\n",
      "        [1.2166, 0.0000, 0.0000, 0.4173, 1.4283]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0843939781188965\n",
      "Batch 82, Loss: 3.0843939781188965\n",
      "Processing Batch 83/157\n",
      "Batch 83 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011331234127283096, Std: 0.08766436576843262\n",
      "z_j Mean: 0.011691344901919365, Std: 0.08691757172346115\n",
      "Similarities: tensor([[1.0825, 0.0000, 0.0000, 0.0000, 0.0073],\n",
      "        [0.0000, 1.4188, 0.1036, 0.0753, 0.1174],\n",
      "        [0.0000, 0.0318, 1.4055, 0.0000, 0.1255],\n",
      "        [0.0000, 0.1478, 0.0000, 1.4008, 0.0000],\n",
      "        [0.5071, 0.2032, 0.1608, 0.0000, 1.3336]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1236109733581543\n",
      "Batch 83, Loss: 3.1236109733581543\n",
      "Processing Batch 84/157\n",
      "Batch 84 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011813574470579624, Std: 0.08760066330432892\n",
      "z_j Mean: 0.01183704100549221, Std: 0.0868978500366211\n",
      "Similarities: tensor([[1.4243, 0.0000, 1.3594, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4228, 0.0000, 0.0000, 0.0000],\n",
      "        [1.0808, 0.0000, 1.1134, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0796, 1.2133],\n",
      "        [0.0000, 0.0000, 0.0000, 1.1565, 1.2798]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.085512399673462\n",
      "Batch 84, Loss: 3.085512399673462\n",
      "Processing Batch 85/157\n",
      "Batch 85 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01193319447338581, Std: 0.08617926388978958\n",
      "z_j Mean: 0.011983685195446014, Std: 0.08546094596385956\n",
      "Similarities: tensor([[1.2530, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.1797, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0927, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3847]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0970938205718994\n",
      "Batch 85, Loss: 3.0970938205718994\n",
      "Processing Batch 86/157\n",
      "Batch 86 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011412663385272026, Std: 0.08765380084514618\n",
      "z_j Mean: 0.011501058004796505, Std: 0.08764224499464035\n",
      "Similarities: tensor([[1.4206, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4284, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3802, 0.0000, 0.7345],\n",
      "        [0.0000, 0.0000, 0.0053, 1.3056, 0.0000],\n",
      "        [0.0000, 0.0000, 0.9287, 0.0000, 1.4286]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0580334663391113\n",
      "Batch 86, Loss: 3.0580334663391113\n",
      "Processing Batch 87/157\n",
      "Batch 87 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011094963178038597, Std: 0.08699572086334229\n",
      "z_j Mean: 0.011145098134875298, Std: 0.08557435870170593\n",
      "Similarities: tensor([[1.3296, 0.0000, 0.5024, 0.0554, 0.3782],\n",
      "        [0.0000, 1.4032, 0.0000, 0.0000, 0.0000],\n",
      "        [1.0918, 0.0000, 1.4064, 0.0000, 1.1601],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4286, 0.0000],\n",
      "        [0.8426, 0.0000, 1.1839, 0.0000, 1.3872]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0765931606292725\n",
      "Batch 87, Loss: 3.0765931606292725\n",
      "Processing Batch 88/157\n",
      "Batch 88 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011460382491350174, Std: 0.08764757961034775\n",
      "z_j Mean: 0.011145070195198059, Std: 0.08557435870170593\n",
      "Similarities: tensor([[1.3508, 0.0155, 1.0403, 0.0000, 0.0000],\n",
      "        [0.0019, 0.8791, 0.0000, 0.0000, 0.0000],\n",
      "        [1.3367, 0.0138, 1.4242, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4087, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3995]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.110696315765381\n",
      "Batch 88, Loss: 3.110696315765381\n",
      "Processing Batch 89/157\n",
      "Batch 89 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011415128596127033, Std: 0.0869542807340622\n",
      "z_j Mean: 0.011618191376328468, Std: 0.0869273841381073\n",
      "Similarities: tensor([[1.4279, 0.0000, 0.0000, 0.0000, 1.3348],\n",
      "        [0.0000, 1.4263, 0.5405, 0.0000, 0.0000],\n",
      "        [0.0000, 0.7282, 1.4024, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4099, 0.0000],\n",
      "        [0.8767, 0.0000, 0.0000, 0.0000, 1.2180]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0892908573150635\n",
      "Batch 89, Loss: 3.0892908573150635\n",
      "Processing Batch 90/157\n",
      "Batch 90 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011242572218179703, Std: 0.0876757800579071\n",
      "z_j Mean: 0.011292539536952972, Std: 0.08766936510801315\n",
      "Similarities: tensor([[1.4051, 0.0000, 0.0000, 0.8863, 0.0000],\n",
      "        [0.0000, 1.3934, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3846, 0.0000, 0.0000],\n",
      "        [0.6800, 0.0000, 0.0000, 1.3976, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4286]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0995395183563232\n",
      "Batch 90, Loss: 3.0995395183563232\n",
      "Processing Batch 91/157\n",
      "Batch 91 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.010928655043244362, Std: 0.08771546930074692\n",
      "z_j Mean: 0.010572854429483414, Std: 0.08635672926902771\n",
      "Similarities: tensor([[1.2855, 0.0060, 0.3214, 0.0000, 1.3046],\n",
      "        [0.1346, 1.3984, 0.6179, 0.0000, 0.0042],\n",
      "        [0.6241, 0.5904, 1.2832, 0.0000, 0.7639],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [1.1023, 0.0000, 0.5043, 0.0000, 1.4272]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1236655712127686\n",
      "Batch 91, Loss: 3.1236655712127686\n",
      "Processing Batch 92/157\n",
      "Batch 92 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011739828623831272, Std: 0.08691103756427765\n",
      "z_j Mean: 0.011753397062420845, Std: 0.08690919727087021\n",
      "Similarities: tensor([[1.4282e+00, 5.1253e-03, 0.0000e+00, 0.0000e+00, 2.9982e-04],\n",
      "        [0.0000e+00, 1.1446e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 1.3389e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 1.3037e+00, 1.1087e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0112e+00, 1.2434e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0266735553741455\n",
      "Batch 92, Loss: 3.0266735553741455\n",
      "Processing Batch 93/157\n",
      "Batch 93 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011963386088609695, Std: 0.08758033066987991\n",
      "z_j Mean: 0.011444013565778732, Std: 0.08695049583911896\n",
      "Similarities: tensor([[1.4189, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0288, 1.1641, 0.0000, 0.4284, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3282, 0.0000, 1.0446],\n",
      "        [0.0000, 0.0000, 0.3376, 1.3132, 0.3402],\n",
      "        [0.0000, 0.0000, 0.7835, 0.0000, 1.3590]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.094755172729492\n",
      "Batch 93, Loss: 3.094755172729492\n",
      "Processing Batch 94/157\n",
      "Batch 94 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011815029196441174, Std: 0.08690084517002106\n",
      "z_j Mean: 0.011745218187570572, Std: 0.08620508760213852\n",
      "Similarities: tensor([[1.4096e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [9.7916e-04, 0.0000e+00, 1.3645e+00, 1.0342e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 1.1271e+00, 1.4241e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.4088e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1434781551361084\n",
      "Batch 94, Loss: 3.1434781551361084\n",
      "Processing Batch 95/157\n",
      "Batch 95 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011640576645731926, Std: 0.08692438900470734\n",
      "z_j Mean: 0.01172834075987339, Std: 0.08761211484670639\n",
      "Similarities: tensor([[1.3908, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4260, 0.5758, 0.0000, 0.0000],\n",
      "        [0.0000, 0.7128, 1.4181, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3654, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4254]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1083269119262695\n",
      "Batch 95, Loss: 3.1083269119262695\n",
      "Processing Batch 96/157\n",
      "Batch 96 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011374030262231827, Std: 0.08695967495441437\n",
      "z_j Mean: 0.011356374248862267, Std: 0.08696197718381882\n",
      "Similarities: tensor([[1.4286, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4219, 0.8521, 1.1611, 0.0000],\n",
      "        [0.0000, 1.0933, 1.3543, 1.3436, 0.0000],\n",
      "        [0.0000, 1.1998, 1.2764, 1.4169, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0115, 0.0000, 1.1556]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.157711982727051\n",
      "Batch 96, Loss: 3.157711982727051\n",
      "Processing Batch 97/157\n",
      "Batch 97 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011825698427855968, Std: 0.08689939230680466\n",
      "z_j Mean: 0.011328076012432575, Std: 0.08626089990139008\n",
      "Similarities: tensor([[1.2967, 0.0000, 0.0140, 0.0000, 0.0272],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1310, 0.0000, 1.3295, 0.6009, 0.0000],\n",
      "        [0.0080, 0.0000, 0.1036, 0.6148, 0.0000],\n",
      "        [0.0356, 0.0000, 0.0000, 0.0000, 1.4285]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1497983932495117\n",
      "Batch 97, Loss: 3.1497983932495117\n",
      "Processing Batch 98/157\n",
      "Batch 98 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011659880168735981, Std: 0.0869217962026596\n",
      "z_j Mean: 0.011768323369324207, Std: 0.08620193600654602\n",
      "Similarities: tensor([[1.3471, 0.0000, 0.5843, 0.0000, 1.2842],\n",
      "        [0.0592, 1.3864, 0.1312, 0.0086, 0.0613],\n",
      "        [0.4903, 0.0000, 1.1297, 0.0000, 0.5079],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3816, 0.0000],\n",
      "        [1.1700, 0.0000, 0.1556, 0.0000, 1.3579]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1015594005584717\n",
      "Batch 98, Loss: 3.1015594005584717\n",
      "Processing Batch 99/157\n",
      "Batch 99 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011884578503668308, Std: 0.08759105205535889\n",
      "z_j Mean: 0.011578790843486786, Std: 0.08622759580612183\n",
      "Similarities: tensor([[1.3993, 0.3162, 0.0000, 0.2044, 0.0997],\n",
      "        [0.0690, 1.4005, 0.0000, 1.3330, 0.2908],\n",
      "        [0.0000, 0.0000, 1.4243, 0.0000, 0.0000],\n",
      "        [0.0600, 1.2910, 0.0000, 1.4118, 0.2527],\n",
      "        [0.0463, 0.6183, 0.0000, 0.3997, 1.3404]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.103468894958496\n",
      "Batch 99, Loss: 3.103468894958496\n",
      "Processing Batch 100/157\n",
      "Batch 100 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011901361867785454, Std: 0.08758877217769623\n",
      "z_j Mean: 0.012027719989418983, Std: 0.08757151663303375\n",
      "Similarities: tensor([[1.4168, 0.0000, 0.0000, 0.5767, 0.0000],\n",
      "        [0.0000, 1.4232, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3828, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3171, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4286]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0624594688415527\n",
      "Batch 100, Loss: 3.0624594688415527\n",
      "Processing Batch 101/157\n",
      "Batch 101 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011926811188459396, Std: 0.08758531510829926\n",
      "z_j Mean: 0.012233804911375046, Std: 0.08684287220239639\n",
      "Similarities: tensor([[1.4271, 0.7847, 1.1862, 0.0000, 0.0000],\n",
      "        [0.7938, 1.4270, 0.3039, 0.0000, 0.0000],\n",
      "        [1.2170, 0.2150, 1.4107, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4274, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3853]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.103478193283081\n",
      "Batch 101, Loss: 3.103478193283081\n",
      "Processing Batch 102/157\n",
      "Batch 102 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011710098013281822, Std: 0.08761455118656158\n",
      "z_j Mean: 0.011747014708817005, Std: 0.08691006898880005\n",
      "Similarities: tensor([[1.4080, 1.4136, 0.7541, 0.0000, 0.0057],\n",
      "        [1.4010, 1.4286, 0.7501, 0.0000, 0.0000],\n",
      "        [0.7441, 0.5018, 1.3722, 0.0000, 1.0060],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2078, 0.0000, 1.0591, 0.0000, 1.4080]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1055028438568115\n",
      "Batch 102, Loss: 3.1055028438568115\n",
      "Processing Batch 103/157\n",
      "Batch 103 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012665282003581524, Std: 0.08748156577348709\n",
      "z_j Mean: 0.012601621448993683, Std: 0.0874907597899437\n",
      "Similarities: tensor([[1.3964, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4072, 0.4153, 0.0000, 1.4247],\n",
      "        [0.0000, 1.0331, 1.1996, 0.0000, 0.7614],\n",
      "        [0.1978, 0.0000, 0.0000, 1.4260, 0.0000],\n",
      "        [0.0000, 1.3640, 0.3227, 0.0000, 1.4065]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.084429979324341\n",
      "Batch 103, Loss: 3.084429979324341\n",
      "Processing Batch 104/157\n",
      "Batch 104 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011612809263169765, Std: 0.0869281068444252\n",
      "z_j Mean: 0.011771970428526402, Std: 0.08690668642520905\n",
      "Similarities: tensor([[1.1998e+00, 8.0202e-01, 1.7957e-04, 0.0000e+00, 1.5219e-01],\n",
      "        [1.4168e+00, 1.2314e+00, 0.0000e+00, 0.0000e+00, 8.8651e-02],\n",
      "        [0.0000e+00, 0.0000e+00, 1.2828e+00, 0.0000e+00, 1.3949e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 1.3020e+00, 3.6226e-03],\n",
      "        [2.1274e-01, 3.4103e-02, 1.2493e+00, 0.0000e+00, 1.3942e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.051771640777588\n",
      "Batch 104, Loss: 3.051771640777588\n",
      "Processing Batch 105/157\n",
      "Batch 105 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011996675282716751, Std: 0.08757577836513519\n",
      "z_j Mean: 0.012322699651122093, Std: 0.08753048628568649\n",
      "Similarities: tensor([[1.4274, 1.0954, 0.1090, 0.0000, 0.4152],\n",
      "        [1.2395, 1.4036, 0.7877, 0.0000, 0.4961],\n",
      "        [0.1529, 0.9685, 1.4246, 0.0000, 0.3260],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0107, 0.0000],\n",
      "        [0.3568, 0.3935, 0.2265, 0.0000, 1.4227]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285035133361816]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0804269313812256\n",
      "Batch 105, Loss: 3.0804269313812256\n",
      "Processing Batch 106/157\n",
      "Batch 106 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012143384665250778, Std: 0.08755555748939514\n",
      "z_j Mean: 0.012215500697493553, Std: 0.08754551410675049\n",
      "Similarities: tensor([[1.4084, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4134, 0.0000, 0.0000, 0.3806],\n",
      "        [0.0000, 0.0000, 1.4246, 0.3391, 0.0000],\n",
      "        [0.0000, 0.0000, 0.3466, 1.4185, 0.0000],\n",
      "        [0.0000, 0.2564, 0.0000, 0.0000, 1.4145]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.082695245742798\n",
      "Batch 106, Loss: 3.082695245742798\n",
      "Processing Batch 107/157\n",
      "Batch 107 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01225079782307148, Std: 0.08754058182239532\n",
      "z_j Mean: 0.012091629207134247, Std: 0.08756271004676819\n",
      "Similarities: tensor([[1.0908, 0.6483, 0.0000, 0.0000, 0.0000],\n",
      "        [0.7954, 1.2532, 0.6664, 0.0000, 0.0000],\n",
      "        [0.0000, 1.0468, 1.4070, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3646, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3891]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0522916316986084\n",
      "Batch 107, Loss: 3.0522916316986084\n",
      "Processing Batch 108/157\n",
      "Batch 108 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012885446660220623, Std: 0.08744940906763077\n",
      "z_j Mean: 0.012803370133042336, Std: 0.087461456656456\n",
      "Similarities: tensor([[1.3682, 0.0000, 0.0000, 0.0000, 1.3994],\n",
      "        [0.0000, 1.3606, 0.0000, 0.9474, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4089, 0.6028, 0.0000],\n",
      "        [0.0000, 1.0702, 0.5321, 1.3918, 0.0000],\n",
      "        [1.2718, 0.0000, 0.0000, 0.0000, 1.4282]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0513880252838135\n",
      "Batch 108, Loss: 3.0513880252838135\n",
      "Processing Batch 109/157\n",
      "Batch 109 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01248136442154646, Std: 0.08750800043344498\n",
      "z_j Mean: 0.01266191340982914, Std: 0.08748205751180649\n",
      "Similarities: tensor([[1.4110, 0.0761, 0.0000, 0.9325, 0.5827],\n",
      "        [0.0977, 1.3586, 0.4661, 0.0000, 0.0000],\n",
      "        [0.0000, 0.5028, 1.4247, 0.0000, 0.0000],\n",
      "        [0.9344, 0.0000, 0.0000, 1.3762, 0.0000],\n",
      "        [0.6550, 0.0000, 0.0000, 0.0000, 1.2671]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.056765556335449\n",
      "Batch 109, Loss: 3.056765556335449\n",
      "Processing Batch 110/157\n",
      "Batch 110 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013539087027311325, Std: 0.08664894104003906\n",
      "z_j Mean: 0.013543910346925259, Std: 0.08734983205795288\n",
      "Similarities: tensor([[1.2852, 0.1687, 0.6130, 0.6197, 0.0000],\n",
      "        [0.0914, 1.1500, 0.0000, 0.0000, 0.0194],\n",
      "        [0.7929, 0.0000, 1.3590, 0.9410, 0.0000],\n",
      "        [0.8932, 0.0233, 0.9372, 1.3808, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3024]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.078486204147339\n",
      "Batch 110, Loss: 3.078486204147339\n",
      "Processing Batch 111/157\n",
      "Batch 111 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013186759315431118, Std: 0.08740447461605072\n",
      "z_j Mean: 0.013189407996833324, Std: 0.0867028683423996\n",
      "Similarities: tensor([[1.2150, 0.3011, 0.0182, 0.0000, 0.2396],\n",
      "        [0.2246, 1.1275, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1286, 0.0000, 1.3756, 0.0000, 0.0347],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4123, 0.0000],\n",
      "        [0.4485, 0.0000, 0.0154, 0.0000, 1.4055]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0965046882629395\n",
      "Batch 111, Loss: 3.0965046882629395\n",
      "Processing Batch 112/157\n",
      "Batch 112 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013659326359629631, Std: 0.08663006871938705\n",
      "z_j Mean: 0.013661273755133152, Std: 0.08662975579500198\n",
      "Similarities: tensor([[1.4250, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3785, 0.0816, 0.0000, 0.8368],\n",
      "        [0.0000, 0.1045, 1.4031, 0.0000, 0.8878],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4275, 0.0000],\n",
      "        [0.0000, 0.8585, 1.0860, 0.0000, 1.4112]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4283052682876587]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.10245680809021\n",
      "Batch 112, Loss: 3.10245680809021\n",
      "Processing Batch 113/157\n",
      "Batch 113 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012461258098483086, Std: 0.08681052178144455\n",
      "z_j Mean: 0.012700310908257961, Std: 0.08677586913108826\n",
      "Similarities: tensor([[1.4286, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4105, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4058, 0.1694, 0.1176],\n",
      "        [0.0000, 0.0000, 0.1843, 1.4154, 0.1186],\n",
      "        [0.0000, 0.0000, 0.1889, 0.2110, 1.4179]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0830087661743164\n",
      "Batch 113, Loss: 3.0830087661743164\n",
      "Processing Batch 114/157\n",
      "Batch 114 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012178851291537285, Std: 0.08614487946033478\n",
      "z_j Mean: 0.012449271976947784, Std: 0.08539435267448425\n",
      "Similarities: tensor([[1.3605, 0.8961, 0.4267, 0.1397, 0.0839],\n",
      "        [1.0933, 1.4117, 0.2260, 0.2583, 0.1039],\n",
      "        [0.8035, 0.3257, 1.1518, 0.0000, 0.0315],\n",
      "        [0.1092, 0.3724, 0.0000, 1.3939, 0.0981],\n",
      "        [0.1054, 0.1689, 0.0368, 0.1059, 1.4159]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1127891540527344\n",
      "Batch 114, Loss: 3.1127891540527344\n",
      "Processing Batch 115/157\n",
      "Batch 115 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012922356836497784, Std: 0.08744395524263382\n",
      "z_j Mean: 0.013121035881340504, Std: 0.08741436153650284\n",
      "Similarities: tensor([[1.4140, 0.8083, 1.0839, 0.5415, 0.8456],\n",
      "        [0.9759, 1.4085, 0.1999, 1.2336, 1.4142],\n",
      "        [1.0248, 0.2062, 1.4182, 0.1137, 0.2127],\n",
      "        [0.6827, 1.3562, 0.0994, 1.4248, 1.3164],\n",
      "        [1.0435, 1.3981, 0.2321, 1.1864, 1.4152]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.105527877807617\n",
      "Batch 115, Loss: 3.105527877807617\n",
      "Processing Batch 116/157\n",
      "Batch 116 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013063326478004456, Std: 0.08672194927930832\n",
      "z_j Mean: 0.012929640710353851, Std: 0.08674199134111404\n",
      "Similarities: tensor([[1.4286e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 7.1131e-01],\n",
      "        [0.0000e+00, 1.0335e+00, 0.0000e+00, 1.2858e+00, 1.7317e-01],\n",
      "        [0.0000e+00, 0.0000e+00, 8.0590e-01, 8.5773e-04, 6.4586e-01],\n",
      "        [0.0000e+00, 7.0790e-01, 3.3947e-01, 1.3310e+00, 2.9737e-01],\n",
      "        [7.0870e-01, 5.9877e-02, 7.9620e-01, 2.7890e-01, 1.3983e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.097534418106079\n",
      "Batch 116, Loss: 3.097534418106079\n",
      "Processing Batch 117/157\n",
      "Batch 117 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01230756938457489, Std: 0.08612658828496933\n",
      "z_j Mean: 0.012283027172088623, Std: 0.08613009005784988\n",
      "Similarities: tensor([[1.4286, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4022, 0.5497, 0.0000, 0.0000],\n",
      "        [0.0000, 0.4719, 1.4225, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4275, 0.0000],\n",
      "        [0.0000, 0.2863, 0.0000, 0.0000, 1.0633]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0892958641052246\n",
      "Batch 117, Loss: 3.0892958641052246\n",
      "Processing Batch 118/157\n",
      "Batch 118 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012091664597392082, Std: 0.08544572442770004\n",
      "z_j Mean: 0.012036677449941635, Std: 0.08545348793268204\n",
      "Similarities: tensor([[1.3289, 0.4057, 0.0822, 0.0261, 0.0000],\n",
      "        [0.6277, 1.3497, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0214, 0.0000, 1.3436, 1.3701, 0.0000],\n",
      "        [0.0059, 0.0000, 1.2724, 1.4039, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.2612]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.139822006225586\n",
      "Batch 118, Loss: 3.139822006225586\n",
      "Processing Batch 119/157\n",
      "Batch 119 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012892385013401508, Std: 0.08604097366333008\n",
      "z_j Mean: 0.013210787437856197, Std: 0.08740084618330002\n",
      "Similarities: tensor([[1.3778, 0.5083, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4451, 1.3304, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4223, 0.2907, 0.0268],\n",
      "        [0.0000, 0.0000, 0.4313, 1.3493, 0.2969],\n",
      "        [0.0000, 0.0000, 0.0167, 0.8511, 1.2844]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0995707511901855\n",
      "Batch 119, Loss: 3.0995707511901855\n",
      "Processing Batch 120/157\n",
      "Batch 120 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01243577804416418, Std: 0.08610816299915314\n",
      "z_j Mean: 0.011870844289660454, Std: 0.08547668904066086\n",
      "Similarities: tensor([[1.3000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4047, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3971, 0.5775, 1.1752],\n",
      "        [0.0000, 0.0926, 0.6553, 1.3296, 0.2898],\n",
      "        [0.0000, 0.0000, 0.9261, 0.4040, 1.3618]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.144833564758301\n",
      "Batch 120, Loss: 3.144833564758301\n",
      "Processing Batch 121/157\n",
      "Batch 121 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01134115643799305, Std: 0.08625917881727219\n",
      "z_j Mean: 0.01139678992331028, Std: 0.08625183999538422\n",
      "Similarities: tensor([[0.8857, 0.0000, 0.0000, 0.7802, 0.2163],\n",
      "        [0.0000, 1.3084, 0.4329, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.2801, 0.0000, 0.0000],\n",
      "        [1.2811, 0.0000, 0.0000, 1.4270, 0.3956],\n",
      "        [0.9528, 0.0000, 0.0000, 1.0613, 1.0782]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.090974807739258\n",
      "Batch 121, Loss: 3.090974807739258\n",
      "Processing Batch 122/157\n",
      "Batch 122 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012436270713806152, Std: 0.08681410551071167\n",
      "z_j Mean: 0.012049652636051178, Std: 0.08401080220937729\n",
      "Similarities: tensor([[1.3712, 0.0000, 0.0000, 0.0000, 0.0181],\n",
      "        [0.0000, 1.3916, 0.0000, 0.0000, 0.9410],\n",
      "        [0.0000, 0.0000, 1.2672, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0346, 0.8277, 0.0000, 0.0000, 1.3995]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1230125427246094\n",
      "Batch 122, Loss: 3.1230125427246094\n",
      "Processing Batch 123/157\n",
      "Batch 123 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012134743854403496, Std: 0.08755674958229065\n",
      "z_j Mean: 0.011791636236011982, Std: 0.08690401911735535\n",
      "Similarities: tensor([[1.3956, 0.0000, 1.3006, 0.0230, 0.3575],\n",
      "        [0.0000, 1.4183, 0.0000, 0.0000, 0.6399],\n",
      "        [1.2030, 0.0000, 1.3762, 0.0635, 0.0000],\n",
      "        [0.0718, 0.0000, 0.0000, 1.2901, 0.0000],\n",
      "        [0.4013, 0.6268, 0.0000, 0.0000, 1.2957]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.033155679702759\n",
      "Batch 123, Loss: 3.033155679702759\n",
      "Processing Batch 124/157\n",
      "Batch 124 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012514578178524971, Std: 0.08750325441360474\n",
      "z_j Mean: 0.012918276712298393, Std: 0.08744456619024277\n",
      "Similarities: tensor([[1.2953e+00, 0.0000e+00, 1.7262e-02, 4.3836e-01, 0.0000e+00],\n",
      "        [0.0000e+00, 1.4256e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [1.1528e-03, 0.0000e+00, 1.3310e+00, 3.4566e-01, 1.1892e+00],\n",
      "        [2.9495e-02, 0.0000e+00, 4.0439e-01, 1.4111e+00, 2.1437e-01],\n",
      "        [0.0000e+00, 0.0000e+00, 1.3182e+00, 1.5528e-01, 1.4034e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.054896831512451\n",
      "Batch 124, Loss: 3.054896831512451\n",
      "Processing Batch 125/157\n",
      "Batch 125 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012430371716618538, Std: 0.08610894531011581\n",
      "z_j Mean: 0.012525530532002449, Std: 0.08680126816034317\n",
      "Similarities: tensor([[1.4275, 0.1838, 1.2909, 0.4619, 0.0000],\n",
      "        [0.2115, 1.3962, 0.1912, 1.1042, 0.0000],\n",
      "        [1.4261, 0.1836, 1.3257, 0.4614, 0.0000],\n",
      "        [0.5169, 1.0894, 0.4674, 1.3834, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3899]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1212995052337646\n",
      "Batch 125, Loss: 3.1212995052337646\n",
      "Processing Batch 126/157\n",
      "Batch 126 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011811351403594017, Std: 0.0876009613275528\n",
      "z_j Mean: 0.011919056996703148, Std: 0.08688663691282272\n",
      "Similarities: tensor([[1.3681, 0.0000, 1.3875, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3761, 0.0000, 0.0000, 0.0000],\n",
      "        [1.2215, 0.0000, 1.3965, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4286, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4237]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0848894119262695\n",
      "Batch 126, Loss: 3.0848894119262695\n",
      "Processing Batch 127/157\n",
      "Batch 127 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01169614028185606, Std: 0.08761642128229141\n",
      "z_j Mean: 0.012039698660373688, Std: 0.0868699923157692\n",
      "Similarities: tensor([[1.3909e+00, 0.0000e+00, 8.2200e-02, 0.0000e+00, 0.0000e+00],\n",
      "        [2.0891e-04, 1.3777e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [1.3352e-02, 0.0000e+00, 1.4257e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 1.4101e+00, 0.0000e+00],\n",
      "        [1.3372e+00, 0.0000e+00, 8.2894e-02, 0.0000e+00, 0.0000e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.145153522491455\n",
      "Batch 127, Loss: 3.145153522491455\n",
      "Processing Batch 128/157\n",
      "Batch 128 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.0123702771961689, Std: 0.08682353049516678\n",
      "z_j Mean: 0.012204298749566078, Std: 0.08614128082990646\n",
      "Similarities: tensor([[1.2367, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4286, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4008, 0.0000, 0.1150],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4116, 0.0000],\n",
      "        [0.0000, 0.0000, 0.1331, 0.0000, 1.3612]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0834243297576904\n",
      "Batch 128, Loss: 3.0834243297576904\n",
      "Processing Batch 129/157\n",
      "Batch 129 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01184520311653614, Std: 0.08619140088558197\n",
      "z_j Mean: 0.012588988989591599, Std: 0.08749258518218994\n",
      "Similarities: tensor([[1.4143, 1.2388, 0.0000, 0.1151, 0.0000],\n",
      "        [1.1648, 1.3465, 0.0000, 0.0192, 0.0000],\n",
      "        [0.0000, 0.0000, 1.1360, 0.0000, 0.0000],\n",
      "        [0.1509, 0.0940, 0.0000, 1.4234, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4276]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.105414390563965\n",
      "Batch 129, Loss: 3.105414390563965\n",
      "Processing Batch 130/157\n",
      "Batch 130 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011620491743087769, Std: 0.08551108837127686\n",
      "z_j Mean: 0.01218993216753006, Std: 0.08684904128313065\n",
      "Similarities: tensor([[1.4258, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4286, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4261, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4241, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3962]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0783355236053467\n",
      "Batch 130, Loss: 3.0783355236053467\n",
      "Processing Batch 131/157\n",
      "Batch 131 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011741213500499725, Std: 0.0876103937625885\n",
      "z_j Mean: 0.011584929190576077, Std: 0.08693181723356247\n",
      "Similarities: tensor([[1.4270, 0.0000, 0.9547, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4286, 0.0000, 0.0000, 0.0000],\n",
      "        [0.8317, 0.1370, 1.3886, 0.0463, 0.0000],\n",
      "        [0.0000, 0.0000, 0.2042, 1.3742, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3481]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.081068277359009\n",
      "Batch 131, Loss: 3.081068277359009\n",
      "Processing Batch 132/157\n",
      "Batch 132 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011734046041965485, Std: 0.08691181242465973\n",
      "z_j Mean: 0.011600746773183346, Std: 0.08551377058029175\n",
      "Similarities: tensor([[1.3368, 0.9028, 0.0000, 0.0000, 0.0000],\n",
      "        [0.9843, 1.3496, 0.2080, 0.0000, 0.0000],\n",
      "        [0.0096, 0.1215, 1.3659, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4147, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3091]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1352453231811523\n",
      "Batch 132, Loss: 3.1352453231811523\n",
      "Processing Batch 133/157\n",
      "Batch 133 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01122482493519783, Std: 0.08697905391454697\n",
      "z_j Mean: 0.011337990872561932, Std: 0.08625960350036621\n",
      "Similarities: tensor([[1.3634, 0.0000, 0.0000, 0.0000, 0.0038],\n",
      "        [0.5622, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3329, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4251, 0.2884],\n",
      "        [0.0475, 0.0000, 0.0000, 0.2365, 1.3457]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.064685344696045\n",
      "Batch 133, Loss: 3.064685344696045\n",
      "Processing Batch 134/157\n",
      "Batch 134 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01099921204149723, Std: 0.08559323102235794\n",
      "z_j Mean: 0.011150984093546867, Std: 0.08628396689891815\n",
      "Similarities: tensor([[1.4286, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3149, 0.1132, 0.0027, 0.0038],\n",
      "        [0.0000, 0.3614, 1.3858, 0.2963, 0.0000],\n",
      "        [0.0000, 0.0000, 0.2691, 1.3562, 0.5966],\n",
      "        [0.0000, 0.0000, 0.0000, 0.4396, 1.4051]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.153092861175537\n",
      "Batch 134, Loss: 3.153092861175537\n",
      "Processing Batch 135/157\n",
      "Batch 135 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01048728171736002, Std: 0.086367167532444\n",
      "z_j Mean: 0.0104584451764822, Std: 0.08637066185474396\n",
      "Similarities: tensor([[1.4195, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4212, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4073, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3436, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3946]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.093820571899414\n",
      "Batch 135, Loss: 3.093820571899414\n",
      "Processing Batch 136/157\n",
      "Batch 136 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.010457354597747326, Std: 0.08566112816333771\n",
      "z_j Mean: 0.010161999613046646, Std: 0.08353241533041\n",
      "Similarities: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3981, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.9275, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4130, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4286]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1307830810546875\n",
      "Batch 136, Loss: 3.1307830810546875\n",
      "Processing Batch 137/157\n",
      "Batch 137 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.010822538286447525, Std: 0.08489977568387985\n",
      "z_j Mean: 0.010704563930630684, Std: 0.08419279009103775\n",
      "Similarities: tensor([[1.3882, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3432, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4286, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4280]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1803574562072754\n",
      "Batch 137, Loss: 3.1803574562072754\n",
      "Processing Batch 138/157\n",
      "Batch 138 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01102658361196518, Std: 0.08629996329545975\n",
      "z_j Mean: 0.011012060567736626, Std: 0.08559157699346542\n",
      "Similarities: tensor([[1.4209, 0.0000, 0.0000, 0.0000, 0.0641],\n",
      "        [0.0000, 1.2731, 0.0000, 0.0000, 0.2737],\n",
      "        [0.0000, 0.0000, 1.4190, 0.4302, 0.0000],\n",
      "        [0.0000, 0.0000, 0.1628, 1.0849, 0.0000],\n",
      "        [0.0000, 0.1243, 0.0000, 0.0000, 1.2119]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1063084602355957\n",
      "Batch 138, Loss: 3.1063084602355957\n",
      "Processing Batch 139/157\n",
      "Batch 139 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.010601378045976162, Std: 0.08775563538074493\n",
      "z_j Mean: 0.010840469971299171, Std: 0.0877264067530632\n",
      "Similarities: tensor([[1.4139, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.2502, 0.0000, 0.0000, 0.8036],\n",
      "        [0.0000, 0.0000, 1.3998, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.2082, 0.2272],\n",
      "        [0.0000, 1.1558, 0.0000, 0.0000, 1.3628]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0470709800720215\n",
      "Batch 139, Loss: 3.0470709800720215\n",
      "Processing Batch 140/157\n",
      "Batch 140 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.010901276022195816, Std: 0.08702021092176437\n",
      "z_j Mean: 0.01084993313997984, Std: 0.08702662587165833\n",
      "Similarities: tensor([[1.4099, 0.0000, 0.0000, 0.0000, 0.2140],\n",
      "        [0.0000, 1.3789, 0.4957, 0.0000, 0.0000],\n",
      "        [0.0000, 0.5623, 1.3344, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0427, 0.0000, 0.0000, 0.0000, 1.4286]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.176593065261841\n",
      "Batch 140, Loss: 3.176593065261841\n",
      "Processing Batch 141/157\n",
      "Batch 141 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.010593786835670471, Std: 0.0870581790804863\n",
      "z_j Mean: 0.010738609358668327, Std: 0.08704043179750443\n",
      "Similarities: tensor([[1.3381, 0.0849, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2384, 1.4258, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3818, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4208, 0.2462],\n",
      "        [0.0000, 0.0000, 0.0000, 0.3684, 1.3920]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1070003509521484\n",
      "Batch 141, Loss: 3.1070003509521484\n",
      "Processing Batch 142/157\n",
      "Batch 142 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011113129556179047, Std: 0.08769228309392929\n",
      "z_j Mean: 0.01095644012093544, Std: 0.08630889654159546\n",
      "Similarities: tensor([[1.3450, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4246, 0.0000, 0.0000, 1.1922],\n",
      "        [0.0000, 0.0000, 1.4251, 0.7862, 0.0000],\n",
      "        [0.0000, 0.0000, 0.6829, 1.2950, 0.0000],\n",
      "        [0.0000, 1.1291, 0.0558, 0.0000, 1.4166]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1022472381591797\n",
      "Batch 142, Loss: 3.1022472381591797\n",
      "Processing Batch 143/157\n",
      "Batch 143 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.010939819738268852, Std: 0.0856008380651474\n",
      "z_j Mean: 0.011089814826846123, Std: 0.0862918570637703\n",
      "Similarities: tensor([[1.4215, 0.6546, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5330, 1.3186, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4264, 0.0000, 0.0377],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4262, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0357, 0.0000, 1.3180]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1631178855895996\n",
      "Batch 143, Loss: 3.1631178855895996\n",
      "Processing Batch 144/157\n",
      "Batch 144 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01197088323533535, Std: 0.08757930248975754\n",
      "z_j Mean: 0.01199578307569027, Std: 0.08617056906223297\n",
      "Similarities: tensor([[1.4105, 0.0362, 0.0000, 0.1163, 0.0312],\n",
      "        [0.0000, 1.3497, 0.0000, 0.0000, 0.6944],\n",
      "        [0.0000, 0.0000, 1.4278, 0.0000, 0.0000],\n",
      "        [0.1318, 0.1423, 0.0000, 1.3625, 0.1229],\n",
      "        [0.0644, 0.8669, 0.0000, 0.2237, 1.2533]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1012730598449707\n",
      "Batch 144, Loss: 3.1012730598449707\n",
      "Processing Batch 145/157\n",
      "Batch 145 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011554546654224396, Std: 0.08763520419597626\n",
      "z_j Mean: 0.011548575945198536, Std: 0.08693666011095047\n",
      "Similarities: tensor([[1.4243, 0.0000, 0.0000, 0.0000, 0.2276],\n",
      "        [0.0000, 1.4274, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4241, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3644, 0.0000],\n",
      "        [0.3123, 0.0000, 0.0000, 0.0000, 1.3971]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0669541358947754\n",
      "Batch 145, Loss: 3.0669541358947754\n",
      "Processing Batch 146/157\n",
      "Batch 146 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011167198419570923, Std: 0.08628187328577042\n",
      "z_j Mean: 0.011148011311888695, Std: 0.08768785744905472\n",
      "Similarities: tensor([[1.4021, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4286, 1.4220, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3437, 1.3375, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4220, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0638, 0.0000, 1.3975]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.067188024520874\n",
      "Batch 146, Loss: 3.067188024520874\n",
      "Processing Batch 147/157\n",
      "Batch 147 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011501751840114594, Std: 0.08694286644458771\n",
      "z_j Mean: 0.011124867014586926, Std: 0.08628734201192856\n",
      "Similarities: tensor([[1.4286, 0.0000, 0.0000, 1.2271, 0.0000],\n",
      "        [0.0000, 1.4286, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4272, 0.0000, 0.2149],\n",
      "        [1.1177, 0.0000, 0.0000, 1.4116, 0.0000],\n",
      "        [0.0000, 0.0000, 0.2277, 0.0000, 1.4192]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.105825424194336\n",
      "Batch 147, Loss: 3.105825424194336\n",
      "Processing Batch 148/157\n",
      "Batch 148 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011317528784275055, Std: 0.08555171638727188\n",
      "z_j Mean: 0.01186520978808403, Std: 0.08618864417076111\n",
      "Similarities: tensor([[1.2844, 0.0000, 0.0000, 0.0000, 0.0765],\n",
      "        [0.0000, 1.4265, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.4219, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.1770, 0.0000],\n",
      "        [0.6563, 0.0000, 0.0000, 0.0000, 1.1036]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.120844841003418\n",
      "Batch 148, Loss: 3.120844841003418\n",
      "Processing Batch 149/157\n",
      "Batch 149 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011498706415295601, Std: 0.08694327622652054\n",
      "z_j Mean: 0.011551429517567158, Std: 0.08693628013134003\n",
      "Similarities: tensor([[1.3233, 0.0000, 0.0000, 0.8920, 0.0000],\n",
      "        [0.0000, 1.3923, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4057, 0.0000, 0.0000],\n",
      "        [0.8704, 0.0000, 0.0000, 1.3361, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4142]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.09021258354187\n",
      "Batch 149, Loss: 3.09021258354187\n",
      "Processing Batch 150/157\n",
      "Batch 150 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011191505938768387, Std: 0.08768230676651001\n",
      "z_j Mean: 0.011669173836708069, Std: 0.08762001991271973\n",
      "Similarities: tensor([[1.4063, 0.0000, 0.0107, 0.1953, 0.0000],\n",
      "        [0.0000, 1.4030, 0.0000, 0.3702, 0.0000],\n",
      "        [0.0138, 0.0000, 1.4102, 0.0000, 0.6561],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0189, 0.6289],\n",
      "        [0.0069, 0.0000, 0.8615, 0.5296, 1.3414]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0694711208343506\n",
      "Batch 150, Loss: 3.0694711208343506\n",
      "Processing Batch 151/157\n",
      "Batch 151 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011903908103704453, Std: 0.08758842945098877\n",
      "z_j Mean: 0.011960037052631378, Std: 0.08688100427389145\n",
      "Similarities: tensor([[1.3730, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3723, 0.3818, 1.3177, 0.0000],\n",
      "        [0.0000, 0.4668, 1.3639, 0.0000, 0.0000],\n",
      "        [0.0000, 0.9686, 0.0000, 1.2439, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4286]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.101508855819702\n",
      "Batch 151, Loss: 3.101508855819702\n",
      "Processing Batch 152/157\n",
      "Batch 152 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011403419077396393, Std: 0.08765500038862228\n",
      "z_j Mean: 0.011791038326919079, Std: 0.08760370314121246\n",
      "Similarities: tensor([[1.3700, 0.0000, 0.0000, 0.0000, 1.3004],\n",
      "        [0.0000, 1.0703, 0.5348, 0.0000, 0.0000],\n",
      "        [0.0000, 1.0120, 1.4240, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3446, 0.0000],\n",
      "        [1.1650, 0.0000, 0.0000, 0.0000, 1.3949]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.103527545928955\n",
      "Batch 152, Loss: 3.103527545928955\n",
      "Processing Batch 153/157\n",
      "Batch 153 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01155753992497921, Std: 0.08763480931520462\n",
      "z_j Mean: 0.011409522965550423, Std: 0.08695501834154129\n",
      "Similarities: tensor([[1.4194, 0.0000, 0.0000, 0.4290, 0.0000],\n",
      "        [0.0000, 1.4286, 1.2784, 0.0000, 0.0000],\n",
      "        [0.0000, 0.8962, 1.2921, 0.0000, 0.0000],\n",
      "        [0.4235, 0.0000, 0.0000, 1.4041, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4122]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1094908714294434\n",
      "Batch 153, Loss: 3.1094908714294434\n",
      "Processing Batch 154/157\n",
      "Batch 154 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01140416506677866, Std: 0.08765490353107452\n",
      "z_j Mean: 0.011420787312090397, Std: 0.08765274286270142\n",
      "Similarities: tensor([[1.4051, 0.0000, 0.0000, 0.7364, 0.0000],\n",
      "        [0.0000, 1.1170, 0.0000, 0.8092, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5068, 0.7625, 0.0000, 1.3870, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4278]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.081916570663452\n",
      "Batch 154, Loss: 3.081916570663452\n",
      "Processing Batch 155/157\n",
      "Batch 155 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011655360460281372, Std: 0.08762186020612717\n",
      "z_j Mean: 0.01176820881664753, Std: 0.0876067653298378\n",
      "Similarities: tensor([[1.0960, 0.0000, 0.0000, 0.9599, 0.0000],\n",
      "        [0.0000, 1.4286, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4276, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.9590, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3495]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0843982696533203\n",
      "Batch 155, Loss: 3.0843982696533203\n",
      "Processing Batch 156/157\n",
      "Batch 156 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01165976282209158, Std: 0.08762127161026001\n",
      "z_j Mean: 0.01186266727745533, Std: 0.08759403228759766\n",
      "Similarities: tensor([[1.3641, 0.0000, 0.0000, 0.0000, 0.5263],\n",
      "        [0.0000, 1.0512, 0.0000, 0.1016, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3890, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4146, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3319]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.043137311935425\n",
      "Batch 156, Loss: 3.043137311935425\n",
      "Processing Batch 157/157\n",
      "Batch 157 loaded: x.shape=torch.Size([16, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([16, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([16, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([16, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([16, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([16, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011342141777276993, Std: 0.08767901360988617\n",
      "z_j Mean: 0.011314848437905312, Std: 0.08768254518508911\n",
      "Similarities: tensor([[1.3641, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4285, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3204, 0.6029],\n",
      "        [0.0000, 0.0000, 0.0000, 0.8619, 1.1957]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285483360290527]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 1.816150426864624\n",
      "Batch 157, Loss: 1.816150426864624\n",
      "Epoch 6/10, Loss: 3.0816\n",
      "Entered Epoch 7\n",
      "Starting Training Loop with 157 batches.\n",
      "Processing Batch 1/157\n",
      "Batch 1 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011848285794258118, Std: 0.08759596943855286\n",
      "z_j Mean: 0.011758835054934025, Std: 0.08690846711397171\n",
      "Similarities: tensor([[1.4286, 0.0000, 0.0000, 0.0000, 0.9926],\n",
      "        [0.0000, 1.3705, 0.0000, 0.0184, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3404, 0.0018, 0.0000],\n",
      "        [0.0000, 0.0799, 0.0000, 0.7068, 0.0000],\n",
      "        [1.2269, 0.0000, 0.0000, 0.0000, 1.3450]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.103715419769287\n",
      "Batch 1, Loss: 3.103715419769287\n",
      "Processing Batch 2/157\n",
      "Batch 2 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011137774214148521, Std: 0.08699025213718414\n",
      "z_j Mean: 0.011610070243477821, Std: 0.08762786537408829\n",
      "Similarities: tensor([[1.4234, 0.2876, 0.0000, 0.0000, 0.2180],\n",
      "        [0.3160, 1.4178, 0.0000, 0.0000, 1.0985],\n",
      "        [0.0000, 0.0000, 1.4113, 0.7165, 0.0000],\n",
      "        [0.0000, 0.0000, 0.6704, 1.3903, 0.0000],\n",
      "        [0.1409, 1.1797, 0.0000, 0.0000, 1.4251]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.094970226287842\n",
      "Batch 2, Loss: 3.094970226287842\n",
      "Processing Batch 3/157\n",
      "Batch 3 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011332368478178978, Std: 0.08696511387825012\n",
      "z_j Mean: 0.011006113141775131, Std: 0.08700700104236603\n",
      "Similarities: tensor([[1.4286, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4151, 0.0000, 0.0000, 0.5134],\n",
      "        [0.0000, 0.0000, 1.4286, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.8931, 0.0000],\n",
      "        [0.0000, 0.7968, 0.0000, 0.0000, 1.3790]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.120797872543335\n",
      "Batch 3, Loss: 3.120797872543335\n",
      "Processing Batch 4/157\n",
      "Batch 4 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011568217538297176, Std: 0.0876334086060524\n",
      "z_j Mean: 0.011689329519867897, Std: 0.08761733025312424\n",
      "Similarities: tensor([[1.4004, 0.0975, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0525, 1.4044, 0.0000, 0.0000, 0.1328],\n",
      "        [0.0000, 0.0000, 1.2921, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4277, 0.0000],\n",
      "        [0.0000, 0.2671, 0.0000, 0.0000, 1.3985]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.086775779724121\n",
      "Batch 4, Loss: 3.086775779724121\n",
      "Processing Batch 5/157\n",
      "Batch 5 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011772658675909042, Std: 0.08760616928339005\n",
      "z_j Mean: 0.012122501619160175, Std: 0.08685848116874695\n",
      "Similarities: tensor([[1.4286e+00, 1.6101e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [3.7261e-04, 1.1062e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 1.4219e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 1.2879e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.4254e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1128101348876953\n",
      "Batch 5, Loss: 3.1128101348876953\n",
      "Processing Batch 6/157\n",
      "Batch 6 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011724481359124184, Std: 0.08620790392160416\n",
      "z_j Mean: 0.011756484396755695, Std: 0.08690878748893738\n",
      "Similarities: tensor([[1.1667, 0.0000, 0.0000, 0.0093, 0.0000],\n",
      "        [0.0000, 1.3681, 0.6117, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0930, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3931, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3412]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1494834423065186\n",
      "Batch 6, Loss: 3.1494834423065186\n",
      "Processing Batch 7/157\n",
      "Batch 7 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011153304018080235, Std: 0.087687186896801\n",
      "z_j Mean: 0.011061832308769226, Std: 0.08699994534254074\n",
      "Similarities: tensor([[1.4244, 0.0000, 0.1140, 0.0000, 1.4079],\n",
      "        [0.0000, 1.3596, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0098, 0.0000, 1.3721, 0.0000, 0.0191],\n",
      "        [0.0000, 0.0000, 0.0000, 0.7116, 0.0000],\n",
      "        [1.3685, 0.0000, 0.1849, 0.0000, 1.4272]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.058854341506958\n",
      "Batch 7, Loss: 3.058854341506958\n",
      "Processing Batch 8/157\n",
      "Batch 8 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012015879154205322, Std: 0.08616776764392853\n",
      "z_j Mean: 0.01212381012737751, Std: 0.08615264296531677\n",
      "Similarities: tensor([[1.3048, 0.0934, 0.0000, 0.0000, 0.0877],\n",
      "        [0.0000, 1.4097, 0.0000, 0.0373, 0.0590],\n",
      "        [0.0000, 0.0000, 0.0439, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0476, 0.0000, 0.9549, 0.0000],\n",
      "        [0.0583, 0.1806, 0.0000, 0.0000, 1.4137]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.090351104736328\n",
      "Batch 8, Loss: 3.090351104736328\n",
      "Processing Batch 9/157\n",
      "Batch 9 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011909500695765018, Std: 0.08758766949176788\n",
      "z_j Mean: 0.011600996367633343, Std: 0.08692967891693115\n",
      "Similarities: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0334],\n",
      "        [0.0000, 1.3824, 0.0000, 0.0627, 0.0179],\n",
      "        [0.1987, 0.0000, 1.4286, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4286, 0.0000],\n",
      "        [1.3938, 0.0094, 0.0000, 0.0000, 1.4013]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0984086990356445\n",
      "Batch 9, Loss: 3.0984086990356445\n",
      "Processing Batch 10/157\n",
      "Batch 10 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.010893643833696842, Std: 0.08702116459608078\n",
      "z_j Mean: 0.01085532084107399, Std: 0.08632167428731918\n",
      "Similarities: tensor([[1.4165, 0.4063, 0.8291, 0.1209, 0.0000],\n",
      "        [0.3962, 1.4128, 1.2867, 0.5711, 0.0000],\n",
      "        [0.9505, 1.2101, 1.4085, 0.3601, 0.0000],\n",
      "        [0.1179, 0.4610, 0.3829, 1.4220, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1254067420959473\n",
      "Batch 10, Loss: 3.1254067420959473\n",
      "Processing Batch 11/157\n",
      "Batch 11 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.010993514209985733, Std: 0.08770736306905746\n",
      "z_j Mean: 0.010936733335256577, Std: 0.08771446347236633\n",
      "Similarities: tensor([[1.4105, 0.0000, 0.4220, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3022, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2008, 0.0000, 1.3203, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3330, 0.1114],\n",
      "        [0.0000, 0.0882, 0.0000, 0.3485, 0.9686]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.044790267944336\n",
      "Batch 11, Loss: 3.044790267944336\n",
      "Processing Batch 12/157\n",
      "Batch 12 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.010976851917803288, Std: 0.0870107039809227\n",
      "z_j Mean: 0.010983021929860115, Std: 0.08700992912054062\n",
      "Similarities: tensor([[1.3286, 0.5880, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5149, 1.4151, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4138, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4128, 0.0000],\n",
      "        [0.1159, 0.0000, 0.0000, 0.0000, 1.4185]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1110026836395264\n",
      "Batch 12, Loss: 3.1110026836395264\n",
      "Processing Batch 13/157\n",
      "Batch 13 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011080616153776646, Std: 0.08769639581441879\n",
      "z_j Mean: 0.011152243241667747, Std: 0.08768732100725174\n",
      "Similarities: tensor([[1.3967, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3099, 0.6590, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1170, 1.0861, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4286, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4284]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.092095136642456\n",
      "Batch 13, Loss: 3.092095136642456\n",
      "Processing Batch 14/157\n",
      "Batch 14 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01200318243354559, Std: 0.08757488429546356\n",
      "z_j Mean: 0.01230420172214508, Std: 0.08753309398889542\n",
      "Similarities: tensor([[1.3574, 0.1703, 0.0000, 0.0000, 0.2170],\n",
      "        [0.0000, 1.2949, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.2520, 0.2789, 0.0000],\n",
      "        [0.0000, 0.0000, 0.1387, 1.2704, 0.0000],\n",
      "        [0.3035, 0.0560, 0.0000, 0.1842, 1.3870]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.096477508544922\n",
      "Batch 14, Loss: 3.096477508544922\n",
      "Processing Batch 15/157\n",
      "Batch 15 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01081783790141344, Std: 0.08632638305425644\n",
      "z_j Mean: 0.011209778487682343, Std: 0.08767998963594437\n",
      "Similarities: tensor([[1.4034, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0183, 1.3227, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.1372, 0.0000, 0.0000],\n",
      "        [0.0000, 0.9446, 0.0000, 0.7682, 0.0146],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4178]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0841360092163086\n",
      "Batch 15, Loss: 3.0841360092163086\n",
      "Processing Batch 16/157\n",
      "Batch 16 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011570112779736519, Std: 0.0876331552863121\n",
      "z_j Mean: 0.011665157973766327, Std: 0.0876205563545227\n",
      "Similarities: tensor([[1.4195, 0.0000, 0.1703, 0.0000, 0.2742],\n",
      "        [0.0000, 1.4053, 0.0000, 0.0000, 0.0476],\n",
      "        [0.3381, 0.0000, 1.2207, 0.0000, 1.1435],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4283, 0.0000],\n",
      "        [0.4845, 0.1895, 0.7615, 0.0000, 1.3720]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0261311531066895\n",
      "Batch 16, Loss: 3.0261311531066895\n",
      "Processing Batch 17/157\n",
      "Batch 17 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011764039285480976, Std: 0.0869077667593956\n",
      "z_j Mean: 0.011807753704488277, Std: 0.0876014456152916\n",
      "Similarities: tensor([[1.3822, 0.3348, 0.0532, 0.0908, 0.0000],\n",
      "        [0.5361, 1.0729, 0.0558, 0.0000, 0.0000],\n",
      "        [0.0335, 0.1241, 1.3658, 0.0437, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0300, 1.3259, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4090]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.093700885772705\n",
      "Batch 17, Loss: 3.093700885772705\n",
      "Processing Batch 18/157\n",
      "Batch 18 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012533057481050491, Std: 0.08750060945749283\n",
      "z_j Mean: 0.012627138756215572, Std: 0.08748707920312881\n",
      "Similarities: tensor([[1.3038, 0.0000, 0.9122, 0.0000, 0.1695],\n",
      "        [0.0000, 1.3975, 0.0000, 0.0000, 0.0000],\n",
      "        [1.0916, 0.0000, 1.0745, 0.0000, 0.1221],\n",
      "        [0.0000, 0.2345, 0.0000, 1.4209, 0.0000],\n",
      "        [0.2377, 0.0000, 0.0000, 0.0000, 1.1524]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0496606826782227\n",
      "Batch 18, Loss: 3.0496606826782227\n",
      "Processing Batch 19/157\n",
      "Batch 19 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012206983752548695, Std: 0.087546706199646\n",
      "z_j Mean: 0.012493964284658432, Std: 0.08750620484352112\n",
      "Similarities: tensor([[1.3892, 0.0186, 0.0090, 0.6489, 0.0000],\n",
      "        [0.0000, 1.4283, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0333, 0.0000, 1.4182, 0.0000, 1.3557],\n",
      "        [0.6837, 0.0038, 0.0000, 1.3505, 0.0000],\n",
      "        [0.0091, 0.0000, 1.2841, 0.0000, 1.3860]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0795300006866455\n",
      "Batch 19, Loss: 3.0795300006866455\n",
      "Processing Batch 20/157\n",
      "Batch 20 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012440013699233532, Std: 0.08751388639211655\n",
      "z_j Mean: 0.011949236504733562, Std: 0.08758226037025452\n",
      "Similarities: tensor([[1.4213, 0.0357, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2380, 1.2838, 1.1193, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3072, 1.3513, 0.0000, 0.2456],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4173, 0.0000],\n",
      "        [0.0000, 0.0000, 0.3415, 0.0000, 1.4276]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285695552825928]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0683889389038086\n",
      "Batch 20, Loss: 3.0683889389038086\n",
      "Processing Batch 21/157\n",
      "Batch 21 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012105340138077736, Std: 0.08756081759929657\n",
      "z_j Mean: 0.012061438523232937, Std: 0.08756688237190247\n",
      "Similarities: tensor([[1.4285, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3735, 0.0000, 1.0157, 0.0000],\n",
      "        [0.0000, 0.0000, 0.3623, 0.0000, 0.8259],\n",
      "        [0.0000, 1.1470, 0.0000, 1.4286, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4196]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0477962493896484\n",
      "Batch 21, Loss: 3.0477962493896484\n",
      "Processing Batch 22/157\n",
      "Batch 22 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012045050971210003, Std: 0.08756913244724274\n",
      "z_j Mean: 0.012074742466211319, Std: 0.08756504207849503\n",
      "Similarities: tensor([[1.4235e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 1.4286e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 1.4269e+00, 1.3510e-01, 0.0000e+00],\n",
      "        [6.4498e-03, 0.0000e+00, 3.4733e-01, 1.3839e+00, 4.0409e-02],\n",
      "        [1.3591e-03, 0.0000e+00, 0.0000e+00, 2.3687e-02, 1.4161e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.042785406112671\n",
      "Batch 22, Loss: 3.042785406112671\n",
      "Processing Batch 23/157\n",
      "Batch 23 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011313706636428833, Std: 0.08766663074493408\n",
      "z_j Mean: 0.0113786356523633, Std: 0.08765821903944016\n",
      "Similarities: tensor([[1.3234, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3908, 0.0136, 0.0539, 0.3094],\n",
      "        [0.0000, 0.0000, 1.4238, 0.0000, 0.3273],\n",
      "        [0.0000, 1.0318, 0.0000, 0.8711, 0.0000],\n",
      "        [0.0000, 0.4212, 0.1847, 0.0000, 1.3809]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.085092544555664\n",
      "Batch 23, Loss: 3.085092544555664\n",
      "Processing Batch 24/157\n",
      "Batch 24 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011616401374340057, Std: 0.08762703090906143\n",
      "z_j Mean: 0.011645975522696972, Std: 0.08762310445308685\n",
      "Similarities: tensor([[1.2774, 0.2552, 0.0000, 0.1417, 0.4762],\n",
      "        [0.2550, 1.4225, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4227, 0.4040, 0.0000],\n",
      "        [0.0000, 0.0000, 0.5031, 1.3565, 0.1652],\n",
      "        [0.5862, 0.0000, 0.0000, 0.6626, 1.3328]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0472049713134766\n",
      "Batch 24, Loss: 3.0472049713134766\n",
      "Processing Batch 25/157\n",
      "Batch 25 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012136677280068398, Std: 0.08755647391080856\n",
      "z_j Mean: 0.012294008396565914, Std: 0.08753452450037003\n",
      "Similarities: tensor([[1.3320, 0.3633, 0.2238, 0.0000, 0.0000],\n",
      "        [0.0140, 1.4286, 0.2449, 0.0000, 0.0000],\n",
      "        [0.0652, 0.3100, 1.3510, 0.0202, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0104, 1.4261, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4050]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0768895149230957\n",
      "Batch 25, Loss: 3.0768895149230957\n",
      "Processing Batch 26/157\n",
      "Batch 26 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01138497143983841, Std: 0.0876573994755745\n",
      "z_j Mean: 0.011493337340652943, Std: 0.08764326572418213\n",
      "Similarities: tensor([[1.4267, 0.6109, 0.0000, 0.0000, 0.2728],\n",
      "        [0.5414, 1.2389, 0.0000, 0.0587, 0.2526],\n",
      "        [0.0000, 0.0000, 1.3360, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4193, 1.1966],\n",
      "        [0.0394, 0.0763, 0.0000, 1.4136, 1.3197]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0564754009246826\n",
      "Batch 26, Loss: 3.0564754009246826\n",
      "Processing Batch 27/157\n",
      "Batch 27 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011872108094394207, Std: 0.08759275078773499\n",
      "z_j Mean: 0.012093004770576954, Std: 0.08756251633167267\n",
      "Similarities: tensor([[1.3186, 0.2730, 0.0000, 0.0000, 0.4541],\n",
      "        [0.2830, 1.4164, 0.0000, 0.0000, 0.4037],\n",
      "        [0.0000, 0.0000, 1.4088, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4119, 0.0000],\n",
      "        [0.6067, 0.2720, 0.0000, 0.0000, 1.3526]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0336313247680664\n",
      "Batch 27, Loss: 3.0336313247680664\n",
      "Processing Batch 28/157\n",
      "Batch 28 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01141764409840107, Std: 0.08765316009521484\n",
      "z_j Mean: 0.011270316317677498, Std: 0.08767221868038177\n",
      "Similarities: tensor([[1.4021, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3605, 0.2531, 0.5946, 0.0000],\n",
      "        [0.0000, 0.2297, 1.2220, 1.3119, 0.0000],\n",
      "        [0.0000, 0.2733, 0.8091, 1.4284, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.9045]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0637083053588867\n",
      "Batch 28, Loss: 3.0637083053588867\n",
      "Processing Batch 29/157\n",
      "Batch 29 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.0114512350410223, Std: 0.08694953471422195\n",
      "z_j Mean: 0.011457479558885098, Std: 0.08764795958995819\n",
      "Similarities: tensor([[1.4074, 0.0000, 0.1267, 0.0000, 0.0000],\n",
      "        [0.0000, 1.2987, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4214, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4225, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4286]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.051254987716675\n",
      "Batch 29, Loss: 3.051254987716675\n",
      "Processing Batch 30/157\n",
      "Batch 30 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013077965006232262, Std: 0.08742082118988037\n",
      "z_j Mean: 0.012695117853581905, Std: 0.08677662909030914\n",
      "Similarities: tensor([[1.3942, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4133, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4274, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4286, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3985]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0611214637756348\n",
      "Batch 30, Loss: 3.0611214637756348\n",
      "Processing Batch 31/157\n",
      "Batch 31 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01127258874475956, Std: 0.08697287738323212\n",
      "z_j Mean: 0.011127647012472153, Std: 0.08699154853820801\n",
      "Similarities: tensor([[1.4221, 0.0000, 0.0000, 0.0423, 0.0000],\n",
      "        [0.0000, 1.4262, 0.0000, 0.0000, 1.3792],\n",
      "        [0.0000, 0.0000, 1.1570, 0.0000, 0.0000],\n",
      "        [0.1486, 0.0000, 0.0000, 1.3595, 0.0000],\n",
      "        [0.0000, 1.3903, 0.0000, 0.0000, 1.4276]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0770456790924072\n",
      "Batch 31, Loss: 3.0770456790924072\n",
      "Processing Batch 32/157\n",
      "Batch 32 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011667629703879356, Std: 0.08762022107839584\n",
      "z_j Mean: 0.011761505156755447, Std: 0.08760766685009003\n",
      "Similarities: tensor([[1.4043, 0.0000, 0.9367, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4061, 0.0000, 0.0000, 0.0000],\n",
      "        [0.7099, 0.0000, 1.4165, 0.0970, 0.0614],\n",
      "        [0.0020, 0.0000, 0.0434, 1.4142, 0.1913],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0165, 1.4083]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0110936164855957\n",
      "Batch 32, Loss: 3.0110936164855957\n",
      "Processing Batch 33/157\n",
      "Batch 33 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011824900284409523, Std: 0.08759913593530655\n",
      "z_j Mean: 0.011949525214731693, Std: 0.08758222311735153\n",
      "Similarities: tensor([[1.4248, 0.0000, 0.0000, 0.0000, 0.2034],\n",
      "        [0.0000, 1.4047, 0.9143, 0.8055, 0.0000],\n",
      "        [0.0000, 0.9157, 1.4282, 0.3671, 0.0000],\n",
      "        [0.0000, 0.6977, 0.3050, 1.4161, 0.0000],\n",
      "        [0.2583, 0.0000, 0.0000, 0.0000, 1.4217]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.085085868835449\n",
      "Batch 33, Loss: 3.085085868835449\n",
      "Processing Batch 34/157\n",
      "Batch 34 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01175718754529953, Std: 0.08760825544595718\n",
      "z_j Mean: 0.01182887889444828, Std: 0.08759859949350357\n",
      "Similarities: tensor([[1.3091, 0.0406, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1498, 1.2062, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3329, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3483, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.1927]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0404412746429443\n",
      "Batch 34, Loss: 3.0404412746429443\n",
      "Processing Batch 35/157\n",
      "Batch 35 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012085853144526482, Std: 0.08756350725889206\n",
      "z_j Mean: 0.012033121660351753, Std: 0.08757076412439346\n",
      "Similarities: tensor([[1.4162, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4065, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4228, 0.0000, 0.5473],\n",
      "        [0.0000, 0.0000, 0.0000, 0.4285, 0.0000],\n",
      "        [0.0000, 0.0000, 0.5974, 0.0000, 1.4264]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.067525863647461\n",
      "Batch 35, Loss: 3.067525863647461\n",
      "Processing Batch 36/157\n",
      "Batch 36 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011474857106804848, Std: 0.08764567971229553\n",
      "z_j Mean: 0.01147063635289669, Std: 0.0876462385058403\n",
      "Similarities: tensor([[1.4087, 0.0000, 0.0000, 1.3629, 0.0000],\n",
      "        [0.0000, 1.3752, 0.0590, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4238, 0.0000, 0.0000],\n",
      "        [1.4282, 0.0000, 0.0000, 1.3819, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0035, 0.0000, 1.4238]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0340380668640137\n",
      "Batch 36, Loss: 3.0340380668640137\n",
      "Processing Batch 37/157\n",
      "Batch 37 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01231092493981123, Std: 0.0875321477651596\n",
      "z_j Mean: 0.011979900300502777, Std: 0.08687826991081238\n",
      "Similarities: tensor([[1.4243, 1.0962, 0.5435, 0.0000, 0.0000],\n",
      "        [1.0580, 1.4178, 0.8796, 0.0551, 0.0000],\n",
      "        [0.7244, 0.9021, 1.3612, 0.0595, 0.0000],\n",
      "        [0.0000, 0.0654, 0.0680, 1.4223, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4286]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.129361867904663\n",
      "Batch 37, Loss: 3.129361867904663\n",
      "Processing Batch 38/157\n",
      "Batch 38 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011913515627384186, Std: 0.08758711814880371\n",
      "z_j Mean: 0.01203166227787733, Std: 0.08757097274065018\n",
      "Similarities: tensor([[1.4112, 0.0000, 0.1209, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3821, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0712, 0.0000, 1.4078, 0.0000, 0.0000],\n",
      "        [0.0070, 0.0000, 0.0000, 1.3071, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4042]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.064981698989868\n",
      "Batch 38, Loss: 3.064981698989868\n",
      "Processing Batch 39/157\n",
      "Batch 39 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01223139837384224, Std: 0.0875432938337326\n",
      "z_j Mean: 0.012376973405480385, Std: 0.08752283453941345\n",
      "Similarities: tensor([[1.3517e+00, 0.0000e+00, 5.4646e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 1.3819e+00, 2.6969e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [4.4954e-01, 1.6625e-01, 1.3532e+00, 3.1107e-04, 4.1070e-02],\n",
      "        [0.0000e+00, 1.5252e-03, 4.1007e-03, 1.2727e+00, 3.0928e-01],\n",
      "        [0.0000e+00, 6.3726e-02, 1.7134e-01, 1.1009e+00, 8.7483e-01]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.04646635055542\n",
      "Batch 39, Loss: 3.04646635055542\n",
      "Processing Batch 40/157\n",
      "Batch 40 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012544846162199974, Std: 0.08749892562627792\n",
      "z_j Mean: 0.012766802683472633, Std: 0.08746680617332458\n",
      "Similarities: tensor([[1.3969, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4066, 0.0000, 1.3740, 0.8779],\n",
      "        [0.0000, 0.0000, 1.0099, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3458, 0.0000, 1.4245, 0.8063],\n",
      "        [0.0000, 0.7004, 0.0000, 0.7467, 1.4247]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0557432174682617\n",
      "Batch 40, Loss: 3.0557432174682617\n",
      "Processing Batch 41/157\n",
      "Batch 41 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012086886912584305, Std: 0.08756336569786072\n",
      "z_j Mean: 0.0118446359410882, Std: 0.08689681440591812\n",
      "Similarities: tensor([[1.4091, 0.0000, 0.2087, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3613, 0.0000, 0.5221, 0.0000],\n",
      "        [0.0939, 0.0000, 1.4220, 0.0590, 0.0000],\n",
      "        [0.0000, 0.7570, 0.0913, 1.4254, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4207]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285696744918823]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0886270999908447\n",
      "Batch 41, Loss: 3.0886270999908447\n",
      "Processing Batch 42/157\n",
      "Batch 42 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01216835342347622, Std: 0.08755207806825638\n",
      "z_j Mean: 0.012445032596588135, Std: 0.08751317113637924\n",
      "Similarities: tensor([[1.3817, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1013, 0.9851, 0.0000, 0.0000, 0.2170],\n",
      "        [0.0000, 0.0000, 1.4055, 0.0881, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0547, 1.4165, 0.0000],\n",
      "        [0.0178, 0.0000, 0.0000, 0.0000, 1.2547]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.428504467010498]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0499250888824463\n",
      "Batch 42, Loss: 3.0499250888824463\n",
      "Processing Batch 43/157\n",
      "Batch 43 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012326106429100037, Std: 0.08753000944852829\n",
      "z_j Mean: 0.012731458991765976, Std: 0.08747196197509766\n",
      "Similarities: tensor([[1.3482, 0.7488, 0.0944, 0.0000, 0.9931],\n",
      "        [0.4871, 1.4164, 0.0000, 0.0000, 0.0316],\n",
      "        [0.0427, 0.0000, 1.2550, 1.2733, 0.0821],\n",
      "        [0.0000, 0.0000, 0.8801, 1.3883, 0.0000],\n",
      "        [1.1238, 0.0096, 0.2541, 0.0000, 1.3972]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.088894844055176\n",
      "Batch 43, Loss: 3.088894844055176\n",
      "Processing Batch 44/157\n",
      "Batch 44 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012380704283714294, Std: 0.08682204782962799\n",
      "z_j Mean: 0.012481896206736565, Std: 0.08680755645036697\n",
      "Similarities: tensor([[1.4151, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1166, 1.4286, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3351, 0.2528, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0509, 1.4196, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3831]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.075028896331787\n",
      "Batch 44, Loss: 3.075028896331787\n",
      "Processing Batch 45/157\n",
      "Batch 45 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012417564168572426, Std: 0.08751708269119263\n",
      "z_j Mean: 0.01206466369330883, Std: 0.08756642788648605\n",
      "Similarities: tensor([[1.4183, 1.2921, 0.3605, 0.0000, 0.2384],\n",
      "        [1.2364, 1.4218, 0.5385, 0.0000, 0.0668],\n",
      "        [0.4051, 0.4724, 1.3298, 0.0000, 0.9051],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4241, 0.0000],\n",
      "        [0.2883, 0.1330, 0.7660, 0.0000, 1.3962]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0503978729248047\n",
      "Batch 45, Loss: 3.0503978729248047\n",
      "Processing Batch 46/157\n",
      "Batch 46 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012128702364861965, Std: 0.0875575840473175\n",
      "z_j Mean: 0.011831428855657578, Std: 0.08689861744642258\n",
      "Similarities: tensor([[1.3145, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4219, 0.6311, 0.6526, 0.0000],\n",
      "        [0.0000, 0.0777, 1.2693, 1.2795, 1.2873],\n",
      "        [0.0000, 0.2976, 1.3548, 1.3137, 1.1012],\n",
      "        [0.0000, 0.0000, 0.9518, 1.0788, 1.3540]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1075315475463867\n",
      "Batch 46, Loss: 3.1075315475463867\n",
      "Processing Batch 47/157\n",
      "Batch 47 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012187274172902107, Std: 0.08754944801330566\n",
      "z_j Mean: 0.012019295245409012, Std: 0.08687282353639603\n",
      "Similarities: tensor([[1.1973, 0.6455, 0.0000, 0.2868, 0.0000],\n",
      "        [0.6428, 1.3427, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4286, 0.0000, 0.0000],\n",
      "        [0.0222, 0.0000, 0.0000, 1.3752, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0724, 1.4110]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.120053291320801\n",
      "Batch 47, Loss: 3.120053291320801\n",
      "Processing Batch 48/157\n",
      "Batch 48 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012081289663910866, Std: 0.0875641405582428\n",
      "z_j Mean: 0.01183185912668705, Std: 0.08759819716215134\n",
      "Similarities: tensor([[1.3656, 0.0000, 0.2029, 0.0352, 0.0000],\n",
      "        [0.0000, 1.2371, 0.0000, 0.0000, 1.0012],\n",
      "        [0.0989, 0.0000, 1.3908, 0.4120, 0.0000],\n",
      "        [0.0877, 0.0000, 0.7356, 1.3053, 0.4822],\n",
      "        [0.0000, 0.9681, 0.0000, 0.7686, 1.3188]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0449442863464355\n",
      "Batch 48, Loss: 3.0449442863464355\n",
      "Processing Batch 49/157\n",
      "Batch 49 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011484568007290363, Std: 0.08694513887166977\n",
      "z_j Mean: 0.011748247779905796, Std: 0.08760944753885269\n",
      "Similarities: tensor([[1.3778, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.0648, 0.0191, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4136, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4227, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4228]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0515010356903076\n",
      "Batch 49, Loss: 3.0515010356903076\n",
      "Processing Batch 50/157\n",
      "Batch 50 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012071602046489716, Std: 0.08686556667089462\n",
      "z_j Mean: 0.012145353481173515, Std: 0.08685528486967087\n",
      "Similarities: tensor([[1.3625, 0.0000, 0.0000, 0.3082, 0.5990],\n",
      "        [0.0000, 1.1910, 0.0000, 0.0989, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3594, 0.0468, 0.0000],\n",
      "        [0.2768, 0.0000, 0.0000, 1.3930, 0.0000],\n",
      "        [0.9112, 0.0000, 0.0000, 0.0000, 1.3792]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1004936695098877\n",
      "Batch 50, Loss: 3.1004936695098877\n",
      "Processing Batch 51/157\n",
      "Batch 51 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012166129425168037, Std: 0.08755238354206085\n",
      "z_j Mean: 0.01223465334624052, Std: 0.08754283934831619\n",
      "Similarities: tensor([[1.4052, 0.0000, 0.0000, 0.0000, 0.0295],\n",
      "        [0.0000, 1.4035, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3850, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4037, 0.0000],\n",
      "        [0.6872, 0.0000, 0.0000, 0.0000, 1.2849]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0629398822784424\n",
      "Batch 51, Loss: 3.0629398822784424\n",
      "Processing Batch 52/157\n",
      "Batch 52 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011410790495574474, Std: 0.08695485442876816\n",
      "z_j Mean: 0.011800093576312065, Std: 0.08760248124599457\n",
      "Similarities: tensor([[1.4286, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4258, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.0643, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4286, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4278]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0447304248809814\n",
      "Batch 52, Loss: 3.0447304248809814\n",
      "Processing Batch 53/157\n",
      "Batch 53 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011578748002648354, Std: 0.08763201534748077\n",
      "z_j Mean: 0.011772888712584972, Std: 0.0869065672159195\n",
      "Similarities: tensor([[1.2703, 0.0000, 0.0000, 1.2186, 0.0000],\n",
      "        [0.0000, 1.4133, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4240, 0.0000, 0.0000],\n",
      "        [0.7329, 0.0000, 0.0000, 1.3932, 0.4774],\n",
      "        [0.0000, 0.0000, 0.0000, 0.3163, 1.4212]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.126613140106201\n",
      "Batch 53, Loss: 3.126613140106201\n",
      "Processing Batch 54/157\n",
      "Batch 54 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012477596290409565, Std: 0.08750853687524796\n",
      "z_j Mean: 0.012485667131841183, Std: 0.0868070125579834\n",
      "Similarities: tensor([[1.3884, 0.0000, 0.6864, 0.0000, 0.0000],\n",
      "        [0.0208, 1.3329, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5257, 0.0000, 1.4173, 0.0000, 0.3786],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3893, 0.0000],\n",
      "        [0.5090, 0.0000, 0.6876, 0.0000, 1.2478]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.056887626647949\n",
      "Batch 54, Loss: 3.056887626647949\n",
      "Processing Batch 55/157\n",
      "Batch 55 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012337064370512962, Std: 0.08752845972776413\n",
      "z_j Mean: 0.012325789779424667, Std: 0.08682986348867416\n",
      "Similarities: tensor([[1.3840, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4065, 0.7365, 0.4211, 0.0000],\n",
      "        [0.0000, 0.9475, 1.3709, 0.3185, 0.0000],\n",
      "        [0.0000, 0.2061, 0.2707, 1.3939, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.2631]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.065093994140625\n",
      "Batch 55, Loss: 3.065093994140625\n",
      "Processing Batch 56/157\n",
      "Batch 56 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012281091883778572, Std: 0.08753634244203568\n",
      "z_j Mean: 0.01203962229192257, Std: 0.08687000721693039\n",
      "Similarities: tensor([[1.3655, 0.0000, 0.0000, 1.1489, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3868, 0.0000, 1.1846],\n",
      "        [1.1864, 0.0000, 0.0000, 1.3881, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3197, 0.0000, 1.4149]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0874791145324707\n",
      "Batch 56, Loss: 3.0874791145324707\n",
      "Processing Batch 57/157\n",
      "Batch 57 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012806953862309456, Std: 0.0874609425663948\n",
      "z_j Mean: 0.012466360814869404, Std: 0.08751014620065689\n",
      "Similarities: tensor([[1.4171, 0.0000, 0.9435, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4179, 0.0000, 0.0910, 0.0000],\n",
      "        [0.9054, 0.0000, 1.3892, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2846, 0.0000, 0.8141, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.1439]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0868265628814697\n",
      "Batch 57, Loss: 3.0868265628814697\n",
      "Processing Batch 58/157\n",
      "Batch 58 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012952674180269241, Std: 0.08743946999311447\n",
      "z_j Mean: 0.013116173446178436, Std: 0.08741509169340134\n",
      "Similarities: tensor([[1.4273, 0.0041, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0028, 1.1487, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4185, 0.5033, 0.0000],\n",
      "        [0.0000, 0.0000, 0.4833, 1.4227, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4274]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.083550214767456\n",
      "Batch 58, Loss: 3.083550214767456\n",
      "Processing Batch 59/157\n",
      "Batch 59 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012398217804729939, Std: 0.0875198245048523\n",
      "z_j Mean: 0.01229793205857277, Std: 0.08683381974697113\n",
      "Similarities: tensor([[1.4121, 0.0000, 0.0000, 0.9666, 0.0000],\n",
      "        [0.0000, 1.4060, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4127, 0.1385, 0.0000],\n",
      "        [1.2450, 0.0000, 0.0689, 1.3162, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.7245]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.070382595062256\n",
      "Batch 59, Loss: 3.070382595062256\n",
      "Processing Batch 60/157\n",
      "Batch 60 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012433001771569252, Std: 0.08751489222049713\n",
      "z_j Mean: 0.01234199944883585, Std: 0.08612165600061417\n",
      "Similarities: tensor([[1.4278, 0.0000, 0.0000, 0.0000, 1.3422],\n",
      "        [0.0000, 0.8260, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4087, 0.0000],\n",
      "        [1.3257, 0.0000, 0.0000, 0.0000, 1.4286]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0829107761383057\n",
      "Batch 60, Loss: 3.0829107761383057\n",
      "Processing Batch 61/157\n",
      "Batch 61 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012182492762804031, Std: 0.08685008436441422\n",
      "z_j Mean: 0.01231132261455059, Std: 0.08683191984891891\n",
      "Similarities: tensor([[1.4286, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3923, 0.5100, 0.4175, 0.0000],\n",
      "        [0.0000, 0.6632, 1.3874, 0.2193, 0.0000],\n",
      "        [0.0000, 0.5729, 0.2245, 1.4259, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1018898487091064\n",
      "Batch 61, Loss: 3.1018898487091064\n",
      "Processing Batch 62/157\n",
      "Batch 62 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01256447471678257, Std: 0.08679564297199249\n",
      "z_j Mean: 0.012656418606638908, Std: 0.08607600629329681\n",
      "Similarities: tensor([[0.8955, 0.0000, 0.0000, 0.0280, 0.0000],\n",
      "        [0.0000, 1.1963, 1.1113, 0.4784, 0.4271],\n",
      "        [0.0000, 1.2707, 1.4217, 0.0395, 0.1004],\n",
      "        [0.0000, 0.0150, 0.0347, 1.3732, 0.2881],\n",
      "        [0.0000, 0.0423, 0.0982, 0.2758, 1.4200]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.087623357772827\n",
      "Batch 62, Loss: 3.087623357772827\n",
      "Processing Batch 63/157\n",
      "Batch 63 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01332834642380476, Std: 0.08738299459218979\n",
      "z_j Mean: 0.012981350533664227, Std: 0.08743522316217422\n",
      "Similarities: tensor([[1.2871, 0.0000, 0.0000, 0.0917, 0.3353],\n",
      "        [0.0000, 1.4046, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4056, 0.0000, 0.0939],\n",
      "        [0.2254, 0.0608, 0.0000, 1.3760, 0.0000],\n",
      "        [0.0696, 0.0000, 0.3489, 0.0158, 1.2155]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0392446517944336\n",
      "Batch 63, Loss: 3.0392446517944336\n",
      "Processing Batch 64/157\n",
      "Batch 64 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01222266536206007, Std: 0.08684444427490234\n",
      "z_j Mean: 0.012411175295710564, Std: 0.08751798421144485\n",
      "Similarities: tensor([[1.2918e+00, 0.0000e+00, 2.5221e-02, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 1.3653e+00, 9.7093e-02, 8.8709e-01, 1.0730e+00],\n",
      "        [3.9227e-01, 2.3698e-01, 1.2128e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 7.8602e-01, 3.2840e-05, 1.4215e+00, 1.1857e+00],\n",
      "        [0.0000e+00, 9.5881e-01, 0.0000e+00, 1.1548e+00, 1.4278e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0821211338043213\n",
      "Batch 64, Loss: 3.0821211338043213\n",
      "Processing Batch 65/157\n",
      "Batch 65 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012627126649022102, Std: 0.08748708665370941\n",
      "z_j Mean: 0.012409001588821411, Std: 0.08681800961494446\n",
      "Similarities: tensor([[1.3086, 0.2472, 0.0000, 0.6493, 0.0000],\n",
      "        [0.0000, 1.4273, 0.0000, 0.4185, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4286, 0.7066, 0.0000],\n",
      "        [0.0000, 0.4043, 0.8443, 1.1380, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4247]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0686793327331543\n",
      "Batch 65, Loss: 3.0686793327331543\n",
      "Processing Batch 66/157\n",
      "Batch 66 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012186635285615921, Std: 0.08754953742027283\n",
      "z_j Mean: 0.012440313585102558, Std: 0.08751384913921356\n",
      "Similarities: tensor([[1.4074e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 1.4272e+00, 3.2926e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 2.9441e-01, 1.3091e+00, 0.0000e+00, 7.4390e-01],\n",
      "        [1.4083e-03, 0.0000e+00, 0.0000e+00, 1.3931e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 2.5922e-01, 1.2683e+00, 0.0000e+00, 1.2870e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.068739414215088\n",
      "Batch 66, Loss: 3.068739414215088\n",
      "Processing Batch 67/157\n",
      "Batch 67 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012481492012739182, Std: 0.08680761605501175\n",
      "z_j Mean: 0.01254688948392868, Std: 0.08679818361997604\n",
      "Similarities: tensor([[1.4003, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4286, 0.1329, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.0962, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.6832, 0.8233],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0340, 1.2516]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0839056968688965\n",
      "Batch 67, Loss: 3.0839056968688965\n",
      "Processing Batch 68/157\n",
      "Batch 68 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012812245637178421, Std: 0.08746016025543213\n",
      "z_j Mean: 0.01306556910276413, Std: 0.087422676384449\n",
      "Similarities: tensor([[1.3879, 0.0000, 0.0000, 0.5970, 0.0000],\n",
      "        [0.0000, 1.2195, 0.7661, 0.0000, 0.3893],\n",
      "        [0.0000, 0.6061, 1.3245, 0.0000, 0.4700],\n",
      "        [0.6708, 0.0000, 0.0000, 1.3674, 0.0000],\n",
      "        [0.0000, 0.4965, 0.5151, 0.0000, 1.4024]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.101478338241577\n",
      "Batch 68, Loss: 3.101478338241577\n",
      "Processing Batch 69/157\n",
      "Batch 69 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012726036831736565, Std: 0.08747275173664093\n",
      "z_j Mean: 0.012682106345891953, Std: 0.0874791294336319\n",
      "Similarities: tensor([[1.3660, 1.0100, 0.0000, 0.0000, 0.0000],\n",
      "        [0.7518, 1.3218, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4265, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.1024, 0.4984],\n",
      "        [0.0986, 0.0000, 0.0000, 0.0000, 1.3819]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0581774711608887\n",
      "Batch 69, Loss: 3.0581774711608887\n",
      "Processing Batch 70/157\n",
      "Batch 70 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012688565999269485, Std: 0.08677758276462555\n",
      "z_j Mean: 0.012775101698935032, Std: 0.08676488697528839\n",
      "Similarities: tensor([[1.1473, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3432, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0063, 0.0000, 1.2847, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4055, 0.0000],\n",
      "        [0.6728, 0.0000, 0.0000, 0.0000, 1.4286]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.097435712814331\n",
      "Batch 70, Loss: 3.097435712814331\n",
      "Processing Batch 71/157\n",
      "Batch 71 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012718256562948227, Std: 0.08747387677431107\n",
      "z_j Mean: 0.012430327013134956, Std: 0.08751526474952698\n",
      "Similarities: tensor([[1.0401, 0.0000, 0.0174, 0.0000, 0.0000],\n",
      "        [0.0000, 1.2810, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.0622, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3947, 1.3601],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3693, 1.4016]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0927231311798096\n",
      "Batch 71, Loss: 3.0927231311798096\n",
      "Processing Batch 72/157\n",
      "Batch 72 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012105461210012436, Std: 0.08756079524755478\n",
      "z_j Mean: 0.012416768819093704, Std: 0.08751719444990158\n",
      "Similarities: tensor([[1.4009, 1.3245, 0.0000, 0.0000, 0.0000],\n",
      "        [1.2793, 1.4166, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4286, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0079, 1.2997]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0703237056732178\n",
      "Batch 72, Loss: 3.0703237056732178\n",
      "Processing Batch 73/157\n",
      "Batch 73 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013359636068344116, Std: 0.08737821131944656\n",
      "z_j Mean: 0.013072950765490532, Std: 0.08742156624794006\n",
      "Similarities: tensor([[1.4275, 0.2238, 0.0000, 0.9640, 0.0000],\n",
      "        [0.1839, 1.4074, 0.1646, 0.0000, 0.7344],\n",
      "        [0.0000, 0.0827, 1.4138, 0.0000, 0.0000],\n",
      "        [0.9684, 0.0000, 0.0000, 1.4271, 0.0000],\n",
      "        [0.0000, 0.7512, 0.0000, 0.0000, 1.4256]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.062225341796875\n",
      "Batch 73, Loss: 3.062225341796875\n",
      "Processing Batch 74/157\n",
      "Batch 74 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012666534632444382, Std: 0.08678080141544342\n",
      "z_j Mean: 0.012413186952471733, Std: 0.08681740611791611\n",
      "Similarities: tensor([[1.3759, 0.0898, 1.3363, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3358, 0.0000, 0.0150, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3990, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0745, 1.3812]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285608530044556]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.138396739959717\n",
      "Batch 74, Loss: 3.138396739959717\n",
      "Processing Batch 75/157\n",
      "Batch 75 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013434221036732197, Std: 0.08736677467823029\n",
      "z_j Mean: 0.013616509735584259, Std: 0.08733854442834854\n",
      "Similarities: tensor([[1.4286, 0.2923, 0.0000, 0.0000, 0.9304],\n",
      "        [0.2772, 1.2776, 0.0000, 0.0000, 0.0043],\n",
      "        [0.0000, 0.0000, 1.4106, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4276, 0.0000],\n",
      "        [0.7123, 0.0727, 0.0000, 0.0000, 1.3846]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0322699546813965\n",
      "Batch 75, Loss: 3.0322699546813965\n",
      "Processing Batch 76/157\n",
      "Batch 76 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012435119599103928, Std: 0.08751458674669266\n",
      "z_j Mean: 0.012613825500011444, Std: 0.08678848296403885\n",
      "Similarities: tensor([[1.4223, 0.0000, 0.0000, 0.2260, 1.1317],\n",
      "        [0.0000, 1.3591, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0342, 0.0000, 1.3698, 0.2279, 0.0804],\n",
      "        [0.2174, 0.0000, 0.7680, 0.8694, 0.0735],\n",
      "        [0.9894, 0.0000, 0.0978, 0.0071, 1.3994]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.070866823196411\n",
      "Batch 76, Loss: 3.070866823196411\n",
      "Processing Batch 77/157\n",
      "Batch 77 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013967997394502163, Std: 0.08728301525115967\n",
      "z_j Mean: 0.014142315834760666, Std: 0.08725493401288986\n",
      "Similarities: tensor([[1.3413, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.1543, 0.0000, 0.4623, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4286, 0.0000, 0.0000],\n",
      "        [0.0000, 0.4325, 0.0000, 1.4087, 0.1653],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3999]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.071767807006836\n",
      "Batch 77, Loss: 3.071767807006836\n",
      "Processing Batch 78/157\n",
      "Batch 78 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013253729790449142, Std: 0.0873943418264389\n",
      "z_j Mean: 0.013304521329700947, Std: 0.08738663047552109\n",
      "Similarities: tensor([[1.4280, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.2585, 0.2685, 1.3705, 0.0000],\n",
      "        [0.0000, 0.3337, 1.3761, 0.5744, 0.3052],\n",
      "        [0.0000, 1.2769, 0.2581, 1.3533, 0.0000],\n",
      "        [0.0000, 0.0000, 0.3311, 0.0000, 1.4280]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0691640377044678\n",
      "Batch 78, Loss: 3.0691640377044678\n",
      "Processing Batch 79/157\n",
      "Batch 79 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013288715854287148, Std: 0.0873890295624733\n",
      "z_j Mean: 0.01312103308737278, Std: 0.08741436898708344\n",
      "Similarities: tensor([[1.4124, 0.7052, 1.1136, 0.0000, 0.0000],\n",
      "        [0.4840, 1.0734, 0.1407, 0.6387, 0.0000],\n",
      "        [1.2727, 0.5217, 1.3627, 0.0000, 0.2579],\n",
      "        [0.1291, 0.3939, 0.1625, 0.5500, 0.0000],\n",
      "        [0.0000, 0.0000, 0.3393, 0.0000, 1.4147]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4283015727996826]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0550131797790527\n",
      "Batch 79, Loss: 3.0550131797790527\n",
      "Processing Batch 80/157\n",
      "Batch 80 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013948135077953339, Std: 0.08728619664907455\n",
      "z_j Mean: 0.014033429324626923, Std: 0.0872725173830986\n",
      "Similarities: tensor([[1.4054, 0.0000, 0.0000, 0.7071, 0.0000],\n",
      "        [0.0000, 1.3952, 0.1494, 0.0000, 0.3524],\n",
      "        [0.0000, 0.1227, 1.3229, 0.0000, 0.0000],\n",
      "        [0.5086, 0.0000, 0.0000, 1.4146, 0.0000],\n",
      "        [0.0000, 0.3789, 0.0000, 0.0000, 1.4174]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0736021995544434\n",
      "Batch 80, Loss: 3.0736021995544434\n",
      "Processing Batch 81/157\n",
      "Batch 81 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013669142499566078, Std: 0.08733031898736954\n",
      "z_j Mean: 0.013867175206542015, Std: 0.0865970328450203\n",
      "Similarities: tensor([[1.3627, 0.2002, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0802, 1.3632, 0.2418, 0.1899, 0.7052],\n",
      "        [0.0000, 0.1674, 1.4200, 0.0748, 0.0595],\n",
      "        [0.0973, 0.4739, 0.0000, 1.1192, 0.0000],\n",
      "        [0.0000, 0.5514, 0.0768, 0.0000, 1.4250]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.067122459411621\n",
      "Batch 81, Loss: 3.067122459411621\n",
      "Processing Batch 82/157\n",
      "Batch 82 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013702946715056896, Std: 0.08732502162456512\n",
      "z_j Mean: 0.01352659147232771, Std: 0.08735252171754837\n",
      "Similarities: tensor([[1.4127, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4043, 0.0000, 0.6150, 0.0000],\n",
      "        [0.0000, 0.0000, 1.0926, 0.0000, 0.0000],\n",
      "        [0.0000, 0.6389, 0.0000, 1.4120, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4285]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285554885864258]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1144862174987793\n",
      "Batch 82, Loss: 3.1144862174987793\n",
      "Processing Batch 83/157\n",
      "Batch 83 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012760326266288757, Std: 0.08746775984764099\n",
      "z_j Mean: 0.012798169627785683, Std: 0.08746222406625748\n",
      "Similarities: tensor([[1.2503, 0.6675, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5287, 1.4239, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0165, 1.4215, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0848, 1.3807, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4030]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285701513290405]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.063936710357666\n",
      "Batch 83, Loss: 3.063936710357666\n",
      "Processing Batch 84/157\n",
      "Batch 84 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.014044348150491714, Std: 0.08656847476959229\n",
      "z_j Mean: 0.014313045889139175, Std: 0.08722709119319916\n",
      "Similarities: tensor([[1.4245, 1.3407, 0.0000, 0.0000, 0.0000],\n",
      "        [1.2627, 1.4151, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3274, 0.4224, 0.0000],\n",
      "        [0.0000, 0.0000, 0.2237, 1.3158, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.0102]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0923099517822266\n",
      "Batch 84, Loss: 3.0923099517822266\n",
      "Processing Batch 85/157\n",
      "Batch 85 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013407396152615547, Std: 0.08737089484930038\n",
      "z_j Mean: 0.01309997122734785, Std: 0.08741752058267593\n",
      "Similarities: tensor([[1.3948, 0.0000, 0.0000, 0.0000, 0.1822],\n",
      "        [0.0000, 1.4069, 0.0058, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4285, 0.9130, 0.0000],\n",
      "        [0.0000, 0.0000, 1.0004, 1.4230, 0.0000],\n",
      "        [0.4221, 0.0000, 0.0000, 0.0000, 1.4075]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0407207012176514\n",
      "Batch 85, Loss: 3.0407207012176514\n",
      "Processing Batch 86/157\n",
      "Batch 86 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012523858807981014, Std: 0.08750192821025848\n",
      "z_j Mean: 0.012552391737699509, Std: 0.08679738640785217\n",
      "Similarities: tensor([[1.4269, 0.2432, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1934, 1.4196, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4166, 0.4738, 0.0165],\n",
      "        [0.0000, 0.0000, 0.6508, 1.4084, 0.1428],\n",
      "        [0.0000, 0.0000, 0.0360, 0.0390, 1.4139]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0587785243988037\n",
      "Batch 86, Loss: 3.0587785243988037\n",
      "Processing Batch 87/157\n",
      "Batch 87 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013107459992170334, Std: 0.0874164029955864\n",
      "z_j Mean: 0.012981395237147808, Std: 0.08673425018787384\n",
      "Similarities: tensor([[0.3842, 0.0000, 1.0277, 0.0000, 0.0000],\n",
      "        [0.0000, 1.2857, 0.0000, 0.4695, 0.0000],\n",
      "        [0.1761, 0.0000, 1.4149, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1650, 0.0000, 1.3724, 0.0000],\n",
      "        [0.1645, 0.0000, 0.0000, 0.0000, 1.4193]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.097874164581299\n",
      "Batch 87, Loss: 3.097874164581299\n",
      "Processing Batch 88/157\n",
      "Batch 88 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012914209626615047, Std: 0.08744516223669052\n",
      "z_j Mean: 0.012706509791314602, Std: 0.08677496016025543\n",
      "Similarities: tensor([[0.7833, 0.0092, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2484, 1.4274, 0.0000, 0.0000, 0.0475],\n",
      "        [0.0000, 0.0000, 1.4133, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4156, 0.0195],\n",
      "        [0.0000, 0.0560, 0.0000, 0.0000, 1.4274]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0601649284362793\n",
      "Batch 88, Loss: 3.0601649284362793\n",
      "Processing Batch 89/157\n",
      "Batch 89 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013362618163228035, Std: 0.08737775683403015\n",
      "z_j Mean: 0.013280152343213558, Std: 0.08739032596349716\n",
      "Similarities: tensor([[1.3987, 0.5054, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3713, 1.3726, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0821, 0.0000, 1.3757, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4101, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4067]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4284974336624146]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.056980609893799\n",
      "Batch 89, Loss: 3.056980609893799\n",
      "Processing Batch 90/157\n",
      "Batch 90 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012883005663752556, Std: 0.08744976669549942\n",
      "z_j Mean: 0.013219241052865982, Std: 0.08739956468343735\n",
      "Similarities: tensor([[1.4093, 0.0000, 0.0588, 1.2861, 0.0000],\n",
      "        [0.0000, 1.4043, 0.0488, 0.0000, 0.0000],\n",
      "        [0.0560, 0.0000, 1.3780, 0.0000, 0.1712],\n",
      "        [1.1052, 0.1022, 0.0000, 1.3821, 0.0000],\n",
      "        [0.0000, 0.0000, 0.3003, 0.0000, 1.0149]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.075308084487915\n",
      "Batch 90, Loss: 3.075308084487915\n",
      "Processing Batch 91/157\n",
      "Batch 91 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013043239712715149, Std: 0.08742600679397583\n",
      "z_j Mean: 0.012956715188920498, Std: 0.08743887394666672\n",
      "Similarities: tensor([[1.3682, 0.0000, 0.0000, 1.3584, 0.0000],\n",
      "        [0.0000, 1.4086, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0491, 0.0000, 1.4271, 0.0166, 1.2692],\n",
      "        [1.0716, 0.0000, 0.3121, 1.3887, 0.2776],\n",
      "        [0.0000, 0.0000, 1.3329, 0.0000, 1.4179]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.040846586227417\n",
      "Batch 91, Loss: 3.040846586227417\n",
      "Processing Batch 92/157\n",
      "Batch 92 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012267469428479671, Std: 0.08753824979066849\n",
      "z_j Mean: 0.011996135115623474, Std: 0.08757584542036057\n",
      "Similarities: tensor([[1.3899, 0.6419, 0.0000, 0.7446, 0.0000],\n",
      "        [0.5790, 1.3495, 0.0044, 0.9332, 0.0565],\n",
      "        [0.0000, 0.0000, 1.3923, 0.0000, 1.3299],\n",
      "        [0.8535, 1.1611, 0.0000, 1.3762, 0.0000],\n",
      "        [0.0000, 0.0000, 1.2310, 0.0000, 1.2588]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.042969226837158\n",
      "Batch 92, Loss: 3.042969226837158\n",
      "Processing Batch 93/157\n",
      "Batch 93 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011984137818217278, Std: 0.08757749199867249\n",
      "z_j Mean: 0.011986193247139454, Std: 0.0875772088766098\n",
      "Similarities: tensor([[0.0000, 1.2975, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.2985, 0.0000, 0.0000, 0.0000],\n",
      "        [1.4242, 0.0000, 1.3874, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.2848, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3630]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.055971384048462\n",
      "Batch 93, Loss: 3.055971384048462\n",
      "Processing Batch 94/157\n",
      "Batch 94 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01189962588250637, Std: 0.08688929677009583\n",
      "z_j Mean: 0.01209090743213892, Std: 0.08756281435489655\n",
      "Similarities: tensor([[1.4276, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.0026, 0.5933, 0.0000, 0.0000],\n",
      "        [0.0000, 0.8410, 1.4096, 0.0520, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0382, 1.4275, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4130]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.040195941925049\n",
      "Batch 94, Loss: 3.040195941925049\n",
      "Processing Batch 95/157\n",
      "Batch 95 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011541951447725296, Std: 0.08763687312602997\n",
      "z_j Mean: 0.011535962112247944, Std: 0.08763765543699265\n",
      "Similarities: tensor([[1.4068, 0.0105, 0.2893, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4219, 1.2035, 0.0000, 0.0000],\n",
      "        [0.1685, 1.2988, 1.3979, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4212, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4286]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0256757736206055\n",
      "Batch 95, Loss: 3.0256757736206055\n",
      "Processing Batch 96/157\n",
      "Batch 96 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011810263618826866, Std: 0.08690149337053299\n",
      "z_j Mean: 0.011963985860347748, Std: 0.08758024871349335\n",
      "Similarities: tensor([[1.2601, 0.6023, 0.0000, 0.0000, 0.7707],\n",
      "        [0.4804, 1.3703, 0.0000, 0.3377, 1.2474],\n",
      "        [0.0000, 0.0000, 1.2675, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1577, 0.0000, 1.2777, 0.2560],\n",
      "        [0.4718, 1.1569, 0.0000, 0.1675, 1.4215]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.051013946533203\n",
      "Batch 96, Loss: 3.051013946533203\n",
      "Processing Batch 97/157\n",
      "Batch 97 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012501204386353493, Std: 0.08680477738380432\n",
      "z_j Mean: 0.013010065071284771, Std: 0.08743095397949219\n",
      "Similarities: tensor([[1.4147, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4152, 0.0000, 1.1991, 0.0000],\n",
      "        [0.0000, 0.0000, 1.1438, 0.0000, 0.0000],\n",
      "        [0.0000, 0.6202, 0.0000, 1.2973, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.1610, 1.2148]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0822813510894775\n",
      "Batch 97, Loss: 3.0822813510894775\n",
      "Processing Batch 98/157\n",
      "Batch 98 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011357571929693222, Std: 0.08696182072162628\n",
      "z_j Mean: 0.010935971513390541, Std: 0.08631148934364319\n",
      "Similarities: tensor([[1.4130, 0.0000, 0.0000, 0.0000, 0.3700],\n",
      "        [0.0000, 1.3968, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3339, 0.0000, 0.0000],\n",
      "        [0.0000, 0.8146, 0.0000, 1.1076, 0.0000],\n",
      "        [0.7798, 0.0000, 0.0000, 0.0000, 1.3439]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.101306438446045\n",
      "Batch 98, Loss: 3.101306438446045\n",
      "Processing Batch 99/157\n",
      "Batch 99 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011861979961395264, Std: 0.0868944451212883\n",
      "z_j Mean: 0.012056555598974228, Std: 0.08616208285093307\n",
      "Similarities: tensor([[1.3473, 0.0000, 0.1823, 0.0000, 1.2866],\n",
      "        [0.0000, 1.4252, 0.2596, 0.0000, 0.0000],\n",
      "        [0.1299, 0.2507, 1.4075, 0.0000, 0.0698],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3370, 0.0000],\n",
      "        [1.1919, 0.0000, 0.1841, 0.0000, 1.3759]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1064319610595703\n",
      "Batch 99, Loss: 3.1064319610595703\n",
      "Processing Batch 100/157\n",
      "Batch 100 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.010600363835692406, Std: 0.08564354479312897\n",
      "z_j Mean: 0.010483785532414913, Std: 0.08636758476495743\n",
      "Similarities: tensor([[1.3584, 1.2825, 0.0000, 0.0000, 0.0000],\n",
      "        [1.2316, 1.2452, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4283, 0.0000, 0.9225],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3985, 0.0000],\n",
      "        [0.0000, 0.0000, 1.2297, 0.0000, 1.3148]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1207964420318604\n",
      "Batch 100, Loss: 3.1207964420318604\n",
      "Processing Batch 101/157\n",
      "Batch 101 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.010442594066262245, Std: 0.08637256920337677\n",
      "z_j Mean: 0.010433855466544628, Std: 0.08637363463640213\n",
      "Similarities: tensor([[1.4002, 0.0000, 0.0000, 0.0000, 1.0366],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4284, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4251, 0.0000],\n",
      "        [1.1137, 0.0000, 0.0000, 0.0000, 1.3734]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0672173500061035\n",
      "Batch 101, Loss: 3.0672173500061035\n",
      "Processing Batch 102/157\n",
      "Batch 102 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011000851169228554, Std: 0.08700767159461975\n",
      "z_j Mean: 0.011127468198537827, Std: 0.0862870067358017\n",
      "Similarities: tensor([[1.3498, 1.4007, 0.0000, 0.0000, 0.9767],\n",
      "        [1.3476, 1.2740, 0.0000, 0.0000, 1.2611],\n",
      "        [0.0000, 0.0000, 1.4273, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.4811, 0.0000],\n",
      "        [1.2230, 1.1477, 0.0000, 0.0000, 1.4122]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1126554012298584\n",
      "Batch 102, Loss: 3.1126554012298584\n",
      "Processing Batch 103/157\n",
      "Batch 103 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.010571502149105072, Std: 0.0856471136212349\n",
      "z_j Mean: 0.010375358164310455, Std: 0.08495558798313141\n",
      "Similarities: tensor([[1.4168, 0.0000, 0.0000, 0.6166, 0.0000],\n",
      "        [0.0000, 1.4132, 0.0000, 0.0596, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4286, 0.0000, 0.0000],\n",
      "        [0.7112, 0.0977, 0.0000, 1.4139, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3742]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1705315113067627\n",
      "Batch 103, Loss: 3.1705315113067627\n",
      "Processing Batch 104/157\n",
      "Batch 104 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.010749315842986107, Std: 0.08633493632078171\n",
      "z_j Mean: 0.010832702741026878, Std: 0.08561446517705917\n",
      "Similarities: tensor([[1.3949, 0.0000, 0.0000, 0.0000, 0.0242],\n",
      "        [0.0000, 1.4118, 0.0000, 0.0000, 0.8840],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3541, 0.0000],\n",
      "        [0.0000, 0.9076, 0.0000, 0.0000, 1.3957]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1341187953948975\n",
      "Batch 104, Loss: 3.1341187953948975\n",
      "Processing Batch 105/157\n",
      "Batch 105 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.010628900490701199, Std: 0.08564000576734543\n",
      "z_j Mean: 0.010866472497582436, Std: 0.08632027357816696\n",
      "Similarities: tensor([[1.4076, 1.3018, 0.0000, 0.0000, 0.0000],\n",
      "        [1.1934, 1.4266, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4194, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4250]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1123971939086914\n",
      "Batch 105, Loss: 3.1123971939086914\n",
      "Processing Batch 106/157\n",
      "Batch 106 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011307675391435623, Std: 0.0869683250784874\n",
      "z_j Mean: 0.01141718216240406, Std: 0.0869540125131607\n",
      "Similarities: tensor([[1.3925, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4286, 0.0000, 0.5401, 0.0000],\n",
      "        [0.0000, 0.0000, 0.7013, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.2014, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4260]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.086369514465332\n",
      "Batch 106, Loss: 3.086369514465332\n",
      "Processing Batch 107/157\n",
      "Batch 107 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.010989530012011528, Std: 0.0863046869635582\n",
      "z_j Mean: 0.010700507089495659, Std: 0.0863410010933876\n",
      "Similarities: tensor([[1.2601, 0.6730, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4286, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3002, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4058, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4089]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1460540294647217\n",
      "Batch 107, Loss: 3.1460540294647217\n",
      "Processing Batch 108/157\n",
      "Batch 108 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011064241640269756, Std: 0.0855848491191864\n",
      "z_j Mean: 0.010925529524683952, Std: 0.08560266345739365\n",
      "Similarities: tensor([[1.3506, 0.0000, 1.0379, 0.0605, 0.0000],\n",
      "        [0.0000, 1.4262, 0.6750, 1.3012, 0.0000],\n",
      "        [1.0793, 0.5856, 1.4253, 0.6549, 0.0000],\n",
      "        [0.0022, 1.2727, 0.7383, 1.4239, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4286]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.175091505050659\n",
      "Batch 108, Loss: 3.175091505050659\n",
      "Processing Batch 109/157\n",
      "Batch 109 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011354136280715466, Std: 0.0869622677564621\n",
      "z_j Mean: 0.010914964601397514, Std: 0.08631414920091629\n",
      "Similarities: tensor([[1.4277, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.2568, 0.1882, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3940, 1.3526, 0.0000, 0.0554],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4217, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4286]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.088498830795288\n",
      "Batch 109, Loss: 3.088498830795288\n",
      "Processing Batch 110/157\n",
      "Batch 110 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.010840082541108131, Std: 0.08772646635770798\n",
      "z_j Mean: 0.010826051235198975, Std: 0.0870295986533165\n",
      "Similarities: tensor([[1.3990, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4286, 0.2546, 0.0000, 0.0000],\n",
      "        [0.3026, 0.0000, 1.3395, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4277, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4144]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0680196285247803\n",
      "Batch 110, Loss: 3.0680196285247803\n",
      "Processing Batch 111/157\n",
      "Batch 111 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.010804921388626099, Std: 0.08632799983024597\n",
      "z_j Mean: 0.011242621578276157, Std: 0.08767577260732651\n",
      "Similarities: tensor([[1.3861, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.2137, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3741, 0.0516, 0.0000],\n",
      "        [0.0000, 0.0000, 0.1145, 0.6098, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4286]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0748162269592285\n",
      "Batch 111, Loss: 3.0748162269592285\n",
      "Processing Batch 112/157\n",
      "Batch 112 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.010992651805281639, Std: 0.08630428463220596\n",
      "z_j Mean: 0.010900928638875484, Std: 0.08560580760240555\n",
      "Similarities: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.2626, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3382, 0.0000, 0.0000],\n",
      "        [0.5342, 0.0000, 0.0000, 1.3755, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4064]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1392405033111572\n",
      "Batch 112, Loss: 3.1392405033111572\n",
      "Processing Batch 113/157\n",
      "Batch 113 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011179650202393532, Std: 0.08698487281799316\n",
      "z_j Mean: 0.011287129484117031, Std: 0.08767005801200867\n",
      "Similarities: tensor([[1.3554, 0.0000, 0.0000, 0.0000, 1.0587],\n",
      "        [0.0000, 1.3814, 0.0000, 0.1813, 0.0000],\n",
      "        [0.0700, 0.0000, 1.3685, 0.0000, 0.4715],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3947, 0.0000],\n",
      "        [1.1034, 0.0000, 0.0451, 0.0000, 1.3223]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.118243932723999\n",
      "Batch 113, Loss: 3.118243932723999\n",
      "Processing Batch 114/157\n",
      "Batch 114 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011373856104910374, Std: 0.08554424345493317\n",
      "z_j Mean: 0.011358609423041344, Std: 0.08625688403844833\n",
      "Similarities: tensor([[1.2440, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4265, 0.0000, 0.0216, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4266, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4174, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3768]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1594021320343018\n",
      "Batch 114, Loss: 3.1594021320343018\n",
      "Processing Batch 115/157\n",
      "Batch 115 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012143254280090332, Std: 0.08755557239055634\n",
      "z_j Mean: 0.012384895235300064, Std: 0.08752170950174332\n",
      "Similarities: tensor([[1.3553, 1.0678, 0.0000, 0.2372, 0.0000],\n",
      "        [1.3851, 1.3727, 0.0000, 0.1177, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3382, 0.0000, 0.0000],\n",
      "        [0.1847, 0.0285, 0.0000, 1.4006, 0.5815],\n",
      "        [0.0000, 0.0000, 0.0000, 0.6346, 1.4284]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0992684364318848\n",
      "Batch 115, Loss: 3.0992684364318848\n",
      "Processing Batch 116/157\n",
      "Batch 116 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011625321581959724, Std: 0.08762584626674652\n",
      "z_j Mean: 0.01176205463707447, Std: 0.08690803498029709\n",
      "Similarities: tensor([[1.3994, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2741, 1.4222, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3404, 0.0000, 1.1223],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4286, 0.0000],\n",
      "        [0.0000, 0.0000, 0.9299, 0.0000, 1.4188]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1308579444885254\n",
      "Batch 116, Loss: 3.1308579444885254\n",
      "Processing Batch 117/157\n",
      "Batch 117 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011675933375954628, Std: 0.0876191183924675\n",
      "z_j Mean: 0.011806306429207325, Std: 0.08760163933038712\n",
      "Similarities: tensor([[1.3747, 0.9603, 0.0000, 1.3862, 0.9976],\n",
      "        [0.8184, 1.4239, 0.0000, 0.7371, 0.1285],\n",
      "        [0.0000, 0.0000, 1.3277, 0.0000, 0.0000],\n",
      "        [1.3333, 0.5080, 0.0000, 1.3956, 1.1998],\n",
      "        [1.0779, 0.1086, 0.0000, 0.8352, 1.3594]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0924837589263916\n",
      "Batch 117, Loss: 3.0924837589263916\n",
      "Processing Batch 118/157\n",
      "Batch 118 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012574976310133934, Std: 0.08749459683895111\n",
      "z_j Mean: 0.012552081607282162, Std: 0.08749788254499435\n",
      "Similarities: tensor([[1.4143, 0.8025, 0.7662, 0.0000, 1.0017],\n",
      "        [0.3753, 1.2031, 0.4620, 0.0000, 0.0203],\n",
      "        [0.7034, 0.9717, 1.3365, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3660, 0.0000],\n",
      "        [1.0998, 0.0407, 0.0000, 0.0000, 1.4099]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.058417320251465\n",
      "Batch 118, Loss: 3.058417320251465\n",
      "Processing Batch 119/157\n",
      "Batch 119 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011494586244225502, Std: 0.08694381266832352\n",
      "z_j Mean: 0.011720611713826656, Std: 0.08691363036632538\n",
      "Similarities: tensor([[1.3994, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4110, 0.0987, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0040, 0.2446, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3862, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4044]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0791871547698975\n",
      "Batch 119, Loss: 3.0791871547698975\n",
      "Processing Batch 120/157\n",
      "Batch 120 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012179329991340637, Std: 0.08685052394866943\n",
      "z_j Mean: 0.012459585443139076, Std: 0.08681076020002365\n",
      "Similarities: tensor([[1.4263, 1.4280, 0.0000, 0.0000, 0.0000],\n",
      "        [1.4237, 1.4263, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4286, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4169]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0834813117980957\n",
      "Batch 120, Loss: 3.0834813117980957\n",
      "Processing Batch 121/157\n",
      "Batch 121 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012221220880746841, Std: 0.08754470944404602\n",
      "z_j Mean: 0.012677936814725399, Std: 0.08747973293066025\n",
      "Similarities: tensor([[1.4036, 0.0941, 0.5422, 0.0000, 0.0000],\n",
      "        [0.1754, 1.3781, 0.0000, 0.0000, 0.0000],\n",
      "        [0.6062, 0.3409, 1.3075, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4066, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3892]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0957419872283936\n",
      "Batch 121, Loss: 3.0957419872283936\n",
      "Processing Batch 122/157\n",
      "Batch 122 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011953459121286869, Std: 0.08758167922496796\n",
      "z_j Mean: 0.012160085141658783, Std: 0.0875532254576683\n",
      "Similarities: tensor([[1.4255e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 1.3360e+00, 5.8137e-03, 0.0000e+00, 1.3665e-01],\n",
      "        [0.0000e+00, 7.9928e-03, 1.3731e+00, 0.0000e+00, 2.6997e-04],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 1.3639e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 3.4105e-01, 0.0000e+00, 0.0000e+00, 1.1449e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.054612636566162\n",
      "Batch 122, Loss: 3.054612636566162\n",
      "Processing Batch 123/157\n",
      "Batch 123 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013227620162069798, Std: 0.08739829808473587\n",
      "z_j Mean: 0.012930795550346375, Std: 0.08603521436452866\n",
      "Similarities: tensor([[1.2916, 0.0000, 0.0000, 0.0000, 0.5295],\n",
      "        [0.0000, 1.4223, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4223, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4254, 0.0000],\n",
      "        [0.9294, 0.0000, 0.0000, 0.0000, 1.2500]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285393953323364]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.107863426208496\n",
      "Batch 123, Loss: 3.107863426208496\n",
      "Processing Batch 124/157\n",
      "Batch 124 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012944376096129417, Std: 0.08673978596925735\n",
      "z_j Mean: 0.013278204947710037, Std: 0.08739062398672104\n",
      "Similarities: tensor([[1.3580, 0.0000, 0.4680, 0.0000, 0.0000],\n",
      "        [0.0544, 1.3855, 0.0000, 0.0000, 0.0000],\n",
      "        [0.7809, 0.0000, 1.3966, 0.0000, 0.0932],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4106, 0.6337],\n",
      "        [0.0016, 0.0000, 0.1172, 0.4087, 1.3337]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.042806625366211\n",
      "Batch 124, Loss: 3.042806625366211\n",
      "Processing Batch 125/157\n",
      "Batch 125 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013143565505743027, Std: 0.08741097897291183\n",
      "z_j Mean: 0.013366210274398327, Std: 0.08737720549106598\n",
      "Similarities: tensor([[1.3736, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4072, 0.4109, 0.4327, 0.0465],\n",
      "        [0.0000, 0.5679, 1.4136, 1.3403, 0.1342],\n",
      "        [0.0000, 0.5410, 1.3881, 1.2880, 0.1269],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.2570]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.043178081512451\n",
      "Batch 125, Loss: 3.043178081512451\n",
      "Processing Batch 126/157\n",
      "Batch 126 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012942947447299957, Std: 0.08744091540575027\n",
      "z_j Mean: 0.01251628715544939, Std: 0.08750300854444504\n",
      "Similarities: tensor([[1.4095, 0.0000, 0.8356, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4151, 0.0000, 0.0000, 0.0000],\n",
      "        [0.6896, 0.0000, 1.3872, 0.0000, 0.0066],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4248, 0.5313],\n",
      "        [0.0000, 0.0000, 0.0000, 0.5563, 1.3932]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.057476758956909\n",
      "Batch 126, Loss: 3.057476758956909\n",
      "Processing Batch 127/157\n",
      "Batch 127 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012447353452444077, Std: 0.08751284331083298\n",
      "z_j Mean: 0.012999370694160461, Std: 0.08743254095315933\n",
      "Similarities: tensor([[1.4286, 0.0000, 0.0000, 0.7747, 0.0000],\n",
      "        [0.0000, 1.4221, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.2379, 0.0000, 0.0000],\n",
      "        [0.0447, 0.0000, 0.0000, 1.0265, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3909]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.051823854446411\n",
      "Batch 127, Loss: 3.051823854446411\n",
      "Processing Batch 128/157\n",
      "Batch 128 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012140318751335144, Std: 0.08755596727132797\n",
      "z_j Mean: 0.012553299777209759, Std: 0.08749771118164062\n",
      "Similarities: tensor([[1.4286, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4017, 0.0000, 0.2130, 0.6752],\n",
      "        [0.0000, 0.0000, 1.4028, 0.4019, 0.6902],\n",
      "        [0.0000, 0.3556, 0.6158, 1.3555, 0.3554],\n",
      "        [0.0000, 0.3750, 0.9727, 0.0914, 1.3597]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0390563011169434\n",
      "Batch 128, Loss: 3.0390563011169434\n",
      "Processing Batch 129/157\n",
      "Batch 129 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012635046616196632, Std: 0.08748593926429749\n",
      "z_j Mean: 0.0126973707228899, Std: 0.08747691661119461\n",
      "Similarities: tensor([[1.3772, 0.0000, 0.1315, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4095, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1733, 0.0000, 1.3989, 0.1170, 0.0000],\n",
      "        [0.0000, 0.1254, 0.0026, 1.3662, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4121]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.051757335662842\n",
      "Batch 129, Loss: 3.051757335662842\n",
      "Processing Batch 130/157\n",
      "Batch 130 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012836163863539696, Std: 0.08745665848255157\n",
      "z_j Mean: 0.013058342970907688, Std: 0.08742375671863556\n",
      "Similarities: tensor([[1.3891, 0.0000, 0.8672, 0.0161, 0.0000],\n",
      "        [0.0000, 1.3008, 0.0000, 0.8458, 1.1922],\n",
      "        [0.3715, 0.0000, 1.1444, 0.0000, 0.0000],\n",
      "        [0.0000, 1.2014, 0.0000, 1.4245, 1.1501],\n",
      "        [0.0000, 1.3445, 0.0000, 1.0484, 1.4031]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0642616748809814\n",
      "Batch 130, Loss: 3.0642616748809814\n",
      "Processing Batch 131/157\n",
      "Batch 131 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012396094389259815, Std: 0.08752012252807617\n",
      "z_j Mean: 0.012183358892798424, Std: 0.08754999935626984\n",
      "Similarities: tensor([[1.4274, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4200, 0.0000, 0.0000, 1.3135],\n",
      "        [0.0000, 0.0000, 1.2850, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4286, 0.0000],\n",
      "        [0.0000, 0.7424, 0.0000, 0.0000, 1.1983]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.060250997543335\n",
      "Batch 131, Loss: 3.060250997543335\n",
      "Processing Batch 132/157\n",
      "Batch 132 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011945368722081184, Std: 0.0875827819108963\n",
      "z_j Mean: 0.01225917600095272, Std: 0.08753940463066101\n",
      "Similarities: tensor([[1.3042, 0.0107, 0.0000, 0.0000, 1.3375],\n",
      "        [0.0619, 1.3861, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0860, 0.0000, 1.3511, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0272, 1.0239, 0.0000],\n",
      "        [1.0199, 0.0000, 0.0000, 0.0000, 1.3864]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.072209596633911\n",
      "Batch 132, Loss: 3.072209596633911\n",
      "Processing Batch 133/157\n",
      "Batch 133 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012086629867553711, Std: 0.0875634029507637\n",
      "z_j Mean: 0.01205517165362835, Std: 0.08756773918867111\n",
      "Similarities: tensor([[1.4220, 0.0000, 0.8628, 0.0000, 0.0000],\n",
      "        [0.0000, 1.1420, 0.0000, 0.0000, 0.0245],\n",
      "        [1.3871, 0.0000, 1.1389, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0095, 1.3938, 0.0559],\n",
      "        [0.0000, 0.1840, 0.0000, 0.1352, 1.3310]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0492398738861084\n",
      "Batch 133, Loss: 3.0492398738861084\n",
      "Processing Batch 134/157\n",
      "Batch 134 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012689577415585518, Std: 0.08747804909944534\n",
      "z_j Mean: 0.012684082612395287, Std: 0.08747883886098862\n",
      "Similarities: tensor([[1.4166, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2354, 1.1286, 0.0223, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1127, 1.4159, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3099, 0.0090],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.1164]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1111128330230713\n",
      "Batch 134, Loss: 3.1111128330230713\n",
      "Processing Batch 135/157\n",
      "Batch 135 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01294144056737423, Std: 0.08744113147258759\n",
      "z_j Mean: 0.012631158344447613, Std: 0.08748650550842285\n",
      "Similarities: tensor([[1.3986, 1.2452, 0.9402, 0.0000, 1.0238],\n",
      "        [1.2633, 1.4244, 0.9956, 0.0000, 0.9584],\n",
      "        [0.8646, 0.8522, 1.3966, 0.0000, 0.6751],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4173, 0.0000],\n",
      "        [1.0237, 0.7891, 0.8573, 0.0000, 1.3435]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0925991535186768\n",
      "Batch 135, Loss: 3.0925991535186768\n",
      "Processing Batch 136/157\n",
      "Batch 136 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012755243107676506, Std: 0.08746849745512009\n",
      "z_j Mean: 0.01281462237238884, Std: 0.08745981752872467\n",
      "Similarities: tensor([[1.4258, 0.1442, 1.4184, 0.0000, 1.1912],\n",
      "        [0.2080, 1.3222, 0.2944, 0.0000, 0.0983],\n",
      "        [1.4248, 0.2096, 1.4218, 0.0000, 1.1862],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3613, 0.0000],\n",
      "        [1.1604, 0.0153, 1.1574, 0.0000, 1.4268]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0521597862243652\n",
      "Batch 136, Loss: 3.0521597862243652\n",
      "Processing Batch 137/157\n",
      "Batch 137 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012823248282074928, Std: 0.0874585509300232\n",
      "z_j Mean: 0.012852596119046211, Std: 0.08745424449443817\n",
      "Similarities: tensor([[1.4136, 0.0000, 0.0000, 0.8953, 0.0242],\n",
      "        [0.0000, 1.3905, 1.1283, 0.0261, 0.0000],\n",
      "        [0.0000, 0.8996, 1.4009, 0.0971, 0.0000],\n",
      "        [0.7822, 0.1825, 0.1117, 1.3260, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.2970]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0197458267211914\n",
      "Batch 137, Loss: 3.0197458267211914\n",
      "Processing Batch 138/157\n",
      "Batch 138 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012535123154520988, Std: 0.08750031143426895\n",
      "z_j Mean: 0.012818731367588043, Std: 0.08745921403169632\n",
      "Similarities: tensor([[1.4096, 0.0000, 0.0000, 0.2354, 0.0000],\n",
      "        [0.0000, 1.3844, 0.3542, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1258, 1.2489, 0.0000, 0.0000],\n",
      "        [0.1160, 0.0000, 0.0000, 1.3954, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3480]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.064694881439209\n",
      "Batch 138, Loss: 3.064694881439209\n",
      "Processing Batch 139/157\n",
      "Batch 139 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012720158323645592, Std: 0.08747360110282898\n",
      "z_j Mean: 0.012881343252956867, Std: 0.08745001256465912\n",
      "Similarities: tensor([[1.4280, 0.0000, 0.0000, 0.0000, 0.3326],\n",
      "        [0.0000, 1.4159, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3643, 0.1627, 0.0000],\n",
      "        [0.0000, 0.0000, 0.2837, 1.4099, 0.0000],\n",
      "        [0.3312, 0.0000, 0.0000, 0.0000, 1.3719]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0488438606262207\n",
      "Batch 139, Loss: 3.0488438606262207\n",
      "Processing Batch 140/157\n",
      "Batch 140 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013035513460636139, Std: 0.08742716163396835\n",
      "z_j Mean: 0.013255754485726357, Std: 0.08739403635263443\n",
      "Similarities: tensor([[1.4253, 0.0000, 0.0470, 1.1525, 0.0000],\n",
      "        [0.0332, 1.1592, 0.0000, 0.0746, 0.6095],\n",
      "        [0.0520, 0.0000, 1.4143, 0.0897, 0.0000],\n",
      "        [1.2893, 0.0000, 0.0560, 1.2860, 0.0000],\n",
      "        [0.0000, 0.3102, 0.0000, 0.0000, 1.4019]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.082568883895874\n",
      "Batch 140, Loss: 3.082568883895874\n",
      "Processing Batch 141/157\n",
      "Batch 141 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01259423978626728, Std: 0.08749182522296906\n",
      "z_j Mean: 0.012683359906077385, Std: 0.08747895061969757\n",
      "Similarities: tensor([[1.4221, 0.4640, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2403, 1.3447, 0.0000, 0.3558, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4188, 0.0000, 0.3543],\n",
      "        [0.0000, 0.3900, 0.0000, 1.3876, 0.0000],\n",
      "        [0.0000, 0.0000, 0.1622, 0.0000, 1.3846]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.02767276763916\n",
      "Batch 141, Loss: 3.02767276763916\n",
      "Processing Batch 142/157\n",
      "Batch 142 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012351663783192635, Std: 0.08752640336751938\n",
      "z_j Mean: 0.012389172799885273, Std: 0.08752109855413437\n",
      "Similarities: tensor([[1.3784, 0.0000, 0.1347, 0.9622, 1.0387],\n",
      "        [0.0000, 1.3406, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1339, 0.0000, 1.4103, 0.1675, 0.8271],\n",
      "        [0.7943, 0.0000, 0.1615, 1.3943, 0.4185],\n",
      "        [1.0047, 0.0000, 0.8176, 0.6351, 1.4200]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.037101984024048\n",
      "Batch 142, Loss: 3.037101984024048\n",
      "Processing Batch 143/157\n",
      "Batch 143 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012575278989970684, Std: 0.08749455213546753\n",
      "z_j Mean: 0.012523151002824306, Std: 0.08750202506780624\n",
      "Similarities: tensor([[1.3820, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3301, 0.0465, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1121, 1.4109, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4210, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3874]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0376248359680176\n",
      "Batch 143, Loss: 3.0376248359680176\n",
      "Processing Batch 144/157\n",
      "Batch 144 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013100657612085342, Std: 0.08741742372512817\n",
      "z_j Mean: 0.013042965903878212, Std: 0.08742604404687881\n",
      "Similarities: tensor([[0.8996, 1.0407, 0.0000, 0.2248, 0.0000],\n",
      "        [1.2209, 1.3184, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4229, 0.0000, 1.2301],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3523, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3239, 0.0000, 1.4026]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0910658836364746\n",
      "Batch 144, Loss: 3.0910658836364746\n",
      "Processing Batch 145/157\n",
      "Batch 145 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012881976552307606, Std: 0.08744991570711136\n",
      "z_j Mean: 0.0126445097848773, Std: 0.08748456835746765\n",
      "Similarities: tensor([[1.3959, 0.0000, 0.9907, 0.0000, 0.0000],\n",
      "        [0.0000, 1.2796, 0.0000, 0.0000, 0.0000],\n",
      "        [1.2049, 0.0000, 1.4168, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4278, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3439]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.049877405166626\n",
      "Batch 145, Loss: 3.049877405166626\n",
      "Processing Batch 146/157\n",
      "Batch 146 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012346680276095867, Std: 0.08752710372209549\n",
      "z_j Mean: 0.012489492073655128, Std: 0.08750683814287186\n",
      "Similarities: tensor([[1.3877, 0.0000, 0.0000, 0.0000, 0.7934],\n",
      "        [0.0000, 1.4286, 0.0000, 0.0000, 0.7522],\n",
      "        [0.0000, 0.0000, 0.0000, 0.7819, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4198, 0.0000],\n",
      "        [0.8398, 0.4648, 0.0000, 0.0000, 1.3721]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.066021680831909\n",
      "Batch 146, Loss: 3.066021680831909\n",
      "Processing Batch 147/157\n",
      "Batch 147 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01251155138015747, Std: 0.08750369399785995\n",
      "z_j Mean: 0.01244010217487812, Std: 0.08751387894153595\n",
      "Similarities: tensor([[1.4270, 0.3515, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4658, 1.3430, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3616, 1.0850, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3971, 1.3162, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.1759]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.032198905944824\n",
      "Batch 147, Loss: 3.032198905944824\n",
      "Processing Batch 148/157\n",
      "Batch 148 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011952544562518597, Std: 0.0875818058848381\n",
      "z_j Mean: 0.011651888489723206, Std: 0.08692286908626556\n",
      "Similarities: tensor([[0.8307, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1329, 1.3435, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0352, 1.3883, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4275, 0.5296],\n",
      "        [0.0000, 0.0000, 0.0000, 0.6581, 1.3989]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0557844638824463\n",
      "Batch 148, Loss: 3.0557844638824463\n",
      "Processing Batch 149/157\n",
      "Batch 149 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012028161436319351, Std: 0.08687159419059753\n",
      "z_j Mean: 0.012212447822093964, Std: 0.08754593878984451\n",
      "Similarities: tensor([[1.4255, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3565, 0.1061, 0.1087, 0.0000],\n",
      "        [0.0000, 0.0105, 1.3784, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0425, 0.0167, 1.1452, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4156]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.05005145072937\n",
      "Batch 149, Loss: 3.05005145072937\n",
      "Processing Batch 150/157\n",
      "Batch 150 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012855226173996925, Std: 0.08745385706424713\n",
      "z_j Mean: 0.012771040201187134, Std: 0.08746618777513504\n",
      "Similarities: tensor([[1.2490, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3606, 0.0000, 0.2433, 0.0023],\n",
      "        [0.0753, 0.0000, 1.3840, 0.0000, 0.0328],\n",
      "        [0.0000, 0.0000, 0.0000, 0.3979, 0.5575],\n",
      "        [0.0000, 0.0000, 0.0708, 0.2520, 1.4251]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.080029249191284\n",
      "Batch 150, Loss: 3.080029249191284\n",
      "Processing Batch 151/157\n",
      "Batch 151 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012003131210803986, Std: 0.08757488429546356\n",
      "z_j Mean: 0.01199842058122158, Std: 0.0875755324959755\n",
      "Similarities: tensor([[1.0531, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.2445, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4045, 0.0000, 0.0238],\n",
      "        [0.0000, 0.0000, 0.0703, 1.4111, 0.0000],\n",
      "        [0.0000, 0.0000, 0.2055, 0.0000, 1.4285]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 2.994887113571167\n",
      "Batch 151, Loss: 2.994887113571167\n",
      "Processing Batch 152/157\n",
      "Batch 152 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012098386883735657, Std: 0.08756177872419357\n",
      "z_j Mean: 0.012385699898004532, Std: 0.08752159029245377\n",
      "Similarities: tensor([[1.4286, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4241, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4163, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.9815, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3997]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.070341110229492\n",
      "Batch 152, Loss: 3.070341110229492\n",
      "Processing Batch 153/157\n",
      "Batch 153 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012586728669703007, Std: 0.08749290555715561\n",
      "z_j Mean: 0.012717507779598236, Std: 0.08747398853302002\n",
      "Similarities: tensor([[1.4248, 0.2918, 0.0000, 0.5135, 0.0000],\n",
      "        [0.4582, 1.2280, 0.0000, 1.4052, 0.0000],\n",
      "        [0.0000, 0.0000, 1.2041, 0.0000, 0.0000],\n",
      "        [0.5102, 0.8067, 0.0000, 1.3830, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.2154]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0736544132232666\n",
      "Batch 153, Loss: 3.0736544132232666\n",
      "Processing Batch 154/157\n",
      "Batch 154 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012349036522209644, Std: 0.08612064272165298\n",
      "z_j Mean: 0.012945078313350677, Std: 0.086739681661129\n",
      "Similarities: tensor([[1.4019, 0.0000, 0.0000, 0.2344, 0.2762],\n",
      "        [0.0000, 1.4275, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3750, 0.0000, 0.0000],\n",
      "        [0.1054, 0.0000, 0.0000, 1.2376, 0.2260],\n",
      "        [0.2937, 0.0000, 0.0000, 0.4860, 1.3883]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0735607147216797\n",
      "Batch 154, Loss: 3.0735607147216797\n",
      "Processing Batch 155/157\n",
      "Batch 155 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01195776928216219, Std: 0.0875810980796814\n",
      "z_j Mean: 0.011863691732287407, Std: 0.08618886023759842\n",
      "Similarities: tensor([[1.3971, 0.0938, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1436, 1.3949, 0.0000, 0.0341, 0.0205],\n",
      "        [0.0000, 0.0000, 1.4101, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2453, 0.0000, 1.3319, 0.4992],\n",
      "        [0.0000, 0.0941, 0.0000, 0.6137, 1.3830]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1117050647735596\n",
      "Batch 155, Loss: 3.1117050647735596\n",
      "Processing Batch 156/157\n",
      "Batch 156 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.0123821422457695, Std: 0.08682184666395187\n",
      "z_j Mean: 0.012384592555463314, Std: 0.0875217467546463\n",
      "Similarities: tensor([[1.3619e+00, 0.0000e+00, 4.5698e-05, 0.0000e+00, 8.2129e-01],\n",
      "        [0.0000e+00, 1.2895e+00, 1.2451e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [1.3097e-01, 1.2531e+00, 1.3696e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [3.5500e-02, 0.0000e+00, 0.0000e+00, 1.3906e+00, 1.1404e-01],\n",
      "        [5.6939e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.3037e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0910277366638184\n",
      "Batch 156, Loss: 3.0910277366638184\n",
      "Processing Batch 157/157\n",
      "Batch 157 loaded: x.shape=torch.Size([16, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([16, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([16, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([16, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([16, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([16, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01000051386654377, Std: 0.08501610159873962\n",
      "z_j Mean: 0.010079434141516685, Std: 0.08500678092241287\n",
      "Similarities: tensor([[1.4279, 1.0095, 1.3639, 0.0000, 0.0000],\n",
      "        [1.3119, 1.3413, 1.4160, 0.0000, 0.0000],\n",
      "        [1.3888, 1.2382, 1.4275, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0388, 0.0000],\n",
      "        [0.0075, 0.0000, 0.0000, 0.0000, 1.4177]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 1.8829693794250488\n",
      "Batch 157, Loss: 1.8829693794250488\n",
      "Epoch 7/10, Loss: 3.0682\n",
      "Entered Epoch 8\n",
      "Starting Training Loop with 157 batches.\n",
      "Processing Batch 1/157\n",
      "Batch 1 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012982103042304516, Std: 0.08743510395288467\n",
      "z_j Mean: 0.013089923188090324, Std: 0.08741902559995651\n",
      "Similarities: tensor([[1.3667, 0.0312, 0.5078, 1.0992, 0.0000],\n",
      "        [0.1110, 1.3999, 0.6743, 0.1197, 0.0000],\n",
      "        [0.6801, 0.5872, 1.3971, 1.0894, 0.0000],\n",
      "        [0.9587, 0.0350, 0.8003, 1.4146, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.2841]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.03275465965271\n",
      "Batch 1, Loss: 3.03275465965271\n",
      "Processing Batch 2/157\n",
      "Batch 2 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012474318966269493, Std: 0.08610258251428604\n",
      "z_j Mean: 0.012455061078071594, Std: 0.08681140840053558\n",
      "Similarities: tensor([[1.3945, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4275, 0.6024, 1.0038, 0.0000],\n",
      "        [0.0000, 0.5163, 1.2803, 0.9832, 0.1693],\n",
      "        [0.0000, 1.0165, 0.7996, 1.4094, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0656, 0.0000, 0.7803]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1131093502044678\n",
      "Batch 2, Loss: 3.1131093502044678\n",
      "Processing Batch 3/157\n",
      "Batch 3 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012917235493659973, Std: 0.0874447152018547\n",
      "z_j Mean: 0.01288240309804678, Std: 0.08674901723861694\n",
      "Similarities: tensor([[1.4229, 0.0000, 0.0680, 0.0000, 0.3321],\n",
      "        [0.0000, 1.0472, 1.2125, 0.0000, 0.5826],\n",
      "        [0.0079, 0.9451, 1.4183, 0.0000, 0.8857],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4171, 0.0000],\n",
      "        [0.4495, 0.5690, 0.7659, 0.0000, 1.3923]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1047298908233643\n",
      "Batch 3, Loss: 3.1047298908233643\n",
      "Processing Batch 4/157\n",
      "Batch 4 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012648027390241623, Std: 0.08748406916856766\n",
      "z_j Mean: 0.012797238305211067, Std: 0.08746236562728882\n",
      "Similarities: tensor([[1.4194, 0.0000, 0.0000, 0.0000, 0.2098],\n",
      "        [0.0000, 1.2538, 0.0221, 0.1409, 0.0000],\n",
      "        [0.0000, 0.0321, 1.4116, 0.6272, 0.0000],\n",
      "        [0.0000, 0.3763, 0.7469, 1.3340, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3940]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0285298824310303\n",
      "Batch 4, Loss: 3.0285298824310303\n",
      "Processing Batch 5/157\n",
      "Batch 5 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012762505561113358, Std: 0.08676674216985703\n",
      "z_j Mean: 0.01310919038951397, Std: 0.0874161422252655\n",
      "Similarities: tensor([[1.3759, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3910, 0.0000, 0.4493, 1.4165],\n",
      "        [0.0000, 0.0000, 1.4009, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3332, 0.0000, 1.3765, 0.2702],\n",
      "        [0.0000, 1.3438, 0.0000, 0.4006, 1.4254]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0512566566467285\n",
      "Batch 5, Loss: 3.0512566566467285\n",
      "Processing Batch 6/157\n",
      "Batch 6 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01323087140917778, Std: 0.0866965502500534\n",
      "z_j Mean: 0.01362922228872776, Std: 0.0866348147392273\n",
      "Similarities: tensor([[1.4031, 0.0024, 0.2789, 0.0000, 0.0000],\n",
      "        [0.0161, 1.4076, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3457, 0.0000, 1.4150, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4253, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4286]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1073577404022217\n",
      "Batch 6, Loss: 3.1073577404022217\n",
      "Processing Batch 7/157\n",
      "Batch 7 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013248845934867859, Std: 0.087395079433918\n",
      "z_j Mean: 0.01271781325340271, Std: 0.08606696128845215\n",
      "Similarities: tensor([[1.0825, 0.0000, 0.0000, 0.0204, 0.0000],\n",
      "        [0.0000, 1.4250, 0.8449, 1.0610, 0.0000],\n",
      "        [0.0000, 0.6269, 1.3664, 0.6286, 0.0000],\n",
      "        [0.0000, 1.0477, 0.8659, 1.3906, 0.0000],\n",
      "        [0.0000, 0.0000, 0.3861, 0.0000, 0.0000]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0976948738098145\n",
      "Batch 7, Loss: 3.0976948738098145\n",
      "Processing Batch 8/157\n",
      "Batch 8 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013148846104741096, Std: 0.08741018176078796\n",
      "z_j Mean: 0.012983692809939384, Std: 0.08673391491174698\n",
      "Similarities: tensor([[1.3707, 0.0000, 0.0000, 0.3597, 0.3911],\n",
      "        [0.0000, 1.4264, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3482, 0.0000, 0.0000],\n",
      "        [0.2680, 0.0000, 0.0000, 1.4158, 0.2034],\n",
      "        [0.1913, 0.0000, 0.0000, 0.0000, 1.2686]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.070284128189087\n",
      "Batch 8, Loss: 3.070284128189087\n",
      "Processing Batch 9/157\n",
      "Batch 9 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013134014792740345, Std: 0.08671128004789352\n",
      "z_j Mean: 0.013300491496920586, Std: 0.08738724142313004\n",
      "Similarities: tensor([[1.4272, 0.0000, 0.0000, 0.0701, 0.0000],\n",
      "        [0.0000, 1.3962, 0.0689, 0.0000, 0.4338],\n",
      "        [0.0000, 0.1750, 1.3986, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3645, 0.0000],\n",
      "        [0.0000, 0.4043, 0.0000, 0.1067, 1.3880]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0471901893615723\n",
      "Batch 9, Loss: 3.0471901893615723\n",
      "Processing Batch 10/157\n",
      "Batch 10 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012804010882973671, Std: 0.08676062524318695\n",
      "z_j Mean: 0.013056240975856781, Std: 0.08742406964302063\n",
      "Similarities: tensor([[1.3578, 0.2642, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1297, 1.4086, 0.1322, 1.1272, 0.0000],\n",
      "        [0.0000, 0.2477, 1.4231, 0.3074, 0.0000],\n",
      "        [0.0000, 0.6684, 0.9294, 1.1558, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4213]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285272359848022]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.131857395172119\n",
      "Batch 10, Loss: 3.131857395172119\n",
      "Processing Batch 11/157\n",
      "Batch 11 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012809756211936474, Std: 0.08746052533388138\n",
      "z_j Mean: 0.012964446097612381, Std: 0.08743772655725479\n",
      "Similarities: tensor([[1.4045, 0.2116, 0.0000, 0.8747, 0.0000],\n",
      "        [0.1457, 1.2652, 0.0000, 0.7231, 0.0000],\n",
      "        [0.0000, 0.0202, 1.3754, 0.1924, 0.0000],\n",
      "        [0.9592, 0.4856, 0.0000, 1.4255, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.0036]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1146280765533447\n",
      "Batch 11, Loss: 3.1146280765533447\n",
      "Processing Batch 12/157\n",
      "Batch 12 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013274190947413445, Std: 0.08739123493432999\n",
      "z_j Mean: 0.013046842068433762, Std: 0.08672443777322769\n",
      "Similarities: tensor([[1.4261, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3394, 0.0000, 0.0000, 0.4698],\n",
      "        [0.0000, 0.0000, 1.4286, 0.0000, 0.0228],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3548, 0.0000],\n",
      "        [0.0000, 0.0053, 0.0378, 0.0000, 1.4239]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0360069274902344\n",
      "Batch 12, Loss: 3.0360069274902344\n",
      "Processing Batch 13/157\n",
      "Batch 13 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012595802545547485, Std: 0.08679109811782837\n",
      "z_j Mean: 0.013187503442168236, Std: 0.08740436285734177\n",
      "Similarities: tensor([[1.4279, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3153, 0.0000, 0.2167, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3290, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4184, 0.0447],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0236, 1.4022]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.093547821044922\n",
      "Batch 13, Loss: 3.093547821044922\n",
      "Processing Batch 14/157\n",
      "Batch 14 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012874380685389042, Std: 0.0874510332942009\n",
      "z_j Mean: 0.012789399363100529, Std: 0.08676277846097946\n",
      "Similarities: tensor([[1.3561, 0.4213, 0.1310, 0.0000, 0.3627],\n",
      "        [0.3442, 1.3898, 0.0106, 0.2054, 0.2914],\n",
      "        [0.2212, 0.0207, 1.4138, 0.0404, 0.2684],\n",
      "        [0.0000, 0.0725, 0.0451, 1.3882, 0.9667],\n",
      "        [0.4199, 0.2296, 0.2369, 1.0782, 1.3998]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.045703411102295\n",
      "Batch 14, Loss: 3.045703411102295\n",
      "Processing Batch 15/157\n",
      "Batch 15 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012881986796855927, Std: 0.08744991570711136\n",
      "z_j Mean: 0.012904414907097816, Std: 0.08744661509990692\n",
      "Similarities: tensor([[1.4240e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 1.4254e+00, 1.1252e-03, 0.0000e+00, 7.3526e-01],\n",
      "        [0.0000e+00, 2.8219e-03, 1.3939e+00, 3.4801e-01, 5.7644e-01],\n",
      "        [0.0000e+00, 0.0000e+00, 3.6190e-01, 1.4209e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 7.7415e-01, 3.9916e-01, 0.0000e+00, 1.4272e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0510714054107666\n",
      "Batch 15, Loss: 3.0510714054107666\n",
      "Processing Batch 16/157\n",
      "Batch 16 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012350769713521004, Std: 0.08682630956172943\n",
      "z_j Mean: 0.012516634538769722, Std: 0.08750296384096146\n",
      "Similarities: tensor([[1.4195, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4163, 0.0251, 0.6802, 0.4672],\n",
      "        [0.0000, 0.0000, 1.4157, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3230, 0.0000, 0.7360, 0.5055],\n",
      "        [0.0000, 0.0543, 0.0000, 0.0302, 1.3528]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4272109270095825]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.064173460006714\n",
      "Batch 16, Loss: 3.064173460006714\n",
      "Processing Batch 17/157\n",
      "Batch 17 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01209947094321251, Std: 0.08756162971258163\n",
      "z_j Mean: 0.012451564893126488, Std: 0.08681191504001617\n",
      "Similarities: tensor([[1.4202, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.0155, 0.0000, 0.2851, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3583, 0.0000, 0.1793],\n",
      "        [0.0000, 1.1282, 0.0000, 1.4256, 0.0000],\n",
      "        [0.1314, 0.0000, 0.1626, 0.0000, 1.1046]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1244306564331055\n",
      "Batch 17, Loss: 3.1244306564331055\n",
      "Processing Batch 18/157\n",
      "Batch 18 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012761309742927551, Std: 0.08606051653623581\n",
      "z_j Mean: 0.012698196806013584, Std: 0.08606985211372375\n",
      "Similarities: tensor([[1.3300, 0.0000, 0.0000, 1.1071, 0.0000],\n",
      "        [0.0000, 1.4013, 0.0000, 0.0000, 0.6515],\n",
      "        [0.0000, 0.0000, 1.4194, 0.0000, 0.0000],\n",
      "        [1.1378, 0.0000, 0.0000, 1.3939, 0.0000],\n",
      "        [0.0000, 0.6961, 0.0000, 0.0000, 1.4204]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.149106979370117\n",
      "Batch 18, Loss: 3.149106979370117\n",
      "Processing Batch 19/157\n",
      "Batch 19 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01224160473793745, Std: 0.08684176951646805\n",
      "z_j Mean: 0.012400039471685886, Std: 0.08681929111480713\n",
      "Similarities: tensor([[1.4207, 0.0000, 0.1258, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3793, 0.0000, 0.0000, 0.1009],\n",
      "        [0.8337, 0.1346, 1.0452, 0.4320, 0.0000],\n",
      "        [0.0000, 0.0000, 0.5550, 1.3195, 0.0000],\n",
      "        [0.0000, 0.0778, 0.0000, 0.0000, 1.4098]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0868418216705322\n",
      "Batch 19, Loss: 3.0868418216705322\n",
      "Processing Batch 20/157\n",
      "Batch 20 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012943316251039505, Std: 0.0874408558011055\n",
      "z_j Mean: 0.013036860153079033, Std: 0.08672593533992767\n",
      "Similarities: tensor([[1.4250, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4276, 0.4631, 0.0000, 0.0000],\n",
      "        [0.0000, 0.4501, 1.4246, 0.1302, 0.0000],\n",
      "        [0.0090, 0.0000, 0.0925, 1.3705, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.2447]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.033381223678589\n",
      "Batch 20, Loss: 3.033381223678589\n",
      "Processing Batch 21/157\n",
      "Batch 21 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012537946924567223, Std: 0.08749991655349731\n",
      "z_j Mean: 0.012837718240916729, Std: 0.08745642751455307\n",
      "Similarities: tensor([[1.3308, 0.1415, 0.0000, 0.2096, 1.4093],\n",
      "        [0.2091, 1.4173, 0.9009, 0.0000, 0.2250],\n",
      "        [0.0000, 0.8404, 1.4199, 0.0000, 0.0000],\n",
      "        [0.2684, 0.0000, 0.0000, 1.3913, 0.2311],\n",
      "        [1.3848, 0.1629, 0.0000, 0.2174, 1.3592]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.428478479385376]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.055262327194214\n",
      "Batch 21, Loss: 3.055262327194214\n",
      "Processing Batch 22/157\n",
      "Batch 22 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012927708216011524, Std: 0.08744316548109055\n",
      "z_j Mean: 0.012933744117617607, Std: 0.08744227141141891\n",
      "Similarities: tensor([[1.4167, 0.0375, 0.0000, 0.9860, 1.3885],\n",
      "        [0.0000, 1.3997, 0.0000, 0.8660, 0.3306],\n",
      "        [0.0000, 0.0000, 1.3482, 0.0000, 0.0000],\n",
      "        [0.8256, 0.8293, 0.0000, 1.4243, 1.1453],\n",
      "        [1.2673, 0.3055, 0.0000, 1.2474, 1.3843]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0136473178863525\n",
      "Batch 22, Loss: 3.0136473178863525\n",
      "Processing Batch 23/157\n",
      "Batch 23 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012504132464528084, Std: 0.0868043527007103\n",
      "z_j Mean: 0.012198634445667267, Std: 0.08684781938791275\n",
      "Similarities: tensor([[1.4246, 0.0000, 0.1285, 0.0050, 0.0000],\n",
      "        [0.0000, 1.0022, 0.0000, 0.7372, 1.0123],\n",
      "        [0.0800, 0.0000, 1.4077, 0.0000, 0.0000],\n",
      "        [0.0067, 0.0000, 0.0000, 1.3020, 0.6931],\n",
      "        [0.0000, 0.0000, 0.0000, 1.1727, 0.9585]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0922904014587402\n",
      "Batch 23, Loss: 3.0922904014587402\n",
      "Processing Batch 24/157\n",
      "Batch 24 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012508372776210308, Std: 0.08750414103269577\n",
      "z_j Mean: 0.012595578096807003, Std: 0.08749163150787354\n",
      "Similarities: tensor([[1.3488e+00, 1.1864e-03, 0.0000e+00, 2.4145e-02, 2.8718e-01],\n",
      "        [0.0000e+00, 1.4249e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 1.3922e+00, 0.0000e+00, 1.1567e+00],\n",
      "        [3.4692e-01, 0.0000e+00, 0.0000e+00, 1.4239e+00, 0.0000e+00],\n",
      "        [5.0792e-01, 0.0000e+00, 8.8317e-01, 0.0000e+00, 1.2771e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.028404712677002\n",
      "Batch 24, Loss: 3.028404712677002\n",
      "Processing Batch 25/157\n",
      "Batch 25 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012279987335205078, Std: 0.08753649145364761\n",
      "z_j Mean: 0.0120496591553092, Std: 0.087568499147892\n",
      "Similarities: tensor([[1.3326e+00, 0.0000e+00, 1.3564e-01, 0.0000e+00, 1.1675e+00],\n",
      "        [0.0000e+00, 1.3597e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [9.6374e-02, 0.0000e+00, 1.3712e+00, 0.0000e+00, 1.3362e-01],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 1.4275e+00, 0.0000e+00],\n",
      "        [9.4755e-01, 0.0000e+00, 2.6417e-01, 1.0347e-04, 1.3502e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.029426336288452\n",
      "Batch 25, Loss: 3.029426336288452\n",
      "Processing Batch 26/157\n",
      "Batch 26 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012448366731405258, Std: 0.08751270174980164\n",
      "z_j Mean: 0.0118407579138875, Std: 0.08619200438261032\n",
      "Similarities: tensor([[1.3980, 0.0000, 0.5114, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3584, 0.0000, 0.0000, 0.4526],\n",
      "        [0.8173, 0.0000, 1.4182, 1.1885, 0.0000],\n",
      "        [0.2605, 0.0000, 1.1705, 1.3995, 0.0000],\n",
      "        [0.0000, 0.5112, 0.0000, 0.0000, 1.4285]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.088562488555908\n",
      "Batch 26, Loss: 3.088562488555908\n",
      "Processing Batch 27/157\n",
      "Batch 27 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013562221080064774, Std: 0.08734699338674545\n",
      "z_j Mean: 0.013506526127457619, Std: 0.08735562860965729\n",
      "Similarities: tensor([[1.3963, 0.0000, 0.0000, 0.5874, 0.2982],\n",
      "        [0.0000, 1.3612, 0.1436, 0.0000, 0.0550],\n",
      "        [0.0000, 0.3934, 1.3533, 0.0045, 0.0000],\n",
      "        [0.7828, 0.0000, 0.0000, 1.4092, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4116]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0312812328338623\n",
      "Batch 27, Loss: 3.0312812328338623\n",
      "Processing Batch 28/157\n",
      "Batch 28 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012786589562892914, Std: 0.08746391534805298\n",
      "z_j Mean: 0.01235172338783741, Std: 0.08682617545127869\n",
      "Similarities: tensor([[1.3836, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4233, 0.0000, 0.0278, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3933, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0664, 0.0124, 1.3354, 0.0000],\n",
      "        [0.0000, 0.0768, 0.3967, 0.1990, 0.0000]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.069706916809082\n",
      "Batch 28, Loss: 3.069706916809082\n",
      "Processing Batch 29/157\n",
      "Batch 29 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012651295401155949, Std: 0.08748359233140945\n",
      "z_j Mean: 0.012356153689324856, Std: 0.08752577006816864\n",
      "Similarities: tensor([[1.4135, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4121, 0.0000, 0.0076, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4286, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3182, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4142]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0261149406433105\n",
      "Batch 29, Loss: 3.0261149406433105\n",
      "Processing Batch 30/157\n",
      "Batch 30 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012655414640903473, Std: 0.0874829962849617\n",
      "z_j Mean: 0.012463606894016266, Std: 0.08751053363084793\n",
      "Similarities: tensor([[1.4010, 0.0000, 0.0436, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4286, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0308, 0.0000, 1.4043, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3917, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0028, 0.0000, 1.4283]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285701513290405]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.013556957244873\n",
      "Batch 30, Loss: 3.013556957244873\n",
      "Processing Batch 31/157\n",
      "Batch 31 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013362913392484188, Std: 0.08737771213054657\n",
      "z_j Mean: 0.013060406781733036, Std: 0.08742344379425049\n",
      "Similarities: tensor([[1.3954, 0.3039, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4630, 1.3592, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4131, 0.1737, 0.0000],\n",
      "        [0.0000, 0.0000, 0.2430, 1.4032, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3018]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4281647205352783]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0268404483795166\n",
      "Batch 31, Loss: 3.0268404483795166\n",
      "Processing Batch 32/157\n",
      "Batch 32 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01307525485754013, Std: 0.08672015368938446\n",
      "z_j Mean: 0.013169363141059875, Std: 0.08740709722042084\n",
      "Similarities: tensor([[1.0917, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.2929, 0.5746, 0.0000, 0.1168],\n",
      "        [0.0000, 0.5048, 1.3848, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4209, 0.0177],\n",
      "        [0.0000, 0.0452, 0.0000, 0.0000, 1.3923]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0708045959472656\n",
      "Batch 32, Loss: 3.0708045959472656\n",
      "Processing Batch 33/157\n",
      "Batch 33 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013216688297688961, Std: 0.08739995211362839\n",
      "z_j Mean: 0.013145561330020428, Std: 0.08741068094968796\n",
      "Similarities: tensor([[1.3963, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3818, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4197, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4273, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4011]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.043187379837036\n",
      "Batch 33, Loss: 3.043187379837036\n",
      "Processing Batch 34/157\n",
      "Batch 34 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012563593685626984, Std: 0.08749623596668243\n",
      "z_j Mean: 0.01258364412933588, Std: 0.08749335259199142\n",
      "Similarities: tensor([[1.4122, 0.8206, 0.0000, 0.9014, 0.8575],\n",
      "        [0.9239, 1.4142, 0.0000, 0.9110, 0.4240],\n",
      "        [0.0000, 0.0000, 1.4119, 0.0000, 0.0000],\n",
      "        [0.7554, 0.8364, 0.0000, 1.4286, 0.0000],\n",
      "        [0.8135, 0.2664, 0.0000, 0.0000, 1.3827]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0131735801696777\n",
      "Batch 34, Loss: 3.0131735801696777\n",
      "Processing Batch 35/157\n",
      "Batch 35 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013225512579083443, Std: 0.08669736981391907\n",
      "z_j Mean: 0.013250925578176975, Std: 0.08669348061084747\n",
      "Similarities: tensor([[1.3076, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3634, 0.0000, 0.0000, 0.0045],\n",
      "        [0.0000, 0.0000, 1.2331, 0.0227, 0.5399],\n",
      "        [0.0000, 0.0000, 0.0124, 1.4003, 0.0000],\n",
      "        [0.0000, 0.0626, 0.5204, 0.0000, 1.3419]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4284871816635132]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.067491292953491\n",
      "Batch 35, Loss: 3.067491292953491\n",
      "Processing Batch 36/157\n",
      "Batch 36 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01315505150705576, Std: 0.08740925043821335\n",
      "z_j Mean: 0.013265242800116539, Std: 0.08739259093999863\n",
      "Similarities: tensor([[1.4149, 0.0000, 0.5896, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3654, 0.0084, 0.0000, 0.0000],\n",
      "        [0.6555, 0.0280, 1.3642, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3655, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3541]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.062628984451294\n",
      "Batch 36, Loss: 3.062628984451294\n",
      "Processing Batch 37/157\n",
      "Batch 37 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01374521479010582, Std: 0.08731838315725327\n",
      "z_j Mean: 0.013689142651855946, Std: 0.08732718974351883\n",
      "Similarities: tensor([[1.4132e+00, 4.0742e-04, 0.0000e+00, 1.6068e-01, 1.4221e-01],\n",
      "        [0.0000e+00, 1.4159e+00, 0.0000e+00, 0.0000e+00, 3.3332e-02],\n",
      "        [0.0000e+00, 0.0000e+00, 1.3424e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [2.0947e-02, 0.0000e+00, 0.0000e+00, 1.3692e+00, 3.1473e-01],\n",
      "        [1.5045e-02, 0.0000e+00, 0.0000e+00, 2.5541e-01, 1.3636e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.427086591720581]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0197651386260986\n",
      "Batch 37, Loss: 3.0197651386260986\n",
      "Processing Batch 38/157\n",
      "Batch 38 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01230231299996376, Std: 0.0875333622097969\n",
      "z_j Mean: 0.01270067598670721, Std: 0.08747643977403641\n",
      "Similarities: tensor([[1.2845, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4162, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4078, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4286, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0139, 0.0000, 1.3791]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0359387397766113\n",
      "Batch 38, Loss: 3.0359387397766113\n",
      "Processing Batch 39/157\n",
      "Batch 39 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01349419541656971, Std: 0.0873575359582901\n",
      "z_j Mean: 0.013205226510763168, Std: 0.08740168064832687\n",
      "Similarities: tensor([[1.4159, 0.0000, 0.0961, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4254, 0.0000, 0.0000, 0.4023],\n",
      "        [0.0800, 0.0000, 1.4198, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4210, 0.0000],\n",
      "        [0.0000, 0.0259, 0.0000, 0.0000, 1.3729]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0526559352874756\n",
      "Batch 39, Loss: 3.0526559352874756\n",
      "Processing Batch 40/157\n",
      "Batch 40 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.0125528983771801, Std: 0.0874977707862854\n",
      "z_j Mean: 0.012645510956645012, Std: 0.08748442679643631\n",
      "Similarities: tensor([[1.3794, 0.0000, 0.0000, 0.0000, 0.1767],\n",
      "        [0.0000, 1.1920, 0.0745, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4099, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.2991, 0.0000],\n",
      "        [0.0732, 0.0000, 0.0000, 0.0000, 1.4252]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0853233337402344\n",
      "Batch 40, Loss: 3.0853233337402344\n",
      "Processing Batch 41/157\n",
      "Batch 41 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013052062131464481, Std: 0.08742469549179077\n",
      "z_j Mean: 0.013175657019019127, Std: 0.08740615099668503\n",
      "Similarities: tensor([[1.4111, 0.0000, 0.0172, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4140, 0.0000, 1.1006, 0.0000],\n",
      "        [0.0440, 0.0000, 1.4004, 0.0000, 0.0000],\n",
      "        [0.0000, 1.0951, 0.0000, 1.3947, 0.0000],\n",
      "        [0.1269, 0.0000, 0.0000, 0.0000, 1.3829]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.057614326477051\n",
      "Batch 41, Loss: 3.057614326477051\n",
      "Processing Batch 42/157\n",
      "Batch 42 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012721994891762733, Std: 0.08677268773317337\n",
      "z_j Mean: 0.013174207881093025, Std: 0.08670517802238464\n",
      "Similarities: tensor([[1.4238, 0.8933, 0.3633, 0.0000, 0.1265],\n",
      "        [0.5363, 1.2984, 0.0090, 0.0000, 0.0000],\n",
      "        [0.3171, 0.0151, 1.3982, 0.0000, 0.6308],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0667, 0.0000],\n",
      "        [0.2515, 0.0000, 0.8172, 0.0000, 1.3459]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.085864543914795\n",
      "Batch 42, Loss: 3.085864543914795\n",
      "Processing Batch 43/157\n",
      "Batch 43 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013234088197350502, Std: 0.08739732205867767\n",
      "z_j Mean: 0.013299260288476944, Std: 0.08738742768764496\n",
      "Similarities: tensor([[1.4229, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.1495, 0.0000, 0.1120, 0.0633],\n",
      "        [0.0000, 0.0000, 1.4043, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2788, 0.0000, 1.2671, 0.5970],\n",
      "        [0.0000, 0.0447, 0.0000, 0.9694, 1.3990]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.040050983428955\n",
      "Batch 43, Loss: 3.040050983428955\n",
      "Processing Batch 44/157\n",
      "Batch 44 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013163620606064796, Std: 0.08740796893835068\n",
      "z_j Mean: 0.01309578400105238, Std: 0.08741815388202667\n",
      "Similarities: tensor([[1.3546, 0.0000, 0.0000, 0.0000, 0.1593],\n",
      "        [0.0000, 1.4026, 0.0019, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0016, 1.4199, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4178, 0.0000],\n",
      "        [0.1819, 0.0000, 0.0000, 0.0000, 1.4283]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.054271936416626\n",
      "Batch 44, Loss: 3.054271936416626\n",
      "Processing Batch 45/157\n",
      "Batch 45 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013081190176308155, Std: 0.08742032945156097\n",
      "z_j Mean: 0.013172786682844162, Std: 0.08740658313035965\n",
      "Similarities: tensor([[1.4066, 0.0000, 0.3952, 0.0000, 0.0000],\n",
      "        [0.0000, 1.2070, 0.0000, 0.9461, 0.0000],\n",
      "        [0.3959, 0.0000, 1.3850, 0.0000, 0.0000],\n",
      "        [0.0000, 1.0657, 0.0000, 1.3846, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3982]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285075664520264]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0649635791778564\n",
      "Batch 45, Loss: 3.0649635791778564\n",
      "Processing Batch 46/157\n",
      "Batch 46 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013799951411783695, Std: 0.08730974048376083\n",
      "z_j Mean: 0.013210463337600231, Std: 0.08669965714216232\n",
      "Similarities: tensor([[1.3471, 0.0000, 0.0000, 0.9895, 0.0000],\n",
      "        [0.0000, 1.3874, 0.0000, 0.2638, 0.0000],\n",
      "        [0.0000, 0.0000, 1.2345, 0.0000, 0.5109],\n",
      "        [1.1438, 0.3884, 0.0000, 1.4058, 0.0000],\n",
      "        [0.0000, 0.0000, 0.5730, 0.0000, 1.4199]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4283173084259033]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.068530797958374\n",
      "Batch 46, Loss: 3.068530797958374\n",
      "Processing Batch 47/157\n",
      "Batch 47 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01313801109790802, Std: 0.08741181343793869\n",
      "z_j Mean: 0.012934606522321701, Std: 0.08674124628305435\n",
      "Similarities: tensor([[1.4056, 0.0000, 0.3163, 0.0134, 0.0000],\n",
      "        [0.0000, 1.4137, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3373, 0.0000, 1.4253, 0.0000, 0.0000],\n",
      "        [0.0016, 0.0000, 0.0000, 1.3620, 0.3051],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0869, 1.3832]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4284857511520386]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.075610637664795\n",
      "Batch 47, Loss: 3.075610637664795\n",
      "Processing Batch 48/157\n",
      "Batch 48 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013590555638074875, Std: 0.08734259009361267\n",
      "z_j Mean: 0.013936001807451248, Std: 0.08728812634944916\n",
      "Similarities: tensor([[1.2639, 0.0000, 0.0000, 0.0060, 0.0339],\n",
      "        [0.0000, 1.3335, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3664, 0.0000, 0.0000],\n",
      "        [0.0029, 0.4018, 0.0000, 1.4162, 0.0143],\n",
      "        [0.0067, 0.0000, 0.0000, 0.0058, 1.1032]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4284476041793823]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0671956539154053\n",
      "Batch 48, Loss: 3.0671956539154053\n",
      "Processing Batch 49/157\n",
      "Batch 49 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013222092762589455, Std: 0.08739913254976273\n",
      "z_j Mean: 0.013116257265210152, Std: 0.08741508424282074\n",
      "Similarities: tensor([[1.4040, 0.7173, 0.0000, 0.0000, 0.0000],\n",
      "        [0.8996, 1.1835, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4273, 0.8411, 0.0923],\n",
      "        [0.0000, 0.0000, 1.0505, 1.3745, 0.9750],\n",
      "        [0.0000, 0.0000, 0.1143, 0.9919, 1.3435]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0794451236724854\n",
      "Batch 49, Loss: 3.0794451236724854\n",
      "Processing Batch 50/157\n",
      "Batch 50 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013119475916028023, Std: 0.08741459250450134\n",
      "z_j Mean: 0.013126684352755547, Std: 0.08741351217031479\n",
      "Similarities: tensor([[1.1578e+00, 0.0000e+00, 0.0000e+00, 2.1830e-03, 0.0000e+00],\n",
      "        [7.0793e-03, 1.3887e+00, 0.0000e+00, 0.0000e+00, 1.1107e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 1.4241e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [1.7302e-02, 0.0000e+00, 0.0000e+00, 1.4226e+00, 1.0026e-03],\n",
      "        [1.2727e-02, 9.0428e-01, 0.0000e+00, 0.0000e+00, 1.4281e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285386800765991]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.040118455886841\n",
      "Batch 50, Loss: 3.040118455886841\n",
      "Processing Batch 51/157\n",
      "Batch 51 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01309664361178875, Std: 0.08741802722215652\n",
      "z_j Mean: 0.013132041320204735, Std: 0.08741271495819092\n",
      "Similarities: tensor([[1.3425, 1.1145, 0.0000, 0.0000, 1.1028],\n",
      "        [0.8296, 1.4286, 0.0000, 0.0000, 1.3729],\n",
      "        [0.0000, 0.0000, 1.4178, 0.6380, 0.0000],\n",
      "        [0.0675, 0.0000, 0.8385, 1.3872, 0.0000],\n",
      "        [0.8251, 1.4202, 0.0000, 0.0000, 1.3931]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0666558742523193\n",
      "Batch 51, Loss: 3.0666558742523193\n",
      "Processing Batch 52/157\n",
      "Batch 52 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012498965486884117, Std: 0.08680509030818939\n",
      "z_j Mean: 0.012814365327358246, Std: 0.08675909787416458\n",
      "Similarities: tensor([[1.4028, 0.0072, 0.2835, 0.0000, 0.0196],\n",
      "        [0.0000, 1.4187, 0.0000, 0.0000, 0.1370],\n",
      "        [0.1909, 0.0000, 1.4063, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3570, 0.0000],\n",
      "        [0.0079, 0.1111, 0.0000, 0.0000, 1.3049]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0760185718536377\n",
      "Batch 52, Loss: 3.0760185718536377\n",
      "Processing Batch 53/157\n",
      "Batch 53 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013479458168148994, Std: 0.08735980838537216\n",
      "z_j Mean: 0.013738037087023258, Std: 0.0873195081949234\n",
      "Similarities: tensor([[1.4167e+00, 0.0000e+00, 1.5360e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 1.4031e+00, 0.0000e+00, 9.3774e-01, 1.8374e-03],\n",
      "        [1.4095e-01, 0.0000e+00, 1.3248e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 7.5729e-01, 0.0000e+00, 1.4286e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 6.8878e-04, 0.0000e+00, 0.0000e+00, 1.3769e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0359411239624023\n",
      "Batch 53, Loss: 3.0359411239624023\n",
      "Processing Batch 54/157\n",
      "Batch 54 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012883197516202927, Std: 0.08744973689317703\n",
      "z_j Mean: 0.013027086853981018, Std: 0.08742842078208923\n",
      "Similarities: tensor([[1.4256, 0.0000, 0.0000, 0.7579, 0.2596],\n",
      "        [0.0000, 1.3822, 0.0000, 0.1304, 0.0000],\n",
      "        [0.0116, 0.0054, 1.2024, 0.0000, 0.0000],\n",
      "        [0.6216, 0.0292, 0.0000, 1.4035, 0.8096],\n",
      "        [0.1699, 0.0000, 0.0000, 0.7024, 1.4195]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.036278247833252\n",
      "Batch 54, Loss: 3.036278247833252\n",
      "Processing Batch 55/157\n",
      "Batch 55 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01327485591173172, Std: 0.08739113062620163\n",
      "z_j Mean: 0.013539491221308708, Std: 0.0873505249619484\n",
      "Similarities: tensor([[1.4092, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3813, 1.1618, 0.0000, 1.1111],\n",
      "        [0.0000, 1.1852, 1.3692, 0.0612, 1.2185],\n",
      "        [0.0000, 0.0000, 0.0256, 1.4198, 0.0198],\n",
      "        [0.0000, 1.2156, 1.1146, 0.0289, 1.3803]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0165281295776367\n",
      "Batch 55, Loss: 3.0165281295776367\n",
      "Processing Batch 56/157\n",
      "Batch 56 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012533202767372131, Std: 0.08750059455633163\n",
      "z_j Mean: 0.012849257327616215, Std: 0.08745472878217697\n",
      "Similarities: tensor([[1.3901, 0.8578, 0.0000, 0.0000, 0.9010],\n",
      "        [0.2826, 1.3228, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4213, 0.1982, 0.0000],\n",
      "        [0.0000, 0.0000, 0.2880, 1.4177, 0.0301],\n",
      "        [1.1704, 0.5790, 0.0000, 0.0000, 1.3494]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0487663745880127\n",
      "Batch 56, Loss: 3.0487663745880127\n",
      "Processing Batch 57/157\n",
      "Batch 57 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012638887390494347, Std: 0.08678483963012695\n",
      "z_j Mean: 0.012682194821536541, Std: 0.08677852153778076\n",
      "Similarities: tensor([[1.4034, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.1815, 0.2826, 0.0000, 0.0000],\n",
      "        [0.0000, 0.5298, 1.1318, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4193, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3000]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285427331924438]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.057276725769043\n",
      "Batch 57, Loss: 3.057276725769043\n",
      "Processing Batch 58/157\n",
      "Batch 58 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012434136122465134, Std: 0.087514728307724\n",
      "z_j Mean: 0.012664968147873878, Std: 0.08748161047697067\n",
      "Similarities: tensor([[1.3835, 0.0029, 0.1096, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3339, 0.0000, 0.8677, 0.1418],\n",
      "        [0.3019, 0.0000, 0.8932, 0.0000, 0.0000],\n",
      "        [0.0000, 1.1088, 0.0000, 1.3952, 0.1032],\n",
      "        [0.0000, 0.0884, 0.0000, 0.0863, 1.4196]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.023169755935669\n",
      "Batch 58, Loss: 3.023169755935669\n",
      "Processing Batch 59/157\n",
      "Batch 59 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012537151575088501, Std: 0.08750002831220627\n",
      "z_j Mean: 0.013120647519826889, Std: 0.08741442114114761\n",
      "Similarities: tensor([[1.3980, 0.0000, 0.1389, 0.2683, 0.0000],\n",
      "        [0.0000, 1.3239, 0.0068, 0.0000, 0.3956],\n",
      "        [0.5320, 0.0000, 1.1975, 0.0000, 0.0000],\n",
      "        [0.0019, 0.0000, 0.0000, 1.4219, 0.0000],\n",
      "        [0.0000, 0.2390, 0.0000, 0.0000, 1.4085]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285602569580078]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0970568656921387\n",
      "Batch 59, Loss: 3.0970568656921387\n",
      "Processing Batch 60/157\n",
      "Batch 60 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011863598600029945, Std: 0.08689422160387039\n",
      "z_j Mean: 0.01166446041315794, Std: 0.0862160474061966\n",
      "Similarities: tensor([[1.4280, 0.0000, 0.4305, 0.6053, 0.0085],\n",
      "        [0.0000, 1.3274, 0.9637, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2099, 0.5299, 0.0000, 0.0000],\n",
      "        [0.6194, 0.6331, 1.2941, 0.8082, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4167]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.103559732437134\n",
      "Batch 60, Loss: 3.103559732437134\n",
      "Processing Batch 61/157\n",
      "Batch 61 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01270473562180996, Std: 0.08747584372758865\n",
      "z_j Mean: 0.01221400685608387, Std: 0.08542831987142563\n",
      "Similarities: tensor([[1.3639, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3193, 0.0000, 0.0000, 0.0760],\n",
      "        [0.0000, 0.0000, 1.3942, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0190, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1129, 0.0000, 0.0000, 1.2650]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.08187198638916\n",
      "Batch 61, Loss: 3.08187198638916\n",
      "Processing Batch 62/157\n",
      "Batch 62 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012778800912201405, Std: 0.08746505528688431\n",
      "z_j Mean: 0.01273988839238882, Std: 0.08747074007987976\n",
      "Similarities: tensor([[1.3550, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3608, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0060, 1.3918, 0.0000, 0.1295],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3490, 0.1934],\n",
      "        [0.0000, 0.0000, 0.1255, 0.1631, 1.3285]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4280357360839844]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.041325330734253\n",
      "Batch 62, Loss: 3.041325330734253\n",
      "Processing Batch 63/157\n",
      "Batch 63 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012269211933016777, Std: 0.08753800392150879\n",
      "z_j Mean: 0.012301514856517315, Std: 0.08753347396850586\n",
      "Similarities: tensor([[1.3653, 0.3799, 0.9625, 0.0000, 0.1403],\n",
      "        [0.2014, 1.1540, 0.0000, 0.0000, 0.0000],\n",
      "        [1.2675, 0.0000, 1.2964, 0.0000, 0.1602],\n",
      "        [0.0000, 0.0000, 0.0000, 1.2693, 0.0000],\n",
      "        [0.1469, 0.0000, 0.0223, 0.0000, 1.4112]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0462374687194824\n",
      "Batch 63, Loss: 3.0462374687194824\n",
      "Processing Batch 64/157\n",
      "Batch 64 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013144787400960922, Std: 0.08741079270839691\n",
      "z_j Mean: 0.012712819501757622, Std: 0.08677403628826141\n",
      "Similarities: tensor([[1.3697, 0.0000, 0.0206, 0.6192, 0.0414],\n",
      "        [0.0000, 1.4022, 0.0000, 0.0000, 0.3624],\n",
      "        [0.0031, 0.0000, 1.3947, 0.1444, 0.6711],\n",
      "        [1.0442, 0.0000, 0.1113, 1.3224, 0.1531],\n",
      "        [0.0073, 0.4098, 0.6898, 0.1704, 1.3733]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0670039653778076\n",
      "Batch 64, Loss: 3.0670039653778076\n",
      "Processing Batch 65/157\n",
      "Batch 65 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012604599818587303, Std: 0.08749032765626907\n",
      "z_j Mean: 0.012664074078202248, Std: 0.08748174458742142\n",
      "Similarities: tensor([[1.3722, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4270, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4286, 0.0000, 0.0000],\n",
      "        [0.1528, 0.0000, 0.0000, 1.3323, 0.0000],\n",
      "        [0.0437, 0.0000, 0.0000, 0.0000, 1.3040]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0137646198272705\n",
      "Batch 65, Loss: 3.0137646198272705\n",
      "Processing Batch 66/157\n",
      "Batch 66 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012428229674696922, Std: 0.08751556277275085\n",
      "z_j Mean: 0.012416341342031956, Std: 0.08751725405454636\n",
      "Similarities: tensor([[1.2930, 0.0506, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0893, 1.3731, 0.3120, 0.2213, 0.0000],\n",
      "        [0.0000, 0.2183, 1.3651, 0.2171, 0.0000],\n",
      "        [0.5158, 0.2300, 0.0807, 1.2605, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4195]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0160529613494873\n",
      "Batch 66, Loss: 3.0160529613494873\n",
      "Processing Batch 67/157\n",
      "Batch 67 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012696076184511185, Std: 0.08747710287570953\n",
      "z_j Mean: 0.012401968240737915, Std: 0.08681901544332504\n",
      "Similarities: tensor([[1.4116, 0.0000, 0.3041, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3436, 0.1259, 0.0000, 0.0000],\n",
      "        [0.5088, 0.2869, 1.3352, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3100, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3981]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.096958637237549\n",
      "Batch 67, Loss: 3.096958637237549\n",
      "Processing Batch 68/157\n",
      "Batch 68 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013004979118704796, Std: 0.0867307260632515\n",
      "z_j Mean: 0.012472262606024742, Std: 0.08750929683446884\n",
      "Similarities: tensor([[1.4197, 0.5415, 0.5416, 0.0992, 0.0000],\n",
      "        [0.6391, 1.4276, 1.4266, 0.0000, 0.0000],\n",
      "        [0.5786, 1.2962, 1.2915, 0.0000, 0.0000],\n",
      "        [0.1951, 0.0000, 0.0000, 1.3770, 0.3209],\n",
      "        [0.0000, 0.0000, 0.0000, 0.4381, 1.1247]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.049283742904663\n",
      "Batch 68, Loss: 3.049283742904663\n",
      "Processing Batch 69/157\n",
      "Batch 69 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012608946301043034, Std: 0.08748970925807953\n",
      "z_j Mean: 0.012399326078593731, Std: 0.08681938797235489\n",
      "Similarities: tensor([[1.4238, 0.0000, 0.7844, 1.1399, 0.0000],\n",
      "        [0.0000, 1.4250, 0.0000, 0.0000, 0.4475],\n",
      "        [0.8378, 0.0000, 1.3812, 1.0055, 0.0000],\n",
      "        [0.8904, 0.0000, 0.8674, 1.3573, 0.0000],\n",
      "        [0.0000, 1.0509, 0.0000, 0.0000, 1.2202]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0905609130859375\n",
      "Batch 69, Loss: 3.0905609130859375\n",
      "Processing Batch 70/157\n",
      "Batch 70 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012362543493509293, Std: 0.08752486854791641\n",
      "z_j Mean: 0.012353445403277874, Std: 0.08752615004777908\n",
      "Similarities: tensor([[1.3625, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4244, 0.5674, 0.0000, 0.0000],\n",
      "        [0.0000, 0.4652, 1.4286, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3608, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.2978]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0862226486206055\n",
      "Batch 70, Loss: 3.0862226486206055\n",
      "Processing Batch 71/157\n",
      "Batch 71 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012667523697018623, Std: 0.08748124539852142\n",
      "z_j Mean: 0.01207056362181902, Std: 0.08616012334823608\n",
      "Similarities: tensor([[1.4286, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4275, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4221, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4087, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4098]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.05509352684021\n",
      "Batch 71, Loss: 3.05509352684021\n",
      "Processing Batch 72/157\n",
      "Batch 72 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012815764173865318, Std: 0.08745964616537094\n",
      "z_j Mean: 0.012685120105743408, Std: 0.08747869729995728\n",
      "Similarities: tensor([[1.3569e+00, 0.0000e+00, 1.0132e-01, 7.9415e-02, 0.0000e+00],\n",
      "        [2.8851e-01, 9.8109e-01, 2.1353e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [5.3532e-04, 5.7337e-02, 1.3802e+00, 1.3158e-02, 0.0000e+00],\n",
      "        [2.3127e-04, 0.0000e+00, 2.0941e-02, 1.4046e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.4116e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.057122230529785\n",
      "Batch 72, Loss: 3.057122230529785\n",
      "Processing Batch 73/157\n",
      "Batch 73 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013098904862999916, Std: 0.08741767704486847\n",
      "z_j Mean: 0.012624084949493408, Std: 0.08678698539733887\n",
      "Similarities: tensor([[1.3943, 0.0000, 0.4649, 0.0000, 0.0000],\n",
      "        [0.0000, 0.9479, 0.0000, 0.1931, 0.0000],\n",
      "        [0.5477, 0.0000, 0.9188, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0902, 0.0000, 1.1908, 0.0000],\n",
      "        [0.0000, 0.6863, 0.0000, 0.0000, 1.3830]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4259376525878906]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0538527965545654\n",
      "Batch 73, Loss: 3.0538527965545654\n",
      "Processing Batch 74/157\n",
      "Batch 74 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012883544899523258, Std: 0.08744969218969345\n",
      "z_j Mean: 0.012980305589735508, Std: 0.08743537962436676\n",
      "Similarities: tensor([[1.3336e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 1.3990e+00, 1.2255e-03, 0.0000e+00, 4.6053e-03],\n",
      "        [0.0000e+00, 0.0000e+00, 1.3995e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 1.2566e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.4121e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0629773139953613\n",
      "Batch 74, Loss: 3.0629773139953613\n",
      "Processing Batch 75/157\n",
      "Batch 75 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011571278795599937, Std: 0.08693364262580872\n",
      "z_j Mean: 0.011822991073131561, Std: 0.0868997648358345\n",
      "Similarities: tensor([[1.3974, 1.0227, 0.0000, 0.0000, 1.3706],\n",
      "        [0.8601, 1.4254, 0.0000, 0.0000, 1.0141],\n",
      "        [0.0000, 0.0000, 1.1755, 1.1987, 0.0000],\n",
      "        [0.0000, 0.0000, 0.5612, 1.1049, 0.0000],\n",
      "        [1.4191, 0.9639, 0.0000, 0.0000, 1.4239]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0286710262298584\n",
      "Batch 75, Loss: 3.0286710262298584\n",
      "Processing Batch 76/157\n",
      "Batch 76 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012363536283373833, Std: 0.08752471953630447\n",
      "z_j Mean: 0.012638160958886147, Std: 0.08748548477888107\n",
      "Similarities: tensor([[1.3231, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4286, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3835, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.8876, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4058]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.045480966567993\n",
      "Batch 76, Loss: 3.045480966567993\n",
      "Processing Batch 77/157\n",
      "Batch 77 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012827792204916477, Std: 0.08745788782835007\n",
      "z_j Mean: 0.012931284494698048, Std: 0.08744263648986816\n",
      "Similarities: tensor([[1.4120, 0.0065, 0.1357, 0.0569, 0.0000],\n",
      "        [0.0567, 1.1392, 0.8295, 0.8820, 0.0000],\n",
      "        [0.0918, 0.1637, 1.3434, 1.4286, 0.0000],\n",
      "        [0.0529, 0.0943, 0.7741, 0.8232, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4282]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.059234380722046\n",
      "Batch 77, Loss: 3.059234380722046\n",
      "Processing Batch 78/157\n",
      "Batch 78 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012664584442973137, Std: 0.08607480674982071\n",
      "z_j Mean: 0.012876847758889198, Std: 0.0867498442530632\n",
      "Similarities: tensor([[1.4187, 0.0000, 0.4546, 0.6433, 0.0000],\n",
      "        [0.0000, 1.4080, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3154, 0.0000, 1.4165, 0.9206, 0.0000],\n",
      "        [0.4563, 0.0000, 0.9414, 1.3321, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.2461]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0621654987335205\n",
      "Batch 78, Loss: 3.0621654987335205\n",
      "Processing Batch 79/157\n",
      "Batch 79 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01159757561981678, Std: 0.08693013340234756\n",
      "z_j Mean: 0.0117595624178648, Std: 0.08760793507099152\n",
      "Similarities: tensor([[1.4185, 0.5268, 0.0658, 0.0000, 0.0000],\n",
      "        [0.4581, 1.4063, 0.0785, 0.0000, 0.0000],\n",
      "        [0.1579, 0.1664, 1.4261, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4281, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4286]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.06496262550354\n",
      "Batch 79, Loss: 3.06496262550354\n",
      "Processing Batch 80/157\n",
      "Batch 80 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012730428017675877, Std: 0.08677145838737488\n",
      "z_j Mean: 0.012710163369774818, Std: 0.08747506141662598\n",
      "Similarities: tensor([[1.4286, 0.0000, 0.4326, 0.0000, 0.0000],\n",
      "        [0.0000, 0.8546, 0.9117, 0.0000, 0.0000],\n",
      "        [0.1450, 0.4480, 1.3033, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4178, 0.0809],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3708]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0722835063934326\n",
      "Batch 80, Loss: 3.0722835063934326\n",
      "Processing Batch 81/157\n",
      "Batch 81 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012828519567847252, Std: 0.08745777606964111\n",
      "z_j Mean: 0.012726053595542908, Std: 0.08606573939323425\n",
      "Similarities: tensor([[1.4256, 0.0000, 1.3837, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4199, 0.0000, 0.0000, 0.1135],\n",
      "        [0.9358, 0.0000, 1.1314, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.2907, 0.0000],\n",
      "        [0.0000, 0.0518, 0.0000, 0.0000, 1.3944]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.073411226272583\n",
      "Batch 81, Loss: 3.073411226272583\n",
      "Processing Batch 82/157\n",
      "Batch 82 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012615950778126717, Std: 0.08678817003965378\n",
      "z_j Mean: 0.012409140355885029, Std: 0.08751827478408813\n",
      "Similarities: tensor([[1.4204, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4286, 0.0000, 0.0000, 1.4161],\n",
      "        [0.0000, 0.0000, 1.3207, 1.2686, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3936, 1.1763, 0.0000],\n",
      "        [0.0000, 1.0829, 0.0000, 0.0000, 1.1952]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0796215534210205\n",
      "Batch 82, Loss: 3.0796215534210205\n",
      "Processing Batch 83/157\n",
      "Batch 83 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012832339853048325, Std: 0.08745721727609634\n",
      "z_j Mean: 0.012780629098415375, Std: 0.08746478706598282\n",
      "Similarities: tensor([[1.2086, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4161, 0.0000, 0.4196, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3608, 0.0000, 0.6371],\n",
      "        [0.0000, 0.4084, 0.0000, 1.4183, 0.0000],\n",
      "        [0.0000, 0.0000, 0.7579, 0.0000, 1.3790]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0321664810180664\n",
      "Batch 83, Loss: 3.0321664810180664\n",
      "Processing Batch 84/157\n",
      "Batch 84 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011910752393305302, Std: 0.08758749812841415\n",
      "z_j Mean: 0.01182479690760374, Std: 0.08759915083646774\n",
      "Similarities: tensor([[1.4100, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3996, 0.3846, 0.0000, 0.0434],\n",
      "        [0.0000, 0.8230, 1.3098, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4164, 0.2517],\n",
      "        [0.0000, 0.1867, 0.0000, 0.0974, 1.4095]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.045633316040039\n",
      "Batch 84, Loss: 3.045633316040039\n",
      "Processing Batch 85/157\n",
      "Batch 85 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011919800192117691, Std: 0.08758627623319626\n",
      "z_j Mean: 0.01166287437081337, Std: 0.0862162634730339\n",
      "Similarities: tensor([[1.4067, 1.0777, 0.0000, 0.0000, 0.0000],\n",
      "        [1.1164, 1.3892, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4286, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4198, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3406]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1167500019073486\n",
      "Batch 85, Loss: 3.1167500019073486\n",
      "Processing Batch 86/157\n",
      "Batch 86 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012902701273560524, Std: 0.08744686096906662\n",
      "z_j Mean: 0.013056261464953423, Std: 0.08742406219244003\n",
      "Similarities: tensor([[1.4247, 0.6469, 0.1065, 0.0000, 0.7813],\n",
      "        [0.7014, 1.4285, 0.3108, 0.0000, 1.1965],\n",
      "        [0.1510, 0.2198, 1.3371, 0.0891, 0.0062],\n",
      "        [0.0000, 0.0000, 0.3166, 1.3228, 0.0000],\n",
      "        [0.8373, 1.1878, 0.0000, 0.0000, 1.4284]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.070768356323242\n",
      "Batch 86, Loss: 3.070768356323242\n",
      "Processing Batch 87/157\n",
      "Batch 87 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012246525846421719, Std: 0.08754117786884308\n",
      "z_j Mean: 0.011841462925076485, Std: 0.08759689331054688\n",
      "Similarities: tensor([[0.4473, 0.0000, 1.3099, 0.1985, 0.0000],\n",
      "        [0.0000, 1.2995, 0.0240, 0.4014, 0.0000],\n",
      "        [0.3515, 0.0000, 1.3472, 0.0000, 0.0000],\n",
      "        [0.0000, 0.4559, 0.1043, 1.1069, 0.4471],\n",
      "        [0.0000, 0.0291, 0.0000, 0.0034, 1.4156]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0606889724731445\n",
      "Batch 87, Loss: 3.0606889724731445\n",
      "Processing Batch 88/157\n",
      "Batch 88 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012237926945090294, Std: 0.08684229105710983\n",
      "z_j Mean: 0.012143200263381004, Std: 0.0861499160528183\n",
      "Similarities: tensor([[1.4124, 0.0000, 0.1681, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1849, 0.0000, 1.4039, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4265, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4286]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.073220729827881\n",
      "Batch 88, Loss: 3.073220729827881\n",
      "Processing Batch 89/157\n",
      "Batch 89 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012823604047298431, Std: 0.08745849877595901\n",
      "z_j Mean: 0.012925922870635986, Std: 0.08744343370199203\n",
      "Similarities: tensor([[1.0993, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4286, 0.0000, 0.0000, 0.2661],\n",
      "        [0.0000, 0.0000, 1.3036, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3384, 0.0000],\n",
      "        [0.0000, 0.3638, 0.0000, 0.0000, 1.4246]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.025515556335449\n",
      "Batch 89, Loss: 3.025515556335449\n",
      "Processing Batch 90/157\n",
      "Batch 90 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012511986307799816, Std: 0.08750362694263458\n",
      "z_j Mean: 0.012328040786087513, Std: 0.0875297337770462\n",
      "Similarities: tensor([[1.4284, 0.0000, 0.0000, 0.7013, 0.4236],\n",
      "        [0.1153, 1.4099, 0.0356, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2672, 1.1380, 0.0000, 0.0000],\n",
      "        [0.5856, 0.0000, 0.0000, 1.3374, 1.1116],\n",
      "        [0.5073, 0.0000, 0.0000, 1.2764, 1.4186]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0651538372039795\n",
      "Batch 90, Loss: 3.0651538372039795\n",
      "Processing Batch 91/157\n",
      "Batch 91 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012199705466628075, Std: 0.08754771947860718\n",
      "z_j Mean: 0.012400262989103794, Std: 0.08751953393220901\n",
      "Similarities: tensor([[1.4015, 0.0000, 0.0000, 0.0000, 0.0827],\n",
      "        [0.0000, 1.3155, 0.0325, 0.3307, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3762, 0.8558, 0.7083],\n",
      "        [0.0000, 0.0275, 1.2185, 1.3505, 0.0395],\n",
      "        [0.1914, 0.0000, 0.0000, 0.0000, 0.8009]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0489304065704346\n",
      "Batch 91, Loss: 3.0489304065704346\n",
      "Processing Batch 92/157\n",
      "Batch 92 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01239047385752201, Std: 0.08752091228961945\n",
      "z_j Mean: 0.012414762750267982, Std: 0.0868171900510788\n",
      "Similarities: tensor([[1.4111, 0.1313, 0.0000, 0.0000, 0.0000],\n",
      "        [0.6895, 1.2727, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3365, 0.0719, 0.0000],\n",
      "        [0.0000, 0.0000, 0.4924, 1.3619, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3342]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.428161859512329]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0853114128112793\n",
      "Batch 92, Loss: 3.0853114128112793\n",
      "Processing Batch 93/157\n",
      "Batch 93 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012073015794157982, Std: 0.08756527304649353\n",
      "z_j Mean: 0.012548837810754776, Std: 0.08749835193157196\n",
      "Similarities: tensor([[1.1214, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.2821, 0.4122, 1.1301, 0.0000],\n",
      "        [0.0000, 0.9426, 1.4006, 0.3197, 0.0000],\n",
      "        [0.0000, 1.1171, 0.1069, 1.4261, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4227]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.074535608291626\n",
      "Batch 93, Loss: 3.074535608291626\n",
      "Processing Batch 94/157\n",
      "Batch 94 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01229940541088581, Std: 0.08753376454114914\n",
      "z_j Mean: 0.012053708545863628, Std: 0.08756794035434723\n",
      "Similarities: tensor([[1.3158, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4102, 0.7481, 0.0000, 0.0000],\n",
      "        [0.0000, 0.4696, 1.3888, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.2649, 1.1442],\n",
      "        [0.0000, 0.0000, 0.0000, 1.1552, 1.2110]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0354230403900146\n",
      "Batch 94, Loss: 3.0354230403900146\n",
      "Processing Batch 95/157\n",
      "Batch 95 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012456802651286125, Std: 0.08681116253137589\n",
      "z_j Mean: 0.012728113681077957, Std: 0.08747244626283646\n",
      "Similarities: tensor([[1.3478, 0.0000, 0.2149, 0.1140, 0.0000],\n",
      "        [0.0000, 1.3958, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2616, 0.0000, 1.4137, 1.3002, 0.0000],\n",
      "        [0.1657, 0.0000, 1.3988, 1.4048, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3995]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0300052165985107\n",
      "Batch 95, Loss: 3.0300052165985107\n",
      "Processing Batch 96/157\n",
      "Batch 96 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012592162936925888, Std: 0.08749212324619293\n",
      "z_j Mean: 0.012191632762551308, Std: 0.08614306896924973\n",
      "Similarities: tensor([[1.4264, 1.3929, 0.0000, 0.0000, 0.0000],\n",
      "        [1.3169, 1.4005, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.6935, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4284, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3552]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.057223320007324\n",
      "Batch 96, Loss: 3.057223320007324\n",
      "Processing Batch 97/157\n",
      "Batch 97 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012169751338660717, Std: 0.08755188435316086\n",
      "z_j Mean: 0.01231276337057352, Std: 0.08753188699483871\n",
      "Similarities: tensor([[1.4117, 0.0000, 0.0000, 0.0000, 1.3216],\n",
      "        [0.0000, 1.2555, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2651, 1.4247, 0.4041, 0.0000],\n",
      "        [0.0000, 0.0000, 0.5894, 1.0614, 0.0000],\n",
      "        [1.3664, 0.0000, 0.0000, 0.0000, 1.4266]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.044605255126953\n",
      "Batch 97, Loss: 3.044605255126953\n",
      "Processing Batch 98/157\n",
      "Batch 98 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011631406843662262, Std: 0.08622051775455475\n",
      "z_j Mean: 0.011810850352048874, Std: 0.08760103583335876\n",
      "Similarities: tensor([[1.4124, 0.0084, 0.0000, 0.1745, 0.6212],\n",
      "        [0.0494, 1.2424, 0.0000, 0.3708, 1.0684],\n",
      "        [0.0000, 0.0000, 1.2953, 0.0000, 0.0000],\n",
      "        [0.1863, 0.7908, 0.0000, 1.4077, 0.3861],\n",
      "        [0.4969, 0.9857, 0.0000, 0.3027, 1.4010]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0758016109466553\n",
      "Batch 98, Loss: 3.0758016109466553\n",
      "Processing Batch 99/157\n",
      "Batch 99 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011986805126070976, Std: 0.08757712692022324\n",
      "z_j Mean: 0.012398594990372658, Std: 0.08751977235078812\n",
      "Similarities: tensor([[1.3876, 0.5332, 0.0440, 0.0000, 0.0000],\n",
      "        [0.5973, 1.3509, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1148, 1.4283, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3786, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4005]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.078828811645508\n",
      "Batch 99, Loss: 3.078828811645508\n",
      "Processing Batch 100/157\n",
      "Batch 100 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011958854272961617, Std: 0.08758094906806946\n",
      "z_j Mean: 0.01205151155591011, Std: 0.0875682383775711\n",
      "Similarities: tensor([[1.4263, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.2929, 0.0313, 0.5904, 0.2010],\n",
      "        [0.0000, 0.0858, 1.4152, 0.0000, 0.0000],\n",
      "        [0.0000, 0.4939, 0.0000, 1.3715, 0.0000],\n",
      "        [0.0000, 0.6286, 0.0000, 0.0697, 1.4236]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.043287992477417\n",
      "Batch 100, Loss: 3.043287992477417\n",
      "Processing Batch 101/157\n",
      "Batch 101 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012713736854493618, Std: 0.08747453987598419\n",
      "z_j Mean: 0.012519216164946556, Std: 0.086802177131176\n",
      "Similarities: tensor([[1.3293, 0.0982, 0.5438, 0.0000, 0.4204],\n",
      "        [0.3580, 1.4264, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4338, 0.0000, 1.4143, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3881, 0.0000],\n",
      "        [0.0000, 0.0412, 0.0000, 0.0000, 0.9630]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285123348236084]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0825283527374268\n",
      "Batch 101, Loss: 3.0825283527374268\n",
      "Processing Batch 102/157\n",
      "Batch 102 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012297568842768669, Std: 0.08753402531147003\n",
      "z_j Mean: 0.012178035452961922, Std: 0.08755072951316833\n",
      "Similarities: tensor([[1.4003, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4112, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4182, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0360, 0.0000, 1.3737, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3087]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0631699562072754\n",
      "Batch 102, Loss: 3.0631699562072754\n",
      "Processing Batch 103/157\n",
      "Batch 103 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011992993764579296, Std: 0.08617095649242401\n",
      "z_j Mean: 0.012285051867365837, Std: 0.08683563768863678\n",
      "Similarities: tensor([[1.3865, 0.0429, 0.5881, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3475, 0.0000, 0.1435, 0.0422],\n",
      "        [0.4941, 0.0000, 1.3807, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2508, 0.0000, 1.4180, 0.0475],\n",
      "        [0.0000, 0.2669, 0.0000, 0.1718, 1.2220]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0659732818603516\n",
      "Batch 103, Loss: 3.0659732818603516\n",
      "Processing Batch 104/157\n",
      "Batch 104 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012426035478711128, Std: 0.08751587569713593\n",
      "z_j Mean: 0.011906009167432785, Std: 0.08618301898241043\n",
      "Similarities: tensor([[1.4144e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.9525e-01],\n",
      "        [0.0000e+00, 1.4279e+00, 0.0000e+00, 5.6967e-05, 0.0000e+00],\n",
      "        [5.4908e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 1.4188e+00, 0.0000e+00],\n",
      "        [7.4560e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.3792e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0710484981536865\n",
      "Batch 104, Loss: 3.0710484981536865\n",
      "Processing Batch 105/157\n",
      "Batch 105 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012976398691534996, Std: 0.08743595331907272\n",
      "z_j Mean: 0.012953522615134716, Std: 0.08743934333324432\n",
      "Similarities: tensor([[1.4188, 0.2623, 0.0000, 0.0888, 0.2782],\n",
      "        [0.1307, 1.3504, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0173, 0.0000, 1.4058, 0.9629, 0.0000],\n",
      "        [0.0588, 0.0000, 1.1338, 1.2730, 0.0000],\n",
      "        [0.3237, 0.2034, 0.0000, 0.0000, 1.4068]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4280790090560913]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0495965480804443\n",
      "Batch 105, Loss: 3.0495965480804443\n",
      "Processing Batch 106/157\n",
      "Batch 106 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012060616165399551, Std: 0.08756699413061142\n",
      "z_j Mean: 0.01221771165728569, Std: 0.08754520863294601\n",
      "Similarities: tensor([[1.4249, 0.6305, 0.0000, 0.0000, 0.0000],\n",
      "        [0.8317, 1.2289, 0.0000, 0.4777, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4210, 0.0000, 0.0000],\n",
      "        [0.0000, 0.5561, 0.0000, 1.2265, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3855]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.029623031616211\n",
      "Batch 106, Loss: 3.029623031616211\n",
      "Processing Batch 107/157\n",
      "Batch 107 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012446140870451927, Std: 0.0875130221247673\n",
      "z_j Mean: 0.012285355478525162, Std: 0.08753573894500732\n",
      "Similarities: tensor([[1.1544, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.8300, 1.4139, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3716, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3735, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.2735]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0323238372802734\n",
      "Batch 107, Loss: 3.0323238372802734\n",
      "Processing Batch 108/157\n",
      "Batch 108 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012531241402029991, Std: 0.08750087022781372\n",
      "z_j Mean: 0.012561330571770668, Std: 0.0874965637922287\n",
      "Similarities: tensor([[1.4266, 0.0000, 0.0000, 0.2583, 0.0000],\n",
      "        [0.0000, 1.4276, 1.1917, 0.0000, 0.0000],\n",
      "        [0.0000, 0.9554, 1.3857, 0.0000, 0.0000],\n",
      "        [0.2500, 0.0000, 0.0000, 1.3578, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3688]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.073477268218994\n",
      "Batch 108, Loss: 3.073477268218994\n",
      "Processing Batch 109/157\n",
      "Batch 109 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012570900842547417, Std: 0.08749518543481827\n",
      "z_j Mean: 0.012667031958699226, Std: 0.08748131990432739\n",
      "Similarities: tensor([[1.3961, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4055, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4048, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4240, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4286]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 2.9995133876800537\n",
      "Batch 109, Loss: 2.9995133876800537\n",
      "Processing Batch 110/157\n",
      "Batch 110 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012769270688295364, Std: 0.08746644854545593\n",
      "z_j Mean: 0.012575921602547169, Std: 0.08749446272850037\n",
      "Similarities: tensor([[1.2773, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4252, 0.4762, 0.0000, 0.0000],\n",
      "        [0.0000, 0.4260, 1.4257, 0.0000, 0.2068],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4286, 0.0000],\n",
      "        [0.0000, 0.0000, 0.2128, 0.0000, 1.4232]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0493671894073486\n",
      "Batch 110, Loss: 3.0493671894073486\n",
      "Processing Batch 111/157\n",
      "Batch 111 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012825867161154747, Std: 0.08745816349983215\n",
      "z_j Mean: 0.013093749061226845, Std: 0.08741845935583115\n",
      "Similarities: tensor([[1.4280, 0.0000, 0.5298, 0.0000, 0.2093],\n",
      "        [0.0000, 1.4285, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5454, 0.0000, 1.4269, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.2633, 0.0159],\n",
      "        [0.1504, 0.0000, 0.0000, 0.0559, 1.4145]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0645558834075928\n",
      "Batch 111, Loss: 3.0645558834075928\n",
      "Processing Batch 112/157\n",
      "Batch 112 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012972505763173103, Std: 0.08743653446435928\n",
      "z_j Mean: 0.013055110350251198, Std: 0.08742424100637436\n",
      "Similarities: tensor([[1.4244, 0.0000, 0.0000, 0.0000, 0.9649],\n",
      "        [0.0000, 1.4214, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3062, 0.0000, 0.0053],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4286, 0.0000],\n",
      "        [0.8985, 0.0000, 0.0000, 0.0000, 1.4156]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.005964517593384\n",
      "Batch 112, Loss: 3.005964517593384\n",
      "Processing Batch 113/157\n",
      "Batch 113 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013142673298716545, Std: 0.08741111308336258\n",
      "z_j Mean: 0.013110669329762459, Std: 0.0874159187078476\n",
      "Similarities: tensor([[1.3994, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4174, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4246, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4121, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4237]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0333991050720215\n",
      "Batch 113, Loss: 3.0333991050720215\n",
      "Processing Batch 114/157\n",
      "Batch 114 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012205329723656178, Std: 0.0875469297170639\n",
      "z_j Mean: 0.012501711957156658, Std: 0.08750509470701218\n",
      "Similarities: tensor([[1.4274, 0.3628, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4553, 1.3368, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.1548, 0.0000, 0.0434],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4285, 0.0000],\n",
      "        [0.0134, 0.0000, 0.3823, 0.0000, 1.4245]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0323288440704346\n",
      "Batch 114, Loss: 3.0323288440704346\n",
      "Processing Batch 115/157\n",
      "Batch 115 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.0119882021099329, Std: 0.08687712252140045\n",
      "z_j Mean: 0.012308815494179726, Std: 0.08753244578838348\n",
      "Similarities: tensor([[1.3280, 0.0000, 0.0000, 0.8457, 0.0000],\n",
      "        [0.0000, 1.4237, 0.0000, 0.0000, 0.5414],\n",
      "        [0.0000, 0.0000, 1.4179, 0.0000, 0.3442],\n",
      "        [0.6118, 0.0000, 0.0000, 1.4033, 0.0000],\n",
      "        [0.0000, 0.6167, 0.1992, 0.0000, 1.4237]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0810015201568604\n",
      "Batch 115, Loss: 3.0810015201568604\n",
      "Processing Batch 116/157\n",
      "Batch 116 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012705137953162193, Std: 0.08677516132593155\n",
      "z_j Mean: 0.012629945762455463, Std: 0.08678613603115082\n",
      "Similarities: tensor([[1.4185, 0.0000, 1.0661, 0.0000, 0.4833],\n",
      "        [0.0000, 1.4075, 0.0000, 0.9356, 0.0000],\n",
      "        [0.9632, 0.0000, 1.4097, 0.0000, 0.5156],\n",
      "        [0.0000, 0.8461, 0.0000, 1.4286, 0.0000],\n",
      "        [0.7456, 0.0000, 0.7756, 0.0000, 1.3770]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0630383491516113\n",
      "Batch 116, Loss: 3.0630383491516113\n",
      "Processing Batch 117/157\n",
      "Batch 117 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012530870735645294, Std: 0.0875009223818779\n",
      "z_j Mean: 0.012347433716058731, Std: 0.08752699941396713\n",
      "Similarities: tensor([[1.0552, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4190, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.2808, 0.2077, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4109, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3765]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0359108448028564\n",
      "Batch 117, Loss: 3.0359108448028564\n",
      "Processing Batch 118/157\n",
      "Batch 118 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012079179286956787, Std: 0.08756443113088608\n",
      "z_j Mean: 0.012334637343883514, Std: 0.08752880245447159\n",
      "Similarities: tensor([[1.4286, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.8393, 0.0000, 0.1907, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4286, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2233, 0.0000, 1.3196, 0.0000],\n",
      "        [0.0000, 0.9200, 0.0000, 0.0000, 1.4123]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.036647319793701\n",
      "Batch 118, Loss: 3.036647319793701\n",
      "Processing Batch 119/157\n",
      "Batch 119 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012969179078936577, Std: 0.08743702620267868\n",
      "z_j Mean: 0.013305801898241043, Std: 0.08738642185926437\n",
      "Similarities: tensor([[1.3295, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3959, 0.0000, 0.0000, 1.1488],\n",
      "        [0.0000, 0.0000, 1.2269, 0.2212, 0.0000],\n",
      "        [0.0105, 0.0000, 0.4208, 1.4138, 0.0000],\n",
      "        [0.0000, 1.0820, 0.0000, 0.0000, 1.1186]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4281924962997437]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0341153144836426\n",
      "Batch 119, Loss: 3.0341153144836426\n",
      "Processing Batch 120/157\n",
      "Batch 120 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01279370579868555, Std: 0.08746287971735\n",
      "z_j Mean: 0.012689974159002304, Std: 0.08677737414836884\n",
      "Similarities: tensor([[1.4070, 0.0165, 0.2694, 0.0809, 0.2352],\n",
      "        [0.0257, 1.4277, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3122, 0.0000, 1.4190, 0.4262, 0.0000],\n",
      "        [0.0492, 0.0000, 0.2237, 1.4129, 0.0000],\n",
      "        [0.2649, 0.0000, 0.0000, 0.0000, 1.2682]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0480597019195557\n",
      "Batch 120, Loss: 3.0480597019195557\n",
      "Processing Batch 121/157\n",
      "Batch 121 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012720970436930656, Std: 0.08747348934412003\n",
      "z_j Mean: 0.012831875123083591, Std: 0.08745728433132172\n",
      "Similarities: tensor([[1.2162, 0.3755, 0.0000, 1.0344, 0.0319],\n",
      "        [0.6247, 1.2381, 0.0000, 0.2309, 0.2813],\n",
      "        [0.0000, 0.0000, 1.4265, 0.0000, 0.0000],\n",
      "        [0.1666, 0.1626, 0.0000, 1.0040, 1.1946],\n",
      "        [0.0000, 0.2200, 0.0000, 0.6719, 1.4265]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0646510124206543\n",
      "Batch 121, Loss: 3.0646510124206543\n",
      "Processing Batch 122/157\n",
      "Batch 122 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012185150757431984, Std: 0.08684971928596497\n",
      "z_j Mean: 0.01209268905222416, Std: 0.08686263114213943\n",
      "Similarities: tensor([[1.3824, 0.6723, 0.7814, 0.0000, 0.0000],\n",
      "        [0.4739, 1.2292, 1.4286, 0.0000, 0.0000],\n",
      "        [0.4712, 1.2640, 1.4204, 0.0895, 0.0000],\n",
      "        [0.0000, 0.3546, 0.0000, 0.4074, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3767]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.096144199371338\n",
      "Batch 122, Loss: 3.096144199371338\n",
      "Processing Batch 123/157\n",
      "Batch 123 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01198537927120924, Std: 0.08757732063531876\n",
      "z_j Mean: 0.012291964143514633, Std: 0.08753480762243271\n",
      "Similarities: tensor([[1.3998, 0.0000, 0.0429, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3483, 0.0000, 1.3259, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4240, 0.0000, 0.0000],\n",
      "        [0.0000, 1.0969, 0.0000, 1.3953, 0.0000],\n",
      "        [0.0194, 0.0000, 0.0000, 0.0000, 1.4073]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4284483194351196]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.052781820297241\n",
      "Batch 123, Loss: 3.052781820297241\n",
      "Processing Batch 124/157\n",
      "Batch 124 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012159631587564945, Std: 0.08755329251289368\n",
      "z_j Mean: 0.012614664621651173, Std: 0.08748888224363327\n",
      "Similarities: tensor([[1.4064, 0.0000, 0.1585, 1.0837, 0.0000],\n",
      "        [0.0000, 1.3666, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0518, 0.0000, 1.3274, 0.7160, 0.0000],\n",
      "        [1.2723, 0.0000, 0.5487, 1.3700, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4230]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1083285808563232\n",
      "Batch 124, Loss: 3.1083285808563232\n",
      "Processing Batch 125/157\n",
      "Batch 125 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012763209640979767, Std: 0.08676663786172867\n",
      "z_j Mean: 0.011761240661144257, Std: 0.0854918360710144\n",
      "Similarities: tensor([[1.2790, 0.0000, 0.0000, 0.2991, 0.0000],\n",
      "        [0.0000, 1.3534, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3623, 0.2189, 0.0000],\n",
      "        [0.0000, 0.0000, 0.1162, 1.3009, 0.5188],\n",
      "        [0.0000, 0.0000, 0.0081, 0.2570, 1.3832]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.428415298461914]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0980656147003174\n",
      "Batch 125, Loss: 3.0980656147003174\n",
      "Processing Batch 126/157\n",
      "Batch 126 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012728164903819561, Std: 0.08747243881225586\n",
      "z_j Mean: 0.01318193506449461, Std: 0.08740520477294922\n",
      "Similarities: tensor([[1.3222, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0634, 1.4260, 0.0000, 0.0000, 0.3020],\n",
      "        [0.0000, 0.0000, 1.3665, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3939, 0.0000],\n",
      "        [0.0893, 0.1255, 0.0000, 0.0000, 1.3488]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.42854905128479]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.036254644393921\n",
      "Batch 126, Loss: 3.036254644393921\n",
      "Processing Batch 127/157\n",
      "Batch 127 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012917718850076199, Std: 0.08744464814662933\n",
      "z_j Mean: 0.012912635691463947, Std: 0.08744540065526962\n",
      "Similarities: tensor([[1.3916, 0.0000, 0.0000, 0.6810, 0.0000],\n",
      "        [0.0000, 1.4286, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4226, 0.0000, 0.0000],\n",
      "        [0.8564, 0.0000, 0.0000, 1.3966, 0.4095],\n",
      "        [0.0628, 0.0000, 0.0000, 0.3736, 1.4189]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0708656311035156\n",
      "Batch 127, Loss: 3.0708656311035156\n",
      "Processing Batch 128/157\n",
      "Batch 128 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012810686603188515, Std: 0.08746039122343063\n",
      "z_j Mean: 0.013017131015658379, Std: 0.08672890067100525\n",
      "Similarities: tensor([[1.3522, 0.0000, 0.0000, 0.0000, 0.1941],\n",
      "        [0.0000, 1.3401, 0.0000, 0.6216, 0.0000],\n",
      "        [0.0000, 0.0312, 1.3683, 0.4832, 0.8826],\n",
      "        [0.0000, 0.3081, 0.7932, 1.3598, 0.0000],\n",
      "        [0.0844, 0.0000, 0.7371, 0.0047, 1.3693]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0878782272338867\n",
      "Batch 128, Loss: 3.0878782272338867\n",
      "Processing Batch 129/157\n",
      "Batch 129 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012128015980124474, Std: 0.08755768090486526\n",
      "z_j Mean: 0.012047691270709038, Std: 0.08686888962984085\n",
      "Similarities: tensor([[1.3429, 0.0000, 0.8951, 0.3412, 0.0000],\n",
      "        [0.0000, 1.3770, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4242, 0.0000, 1.3044, 1.0268, 0.0000],\n",
      "        [0.2502, 0.0000, 0.9255, 1.3563, 0.1899],\n",
      "        [0.0000, 0.0000, 0.0000, 0.4847, 1.4286]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.112183094024658\n",
      "Batch 129, Loss: 3.112183094024658\n",
      "Processing Batch 130/157\n",
      "Batch 130 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012121468782424927, Std: 0.08685863018035889\n",
      "z_j Mean: 0.012446889653801918, Std: 0.0868125855922699\n",
      "Similarities: tensor([[1.3808, 0.0000, 0.0750, 0.0497, 1.2036],\n",
      "        [0.0000, 1.4267, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0180, 0.0000, 1.3197, 0.8308, 0.0000],\n",
      "        [0.0826, 0.0000, 0.5666, 1.3014, 0.0893],\n",
      "        [0.9679, 0.0000, 0.0000, 0.0079, 1.3824]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0646250247955322\n",
      "Batch 130, Loss: 3.0646250247955322\n",
      "Processing Batch 131/157\n",
      "Batch 131 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013034781441092491, Std: 0.08742726594209671\n",
      "z_j Mean: 0.013166001997888088, Std: 0.08670642226934433\n",
      "Similarities: tensor([[1.3266, 0.0483, 0.0000, 0.0167, 0.0403],\n",
      "        [0.0298, 1.3265, 0.0157, 0.2302, 0.0000],\n",
      "        [0.0000, 0.0265, 1.2992, 0.0015, 0.0000],\n",
      "        [0.0268, 0.3404, 0.1406, 1.3376, 0.0000],\n",
      "        [0.1328, 0.0000, 0.0000, 0.0000, 1.4278]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.428331971168518]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0992467403411865\n",
      "Batch 131, Loss: 3.0992467403411865\n",
      "Processing Batch 132/157\n",
      "Batch 132 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013027946464717388, Std: 0.08742828667163849\n",
      "z_j Mean: 0.01282680593430996, Std: 0.08745802938938141\n",
      "Similarities: tensor([[1.2772, 0.0000, 0.1152, 0.0000, 0.0000],\n",
      "        [0.0000, 1.2437, 0.0279, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1425, 1.3006, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4226, 0.0799],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0499, 1.3015]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.028404474258423\n",
      "Batch 132, Loss: 3.028404474258423\n",
      "Processing Batch 133/157\n",
      "Batch 133 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013068054802715778, Std: 0.08742229640483856\n",
      "z_j Mean: 0.013321883045136929, Std: 0.08738397806882858\n",
      "Similarities: tensor([[1.4166, 0.1535, 0.0000, 0.0357, 0.0199],\n",
      "        [0.3710, 1.3193, 0.0000, 0.3181, 0.7095],\n",
      "        [0.0000, 0.0000, 1.2522, 0.0000, 0.0000],\n",
      "        [0.0161, 0.1329, 0.0000, 1.3854, 0.0172],\n",
      "        [0.0188, 0.4177, 0.0000, 0.0360, 1.3914]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4282163381576538]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0499227046966553\n",
      "Batch 133, Loss: 3.0499227046966553\n",
      "Processing Batch 134/157\n",
      "Batch 134 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01221926137804985, Std: 0.0875449851155281\n",
      "z_j Mean: 0.012622373178601265, Std: 0.08748777210712433\n",
      "Similarities: tensor([[1.4277, 0.0000, 0.7310, 0.0000, 0.0077],\n",
      "        [0.0000, 1.4286, 0.0000, 0.6314, 0.0000],\n",
      "        [0.5342, 0.0000, 1.2883, 0.0000, 0.1827],\n",
      "        [0.0000, 0.4061, 0.0000, 1.4038, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0878, 0.0000, 1.2873]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0576109886169434\n",
      "Batch 134, Loss: 3.0576109886169434\n",
      "Processing Batch 135/157\n",
      "Batch 135 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01265276875346899, Std: 0.08748337626457214\n",
      "z_j Mean: 0.012888073921203613, Std: 0.08744902908802032\n",
      "Similarities: tensor([[1.3565, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4163, 0.3027, 0.2908, 0.0271],\n",
      "        [0.0000, 0.4774, 1.2875, 0.7801, 0.6706],\n",
      "        [0.0000, 0.3538, 1.0159, 1.4192, 1.0885],\n",
      "        [0.0000, 0.0380, 0.4908, 1.2288, 1.0555]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0745105743408203\n",
      "Batch 135, Loss: 3.0745105743408203\n",
      "Processing Batch 136/157\n",
      "Batch 136 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012516065500676632, Std: 0.08750304579734802\n",
      "z_j Mean: 0.012249385006725788, Std: 0.0868406742811203\n",
      "Similarities: tensor([[1.4286, 1.4023, 0.0000, 0.0000, 1.2813],\n",
      "        [1.4164, 1.4259, 0.0000, 0.0000, 1.3488],\n",
      "        [0.0000, 0.1620, 1.0466, 0.0000, 0.4722],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4014, 0.0000],\n",
      "        [1.2399, 1.3442, 0.1797, 0.0000, 1.4251]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.074169874191284\n",
      "Batch 136, Loss: 3.074169874191284\n",
      "Processing Batch 137/157\n",
      "Batch 137 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01336074247956276, Std: 0.08737803995609283\n",
      "z_j Mean: 0.013269713148474693, Std: 0.0873919129371643\n",
      "Similarities: tensor([[1.3702, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.2633, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3595, 0.4022, 0.0000],\n",
      "        [0.0000, 0.0000, 0.5045, 1.3959, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4061]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0491676330566406\n",
      "Batch 137, Loss: 3.0491676330566406\n",
      "Processing Batch 138/157\n",
      "Batch 138 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011960763484239578, Std: 0.08758068829774857\n",
      "z_j Mean: 0.011937154456973076, Std: 0.08758390694856644\n",
      "Similarities: tensor([[1.4111e+00, 0.0000e+00, 1.4059e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 1.3800e+00, 7.5505e-01, 6.0088e-01, 1.2408e-02],\n",
      "        [0.0000e+00, 7.6487e-01, 1.4265e+00, 2.9750e-01, 8.1401e-03],\n",
      "        [0.0000e+00, 5.2059e-01, 3.7870e-01, 1.2627e+00, 2.5340e-02],\n",
      "        [0.0000e+00, 0.0000e+00, 1.7623e-04, 0.0000e+00, 1.4238e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0088024139404297\n",
      "Batch 138, Loss: 3.0088024139404297\n",
      "Processing Batch 139/157\n",
      "Batch 139 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011547403410077095, Std: 0.08623180538415909\n",
      "z_j Mean: 0.011479600332677364, Std: 0.08624085783958435\n",
      "Similarities: tensor([[1.4206, 0.0000, 0.0000, 0.0000, 0.0795],\n",
      "        [0.0000, 1.3965, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3934, 0.0000, 0.0000],\n",
      "        [0.0473, 0.0000, 0.0000, 0.9348, 1.4122],\n",
      "        [0.0000, 0.0000, 0.0000, 0.9148, 1.4124]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.141071081161499\n",
      "Batch 139, Loss: 3.141071081161499\n",
      "Processing Batch 140/157\n",
      "Batch 140 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012283353134989738, Std: 0.08753602206707001\n",
      "z_j Mean: 0.012133311480283737, Std: 0.08755694329738617\n",
      "Similarities: tensor([[1.4215, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3268, 0.2766, 0.1948, 0.0000],\n",
      "        [0.0000, 0.1137, 1.4110, 0.1851, 0.0000],\n",
      "        [0.0000, 0.0234, 0.2495, 1.2652, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0469, 1.3580]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0372793674468994\n",
      "Batch 140, Loss: 3.0372793674468994\n",
      "Processing Batch 141/157\n",
      "Batch 141 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01296619325876236, Std: 0.0874374732375145\n",
      "z_j Mean: 0.01309763453900814, Std: 0.08741787821054459\n",
      "Similarities: tensor([[1.3460, 0.0000, 0.0000, 0.0000, 0.3603],\n",
      "        [0.0000, 1.4049, 0.0000, 0.1217, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3826, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1134, 0.0000, 1.3889, 0.0000],\n",
      "        [0.7612, 0.0000, 0.0000, 0.0000, 1.3406]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285542964935303]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.056655168533325\n",
      "Batch 141, Loss: 3.056655168533325\n",
      "Processing Batch 142/157\n",
      "Batch 142 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01262595597654581, Std: 0.08748725056648254\n",
      "z_j Mean: 0.012720896862447262, Std: 0.08747349679470062\n",
      "Similarities: tensor([[1.4191, 0.0000, 0.0000, 0.0000, 0.0759],\n",
      "        [0.1036, 1.3925, 0.0000, 0.1807, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3996, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2208, 0.0000, 1.3386, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3784]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.074748992919922\n",
      "Batch 142, Loss: 3.074748992919922\n",
      "Processing Batch 143/157\n",
      "Batch 143 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012760370038449764, Std: 0.0867670550942421\n",
      "z_j Mean: 0.01311679556965828, Std: 0.08741500228643417\n",
      "Similarities: tensor([[1.3372, 0.0000, 1.3606, 0.3233, 0.3872],\n",
      "        [0.0000, 1.1337, 0.0000, 0.0000, 0.0000],\n",
      "        [0.9764, 0.0000, 1.4112, 0.0769, 0.3426],\n",
      "        [0.5547, 0.0000, 0.2011, 1.4222, 0.0812],\n",
      "        [0.2708, 0.0000, 0.2737, 0.0436, 1.3636]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.05483341217041\n",
      "Batch 143, Loss: 3.05483341217041\n",
      "Processing Batch 144/157\n",
      "Batch 144 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013103324919939041, Std: 0.08741702139377594\n",
      "z_j Mean: 0.01304652076214552, Std: 0.08742552250623703\n",
      "Similarities: tensor([[1.2941, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4026, 0.0000, 0.0915, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3040, 0.3795, 0.0000],\n",
      "        [0.0000, 0.1614, 0.3503, 1.3867, 0.0947],\n",
      "        [0.0000, 0.0000, 0.0599, 0.0366, 1.3723]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0523712635040283\n",
      "Batch 144, Loss: 3.0523712635040283\n",
      "Processing Batch 145/157\n",
      "Batch 145 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012586872093379498, Std: 0.08749288320541382\n",
      "z_j Mean: 0.012540863826870918, Std: 0.08679905533790588\n",
      "Similarities: tensor([[1.3915e+00, 0.0000e+00, 2.4507e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 1.4054e+00, 0.0000e+00, 0.0000e+00, 8.7234e-02],\n",
      "        [2.1926e-01, 0.0000e+00, 1.4269e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 5.8082e-04, 1.3784e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 1.5406e-01, 2.3770e-02, 0.0000e+00, 1.3008e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0129575729370117\n",
      "Batch 145, Loss: 3.0129575729370117\n",
      "Processing Batch 146/157\n",
      "Batch 146 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012247342616319656, Std: 0.08684095740318298\n",
      "z_j Mean: 0.01250615157186985, Std: 0.08680406212806702\n",
      "Similarities: tensor([[1.4254, 0.0000, 0.0000, 0.0000, 1.4045],\n",
      "        [0.0000, 1.3327, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.2034, 0.7621, 0.0000],\n",
      "        [0.0000, 0.0000, 1.2352, 1.2617, 0.0000],\n",
      "        [1.3834, 0.0000, 0.0000, 0.0000, 1.4152]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0676586627960205\n",
      "Batch 146, Loss: 3.0676586627960205\n",
      "Processing Batch 147/157\n",
      "Batch 147 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011790651828050613, Std: 0.08760375529527664\n",
      "z_j Mean: 0.011662814766168594, Std: 0.08762086927890778\n",
      "Similarities: tensor([[1.4116, 0.6285, 0.0000, 0.0000, 0.0000],\n",
      "        [0.9476, 1.2395, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4248, 0.0896, 0.8337],\n",
      "        [0.0000, 0.0000, 0.0858, 1.3364, 0.8023],\n",
      "        [0.0000, 0.0000, 0.5131, 0.5570, 1.3880]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.022218942642212\n",
      "Batch 147, Loss: 3.022218942642212\n",
      "Processing Batch 148/157\n",
      "Batch 148 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012174427509307861, Std: 0.08755123615264893\n",
      "z_j Mean: 0.011954538524150848, Std: 0.08758153766393661\n",
      "Similarities: tensor([[1.4085, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4192, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0672, 0.0000, 1.3858, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4277, 0.0042],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3715]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0370285511016846\n",
      "Batch 148, Loss: 3.0370285511016846\n",
      "Processing Batch 149/157\n",
      "Batch 149 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012542415410280228, Std: 0.08749927580356598\n",
      "z_j Mean: 0.01245032250881195, Std: 0.08751242607831955\n",
      "Similarities: tensor([[1.4284, 0.3746, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5591, 1.2696, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1205, 1.4078, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4239, 0.1584],\n",
      "        [0.0000, 0.0000, 0.0000, 0.1259, 1.4060]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0284416675567627\n",
      "Batch 149, Loss: 3.0284416675567627\n",
      "Processing Batch 150/157\n",
      "Batch 150 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.0130746029317379, Std: 0.08742132037878036\n",
      "z_j Mean: 0.013207044452428818, Std: 0.08740141242742538\n",
      "Similarities: tensor([[1.3211, 0.0000, 0.0000, 0.0529, 0.0000],\n",
      "        [0.0000, 1.3121, 0.0000, 0.7118, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4230, 0.0000, 0.0000],\n",
      "        [0.0000, 1.2081, 0.0000, 1.3062, 0.2013],\n",
      "        [0.0000, 0.0648, 0.0000, 0.1519, 1.4188]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0350046157836914\n",
      "Batch 150, Loss: 3.0350046157836914\n",
      "Processing Batch 151/157\n",
      "Batch 151 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013057333417236805, Std: 0.0874239057302475\n",
      "z_j Mean: 0.012960163876414299, Std: 0.08743836730718613\n",
      "Similarities: tensor([[1.3270, 0.0139, 0.0000, 0.7258, 0.0000],\n",
      "        [0.0364, 1.3969, 0.0000, 0.0000, 0.2596],\n",
      "        [0.0000, 0.0000, 1.3847, 0.0000, 0.0000],\n",
      "        [0.7698, 0.0000, 0.0000, 1.4278, 0.0000],\n",
      "        [0.0000, 0.3191, 0.0000, 0.0000, 1.2924]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.428571105003357]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0391573905944824\n",
      "Batch 151, Loss: 3.0391573905944824\n",
      "Processing Batch 152/157\n",
      "Batch 152 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012989683076739311, Std: 0.08743397891521454\n",
      "z_j Mean: 0.012736787088215351, Std: 0.08677051961421967\n",
      "Similarities: tensor([[1.3986, 0.0000, 0.0634, 0.0000, 0.0000],\n",
      "        [0.0000, 1.2823, 0.0000, 0.7196, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3933, 0.0000, 0.0000],\n",
      "        [0.0000, 0.9154, 0.0000, 1.3727, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4279]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4279288053512573]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.095186233520508\n",
      "Batch 152, Loss: 3.095186233520508\n",
      "Processing Batch 153/157\n",
      "Batch 153 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013786773197352886, Std: 0.08731183409690857\n",
      "z_j Mean: 0.014026764780282974, Std: 0.08727359026670456\n",
      "Similarities: tensor([[1.2195, 0.0000, 0.0000, 0.0000, 0.1750],\n",
      "        [0.0000, 0.7567, 1.3107, 0.0000, 0.1018],\n",
      "        [0.0000, 0.8555, 1.0707, 0.0000, 0.1698],\n",
      "        [0.0208, 0.0000, 0.0000, 1.3971, 0.1502],\n",
      "        [0.3339, 0.4701, 0.0000, 0.0612, 1.4006]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0846261978149414\n",
      "Batch 153, Loss: 3.0846261978149414\n",
      "Processing Batch 154/157\n",
      "Batch 154 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013080998323857784, Std: 0.08742036670446396\n",
      "z_j Mean: 0.013434259220957756, Std: 0.08736676722764969\n",
      "Similarities: tensor([[1.3947, 1.0876, 0.6240, 0.1523, 0.0000],\n",
      "        [0.9919, 1.0658, 0.1372, 0.0000, 0.0000],\n",
      "        [0.7103, 0.0000, 1.4172, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.9623, 0.0209],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0026, 1.4097]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.428331732749939]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0635266304016113\n",
      "Batch 154, Loss: 3.0635266304016113\n",
      "Processing Batch 155/157\n",
      "Batch 155 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012641564011573792, Std: 0.08748500049114227\n",
      "z_j Mean: 0.012888297438621521, Std: 0.08744898438453674\n",
      "Similarities: tensor([[1.4217, 0.0000, 0.0000, 0.7103, 0.0000],\n",
      "        [0.0000, 1.1855, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3973, 0.0000, 0.0000],\n",
      "        [0.7925, 0.0000, 0.0000, 1.4181, 0.0000],\n",
      "        [0.0000, 0.0081, 0.0000, 0.0000, 1.4252]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0116946697235107\n",
      "Batch 155, Loss: 3.0116946697235107\n",
      "Processing Batch 156/157\n",
      "Batch 156 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012400541454553604, Std: 0.08751948922872543\n",
      "z_j Mean: 0.012187929823994637, Std: 0.0875493586063385\n",
      "Similarities: tensor([[1.4275, 0.0000, 1.0587, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4225, 0.0000, 0.0000, 0.0000],\n",
      "        [1.0428, 0.0000, 1.4285, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.2874, 0.0000],\n",
      "        [0.0000, 0.2604, 0.0000, 0.0000, 1.3518]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0268449783325195\n",
      "Batch 156, Loss: 3.0268449783325195\n",
      "Processing Batch 157/157\n",
      "Batch 157 loaded: x.shape=torch.Size([16, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([16, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([16, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([16, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([16, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([16, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013892188668251038, Std: 0.08731110394001007\n",
      "z_j Mean: 0.013587823137640953, Std: 0.08735901117324829\n",
      "Similarities: tensor([[1.3954, 0.0677, 0.0000, 0.7153, 0.2055],\n",
      "        [0.2329, 1.2025, 0.3973, 0.7172, 0.0273],\n",
      "        [0.0000, 0.2856, 1.3001, 0.0000, 0.0000],\n",
      "        [1.2344, 0.1374, 0.0000, 1.0711, 0.0931],\n",
      "        [0.2067, 0.0394, 0.0000, 0.6804, 1.3448]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 1.756925106048584\n",
      "Batch 157, Loss: 1.756925106048584\n",
      "Epoch 8/10, Loss: 3.0507\n",
      "Entered Epoch 9\n",
      "Starting Training Loop with 157 batches.\n",
      "Processing Batch 1/157\n",
      "Batch 1 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011729517951607704, Std: 0.08620721846818924\n",
      "z_j Mean: 0.012278241105377674, Std: 0.08683659881353378\n",
      "Similarities: tensor([[1.2114, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3638, 0.0000, 0.2975, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4277, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0335, 0.0000, 1.3836, 0.2365],\n",
      "        [0.0000, 0.0000, 0.0000, 0.2388, 1.4083]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.070444107055664\n",
      "Batch 1, Loss: 3.070444107055664\n",
      "Processing Batch 2/157\n",
      "Batch 2 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013621455989778042, Std: 0.08733777701854706\n",
      "z_j Mean: 0.01395857147872448, Std: 0.08728452026844025\n",
      "Similarities: tensor([[1.3896e+00, 0.0000e+00, 1.3059e+00, 0.0000e+00, 3.2533e-01],\n",
      "        [0.0000e+00, 1.4188e+00, 0.0000e+00, 3.5395e-04, 0.0000e+00],\n",
      "        [1.3452e+00, 0.0000e+00, 1.1926e+00, 0.0000e+00, 2.8784e-01],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 1.4077e+00, 0.0000e+00],\n",
      "        [3.5581e-01, 0.0000e+00, 4.6815e-01, 0.0000e+00, 1.3594e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0541534423828125\n",
      "Batch 2, Loss: 3.0541534423828125\n",
      "Processing Batch 3/157\n",
      "Batch 3 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.0133376345038414, Std: 0.08738157898187637\n",
      "z_j Mean: 0.01329183578491211, Std: 0.08668722212314606\n",
      "Similarities: tensor([[1.3219, 0.6624, 0.0000, 0.0000, 0.0000],\n",
      "        [1.0993, 1.2717, 0.0000, 0.1057, 0.1825],\n",
      "        [0.0000, 0.0000, 1.3760, 0.0000, 0.0952],\n",
      "        [0.0000, 0.3620, 0.0000, 0.9846, 0.0000],\n",
      "        [0.1141, 0.3937, 0.0000, 0.0000, 1.2524]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.428550362586975]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0819575786590576\n",
      "Batch 3, Loss: 3.0819575786590576\n",
      "Processing Batch 4/157\n",
      "Batch 4 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012407593429088593, Std: 0.08751849085092545\n",
      "z_j Mean: 0.01238669827580452, Std: 0.08752145618200302\n",
      "Similarities: tensor([[1.4175, 0.0000, 0.0000, 0.1259, 0.0000],\n",
      "        [0.0000, 1.3240, 0.0000, 0.0000, 0.4616],\n",
      "        [0.0000, 0.0000, 1.4286, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3813, 0.0000],\n",
      "        [0.0000, 0.3002, 0.0000, 0.0000, 1.4262]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0348756313323975\n",
      "Batch 4, Loss: 3.0348756313323975\n",
      "Processing Batch 5/157\n",
      "Batch 5 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013309933245182037, Std: 0.08738579601049423\n",
      "z_j Mean: 0.01323020365089178, Std: 0.08739790320396423\n",
      "Similarities: tensor([[1.3991, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3578, 0.0000, 0.1816, 0.0409],\n",
      "        [0.0000, 0.0000, 1.1224, 0.0000, 0.0523],\n",
      "        [0.0000, 0.0788, 0.0000, 1.4071, 0.0030],\n",
      "        [0.0000, 0.2434, 0.1337, 0.0443, 1.3833]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0228309631347656\n",
      "Batch 5, Loss: 3.0228309631347656\n",
      "Processing Batch 6/157\n",
      "Batch 6 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013007419183850288, Std: 0.08743134886026382\n",
      "z_j Mean: 0.012812837027013302, Std: 0.08675932139158249\n",
      "Similarities: tensor([[1.4142, 0.4116, 1.2580, 0.0000, 0.0000],\n",
      "        [0.6158, 1.3616, 0.1667, 0.0000, 0.0000],\n",
      "        [1.1424, 0.0580, 1.4071, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4234, 1.0556],\n",
      "        [0.0000, 0.0000, 0.0000, 1.1354, 1.1589]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0555663108825684\n",
      "Batch 6, Loss: 3.0555663108825684\n",
      "Processing Batch 7/157\n",
      "Batch 7 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012591859325766563, Std: 0.08749216794967651\n",
      "z_j Mean: 0.012315248139202595, Std: 0.08753152936697006\n",
      "Similarities: tensor([[1.4083, 0.2582, 0.1927, 0.0000, 0.0000],\n",
      "        [0.1459, 1.4286, 1.0662, 0.0000, 0.0000],\n",
      "        [0.0967, 0.9470, 1.1939, 0.0000, 0.0000],\n",
      "        [0.1411, 0.0000, 0.0000, 1.3625, 0.8570],\n",
      "        [0.1283, 0.0000, 0.0000, 1.1436, 1.4051]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0487871170043945\n",
      "Batch 7, Loss: 3.0487871170043945\n",
      "Processing Batch 8/157\n",
      "Batch 8 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012165049090981483, Std: 0.08755253255367279\n",
      "z_j Mean: 0.012129122391343117, Std: 0.08685755729675293\n",
      "Similarities: tensor([[1.2194, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3897, 0.0000, 0.0000, 1.4214],\n",
      "        [0.0000, 0.0000, 1.3295, 0.2600, 0.0000],\n",
      "        [0.0000, 0.0000, 0.1165, 1.0714, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0650758743286133\n",
      "Batch 8, Loss: 3.0650758743286133\n",
      "Processing Batch 9/157\n",
      "Batch 9 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012937923893332481, Std: 0.08674074709415436\n",
      "z_j Mean: 0.013436533510684967, Std: 0.08736641705036163\n",
      "Similarities: tensor([[1.2710e+00, 7.8897e-01, 4.6917e-01, 5.6063e-01, 6.4705e-01],\n",
      "        [5.6700e-01, 1.3947e+00, 1.2162e+00, 6.3552e-01, 9.7682e-02],\n",
      "        [7.9662e-01, 1.0290e+00, 7.9625e-01, 8.6320e-01, 1.4130e-01],\n",
      "        [7.4284e-01, 6.5867e-01, 5.6577e-01, 1.4071e+00, 1.5053e-01],\n",
      "        [4.7281e-01, 5.8614e-04, 0.0000e+00, 1.6911e-02, 1.4210e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1072046756744385\n",
      "Batch 9, Loss: 3.1072046756744385\n",
      "Processing Batch 10/157\n",
      "Batch 10 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013392668217420578, Std: 0.08737315237522125\n",
      "z_j Mean: 0.013163733296096325, Std: 0.08670676499605179\n",
      "Similarities: tensor([[1.4184, 0.0000, 0.0000, 0.1270, 0.5511],\n",
      "        [0.0000, 1.3995, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4175, 0.1203, 0.0000],\n",
      "        [0.2892, 0.0000, 0.0060, 1.3736, 0.0000],\n",
      "        [0.2539, 0.0000, 0.0000, 0.0000, 1.2367]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285684823989868]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.104231834411621\n",
      "Batch 10, Loss: 3.104231834411621\n",
      "Processing Batch 11/157\n",
      "Batch 11 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01275559514760971, Std: 0.0867677628993988\n",
      "z_j Mean: 0.012974334880709648, Std: 0.0867353156208992\n",
      "Similarities: tensor([[1.4224, 0.1817, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2651, 1.4275, 0.0000, 0.0000, 0.0362],\n",
      "        [0.0000, 0.0000, 1.4203, 0.2422, 0.0000],\n",
      "        [0.0000, 0.0000, 0.9665, 1.1497, 0.0000],\n",
      "        [0.0000, 0.0113, 0.0000, 0.0000, 1.3869]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0308849811553955\n",
      "Batch 11, Loss: 3.0308849811553955\n",
      "Processing Batch 12/157\n",
      "Batch 12 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01305319182574749, Std: 0.08742453157901764\n",
      "z_j Mean: 0.012707872316241264, Std: 0.08747538924217224\n",
      "Similarities: tensor([[1.3626, 0.1781, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2243, 1.3527, 0.0000, 0.4609, 0.0000],\n",
      "        [0.0000, 0.2871, 1.1120, 0.0000, 0.0819],\n",
      "        [0.0000, 0.1835, 0.0000, 1.3030, 0.0000],\n",
      "        [0.0000, 0.0000, 0.2930, 0.0000, 1.3905]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.021287441253662\n",
      "Batch 12, Loss: 3.021287441253662\n",
      "Processing Batch 13/157\n",
      "Batch 13 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012606140226125717, Std: 0.08749011158943176\n",
      "z_j Mean: 0.012639664113521576, Std: 0.0867847204208374\n",
      "Similarities: tensor([[1.4158, 1.3005, 0.0049, 0.0000, 0.1046],\n",
      "        [1.2973, 1.4183, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.2511, 1.3010, 0.0000],\n",
      "        [0.0000, 0.0000, 0.4987, 0.4609, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0670, 0.0000, 1.4194]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1007354259490967\n",
      "Batch 13, Loss: 3.1007354259490967\n",
      "Processing Batch 14/157\n",
      "Batch 14 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013071859255433083, Std: 0.0874217301607132\n",
      "z_j Mean: 0.012858566828072071, Std: 0.08745335787534714\n",
      "Similarities: tensor([[1.3375, 0.3345, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1080, 1.3530, 0.0000, 0.0000, 0.3410],\n",
      "        [0.0000, 0.0000, 1.4281, 1.3486, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3795, 1.4269, 0.0000],\n",
      "        [0.1835, 0.2434, 0.0000, 0.0000, 1.2870]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.012883186340332\n",
      "Batch 14, Loss: 3.012883186340332\n",
      "Processing Batch 15/157\n",
      "Batch 15 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012424521148204803, Std: 0.08751609176397324\n",
      "z_j Mean: 0.012280050665140152, Std: 0.08683633804321289\n",
      "Similarities: tensor([[1.4031, 0.0216, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4279, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.2897, 0.0220, 0.8527],\n",
      "        [0.0000, 0.0000, 0.1030, 1.4098, 0.0025],\n",
      "        [0.0000, 0.0000, 0.3489, 0.0506, 1.3025]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.086956739425659\n",
      "Batch 15, Loss: 3.086956739425659\n",
      "Processing Batch 16/157\n",
      "Batch 16 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012142985127866268, Std: 0.08755560219287872\n",
      "z_j Mean: 0.012132668867707253, Std: 0.08755704015493393\n",
      "Similarities: tensor([[1.3970, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4029, 0.2549, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1675, 1.3918, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4220, 0.3404],\n",
      "        [0.0000, 0.0000, 0.0000, 0.2703, 1.3668]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285614490509033]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0458736419677734\n",
      "Batch 16, Loss: 3.0458736419677734\n",
      "Processing Batch 17/157\n",
      "Batch 17 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012297574430704117, Std: 0.08753401786088943\n",
      "z_j Mean: 0.012485034763813019, Std: 0.08750747889280319\n",
      "Similarities: tensor([[1.3818, 1.3893, 0.0000, 0.0571, 0.0000],\n",
      "        [1.3785, 1.4038, 0.0000, 0.4153, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4274, 0.0000, 0.0000],\n",
      "        [0.1683, 0.2834, 0.0000, 1.4274, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4175]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285542964935303]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.043991804122925\n",
      "Batch 17, Loss: 3.043991804122925\n",
      "Processing Batch 18/157\n",
      "Batch 18 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012887977063655853, Std: 0.08744903653860092\n",
      "z_j Mean: 0.012925630435347557, Std: 0.08744347095489502\n",
      "Similarities: tensor([[1.4059, 0.0000, 0.0000, 1.1021, 0.2736],\n",
      "        [0.0000, 1.4255, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4171, 0.0000, 0.0000],\n",
      "        [1.3464, 0.0000, 0.0000, 1.3027, 0.0438],\n",
      "        [0.0845, 0.0000, 0.0077, 0.0000, 1.4038]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0545694828033447\n",
      "Batch 18, Loss: 3.0545694828033447\n",
      "Processing Batch 19/157\n",
      "Batch 19 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013089756481349468, Std: 0.0874190554022789\n",
      "z_j Mean: 0.013065662235021591, Std: 0.08742265403270721\n",
      "Similarities: tensor([[1.3668e+00, 6.8467e-01, 9.4966e-01, 1.0252e-01, 0.0000e+00],\n",
      "        [8.1939e-01, 1.4286e+00, 4.9965e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [1.1962e+00, 6.5019e-01, 1.4052e+00, 0.0000e+00, 1.4193e-01],\n",
      "        [3.4713e-05, 0.0000e+00, 0.0000e+00, 1.2906e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 1.5211e-01, 0.0000e+00, 1.3925e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.048084020614624\n",
      "Batch 19, Loss: 3.048084020614624\n",
      "Processing Batch 20/157\n",
      "Batch 20 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012687092646956444, Std: 0.087478406727314\n",
      "z_j Mean: 0.012571359984576702, Std: 0.0874951183795929\n",
      "Similarities: tensor([[1.3857, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.8632, 0.0043, 1.1349, 0.8294],\n",
      "        [0.0000, 0.0000, 1.3471, 0.0000, 0.3079],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3224, 0.8707],\n",
      "        [0.0000, 0.0000, 0.1389, 1.0891, 1.3804]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.074207067489624\n",
      "Batch 20, Loss: 3.074207067489624\n",
      "Processing Batch 21/157\n",
      "Batch 21 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012110603973269463, Std: 0.08756007999181747\n",
      "z_j Mean: 0.01217651553452015, Std: 0.08755094558000565\n",
      "Similarities: tensor([[1.3755, 0.0000, 0.0000, 0.0000, 1.0866],\n",
      "        [0.0000, 1.4241, 0.0520, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1770, 1.1330, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4261, 0.0000],\n",
      "        [1.0253, 0.0000, 0.0000, 0.0000, 1.3711]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0167884826660156\n",
      "Batch 21, Loss: 3.0167884826660156\n",
      "Processing Batch 22/157\n",
      "Batch 22 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012679418548941612, Std: 0.08747951686382294\n",
      "z_j Mean: 0.013118039816617966, Std: 0.08671369403600693\n",
      "Similarities: tensor([[1.4277, 1.3152, 0.0000, 0.0000, 0.0000],\n",
      "        [1.2668, 1.4200, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3005, 0.9925, 0.0000],\n",
      "        [0.0000, 0.0000, 1.2239, 1.4283, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3848]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1291327476501465\n",
      "Batch 22, Loss: 3.1291327476501465\n",
      "Processing Batch 23/157\n",
      "Batch 23 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012159503996372223, Std: 0.08755331486463547\n",
      "z_j Mean: 0.011781159788370132, Std: 0.08760503679513931\n",
      "Similarities: tensor([[1.3988, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4122, 0.0000, 0.0000, 0.9887],\n",
      "        [0.0000, 0.0000, 1.4181, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3871, 0.0000],\n",
      "        [0.0000, 0.9184, 0.0000, 0.0000, 1.3936]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.046722888946533\n",
      "Batch 23, Loss: 3.046722888946533\n",
      "Processing Batch 24/157\n",
      "Batch 24 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01337373536080122, Std: 0.08737605065107346\n",
      "z_j Mean: 0.013693494722247124, Std: 0.08732650429010391\n",
      "Similarities: tensor([[1.3229, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3769, 0.5239, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2788, 1.2458, 0.1382, 0.0749],\n",
      "        [0.0000, 0.0000, 0.1196, 1.3719, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0900, 0.0000, 1.2496]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.07716703414917\n",
      "Batch 24, Loss: 3.07716703414917\n",
      "Processing Batch 25/157\n",
      "Batch 25 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012462256476283073, Std: 0.08751072734594345\n",
      "z_j Mean: 0.012446386739611626, Std: 0.08751297742128372\n",
      "Similarities: tensor([[1.4275, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.5957, 0.0000, 0.2995, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3875, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4024, 0.0000, 1.4147, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.2276]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.03489351272583\n",
      "Batch 25, Loss: 3.03489351272583\n",
      "Processing Batch 26/157\n",
      "Batch 26 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013406066223978996, Std: 0.0873711034655571\n",
      "z_j Mean: 0.013509444892406464, Std: 0.08735517412424088\n",
      "Similarities: tensor([[1.4139, 0.0000, 0.0000, 0.1138, 0.0127],\n",
      "        [0.0000, 1.4225, 0.4374, 0.0000, 0.0000],\n",
      "        [0.0000, 0.5041, 1.3717, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3958, 0.0000],\n",
      "        [0.0113, 0.0000, 0.0000, 0.0000, 1.4280]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.074570655822754\n",
      "Batch 26, Loss: 3.074570655822754\n",
      "Processing Batch 27/157\n",
      "Batch 27 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012322784401476383, Std: 0.08683028817176819\n",
      "z_j Mean: 0.011911354027688503, Std: 0.08618228137493134\n",
      "Similarities: tensor([[0.7380, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.7444, 0.0000, 0.0000, 0.0570],\n",
      "        [0.0000, 0.0000, 1.4243, 0.6702, 0.0000],\n",
      "        [0.0000, 0.0000, 0.7196, 1.4196, 0.0000],\n",
      "        [0.0000, 0.1090, 0.0000, 0.0000, 1.2928]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0680859088897705\n",
      "Batch 27, Loss: 3.0680859088897705\n",
      "Processing Batch 28/157\n",
      "Batch 28 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012753186747431755, Std: 0.08746879547834396\n",
      "z_j Mean: 0.0127226198092103, Std: 0.08747324347496033\n",
      "Similarities: tensor([[0.0788, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.0980, 0.1634, 0.0000, 0.1993],\n",
      "        [0.0000, 0.1062, 1.4044, 0.0000, 0.4472],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4114, 0.0000],\n",
      "        [0.0000, 0.2333, 0.5296, 0.0000, 1.4053]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.072859525680542\n",
      "Batch 28, Loss: 3.072859525680542\n",
      "Processing Batch 29/157\n",
      "Batch 29 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01218478474766016, Std: 0.08684976398944855\n",
      "z_j Mean: 0.012588581070303917, Std: 0.08679214864969254\n",
      "Similarities: tensor([[1.3624, 0.0000, 0.5140, 0.2817, 0.0000],\n",
      "        [0.0000, 1.4282, 0.0000, 0.0000, 0.2268],\n",
      "        [0.5670, 0.0000, 1.3938, 0.0611, 0.0000],\n",
      "        [0.2494, 0.0000, 0.0810, 1.4249, 0.0000],\n",
      "        [0.0000, 0.2228, 0.0000, 0.0000, 1.3881]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0491652488708496\n",
      "Batch 29, Loss: 3.0491652488708496\n",
      "Processing Batch 30/157\n",
      "Batch 30 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011861031875014305, Std: 0.08759424835443497\n",
      "z_j Mean: 0.011847549118101597, Std: 0.08759607374668121\n",
      "Similarities: tensor([[1.2059, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3565, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4286, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4248, 0.0000],\n",
      "        [0.0633, 0.0000, 0.0000, 0.0000, 1.4160]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.030564069747925\n",
      "Batch 30, Loss: 3.030564069747925\n",
      "Processing Batch 31/157\n",
      "Batch 31 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01322891004383564, Std: 0.08739809691905975\n",
      "z_j Mean: 0.013222839683294296, Std: 0.08739901334047318\n",
      "Similarities: tensor([[1.3287, 0.7301, 0.0000, 0.0000, 0.4294],\n",
      "        [0.4678, 1.3922, 0.1036, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2035, 1.3816, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3903, 0.0000],\n",
      "        [0.7268, 0.0000, 0.0000, 0.0000, 1.3496]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0417277812957764\n",
      "Batch 31, Loss: 3.0417277812957764\n",
      "Processing Batch 32/157\n",
      "Batch 32 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012299742549657822, Std: 0.08753371983766556\n",
      "z_j Mean: 0.012172320857644081, Std: 0.08685150742530823\n",
      "Similarities: tensor([[1.4258, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4286, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3147, 0.0000, 1.1501],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4196, 0.0000],\n",
      "        [0.0000, 0.0000, 0.8139, 0.0000, 1.3326]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0313141345977783\n",
      "Batch 32, Loss: 3.0313141345977783\n",
      "Processing Batch 33/157\n",
      "Batch 33 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012378145009279251, Std: 0.08682241290807724\n",
      "z_j Mean: 0.012498043477535248, Std: 0.08750562369823456\n",
      "Similarities: tensor([[1.4279, 0.1155, 0.0000, 0.0429, 0.0000],\n",
      "        [0.2374, 1.0636, 0.1144, 0.5769, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4286, 0.0000, 0.0000],\n",
      "        [0.0969, 0.8054, 0.0000, 1.3425, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4269]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0751264095306396\n",
      "Batch 33, Loss: 3.0751264095306396\n",
      "Processing Batch 34/157\n",
      "Batch 34 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012771029025316238, Std: 0.08746619522571564\n",
      "z_j Mean: 0.01280001737177372, Std: 0.08746195584535599\n",
      "Similarities: tensor([[1.4285e+00, 0.0000e+00, 3.5685e-04, 7.1924e-02, 4.6652e-03],\n",
      "        [0.0000e+00, 1.4216e+00, 7.8186e-03, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 1.3767e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [2.3447e-01, 0.0000e+00, 7.9491e-02, 1.3976e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 9.0490e-03, 0.0000e+00, 1.3949e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285147190093994]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.043057918548584\n",
      "Batch 34, Loss: 3.043057918548584\n",
      "Processing Batch 35/157\n",
      "Batch 35 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012935181148350239, Std: 0.0874420627951622\n",
      "z_j Mean: 0.01292820181697607, Std: 0.08674220740795135\n",
      "Similarities: tensor([[1.4148, 0.1209, 0.0000, 0.7959, 0.0000],\n",
      "        [0.0612, 1.2442, 0.0126, 0.3416, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4281, 0.0000, 0.0000],\n",
      "        [0.6380, 0.2485, 0.0000, 1.3506, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4114]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0973103046417236\n",
      "Batch 35, Loss: 3.0973103046417236\n",
      "Processing Batch 36/157\n",
      "Batch 36 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012131035327911377, Std: 0.08755726367235184\n",
      "z_j Mean: 0.012531077489256859, Std: 0.08750089257955551\n",
      "Similarities: tensor([[1.4255, 0.0000, 0.0000, 0.0000, 0.3865],\n",
      "        [0.1053, 1.2441, 0.0000, 0.0000, 0.0688],\n",
      "        [0.0000, 0.0000, 1.3421, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4227, 0.0000],\n",
      "        [0.2650, 0.0116, 0.0000, 0.0000, 1.4221]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0100162029266357\n",
      "Batch 36, Loss: 3.0100162029266357\n",
      "Processing Batch 37/157\n",
      "Batch 37 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012563273310661316, Std: 0.08749628067016602\n",
      "z_j Mean: 0.012480308301746845, Std: 0.08750814944505692\n",
      "Similarities: tensor([[1.3611, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3744, 0.5193, 0.0000, 1.1046],\n",
      "        [0.0000, 0.4265, 1.4215, 0.0000, 0.0315],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4253, 0.0000],\n",
      "        [0.0000, 1.3243, 0.1320, 0.0000, 1.3847]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0405750274658203\n",
      "Batch 37, Loss: 3.0405750274658203\n",
      "Processing Batch 38/157\n",
      "Batch 38 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012269571423530579, Std: 0.08753795176744461\n",
      "z_j Mean: 0.012152320705354214, Std: 0.08755430579185486\n",
      "Similarities: tensor([[1.3986, 0.0000, 0.0000, 0.1359, 0.0000],\n",
      "        [0.0000, 1.3863, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.2854, 0.0000, 0.0000],\n",
      "        [0.1857, 0.0000, 0.0000, 1.3996, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4273]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4284381866455078]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 2.999950647354126\n",
      "Batch 38, Loss: 2.999950647354126\n",
      "Processing Batch 39/157\n",
      "Batch 39 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01229905430227518, Std: 0.08753381669521332\n",
      "z_j Mean: 0.012833122164011002, Std: 0.08745710551738739\n",
      "Similarities: tensor([[1.2949, 0.0000, 0.0000, 0.4682, 0.0000],\n",
      "        [0.0000, 1.4269, 0.0000, 0.0573, 0.0866],\n",
      "        [0.0000, 0.0000, 1.4251, 0.0000, 0.0000],\n",
      "        [0.7101, 0.0277, 0.0000, 1.3992, 0.7492],\n",
      "        [0.2643, 0.0777, 0.0000, 1.1117, 1.2501]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.037750244140625\n",
      "Batch 39, Loss: 3.037750244140625\n",
      "Processing Batch 40/157\n",
      "Batch 40 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012710257433354855, Std: 0.08747504651546478\n",
      "z_j Mean: 0.012911379337310791, Std: 0.08744558691978455\n",
      "Similarities: tensor([[1.3252, 0.0362, 0.0000, 0.9163, 0.0000],\n",
      "        [0.0000, 1.4130, 0.1627, 0.0000, 0.4589],\n",
      "        [0.0000, 0.1672, 1.2161, 0.0000, 0.0000],\n",
      "        [0.9551, 0.0000, 0.0000, 1.4143, 0.0000],\n",
      "        [0.0000, 0.0295, 0.0000, 0.0000, 0.0725]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0881872177124023\n",
      "Batch 40, Loss: 3.0881872177124023\n",
      "Processing Batch 41/157\n",
      "Batch 41 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011951887980103493, Std: 0.08617666363716125\n",
      "z_j Mean: 0.012207478284835815, Std: 0.08614082634449005\n",
      "Similarities: tensor([[1.3603, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4170, 0.0000, 0.0000, 0.8852],\n",
      "        [0.0000, 0.0000, 1.4061, 0.0000, 0.0000],\n",
      "        [0.0036, 0.0000, 0.0000, 1.3393, 0.0000],\n",
      "        [0.0000, 1.0467, 0.0000, 0.0000, 1.3994]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0616700649261475\n",
      "Batch 41, Loss: 3.0616700649261475\n",
      "Processing Batch 42/157\n",
      "Batch 42 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013713419437408447, Std: 0.0873233824968338\n",
      "z_j Mean: 0.013170585036277771, Std: 0.08670572936534882\n",
      "Similarities: tensor([[1.3246, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3277, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4173, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4188, 0.0186],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0298, 1.3978]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0678558349609375\n",
      "Batch 42, Loss: 3.0678558349609375\n",
      "Processing Batch 43/157\n",
      "Batch 43 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013269186019897461, Std: 0.08739199489355087\n",
      "z_j Mean: 0.013446359895169735, Std: 0.08736490458250046\n",
      "Similarities: tensor([[1.4145, 0.0000, 0.2810, 0.0000, 0.7464],\n",
      "        [0.0716, 1.3859, 0.0000, 0.0000, 0.2355],\n",
      "        [0.4372, 0.0000, 1.3417, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3038, 0.0000],\n",
      "        [0.7870, 0.0000, 0.0000, 0.0000, 1.4045]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0527405738830566\n",
      "Batch 43, Loss: 3.0527405738830566\n",
      "Processing Batch 44/157\n",
      "Batch 44 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012807994149625301, Std: 0.08746078610420227\n",
      "z_j Mean: 0.012959165498614311, Std: 0.08743850886821747\n",
      "Similarities: tensor([[1.3714, 0.0000, 1.2069, 0.8959, 0.8517],\n",
      "        [0.2542, 1.3845, 0.0000, 0.0000, 0.0000],\n",
      "        [1.0209, 0.0000, 1.2807, 0.4678, 0.5977],\n",
      "        [0.8966, 0.0000, 0.8500, 1.3905, 1.2974],\n",
      "        [0.6154, 0.0000, 0.5490, 1.2188, 1.3107]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.021928310394287\n",
      "Batch 44, Loss: 3.021928310394287\n",
      "Processing Batch 45/157\n",
      "Batch 45 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011903302744030952, Std: 0.08758851140737534\n",
      "z_j Mean: 0.012265417724847794, Std: 0.08753853291273117\n",
      "Similarities: tensor([[1.3948, 0.0000, 0.0000, 0.0000, 0.0244],\n",
      "        [0.0000, 1.4286, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.1337, 0.1862, 0.0000],\n",
      "        [0.0000, 0.0000, 0.3245, 1.2566, 0.0000],\n",
      "        [0.0731, 0.0000, 0.0000, 0.0000, 1.3411]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0580615997314453\n",
      "Batch 45, Loss: 3.0580615997314453\n",
      "Processing Batch 46/157\n",
      "Batch 46 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013441132381558418, Std: 0.08736571669578552\n",
      "z_j Mean: 0.013305002823472023, Std: 0.08738654851913452\n",
      "Similarities: tensor([[1.3988, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4273, 0.0209, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3988, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4286, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3562]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0433719158172607\n",
      "Batch 46, Loss: 3.0433719158172607\n",
      "Processing Batch 47/157\n",
      "Batch 47 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012231098487973213, Std: 0.08754333853721619\n",
      "z_j Mean: 0.012025837786495686, Std: 0.08757176995277405\n",
      "Similarities: tensor([[1.4231, 0.1216, 0.1757, 0.0000, 0.0000],\n",
      "        [0.0744, 1.2839, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2869, 0.0000, 1.4238, 0.2174, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0904, 1.3185, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4152]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0764260292053223\n",
      "Batch 47, Loss: 3.0764260292053223\n",
      "Processing Batch 48/157\n",
      "Batch 48 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012407022528350353, Std: 0.08751857280731201\n",
      "z_j Mean: 0.012747456319630146, Std: 0.08746962994337082\n",
      "Similarities: tensor([[1.2876, 0.1534, 0.0000, 1.2328, 1.1911],\n",
      "        [0.3319, 0.2615, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4051, 0.0000, 0.0000],\n",
      "        [1.3437, 0.0000, 0.0000, 1.3950, 1.3519],\n",
      "        [1.3156, 0.0000, 0.0000, 1.3841, 1.4174]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.010148048400879\n",
      "Batch 48, Loss: 3.010148048400879\n",
      "Processing Batch 49/157\n",
      "Batch 49 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013350307941436768, Std: 0.08737964183092117\n",
      "z_j Mean: 0.013122997246682644, Std: 0.08741407096385956\n",
      "Similarities: tensor([[1.4034, 0.0000, 0.1534, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4286, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2082, 0.0000, 1.3225, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3723, 0.3089],\n",
      "        [0.0000, 0.0000, 0.0000, 0.6512, 1.4056]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.040580987930298\n",
      "Batch 49, Loss: 3.040580987930298\n",
      "Processing Batch 50/157\n",
      "Batch 50 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012281210161745548, Std: 0.08753632009029388\n",
      "z_j Mean: 0.012300999835133553, Std: 0.08753354102373123\n",
      "Similarities: tensor([[1.0552e+00, 1.7188e-01, 2.5671e-01, 0.0000e+00, 8.6164e-01],\n",
      "        [3.2527e-01, 1.4076e+00, 0.0000e+00, 0.0000e+00, 2.4434e-01],\n",
      "        [0.0000e+00, 0.0000e+00, 1.3264e+00, 0.0000e+00, 1.2957e-03],\n",
      "        [0.0000e+00, 3.2824e-03, 0.0000e+00, 1.3892e+00, 0.0000e+00],\n",
      "        [8.9834e-01, 3.4647e-01, 2.4461e-01, 0.0000e+00, 1.3696e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0151169300079346\n",
      "Batch 50, Loss: 3.0151169300079346\n",
      "Processing Batch 51/157\n",
      "Batch 51 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01224440149962902, Std: 0.08754147589206696\n",
      "z_j Mean: 0.012264050543308258, Std: 0.08753872662782669\n",
      "Similarities: tensor([[1.4214, 0.6821, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4182, 1.3593, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4160, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3942, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3734]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0628244876861572\n",
      "Batch 51, Loss: 3.0628244876861572\n",
      "Processing Batch 52/157\n",
      "Batch 52 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01297779195010662, Std: 0.0874357596039772\n",
      "z_j Mean: 0.012783984653651714, Std: 0.08746430277824402\n",
      "Similarities: tensor([[1.4157, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4004, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4148, 0.0000, 0.0528],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4284, 0.1742],\n",
      "        [0.0000, 0.0000, 0.1909, 0.6395, 0.5474]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.034411907196045\n",
      "Batch 52, Loss: 3.034411907196045\n",
      "Processing Batch 53/157\n",
      "Batch 53 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013032053597271442, Std: 0.08742767572402954\n",
      "z_j Mean: 0.013261346146464348, Std: 0.08739318698644638\n",
      "Similarities: tensor([[1.4281, 0.0000, 0.4627, 0.0000, 0.0651],\n",
      "        [0.0000, 1.4277, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1831, 0.0000, 1.2574, 0.0000, 0.8390],\n",
      "        [0.0000, 0.0000, 0.0000, 1.2456, 0.0000],\n",
      "        [0.1777, 0.0000, 1.1079, 0.0000, 1.4052]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0500645637512207\n",
      "Batch 53, Loss: 3.0500645637512207\n",
      "Processing Batch 54/157\n",
      "Batch 54 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012977506965398788, Std: 0.08743578940629959\n",
      "z_j Mean: 0.012886839918792248, Std: 0.08744920045137405\n",
      "Similarities: tensor([[1.4173e+00, 1.2882e-02, 0.0000e+00, 6.6376e-04, 0.0000e+00],\n",
      "        [2.4300e-02, 1.3349e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 1.3273e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [3.0488e-04, 2.3898e-03, 0.0000e+00, 1.4207e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.4194e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.039980411529541\n",
      "Batch 54, Loss: 3.039980411529541\n",
      "Processing Batch 55/157\n",
      "Batch 55 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012714321725070477, Std: 0.08747445046901703\n",
      "z_j Mean: 0.012283280491828918, Std: 0.0875360295176506\n",
      "Similarities: tensor([[1.3897, 0.0000, 0.0000, 0.0035, 0.0000],\n",
      "        [0.0000, 1.3877, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3911, 0.0000, 0.0000],\n",
      "        [0.0326, 0.0000, 0.0000, 1.4277, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4250]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.038172721862793\n",
      "Batch 55, Loss: 3.038172721862793\n",
      "Processing Batch 56/157\n",
      "Batch 56 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012918512336909771, Std: 0.08744452893733978\n",
      "z_j Mean: 0.013349004089832306, Std: 0.0873798355460167\n",
      "Similarities: tensor([[1.4211, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.1978, 0.2885, 0.0000, 0.4846],\n",
      "        [0.0000, 0.5984, 1.4267, 0.0000, 0.5493],\n",
      "        [0.0000, 0.0594, 0.0089, 1.4197, 0.2645],\n",
      "        [0.0000, 0.9627, 0.9142, 0.0000, 1.2868]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0374021530151367\n",
      "Batch 56, Loss: 3.0374021530151367\n",
      "Processing Batch 57/157\n",
      "Batch 57 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013378763571381569, Std: 0.08737529069185257\n",
      "z_j Mean: 0.013281848281621933, Std: 0.08739007264375687\n",
      "Similarities: tensor([[1.4239e+00, 1.3830e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [8.4967e-03, 1.3703e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 2.1374e-01, 1.4121e+00, 0.0000e+00, 9.2754e-01],\n",
      "        [0.0000e+00, 0.0000e+00, 1.3257e-03, 1.3777e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 5.3274e-01, 1.1929e+00, 0.0000e+00, 1.2514e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0204107761383057\n",
      "Batch 57, Loss: 3.0204107761383057\n",
      "Processing Batch 58/157\n",
      "Batch 58 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01333625428378582, Std: 0.08738178759813309\n",
      "z_j Mean: 0.013494198210537434, Std: 0.0873575285077095\n",
      "Similarities: tensor([[1.4108, 0.0505, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0273, 1.3544, 0.1330, 0.0000, 0.2419],\n",
      "        [0.0000, 0.2153, 1.2955, 0.9445, 0.0000],\n",
      "        [0.0000, 0.0000, 0.9145, 1.4269, 0.0000],\n",
      "        [0.0000, 0.2151, 0.0000, 0.0000, 1.4202]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0510246753692627\n",
      "Batch 58, Loss: 3.0510246753692627\n",
      "Processing Batch 59/157\n",
      "Batch 59 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013483044691383839, Std: 0.08735924959182739\n",
      "z_j Mean: 0.013025294989347458, Std: 0.08672767132520676\n",
      "Similarities: tensor([[1.4094, 0.0000, 0.9166, 0.0000, 0.5300],\n",
      "        [0.0000, 1.4263, 0.0000, 0.0000, 0.0000],\n",
      "        [0.7582, 0.0000, 1.2863, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4262, 0.0163],\n",
      "        [0.6775, 0.0000, 0.0000, 0.0512, 1.3803]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0472917556762695\n",
      "Batch 59, Loss: 3.0472917556762695\n",
      "Processing Batch 60/157\n",
      "Batch 60 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01291591301560402, Std: 0.08744491636753082\n",
      "z_j Mean: 0.012581881135702133, Std: 0.08679312467575073\n",
      "Similarities: tensor([[1.3085, 0.0000, 0.2630, 0.0036, 0.5629],\n",
      "        [0.0000, 1.3009, 0.2058, 0.7302, 0.0000],\n",
      "        [0.0000, 0.0488, 1.4093, 0.0000, 0.6900],\n",
      "        [0.0000, 1.0475, 0.0000, 1.4181, 0.0000],\n",
      "        [0.0000, 0.0000, 0.6695, 0.0000, 1.4232]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0660715103149414\n",
      "Batch 60, Loss: 3.0660715103149414\n",
      "Processing Batch 61/157\n",
      "Batch 61 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013494121842086315, Std: 0.0873575434088707\n",
      "z_j Mean: 0.013488898053765297, Std: 0.08735834807157516\n",
      "Similarities: tensor([[1.3175, 0.0000, 1.1561, 0.0000, 0.8114],\n",
      "        [0.0000, 1.2929, 0.0000, 0.0000, 0.0000],\n",
      "        [1.1150, 0.0000, 1.4286, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4197, 0.0000],\n",
      "        [0.6538, 0.0000, 0.0000, 0.0000, 1.3968]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.078169345855713\n",
      "Batch 61, Loss: 3.078169345855713\n",
      "Processing Batch 62/157\n",
      "Batch 62 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013440795242786407, Std: 0.0873657613992691\n",
      "z_j Mean: 0.013531212694942951, Std: 0.08735180646181107\n",
      "Similarities: tensor([[1.3956, 0.0000, 0.0000, 0.0000, 0.9729],\n",
      "        [0.0000, 1.4175, 1.4036, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4071, 1.4225, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0990, 1.0449, 0.0000],\n",
      "        [0.7293, 0.0000, 0.0000, 0.0000, 1.4181]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0749173164367676\n",
      "Batch 62, Loss: 3.0749173164367676\n",
      "Processing Batch 63/157\n",
      "Batch 63 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013083366677165031, Std: 0.08671893179416656\n",
      "z_j Mean: 0.013524743728339672, Std: 0.08735280483961105\n",
      "Similarities: tensor([[1.4274, 0.0000, 0.0000, 0.0000, 0.0956],\n",
      "        [0.0000, 1.3522, 0.7839, 0.0000, 0.0000],\n",
      "        [0.0000, 1.0795, 1.4286, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1510, 0.0000, 0.0000, 1.0277, 1.2789]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.065250873565674\n",
      "Batch 63, Loss: 3.065250873565674\n",
      "Processing Batch 64/157\n",
      "Batch 64 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.014012688770890236, Std: 0.08727584779262543\n",
      "z_j Mean: 0.013895392417907715, Std: 0.08729460835456848\n",
      "Similarities: tensor([[1.4254, 0.0127, 0.0000, 0.0000, 0.0341],\n",
      "        [0.0741, 1.3777, 0.0066, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.7499, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0259, 0.0000],\n",
      "        [0.0431, 0.0000, 0.0000, 0.6285, 1.1974]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285554885864258]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0810365676879883\n",
      "Batch 64, Loss: 3.0810365676879883\n",
      "Processing Batch 65/157\n",
      "Batch 65 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013723489828407764, Std: 0.08732179552316666\n",
      "z_j Mean: 0.01371322013437748, Std: 0.08732341974973679\n",
      "Similarities: tensor([[1.4286, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.2842, 0.5477, 1.1279, 0.0000],\n",
      "        [0.0000, 0.7026, 1.3988, 0.6334, 0.2913],\n",
      "        [0.0000, 1.0446, 0.6415, 1.2978, 0.0463],\n",
      "        [0.0000, 0.0000, 0.3893, 0.0000, 1.3167]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.034320831298828\n",
      "Batch 65, Loss: 3.034320831298828\n",
      "Processing Batch 66/157\n",
      "Batch 66 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013111338019371033, Std: 0.08741581439971924\n",
      "z_j Mean: 0.013404998928308487, Std: 0.08737125992774963\n",
      "Similarities: tensor([[1.3932, 0.0000, 0.0000, 0.0000, 0.0020],\n",
      "        [0.0000, 1.4028, 0.1689, 0.3451, 0.0000],\n",
      "        [0.0000, 0.0259, 0.9581, 0.0405, 0.0000],\n",
      "        [0.0000, 0.0054, 0.0000, 0.7626, 0.1060],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3164]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0332651138305664\n",
      "Batch 66, Loss: 3.0332651138305664\n",
      "Processing Batch 67/157\n",
      "Batch 67 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012721620500087738, Std: 0.08747339248657227\n",
      "z_j Mean: 0.012805324979126453, Std: 0.08746117353439331\n",
      "Similarities: tensor([[1.3863, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3967, 0.0000, 0.0000, 0.0138],\n",
      "        [0.0000, 0.0000, 1.3084, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.2654, 0.9004],\n",
      "        [0.0000, 0.0000, 0.0000, 0.3108, 1.4284]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0214953422546387\n",
      "Batch 67, Loss: 3.0214953422546387\n",
      "Processing Batch 68/157\n",
      "Batch 68 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01252778246998787, Std: 0.0868009403347969\n",
      "z_j Mean: 0.012526407837867737, Std: 0.08680114150047302\n",
      "Similarities: tensor([[1.4271, 1.0078, 0.0000, 0.0593, 0.0000],\n",
      "        [0.0000, 0.9708, 0.0000, 0.1824, 0.0000],\n",
      "        [0.0000, 0.0000, 1.2964, 0.0000, 1.0708],\n",
      "        [0.0000, 0.3015, 0.0000, 1.3992, 0.0000],\n",
      "        [0.0000, 0.0000, 0.9407, 0.0148, 1.2840]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.065614700317383\n",
      "Batch 68, Loss: 3.065614700317383\n",
      "Processing Batch 69/157\n",
      "Batch 69 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013519505970180035, Std: 0.08735361695289612\n",
      "z_j Mean: 0.01346234604716301, Std: 0.08666089922189713\n",
      "Similarities: tensor([[1.3663, 0.0000, 0.2145, 0.0000, 0.4095],\n",
      "        [0.0000, 1.4258, 0.0000, 0.7034, 0.0000],\n",
      "        [0.3106, 0.0000, 1.3693, 0.0243, 0.9868],\n",
      "        [0.0000, 0.8000, 0.0000, 1.3868, 0.0000],\n",
      "        [0.6838, 0.0000, 0.8574, 0.0000, 0.8226]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.098069429397583\n",
      "Batch 69, Loss: 3.098069429397583\n",
      "Processing Batch 70/157\n",
      "Batch 70 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013456274755299091, Std: 0.08736337721347809\n",
      "z_j Mean: 0.013361330144107342, Std: 0.08667653799057007\n",
      "Similarities: tensor([[1.4282, 0.0000, 0.0259, 1.4157, 0.0000],\n",
      "        [0.0000, 1.4190, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0322, 0.0000, 1.3544, 0.0055, 0.0000],\n",
      "        [1.4265, 0.0000, 0.0231, 1.4182, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4286]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0358967781066895\n",
      "Batch 70, Loss: 3.0358967781066895\n",
      "Processing Batch 71/157\n",
      "Batch 71 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012129217386245728, Std: 0.08755751699209213\n",
      "z_j Mean: 0.01237355824559927, Std: 0.08752331137657166\n",
      "Similarities: tensor([[1.3510, 0.0000, 0.3401, 0.3882, 0.0000],\n",
      "        [0.0000, 1.4286, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1660, 0.0000, 1.3673, 0.0000, 0.0000],\n",
      "        [0.6042, 0.0000, 0.0000, 1.2903, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.2227]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.014813184738159\n",
      "Batch 71, Loss: 3.014813184738159\n",
      "Processing Batch 72/157\n",
      "Batch 72 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013122586533427238, Std: 0.08741413056850433\n",
      "z_j Mean: 0.013288715854287148, Std: 0.0873890370130539\n",
      "Similarities: tensor([[1.3750, 0.1060, 0.1004, 0.0000, 0.7696],\n",
      "        [0.1044, 1.3738, 0.2226, 0.0000, 0.0244],\n",
      "        [0.3189, 0.6059, 1.1541, 0.5472, 0.0746],\n",
      "        [0.0000, 0.0000, 0.9137, 1.3877, 0.0000],\n",
      "        [0.9471, 0.0248, 0.0966, 0.0000, 1.3839]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4283654689788818]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.066465377807617\n",
      "Batch 72, Loss: 3.066465377807617\n",
      "Processing Batch 73/157\n",
      "Batch 73 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012627833522856236, Std: 0.08748698234558105\n",
      "z_j Mean: 0.012777143158018589, Std: 0.08746529370546341\n",
      "Similarities: tensor([[1.4253, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3766, 0.0000, 1.2548, 0.0195],\n",
      "        [0.0000, 0.0000, 1.4204, 0.0000, 0.7582],\n",
      "        [0.0000, 1.2802, 0.0000, 1.3403, 0.0000],\n",
      "        [0.0000, 0.0419, 0.0000, 0.0000, 1.2059]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0842642784118652\n",
      "Batch 73, Loss: 3.0842642784118652\n",
      "Processing Batch 74/157\n",
      "Batch 74 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012647312134504318, Std: 0.08748416602611542\n",
      "z_j Mean: 0.012685977853834629, Std: 0.08677796274423599\n",
      "Similarities: tensor([[1.3433, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4232, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3696, 0.2473, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.6908, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4230]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0667712688446045\n",
      "Batch 74, Loss: 3.0667712688446045\n",
      "Processing Batch 75/157\n",
      "Batch 75 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01264612004160881, Std: 0.08748433738946915\n",
      "z_j Mean: 0.012852905318140984, Std: 0.08745419979095459\n",
      "Similarities: tensor([[1.4044, 0.3792, 0.0000, 0.2480, 0.0000],\n",
      "        [0.4876, 1.3871, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3872, 0.0000, 0.0000],\n",
      "        [0.1144, 0.0000, 0.0000, 1.4183, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.2623]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0390818119049072\n",
      "Batch 75, Loss: 3.0390818119049072\n",
      "Processing Batch 76/157\n",
      "Batch 76 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012780699878931046, Std: 0.08746477961540222\n",
      "z_j Mean: 0.0124592874199152, Std: 0.08751115202903748\n",
      "Similarities: tensor([[1.3058, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4212, 0.5716, 0.0000, 0.0000],\n",
      "        [0.0000, 0.4490, 1.4244, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.2629, 0.3209],\n",
      "        [0.0000, 0.0000, 0.0000, 0.4830, 1.4078]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.039710760116577\n",
      "Batch 76, Loss: 3.039710760116577\n",
      "Processing Batch 77/157\n",
      "Batch 77 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01282990537583828, Std: 0.087457574903965\n",
      "z_j Mean: 0.012661952525377274, Std: 0.0874820426106453\n",
      "Similarities: tensor([[1.4251, 0.0000, 0.6142, 0.0000, 0.0000],\n",
      "        [0.0000, 1.2221, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3791, 0.0000, 1.3908, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4282, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3833]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0233161449432373\n",
      "Batch 77, Loss: 3.0233161449432373\n",
      "Processing Batch 78/157\n",
      "Batch 78 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013039330951869488, Std: 0.08742659538984299\n",
      "z_j Mean: 0.013470862060785294, Std: 0.08736113458871841\n",
      "Similarities: tensor([[1.4116, 0.0000, 0.0000, 1.1016, 0.3089],\n",
      "        [0.0000, 1.4175, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4095, 0.0000, 0.0000],\n",
      "        [1.0030, 0.0000, 0.0000, 1.4051, 0.0000],\n",
      "        [1.1599, 0.0000, 0.0000, 0.8131, 0.7623]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.428342342376709]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.081622838973999\n",
      "Batch 78, Loss: 3.081622838973999\n",
      "Processing Batch 79/157\n",
      "Batch 79 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013255387544631958, Std: 0.08739408850669861\n",
      "z_j Mean: 0.012897412292659283, Std: 0.0874476432800293\n",
      "Similarities: tensor([[1.4004, 0.2374, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3552, 1.3666, 0.4178, 0.0188, 0.0000],\n",
      "        [0.0000, 0.4193, 1.4242, 0.0000, 0.0035],\n",
      "        [0.0000, 0.2826, 0.0000, 1.3058, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0460, 0.0000, 1.3520]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 2.998112440109253\n",
      "Batch 79, Loss: 2.998112440109253\n",
      "Processing Batch 80/157\n",
      "Batch 80 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012866925448179245, Std: 0.08745213598012924\n",
      "z_j Mean: 0.013242179527878761, Std: 0.08739610016345978\n",
      "Similarities: tensor([[1.3223, 0.0000, 0.4390, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3113, 0.0000, 0.0000, 0.6586],\n",
      "        [0.3908, 0.0000, 1.4147, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3482, 0.0000],\n",
      "        [0.0000, 0.7386, 0.0000, 0.0000, 1.3903]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4281476736068726]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.040720224380493\n",
      "Batch 80, Loss: 3.040720224380493\n",
      "Processing Batch 81/157\n",
      "Batch 81 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013576315715909004, Std: 0.08734480291604996\n",
      "z_j Mean: 0.014011632651090622, Std: 0.08727601915597916\n",
      "Similarities: tensor([[1.4128, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3894, 0.0000, 0.0340, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3925, 0.0169, 0.0000],\n",
      "        [0.0000, 0.1983, 0.0182, 1.4217, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0231, 0.0000, 0.7758]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.047224998474121\n",
      "Batch 81, Loss: 3.047224998474121\n",
      "Processing Batch 82/157\n",
      "Batch 82 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013583803549408913, Std: 0.08734364062547684\n",
      "z_j Mean: 0.01320456899702549, Std: 0.08740178495645523\n",
      "Similarities: tensor([[1.4019, 0.5476, 0.6529, 0.0000, 0.0000],\n",
      "        [0.4246, 1.3378, 0.2011, 0.0000, 0.0000],\n",
      "        [0.8141, 0.4816, 1.4000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4264, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4178]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4279568195343018]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.040228843688965\n",
      "Batch 82, Loss: 3.040228843688965\n",
      "Processing Batch 83/157\n",
      "Batch 83 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012923795729875565, Std: 0.0874437466263771\n",
      "z_j Mean: 0.01327198650687933, Std: 0.08739157021045685\n",
      "Similarities: tensor([[1.4013, 0.0000, 0.1633, 0.2824, 0.0000],\n",
      "        [0.0000, 1.3410, 0.0000, 0.0000, 0.3866],\n",
      "        [0.2771, 0.0000, 1.2375, 1.2243, 0.0000],\n",
      "        [0.3210, 0.0000, 0.8286, 1.4285, 0.0000],\n",
      "        [0.0000, 0.1909, 0.0000, 0.0000, 1.3531]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0210657119750977\n",
      "Batch 83, Loss: 3.0210657119750977\n",
      "Processing Batch 84/157\n",
      "Batch 84 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012861097231507301, Std: 0.08745299279689789\n",
      "z_j Mean: 0.012910343706607819, Std: 0.08674485981464386\n",
      "Similarities: tensor([[1.4163, 0.0000, 0.0524, 0.1918, 0.1275],\n",
      "        [0.0000, 1.2214, 0.0093, 0.0000, 0.0117],\n",
      "        [0.0000, 0.0732, 1.3439, 0.0000, 0.5139],\n",
      "        [0.0336, 0.0000, 0.0000, 1.4035, 0.1192],\n",
      "        [0.0667, 0.0000, 0.2960, 0.0535, 1.2606]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.078892469406128\n",
      "Batch 84, Loss: 3.078892469406128\n",
      "Processing Batch 85/157\n",
      "Batch 85 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013170409947633743, Std: 0.0867057591676712\n",
      "z_j Mean: 0.013402542099356651, Std: 0.0866701677441597\n",
      "Similarities: tensor([[1.4241e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.0789e-04],\n",
      "        [0.0000e+00, 1.4271e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 1.3392e+00, 5.3749e-01, 0.0000e+00],\n",
      "        [8.5054e-03, 0.0000e+00, 2.2832e-01, 1.3383e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.3596e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.085883855819702\n",
      "Batch 85, Loss: 3.085883855819702\n",
      "Processing Batch 86/157\n",
      "Batch 86 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012590350583195686, Std: 0.08749238401651382\n",
      "z_j Mean: 0.012762598693370819, Std: 0.08746742457151413\n",
      "Similarities: tensor([[1.4280, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4226, 0.1092, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1888, 1.3512, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3816, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3523]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.047203302383423\n",
      "Batch 86, Loss: 3.047203302383423\n",
      "Processing Batch 87/157\n",
      "Batch 87 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01270601898431778, Std: 0.08747565746307373\n",
      "z_j Mean: 0.012919695116579533, Std: 0.08744435757398605\n",
      "Similarities: tensor([[1.3022, 0.0000, 0.0000, 0.0260, 0.4112],\n",
      "        [0.1295, 1.1271, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4109, 0.0000, 0.0000],\n",
      "        [0.0454, 0.0000, 0.0000, 1.3965, 1.1664],\n",
      "        [0.2938, 0.0000, 0.0000, 0.6879, 1.3050]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4283256530761719]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0202744007110596\n",
      "Batch 87, Loss: 3.0202744007110596\n",
      "Processing Batch 88/157\n",
      "Batch 88 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01276382990181446, Std: 0.0874672457575798\n",
      "z_j Mean: 0.012388396076858044, Std: 0.08752121031284332\n",
      "Similarities: tensor([[1.4264, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4215, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.1871, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3950, 1.2205],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0361, 1.2952]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.03495192527771\n",
      "Batch 88, Loss: 3.03495192527771\n",
      "Processing Batch 89/157\n",
      "Batch 89 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012802636250853539, Std: 0.08746156841516495\n",
      "z_j Mean: 0.01310552004724741, Std: 0.08741669356822968\n",
      "Similarities: tensor([[1.3869, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2565, 0.9022, 0.0000, 0.2832, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4012, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.2146, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4151]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0289247035980225\n",
      "Batch 89, Loss: 3.0289247035980225\n",
      "Processing Batch 90/157\n",
      "Batch 90 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012189293280243874, Std: 0.08684913069009781\n",
      "z_j Mean: 0.01230571698397398, Std: 0.08683270961046219\n",
      "Similarities: tensor([[1.3937, 0.0000, 0.0000, 0.0602, 0.0000],\n",
      "        [0.0000, 1.3925, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3825, 0.0000, 1.1812],\n",
      "        [0.0078, 0.0000, 0.0000, 1.1069, 0.0000],\n",
      "        [0.0000, 0.0000, 1.0556, 0.0000, 1.4098]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.048708438873291\n",
      "Batch 90, Loss: 3.048708438873291\n",
      "Processing Batch 91/157\n",
      "Batch 91 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012702913023531437, Std: 0.08747611939907074\n",
      "z_j Mean: 0.012433507479727268, Std: 0.0868145003914833\n",
      "Similarities: tensor([[1.4286, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4084, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4284, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3646, 0.0000, 1.2383, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3658]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0344207286834717\n",
      "Batch 91, Loss: 3.0344207286834717\n",
      "Processing Batch 92/157\n",
      "Batch 92 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013167411088943481, Std: 0.08740738779306412\n",
      "z_j Mean: 0.013491070829331875, Std: 0.0873580127954483\n",
      "Similarities: tensor([[1.3714, 0.0000, 0.5895, 0.2622, 0.0000],\n",
      "        [0.0233, 1.4069, 0.0000, 0.8603, 0.0000],\n",
      "        [0.5973, 0.0000, 1.3722, 0.0997, 0.7094],\n",
      "        [0.4421, 0.7733, 0.0791, 1.1664, 0.0000],\n",
      "        [0.0043, 0.0000, 0.8471, 0.0000, 1.4221]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.066715955734253\n",
      "Batch 92, Loss: 3.066715955734253\n",
      "Processing Batch 93/157\n",
      "Batch 93 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012972848489880562, Std: 0.0874364823102951\n",
      "z_j Mean: 0.013172997161746025, Std: 0.08740654587745667\n",
      "Similarities: tensor([[1.3821, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4013, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.6374, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3738, 0.0000],\n",
      "        [0.0000, 0.4939, 0.0000, 0.0680, 1.3113]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0247278213500977\n",
      "Batch 93, Loss: 3.0247278213500977\n",
      "Processing Batch 94/157\n",
      "Batch 94 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012591593898832798, Std: 0.0874922052025795\n",
      "z_j Mean: 0.01262576598674059, Std: 0.08748728036880493\n",
      "Similarities: tensor([[1.3719, 0.0000, 0.4158, 0.2113, 0.0000],\n",
      "        [0.0000, 1.3796, 0.0000, 0.0000, 0.7390],\n",
      "        [0.5108, 0.0000, 1.4214, 0.0000, 0.0000],\n",
      "        [0.3129, 0.0000, 0.0000, 1.4060, 0.0000],\n",
      "        [0.0000, 0.8206, 0.0000, 0.0000, 1.2459]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0340447425842285\n",
      "Batch 94, Loss: 3.0340447425842285\n",
      "Processing Batch 95/157\n",
      "Batch 95 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01274050772190094, Std: 0.08606360107660294\n",
      "z_j Mean: 0.012925442308187485, Std: 0.08603601157665253\n",
      "Similarities: tensor([[1.2817, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3631, 0.0000, 0.0000, 0.8917],\n",
      "        [0.0128, 0.0000, 1.3951, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.2433, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0792768001556396\n",
      "Batch 95, Loss: 3.0792768001556396\n",
      "Processing Batch 96/157\n",
      "Batch 96 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012350831180810928, Std: 0.08682630211114883\n",
      "z_j Mean: 0.012062953785061836, Std: 0.08616118878126144\n",
      "Similarities: tensor([[1.3931, 0.0000, 0.0000, 0.0000, 0.0659],\n",
      "        [0.0000, 1.3672, 0.0709, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1264, 1.3913, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.1379, 0.0000],\n",
      "        [0.1342, 0.0000, 0.0000, 0.0000, 0.7965]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0896003246307373\n",
      "Batch 96, Loss: 3.0896003246307373\n",
      "Processing Batch 97/157\n",
      "Batch 97 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012576982378959656, Std: 0.08749430626630783\n",
      "z_j Mean: 0.012214895337820053, Std: 0.08754559606313705\n",
      "Similarities: tensor([[1.2440, 0.0000, 0.4797, 0.3854, 0.0000],\n",
      "        [0.0000, 1.3082, 0.0496, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4208, 0.0995, 0.0000],\n",
      "        [0.1108, 0.0000, 0.0380, 1.4270, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4045]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.037818670272827\n",
      "Batch 97, Loss: 3.037818670272827\n",
      "Processing Batch 98/157\n",
      "Batch 98 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012660283595323563, Std: 0.08748229593038559\n",
      "z_j Mean: 0.013094035908579826, Std: 0.08741840720176697\n",
      "Similarities: tensor([[1.3979, 0.0000, 1.0003, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4191, 0.0025, 0.0000, 0.0000],\n",
      "        [0.8284, 0.0000, 1.4214, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4286, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4286]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0583102703094482\n",
      "Batch 98, Loss: 3.0583102703094482\n",
      "Processing Batch 99/157\n",
      "Batch 99 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012848224490880966, Std: 0.08675408363342285\n",
      "z_j Mean: 0.012718211859464645, Std: 0.08747389167547226\n",
      "Similarities: tensor([[1.4151, 0.0043, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0028, 1.4010, 0.0000, 0.0015, 0.0000],\n",
      "        [0.0243, 0.0000, 1.3797, 0.0000, 0.1236],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4167, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.024486780166626\n",
      "Batch 99, Loss: 3.024486780166626\n",
      "Processing Batch 100/157\n",
      "Batch 100 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013044560328125954, Std: 0.08742581307888031\n",
      "z_j Mean: 0.013193833641707897, Std: 0.08740340918302536\n",
      "Similarities: tensor([[1.4285, 0.0000, 1.2711, 1.4038, 0.0072],\n",
      "        [0.0000, 1.2963, 0.0000, 0.0326, 0.9083],\n",
      "        [0.9247, 0.0000, 1.3149, 0.9251, 0.0047],\n",
      "        [1.4148, 0.0000, 1.2816, 1.4198, 0.0265],\n",
      "        [0.0000, 0.3274, 0.0000, 0.1350, 1.3408]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.066335916519165\n",
      "Batch 100, Loss: 3.066335916519165\n",
      "Processing Batch 101/157\n",
      "Batch 101 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012498131021857262, Std: 0.08750560879707336\n",
      "z_j Mean: 0.012409393675625324, Std: 0.08681795001029968\n",
      "Similarities: tensor([[1.4273, 0.0000, 0.0000, 0.0000, 0.3164],\n",
      "        [0.0000, 0.6074, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4215, 0.0000, 0.3313],\n",
      "        [0.0000, 0.0000, 0.0000, 1.2482, 0.0126],\n",
      "        [0.4056, 0.0000, 0.5240, 0.0078, 1.4047]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.054840087890625\n",
      "Batch 101, Loss: 3.054840087890625\n",
      "Processing Batch 102/157\n",
      "Batch 102 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01209373027086258, Std: 0.0868624895811081\n",
      "z_j Mean: 0.012451015412807465, Std: 0.08751232922077179\n",
      "Similarities: tensor([[1.2615, 0.0000, 0.4555, 0.9787, 0.0000],\n",
      "        [0.1717, 1.2261, 0.5477, 0.0601, 0.4766],\n",
      "        [0.8635, 0.0275, 1.1791, 1.0309, 0.0374],\n",
      "        [0.8840, 0.0000, 0.3510, 0.7678, 0.0000],\n",
      "        [0.0477, 0.2499, 0.2314, 0.0000, 1.2538]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0990147590637207\n",
      "Batch 102, Loss: 3.0990147590637207\n",
      "Processing Batch 103/157\n",
      "Batch 103 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012465808540582657, Std: 0.08751022070646286\n",
      "z_j Mean: 0.012288655154407024, Std: 0.08753527700901031\n",
      "Similarities: tensor([[1.3817, 0.0260, 0.0018, 0.4598, 0.0000],\n",
      "        [0.2490, 1.4218, 0.7220, 0.0000, 0.0000],\n",
      "        [0.0256, 0.8261, 1.2958, 0.0000, 0.0000],\n",
      "        [0.4287, 0.0138, 0.0000, 1.4209, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.8844]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0382370948791504\n",
      "Batch 103, Loss: 3.0382370948791504\n",
      "Processing Batch 104/157\n",
      "Batch 104 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012560208328068256, Std: 0.08749672025442123\n",
      "z_j Mean: 0.012645531445741653, Std: 0.08748442679643631\n",
      "Similarities: tensor([[1.2866, 0.4089, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2271, 1.4185, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3981, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4247, 0.9556],\n",
      "        [0.0000, 0.0000, 0.0000, 1.2977, 1.2687]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.008126974105835\n",
      "Batch 104, Loss: 3.008126974105835\n",
      "Processing Batch 105/157\n",
      "Batch 105 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012226707302033901, Std: 0.08754394948482513\n",
      "z_j Mean: 0.012565473094582558, Std: 0.08749596774578094\n",
      "Similarities: tensor([[1.3586, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4271, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.2955, 0.0000, 0.3027],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4286, 0.0000],\n",
      "        [0.0000, 0.0000, 0.6647, 0.0000, 1.4246]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0239672660827637\n",
      "Batch 105, Loss: 3.0239672660827637\n",
      "Processing Batch 106/157\n",
      "Batch 106 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012836983427405357, Std: 0.08675575256347656\n",
      "z_j Mean: 0.012869960628449917, Std: 0.08675086498260498\n",
      "Similarities: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3686, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.1394, 0.0783, 0.7231],\n",
      "        [0.0000, 0.0000, 0.0634, 1.3493, 0.0189],\n",
      "        [0.0000, 0.0000, 1.2092, 0.0268, 1.4144]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.051802635192871\n",
      "Batch 106, Loss: 3.051802635192871\n",
      "Processing Batch 107/157\n",
      "Batch 107 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012215322814881802, Std: 0.08754553645849228\n",
      "z_j Mean: 0.01215428113937378, Std: 0.08755403757095337\n",
      "Similarities: tensor([[1.4044, 0.0000, 0.0000, 0.3143, 0.0000],\n",
      "        [0.0000, 1.3939, 0.1250, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0563, 1.4258, 0.0000, 0.0000],\n",
      "        [0.4999, 0.0000, 0.0000, 1.4118, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.1138]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 2.998790979385376\n",
      "Batch 107, Loss: 2.998790979385376\n",
      "Processing Batch 108/157\n",
      "Batch 108 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012726382352411747, Std: 0.08677204698324203\n",
      "z_j Mean: 0.012436628341674805, Std: 0.08681405335664749\n",
      "Similarities: tensor([[1.2612, 0.0000, 1.1454, 0.5143, 0.6208],\n",
      "        [0.0000, 1.4105, 0.0000, 0.0990, 0.0000],\n",
      "        [1.1095, 0.0000, 1.3595, 0.1204, 0.5878],\n",
      "        [0.3277, 0.0647, 0.2381, 1.3895, 0.0035],\n",
      "        [0.6000, 0.0000, 0.2592, 0.0000, 1.3052]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0633816719055176\n",
      "Batch 108, Loss: 3.0633816719055176\n",
      "Processing Batch 109/157\n",
      "Batch 109 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012220891192555428, Std: 0.0875447615981102\n",
      "z_j Mean: 0.012074765749275684, Std: 0.08756504207849503\n",
      "Similarities: tensor([[1.3992, 0.0000, 0.0000, 1.1949, 0.0000],\n",
      "        [0.0000, 1.4100, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4010, 0.0000, 0.0000],\n",
      "        [1.1487, 0.0000, 0.0000, 1.4150, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4012]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0575342178344727\n",
      "Batch 109, Loss: 3.0575342178344727\n",
      "Processing Batch 110/157\n",
      "Batch 110 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012420914135873318, Std: 0.08751660585403442\n",
      "z_j Mean: 0.012293638661503792, Std: 0.0875345766544342\n",
      "Similarities: tensor([[1.4286, 0.0000, 0.0000, 1.1926, 0.0000],\n",
      "        [0.0000, 0.2730, 0.0000, 0.0000, 0.1008],\n",
      "        [0.0000, 0.0000, 1.4103, 0.0000, 0.0000],\n",
      "        [1.3232, 0.0121, 0.0000, 1.2707, 0.3327],\n",
      "        [0.0000, 0.9787, 0.0000, 0.5658, 1.0868]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0595991611480713\n",
      "Batch 110, Loss: 3.0595991611480713\n",
      "Processing Batch 111/157\n",
      "Batch 111 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013119583949446678, Std: 0.08671345561742783\n",
      "z_j Mean: 0.013529032468795776, Std: 0.08665050566196442\n",
      "Similarities: tensor([[1.3035, 0.6714, 0.0000, 0.4664, 0.0000],\n",
      "        [0.2531, 1.3003, 0.0597, 0.0352, 0.0000],\n",
      "        [0.0000, 0.1220, 1.3839, 0.1743, 0.1932],\n",
      "        [0.9616, 0.4088, 0.0829, 1.2831, 0.0606],\n",
      "        [0.0000, 0.0000, 0.1905, 0.1785, 1.4021]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4279342889785767]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.053331136703491\n",
      "Batch 111, Loss: 3.053331136703491\n",
      "Processing Batch 112/157\n",
      "Batch 112 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012354493141174316, Std: 0.08752600848674774\n",
      "z_j Mean: 0.012063274160027504, Std: 0.08686672151088715\n",
      "Similarities: tensor([[1.3937, 0.0000, 0.1031, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4193, 0.0000, 0.0000, 0.4472],\n",
      "        [0.4931, 0.0000, 1.1937, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3653, 0.0000],\n",
      "        [0.0000, 0.3771, 0.0000, 0.0000, 1.4218]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0343921184539795\n",
      "Batch 112, Loss: 3.0343921184539795\n",
      "Processing Batch 113/157\n",
      "Batch 113 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012498186901211739, Std: 0.08750560134649277\n",
      "z_j Mean: 0.01244159135967493, Std: 0.08751366287469864\n",
      "Similarities: tensor([[1.4255e+00, 0.0000e+00, 0.0000e+00, 1.3071e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 1.2608e+00, 1.7691e-04, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 1.4031e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [1.3408e+00, 0.0000e+00, 0.0000e+00, 1.3709e+00, 1.0536e-02],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 3.1294e-02, 1.4248e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.059122323989868\n",
      "Batch 113, Loss: 3.059122323989868\n",
      "Processing Batch 114/157\n",
      "Batch 114 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01294548250734806, Std: 0.08744054287672043\n",
      "z_j Mean: 0.012415716424584389, Std: 0.08751734346151352\n",
      "Similarities: tensor([[1.2741, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3363, 0.0000, 0.0000, 0.6395],\n",
      "        [0.0190, 0.1535, 1.2078, 0.0297, 0.0515],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4002, 0.0000],\n",
      "        [0.0025, 0.8173, 0.0000, 0.0000, 1.3476]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.018763542175293\n",
      "Batch 114, Loss: 3.018763542175293\n",
      "Processing Batch 115/157\n",
      "Batch 115 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012459656223654747, Std: 0.0875110924243927\n",
      "z_j Mean: 0.011990202590823174, Std: 0.08757665753364563\n",
      "Similarities: tensor([[1.4280, 0.0000, 0.2528, 0.0000, 0.1641],\n",
      "        [0.0000, 1.3072, 0.0000, 0.0523, 0.0000],\n",
      "        [0.4511, 0.0000, 1.3195, 0.0000, 0.1224],\n",
      "        [0.0000, 0.2859, 0.0000, 1.2567, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0460, 0.0000, 1.3677]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0524399280548096\n",
      "Batch 115, Loss: 3.0524399280548096\n",
      "Processing Batch 116/157\n",
      "Batch 116 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013522149994969368, Std: 0.08665158599615097\n",
      "z_j Mean: 0.013392770662903786, Std: 0.08667168021202087\n",
      "Similarities: tensor([[1.4180, 0.0000, 0.0261, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4017, 0.0000, 0.0433, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4268, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0733, 0.5238],\n",
      "        [0.0000, 0.1165, 0.0000, 0.5055, 1.4093]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0738723278045654\n",
      "Batch 116, Loss: 3.0738723278045654\n",
      "Processing Batch 117/157\n",
      "Batch 117 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011800602078437805, Std: 0.0876024141907692\n",
      "z_j Mean: 0.011624107137322426, Std: 0.08692658692598343\n",
      "Similarities: tensor([[1.4286, 0.0000, 0.0000, 0.0000, 0.8820],\n",
      "        [0.0000, 1.3778, 0.8026, 0.0000, 0.0000],\n",
      "        [0.0000, 0.5244, 1.2460, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4286, 0.0000],\n",
      "        [0.7182, 0.0000, 0.0000, 0.0000, 1.3309]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0334651470184326\n",
      "Batch 117, Loss: 3.0334651470184326\n",
      "Processing Batch 118/157\n",
      "Batch 118 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012326156720519066, Std: 0.08753000199794769\n",
      "z_j Mean: 0.01269950158894062, Std: 0.08747661113739014\n",
      "Similarities: tensor([[1.3450e+00, 1.0912e-03, 2.8258e-01, 5.1123e-01, 8.9653e-04],\n",
      "        [1.1490e-03, 1.3862e+00, 0.0000e+00, 7.8907e-03, 1.5116e-02],\n",
      "        [2.7060e-01, 0.0000e+00, 1.2125e+00, 2.6131e-02, 0.0000e+00],\n",
      "        [3.3181e-01, 7.0508e-03, 1.8673e-01, 1.3958e+00, 5.7931e-03],\n",
      "        [3.5257e-04, 5.6453e-03, 0.0000e+00, 3.6236e-03, 1.4003e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.003279447555542\n",
      "Batch 118, Loss: 3.003279447555542\n",
      "Processing Batch 119/157\n",
      "Batch 119 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012412680312991142, Std: 0.08751777559518814\n",
      "z_j Mean: 0.011973843909800053, Std: 0.0875789001584053\n",
      "Similarities: tensor([[1.2806, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3022, 0.0000, 1.1001, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4089, 0.0000, 0.2637],\n",
      "        [0.0000, 0.8757, 0.0000, 1.3368, 0.0000],\n",
      "        [0.0000, 0.0000, 0.1273, 0.0000, 1.4221]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0215017795562744\n",
      "Batch 119, Loss: 3.0215017795562744\n",
      "Processing Batch 120/157\n",
      "Batch 120 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013088931329548359, Std: 0.08741917461156845\n",
      "z_j Mean: 0.0132271908223629, Std: 0.08739836513996124\n",
      "Similarities: tensor([[1.4080e+00, 1.2681e-01, 9.2222e-02, 0.0000e+00, 0.0000e+00],\n",
      "        [6.0040e-03, 1.3569e+00, 0.0000e+00, 1.0448e+00, 1.4287e-01],\n",
      "        [1.3923e-03, 1.6395e-01, 1.3898e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 4.7865e-01, 0.0000e+00, 7.6952e-01, 1.2887e+00],\n",
      "        [0.0000e+00, 7.4591e-02, 0.0000e+00, 1.1992e-01, 1.4253e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.078014373779297\n",
      "Batch 120, Loss: 3.078014373779297\n",
      "Processing Batch 121/157\n",
      "Batch 121 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013455032370984554, Std: 0.0873635783791542\n",
      "z_j Mean: 0.01343991607427597, Std: 0.08736589550971985\n",
      "Similarities: tensor([[1.4172e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 1.2261e+00, 4.8380e-01, 1.8669e-01, 6.7372e-01],\n",
      "        [0.0000e+00, 3.0995e-01, 1.4102e+00, 8.3600e-01, 4.1314e-04],\n",
      "        [0.0000e+00, 6.7815e-02, 9.8893e-01, 1.3462e+00, 0.0000e+00],\n",
      "        [1.4697e-01, 5.9954e-01, 0.0000e+00, 0.0000e+00, 1.2890e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0532569885253906\n",
      "Batch 121, Loss: 3.0532569885253906\n",
      "Processing Batch 122/157\n",
      "Batch 122 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01233319379389286, Std: 0.0868288055062294\n",
      "z_j Mean: 0.012227682396769524, Std: 0.08684373646974564\n",
      "Similarities: tensor([[1.4245, 0.0000, 0.0000, 0.8894, 0.1300],\n",
      "        [0.0000, 1.4209, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3897, 0.0000, 0.0000],\n",
      "        [1.0185, 0.0000, 0.0000, 1.4008, 0.0930],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4226]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.015899896621704\n",
      "Batch 122, Loss: 3.015899896621704\n",
      "Processing Batch 123/157\n",
      "Batch 123 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011998110450804234, Std: 0.08757557719945908\n",
      "z_j Mean: 0.01198914647102356, Std: 0.08757680654525757\n",
      "Similarities: tensor([[1.3485, 0.0054, 1.2947, 0.0000, 0.0000],\n",
      "        [0.0443, 1.2143, 0.0000, 0.0098, 0.0000],\n",
      "        [1.3631, 0.0000, 1.3983, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0630, 0.0000, 1.4078, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4273]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0151278972625732\n",
      "Batch 123, Loss: 3.0151278972625732\n",
      "Processing Batch 124/157\n",
      "Batch 124 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012761928141117096, Std: 0.08746752142906189\n",
      "z_j Mean: 0.012741362676024437, Std: 0.08676984906196594\n",
      "Similarities: tensor([[1.3571, 0.0000, 0.0000, 0.8113, 0.0497],\n",
      "        [0.0000, 1.4133, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4054, 0.0000, 0.0000],\n",
      "        [1.1462, 0.0000, 0.0000, 1.3229, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4279]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.428496241569519]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0974502563476562\n",
      "Batch 124, Loss: 3.0974502563476562\n",
      "Processing Batch 125/157\n",
      "Batch 125 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01355714350938797, Std: 0.08734778314828873\n",
      "z_j Mean: 0.013407121412456036, Std: 0.08737093955278397\n",
      "Similarities: tensor([[1.3351, 0.6303, 0.0000, 0.0000, 0.0000],\n",
      "        [0.6012, 1.3958, 0.0528, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3680, 1.3279, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3448, 0.0000],\n",
      "        [0.0017, 0.0106, 0.0123, 0.0000, 1.3377]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0254454612731934\n",
      "Batch 125, Loss: 3.0254454612731934\n",
      "Processing Batch 126/157\n",
      "Batch 126 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012266137637197971, Std: 0.08613249659538269\n",
      "z_j Mean: 0.012708482332527637, Std: 0.08747530728578568\n",
      "Similarities: tensor([[1.4208, 0.0000, 0.0946, 0.1249, 0.0000],\n",
      "        [0.0000, 1.3867, 0.0000, 0.0588, 0.6704],\n",
      "        [0.0778, 0.0000, 1.2794, 0.2637, 0.0000],\n",
      "        [0.1031, 0.0000, 0.1389, 1.3796, 0.0000],\n",
      "        [0.0000, 0.8015, 0.0000, 0.0039, 1.4219]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0498316287994385\n",
      "Batch 126, Loss: 3.0498316287994385\n",
      "Processing Batch 127/157\n",
      "Batch 127 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012758186087012291, Std: 0.08746807277202606\n",
      "z_j Mean: 0.01282308716326952, Std: 0.08745857328176498\n",
      "Similarities: tensor([[1.2619, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.5719, 0.0000, 0.2028, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4141, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4146, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3017]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0546505451202393\n",
      "Batch 127, Loss: 3.0546505451202393\n",
      "Processing Batch 128/157\n",
      "Batch 128 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012456247583031654, Std: 0.0875115841627121\n",
      "z_j Mean: 0.012623459100723267, Std: 0.0874876156449318\n",
      "Similarities: tensor([[1.3467, 0.3752, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5519, 1.3954, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3519, 0.1412, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0198, 1.3357, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4286]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0180160999298096\n",
      "Batch 128, Loss: 3.0180160999298096\n",
      "Processing Batch 129/157\n",
      "Batch 129 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01197359524667263, Std: 0.0861736536026001\n",
      "z_j Mean: 0.012029214762151241, Std: 0.0861659049987793\n",
      "Similarities: tensor([[0.8575, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3494, 0.1135, 1.0775, 0.0000],\n",
      "        [0.0000, 0.1121, 1.3481, 0.0000, 0.0000],\n",
      "        [0.0000, 1.2294, 0.0000, 1.4286, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.131877899169922\n",
      "Batch 129, Loss: 3.131877899169922\n",
      "Processing Batch 130/157\n",
      "Batch 130 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01303170621395111, Std: 0.08742772787809372\n",
      "z_j Mean: 0.012960463762283325, Std: 0.08743831515312195\n",
      "Similarities: tensor([[1.4232e+00, 8.8733e-04, 1.7214e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 1.2306e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [2.5628e-01, 2.5252e-04, 1.4185e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 1.3950e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.3640e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0496017932891846\n",
      "Batch 130, Loss: 3.0496017932891846\n",
      "Processing Batch 131/157\n",
      "Batch 131 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012774724513292313, Std: 0.08746565133333206\n",
      "z_j Mean: 0.012851635925471783, Std: 0.08745438605546951\n",
      "Similarities: tensor([[1.4243, 0.0000, 0.0905, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4127, 0.0000, 0.3154, 1.2062],\n",
      "        [0.0000, 0.0000, 1.4191, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1549, 0.0000, 1.3691, 0.0000],\n",
      "        [0.0000, 1.2780, 0.0000, 0.0906, 1.3759]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.084695339202881\n",
      "Batch 131, Loss: 3.084695339202881\n",
      "Processing Batch 132/157\n",
      "Batch 132 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01214507780969143, Std: 0.08685532957315445\n",
      "z_j Mean: 0.012106320820748806, Std: 0.08615510165691376\n",
      "Similarities: tensor([[1.4275, 0.0000, 0.0000, 0.0000, 0.0130],\n",
      "        [0.0000, 1.4286, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.2206, 0.9619, 0.0000],\n",
      "        [0.0000, 0.0000, 0.3740, 1.3764, 0.0000],\n",
      "        [0.1228, 0.0000, 0.0000, 0.0000, 0.9907]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1123483180999756\n",
      "Batch 132, Loss: 3.1123483180999756\n",
      "Processing Batch 133/157\n",
      "Batch 133 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012665239162743092, Std: 0.08748157322406769\n",
      "z_j Mean: 0.012709529139101505, Std: 0.08747515082359314\n",
      "Similarities: tensor([[1.4197, 0.0000, 0.0000, 0.8217, 0.0000],\n",
      "        [0.0000, 0.8083, 0.6274, 0.0000, 1.3783],\n",
      "        [0.0000, 0.2147, 1.3799, 0.0000, 0.3947],\n",
      "        [0.8653, 0.0000, 0.0000, 1.4157, 0.0000],\n",
      "        [0.0000, 0.4843, 0.3871, 0.3107, 0.9137]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0588204860687256\n",
      "Batch 133, Loss: 3.0588204860687256\n",
      "Processing Batch 134/157\n",
      "Batch 134 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012868324294686317, Std: 0.08745193481445312\n",
      "z_j Mean: 0.013237479142844677, Std: 0.08739680051803589\n",
      "Similarities: tensor([[1.2349, 1.1096, 0.0000, 0.7720, 0.0000],\n",
      "        [0.8744, 1.4281, 0.0000, 0.6321, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3429, 0.0000, 0.0000],\n",
      "        [0.5148, 0.5348, 0.0000, 1.3995, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4265]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0686705112457275\n",
      "Batch 134, Loss: 3.0686705112457275\n",
      "Processing Batch 135/157\n",
      "Batch 135 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013152290135622025, Std: 0.08670850843191147\n",
      "z_j Mean: 0.012899567373096943, Std: 0.0867464691400528\n",
      "Similarities: tensor([[1.4071, 0.0000, 0.6459, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4147, 0.0000, 0.0000, 0.0000],\n",
      "        [1.0260, 0.0000, 1.3469, 0.1773, 0.0580],\n",
      "        [0.6268, 0.0000, 0.9725, 0.8306, 0.0599],\n",
      "        [0.0000, 0.0000, 0.0913, 0.0024, 1.4077]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4279546737670898]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0401816368103027\n",
      "Batch 135, Loss: 3.0401816368103027\n",
      "Processing Batch 136/157\n",
      "Batch 136 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012097637169063091, Std: 0.08756188303232193\n",
      "z_j Mean: 0.01213059201836586, Std: 0.08755732327699661\n",
      "Similarities: tensor([[1.4043, 0.0000, 0.0000, 0.0000, 0.3301],\n",
      "        [0.0000, 1.4286, 0.0000, 0.0000, 0.6917],\n",
      "        [0.0000, 0.0000, 1.1348, 0.0000, 0.0418],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4128, 0.0000],\n",
      "        [0.1541, 0.6996, 0.4244, 0.0000, 1.2783]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0325865745544434\n",
      "Batch 136, Loss: 3.0325865745544434\n",
      "Processing Batch 137/157\n",
      "Batch 137 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012477790005505085, Std: 0.08750850707292557\n",
      "z_j Mean: 0.01216457411646843, Std: 0.08614690601825714\n",
      "Similarities: tensor([[1.1187, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4233, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3731, 1.2743, 0.0000],\n",
      "        [0.0000, 0.0000, 1.2123, 1.4087, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.1095]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0523681640625\n",
      "Batch 137, Loss: 3.0523681640625\n",
      "Processing Batch 138/157\n",
      "Batch 138 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013536161743104458, Std: 0.08664939552545547\n",
      "z_j Mean: 0.01367691345512867, Std: 0.08732911199331284\n",
      "Similarities: tensor([[1.3868, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0562, 1.4145, 0.0000, 0.6680, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3935, 0.0000, 0.0000],\n",
      "        [0.0000, 0.5793, 0.0000, 1.3977, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.1786]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0660202503204346\n",
      "Batch 138, Loss: 3.0660202503204346\n",
      "Processing Batch 139/157\n",
      "Batch 139 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013523600995540619, Std: 0.08735298365354538\n",
      "z_j Mean: 0.013368387706577778, Std: 0.08737687021493912\n",
      "Similarities: tensor([[1.2385, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3870, 0.8374, 0.0000, 0.9134],\n",
      "        [0.0000, 1.0848, 1.3451, 0.0000, 1.2879],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3892, 0.0000],\n",
      "        [0.0000, 1.3578, 1.1331, 0.0000, 1.1785]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0704877376556396\n",
      "Batch 139, Loss: 3.0704877376556396\n",
      "Processing Batch 140/157\n",
      "Batch 140 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012383424676954746, Std: 0.08752191811800003\n",
      "z_j Mean: 0.012674021534621716, Std: 0.08748029917478561\n",
      "Similarities: tensor([[1.4144, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3633, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4286, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4160, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.0879]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0863089561462402\n",
      "Batch 140, Loss: 3.0863089561462402\n",
      "Processing Batch 141/157\n",
      "Batch 141 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01255930494517088, Std: 0.08749684691429138\n",
      "z_j Mean: 0.0126044861972332, Std: 0.08678983896970749\n",
      "Similarities: tensor([[1.3424, 0.0000, 0.0000, 0.0000, 0.0059],\n",
      "        [0.0000, 1.4073, 0.0000, 0.0000, 0.3318],\n",
      "        [0.0000, 0.0000, 1.4045, 0.0895, 0.0000],\n",
      "        [0.0000, 0.0000, 0.8627, 1.0176, 0.0000],\n",
      "        [0.0000, 0.6753, 0.0000, 0.0000, 1.3864]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.092881679534912\n",
      "Batch 141, Loss: 3.092881679534912\n",
      "Processing Batch 142/157\n",
      "Batch 142 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013080107048153877, Std: 0.0874205008149147\n",
      "z_j Mean: 0.013146830722689629, Std: 0.08741049468517303\n",
      "Similarities: tensor([[1.3869, 0.0000, 0.0000, 0.1842, 0.6922],\n",
      "        [0.0000, 1.3935, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0133, 1.1732, 0.0000, 0.0000],\n",
      "        [0.1772, 0.0000, 0.0000, 0.9939, 0.3053],\n",
      "        [0.6893, 0.0000, 0.0000, 0.3594, 1.4173]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0462167263031006\n",
      "Batch 142, Loss: 3.0462167263031006\n",
      "Processing Batch 143/157\n",
      "Batch 143 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013118396513164043, Std: 0.08741476386785507\n",
      "z_j Mean: 0.012672912329435349, Std: 0.08748047053813934\n",
      "Similarities: tensor([[0.8915, 0.0000, 0.4229, 0.2446, 0.0000],\n",
      "        [0.0000, 1.4167, 0.0000, 0.0000, 0.0000],\n",
      "        [0.6460, 0.0000, 1.2013, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.2558, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3702]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0129899978637695\n",
      "Batch 143, Loss: 3.0129899978637695\n",
      "Processing Batch 144/157\n",
      "Batch 144 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012349261902272701, Std: 0.08752674609422684\n",
      "z_j Mean: 0.012192506343126297, Std: 0.0868486762046814\n",
      "Similarities: tensor([[1.4200, 0.3468, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2566, 1.4286, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3769, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3715, 0.4920],\n",
      "        [0.0000, 0.0000, 0.0000, 1.2717, 1.2139]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0456159114837646\n",
      "Batch 144, Loss: 3.0456159114837646\n",
      "Processing Batch 145/157\n",
      "Batch 145 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013322977349162102, Std: 0.08738381415605545\n",
      "z_j Mean: 0.01347943115979433, Std: 0.08735980838537216\n",
      "Similarities: tensor([[1.2592e+00, 3.9456e-03, 0.0000e+00, 9.1917e-01, 3.2831e-01],\n",
      "        [2.7289e-03, 1.3534e+00, 5.1678e-01, 1.1634e-02, 7.1559e-02],\n",
      "        [0.0000e+00, 5.9783e-01, 1.3811e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [1.1282e+00, 1.7012e-02, 6.0124e-04, 1.3715e+00, 5.5803e-02],\n",
      "        [4.6686e-02, 6.9006e-02, 0.0000e+00, 3.4652e-02, 1.3942e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0561816692352295\n",
      "Batch 145, Loss: 3.0561816692352295\n",
      "Processing Batch 146/157\n",
      "Batch 146 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012179278768599033, Std: 0.08614481985569\n",
      "z_j Mean: 0.011963204480707645, Std: 0.08688056468963623\n",
      "Similarities: tensor([[1.2402, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3789, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4214, 0.8261, 0.0000],\n",
      "        [0.0000, 0.0000, 0.8785, 1.4138, 0.0000],\n",
      "        [0.0000, 0.0200, 0.0000, 0.0000, 1.3305]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1083152294158936\n",
      "Batch 146, Loss: 3.1083152294158936\n",
      "Processing Batch 147/157\n",
      "Batch 147 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012084347195923328, Std: 0.08756371587514877\n",
      "z_j Mean: 0.01213999092578888, Std: 0.08755601942539215\n",
      "Similarities: tensor([[1.4225, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4279, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3872, 0.0041, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0104, 1.3373, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4286]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0637879371643066\n",
      "Batch 147, Loss: 3.0637879371643066\n",
      "Processing Batch 148/157\n",
      "Batch 148 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012936385348439217, Std: 0.08744188398122787\n",
      "z_j Mean: 0.013107388280332088, Std: 0.08741641044616699\n",
      "Similarities: tensor([[1.3845, 0.0017, 0.0183, 1.0908, 1.0383],\n",
      "        [0.0000, 1.3916, 0.7579, 0.0000, 0.0000],\n",
      "        [0.0208, 0.9076, 1.4165, 0.0000, 0.0000],\n",
      "        [1.1448, 0.0000, 0.0000, 1.3752, 1.2211],\n",
      "        [0.8413, 0.0000, 0.0000, 1.3247, 1.4158]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0399279594421387\n",
      "Batch 148, Loss: 3.0399279594421387\n",
      "Processing Batch 149/157\n",
      "Batch 149 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01298241876065731, Std: 0.08743505924940109\n",
      "z_j Mean: 0.012376097030937672, Std: 0.08682270348072052\n",
      "Similarities: tensor([[1.4131, 1.0783, 0.0000, 0.3290, 0.1114],\n",
      "        [0.8557, 1.2567, 0.0000, 0.6095, 1.0103],\n",
      "        [0.0000, 0.0000, 1.4182, 0.0000, 0.0000],\n",
      "        [0.4547, 0.1528, 0.0000, 1.3751, 0.5345],\n",
      "        [0.2148, 0.4085, 0.0000, 0.8617, 1.4286]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.080156087875366\n",
      "Batch 149, Loss: 3.080156087875366\n",
      "Processing Batch 150/157\n",
      "Batch 150 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01243022084236145, Std: 0.08751528710126877\n",
      "z_j Mean: 0.012559238821268082, Std: 0.08749685436487198\n",
      "Similarities: tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 1.4052e+00, 0.0000e+00, 1.0235e-02, 3.3460e-01],\n",
      "        [4.3950e-02, 1.1214e-03, 1.3643e+00, 2.3883e-02, 2.1091e-01],\n",
      "        [0.0000e+00, 1.8195e-03, 4.9074e-02, 1.3170e+00, 1.3869e-02],\n",
      "        [1.4075e-02, 6.4219e-02, 4.8601e-01, 1.3707e-02, 1.1112e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.005916118621826\n",
      "Batch 150, Loss: 3.005916118621826\n",
      "Processing Batch 151/157\n",
      "Batch 151 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012759101577103138, Std: 0.08746793866157532\n",
      "z_j Mean: 0.012393496930599213, Std: 0.08682022988796234\n",
      "Similarities: tensor([[1.3253e+00, 2.4069e-02, 0.0000e+00, 0.0000e+00, 2.6838e-01],\n",
      "        [0.0000e+00, 1.4081e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 1.4142e+00, 3.2557e-04, 0.0000e+00],\n",
      "        [0.0000e+00, 2.6482e-02, 1.8423e-04, 1.4229e+00, 0.0000e+00],\n",
      "        [7.8368e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.8849e-01]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.044931173324585\n",
      "Batch 151, Loss: 3.044931173324585\n",
      "Processing Batch 152/157\n",
      "Batch 152 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013108638115227222, Std: 0.08671511709690094\n",
      "z_j Mean: 0.013448864221572876, Std: 0.08736451715230942\n",
      "Similarities: tensor([[1.3835, 0.0000, 0.0565, 0.0000, 0.0000],\n",
      "        [0.0000, 1.0278, 0.1840, 0.0000, 0.0000],\n",
      "        [0.0029, 0.7371, 1.4139, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4284, 1.4284],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4286, 1.4286]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0435166358947754\n",
      "Batch 152, Loss: 3.0435166358947754\n",
      "Processing Batch 153/157\n",
      "Batch 153 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012916835956275463, Std: 0.08744478225708008\n",
      "z_j Mean: 0.012869593687355518, Std: 0.0874517410993576\n",
      "Similarities: tensor([[1.4234, 0.0000, 0.0000, 0.0254, 0.0000],\n",
      "        [0.0000, 1.4276, 0.1002, 0.0000, 0.0895],\n",
      "        [0.0000, 0.1171, 1.4286, 0.0000, 1.2759],\n",
      "        [0.0119, 0.0000, 0.0000, 0.9481, 0.0000],\n",
      "        [0.0000, 0.1171, 1.4286, 0.0000, 1.2759]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.059614658355713\n",
      "Batch 153, Loss: 3.059614658355713\n",
      "Processing Batch 154/157\n",
      "Batch 154 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012712262570858002, Std: 0.0874747559428215\n",
      "z_j Mean: 0.012857794761657715, Std: 0.08745347708463669\n",
      "Similarities: tensor([[1.3981, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4162, 0.0000, 0.0000, 0.7973],\n",
      "        [0.0000, 0.0000, 1.3756, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0068, 1.4187, 0.0000],\n",
      "        [0.0000, 0.7922, 0.0000, 0.0000, 1.4201]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285235404968262]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.032109022140503\n",
      "Batch 154, Loss: 3.032109022140503\n",
      "Processing Batch 155/157\n",
      "Batch 155 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012946510687470436, Std: 0.0874403864145279\n",
      "z_j Mean: 0.01264649722725153, Std: 0.08748429268598557\n",
      "Similarities: tensor([[0.6434, 0.0000, 0.2333, 0.0438, 0.0000],\n",
      "        [0.0000, 1.4075, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1139, 0.1050, 1.3647, 0.0000, 0.0000],\n",
      "        [0.0533, 0.0000, 0.0000, 1.3469, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3629]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0310347080230713\n",
      "Batch 155, Loss: 3.0310347080230713\n",
      "Processing Batch 156/157\n",
      "Batch 156 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012477420270442963, Std: 0.08750856667757034\n",
      "z_j Mean: 0.012743780389428139, Std: 0.0874701663851738\n",
      "Similarities: tensor([[1.3622, 0.0000, 0.0000, 0.1428, 0.0000],\n",
      "        [0.0000, 1.4097, 0.0159, 0.0000, 0.0198],\n",
      "        [0.0000, 0.0214, 1.2563, 0.0000, 0.8107],\n",
      "        [0.0362, 0.0000, 0.0000, 1.3718, 0.0000],\n",
      "        [0.0000, 0.0272, 1.0319, 0.0000, 1.1794]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1040217876434326\n",
      "Batch 156, Loss: 3.1040217876434326\n",
      "Processing Batch 157/157\n",
      "Batch 157 loaded: x.shape=torch.Size([16, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([16, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([16, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([16, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([16, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([16, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013946561142802238, Std: 0.08730243891477585\n",
      "z_j Mean: 0.013713400810956955, Std: 0.0873393788933754\n",
      "Similarities: tensor([[1.4177, 0.0614, 0.0000, 0.0726, 0.0000],\n",
      "        [0.1096, 1.0845, 0.0000, 1.2099, 0.4427],\n",
      "        [0.0000, 0.0000, 1.3420, 0.0000, 0.0475],\n",
      "        [0.0342, 0.9669, 0.0526, 1.2258, 0.5410],\n",
      "        [0.0000, 0.4042, 0.0832, 0.3304, 1.4178]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4272123575210571]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 1.724151372909546\n",
      "Batch 157, Loss: 1.724151372909546\n",
      "Epoch 9/10, Loss: 3.0433\n",
      "Entered Epoch 10\n",
      "Starting Training Loop with 157 batches.\n",
      "Processing Batch 1/157\n",
      "Batch 1 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013026531785726547, Std: 0.0874285027384758\n",
      "z_j Mean: 0.012658954598009586, Std: 0.08748248219490051\n",
      "Similarities: tensor([[1.4286, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3703, 0.2906, 0.0327, 0.0000],\n",
      "        [0.0000, 0.1911, 1.3740, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0136, 0.0000, 1.4206, 0.0753],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0118, 1.3628]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.011070489883423\n",
      "Batch 1, Loss: 3.011070489883423\n",
      "Processing Batch 2/157\n",
      "Batch 2 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01397806778550148, Std: 0.08728139847517014\n",
      "z_j Mean: 0.013545848429203033, Std: 0.08594051003456116\n",
      "Similarities: tensor([[0.0000, 0.0000, 0.7604, 0.0000, 0.0298],\n",
      "        [0.0000, 1.4262, 0.0000, 0.0570, 0.0000],\n",
      "        [0.0000, 0.0000, 1.0365, 0.0000, 0.0622],\n",
      "        [0.0000, 0.0043, 0.0000, 0.4237, 0.1399],\n",
      "        [0.0000, 0.0000, 0.0000, 0.3241, 1.3791]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1005983352661133\n",
      "Batch 2, Loss: 3.1005983352661133\n",
      "Processing Batch 3/157\n",
      "Batch 3 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013700387440621853, Std: 0.08732542395591736\n",
      "z_j Mean: 0.013267852365970612, Std: 0.08598387241363525\n",
      "Similarities: tensor([[1.3922, 0.3514, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4069, 1.3911, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.0244],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4236, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3807]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0883169174194336\n",
      "Batch 3, Loss: 3.0883169174194336\n",
      "Processing Batch 4/157\n",
      "Batch 4 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013504723086953163, Std: 0.08735590428113937\n",
      "z_j Mean: 0.013491714373230934, Std: 0.08735791593790054\n",
      "Similarities: tensor([[1.3082e+00, 0.0000e+00, 1.0202e-03, 6.5044e-01, 9.7385e-01],\n",
      "        [0.0000e+00, 1.4224e+00, 5.2519e-02, 0.0000e+00, 0.0000e+00],\n",
      "        [1.8310e-01, 1.2315e-01, 1.3688e+00, 4.7024e-02, 3.0428e-02],\n",
      "        [9.9926e-01, 0.0000e+00, 3.4561e-02, 1.3519e+00, 1.0584e+00],\n",
      "        [7.0041e-01, 0.0000e+00, 1.0010e-02, 8.0625e-01, 1.3286e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.054871082305908\n",
      "Batch 4, Loss: 3.054871082305908\n",
      "Processing Batch 5/157\n",
      "Batch 5 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013066942803561687, Std: 0.08742246776819229\n",
      "z_j Mean: 0.013438320718705654, Std: 0.08736614137887955\n",
      "Similarities: tensor([[1.2477, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4116, 0.0000, 0.0000, 0.9816],\n",
      "        [0.0000, 0.0000, 0.1742, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3467, 0.0000],\n",
      "        [0.0000, 0.9583, 0.5070, 0.0000, 1.4053]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0488717555999756\n",
      "Batch 5, Loss: 3.0488717555999756\n",
      "Processing Batch 6/157\n",
      "Batch 6 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013166507706046104, Std: 0.08740752935409546\n",
      "z_j Mean: 0.012976374477148056, Std: 0.08743596076965332\n",
      "Similarities: tensor([[1.3542, 0.7101, 0.6889, 0.0000, 0.0000],\n",
      "        [0.9308, 1.3541, 1.2097, 0.0000, 0.0000],\n",
      "        [0.5382, 1.2452, 1.4104, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4241, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.2438]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0382802486419678\n",
      "Batch 6, Loss: 3.0382802486419678\n",
      "Processing Batch 7/157\n",
      "Batch 7 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01289983931928873, Std: 0.08744728565216064\n",
      "z_j Mean: 0.013031447306275368, Std: 0.0874277725815773\n",
      "Similarities: tensor([[1.4209, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3434, 0.0355, 0.1690, 0.0000],\n",
      "        [0.0036, 0.0000, 1.4257, 1.1558, 0.0495],\n",
      "        [0.0000, 0.0571, 1.2166, 1.3902, 0.0057],\n",
      "        [0.0000, 0.0000, 0.0649, 0.0000, 1.4268]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0365676879882812\n",
      "Batch 7, Loss: 3.0365676879882812\n",
      "Processing Batch 8/157\n",
      "Batch 8 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013459673151373863, Std: 0.0873628631234169\n",
      "z_j Mean: 0.01406877487897873, Std: 0.08726682513952255\n",
      "Similarities: tensor([[1.3955, 0.0000, 0.0000, 0.7430, 0.0000],\n",
      "        [0.0000, 1.3960, 0.0000, 0.0000, 0.3656],\n",
      "        [0.0000, 0.0000, 1.4251, 0.0000, 0.0000],\n",
      "        [1.1577, 0.0000, 0.0000, 1.3151, 0.0000],\n",
      "        [0.0000, 0.3346, 0.0000, 0.0000, 1.3309]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4284954071044922]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0598833560943604\n",
      "Batch 8, Loss: 3.0598833560943604\n",
      "Processing Batch 9/157\n",
      "Batch 9 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01323282066732645, Std: 0.0873975083231926\n",
      "z_j Mean: 0.013413611799478531, Std: 0.08736994117498398\n",
      "Similarities: tensor([[1.3947, 0.0000, 0.5197, 0.1033, 0.0000],\n",
      "        [0.0000, 1.1676, 0.0000, 0.0053, 0.0073],\n",
      "        [0.5662, 0.0000, 1.3973, 0.0000, 0.0000],\n",
      "        [0.1703, 0.0000, 0.0000, 1.4183, 0.0000],\n",
      "        [0.0000, 0.0984, 0.0000, 0.0000, 1.4015]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0361106395721436\n",
      "Batch 9, Loss: 3.0361106395721436\n",
      "Processing Batch 10/157\n",
      "Batch 10 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013510355725884438, Std: 0.08735502511262894\n",
      "z_j Mean: 0.013123610988259315, Std: 0.0874139741063118\n",
      "Similarities: tensor([[1.3692, 0.0000, 0.0000, 1.2833, 0.2386],\n",
      "        [0.0000, 1.1115, 0.0000, 0.0000, 0.3041],\n",
      "        [0.0000, 0.0000, 1.4238, 0.0000, 0.2615],\n",
      "        [1.1934, 0.0423, 0.0000, 1.3908, 0.2515],\n",
      "        [0.1051, 0.1749, 0.0000, 0.1001, 1.3956]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4269026517868042]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0336172580718994\n",
      "Batch 10, Loss: 3.0336172580718994\n",
      "Processing Batch 11/157\n",
      "Batch 11 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.0129783283919096, Std: 0.08743567019701004\n",
      "z_j Mean: 0.013214239850640297, Std: 0.08740031719207764\n",
      "Similarities: tensor([[1.3116, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4182, 0.0000, 0.0000, 0.4246],\n",
      "        [0.0000, 0.0000, 1.0112, 0.0425, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0341, 1.3731, 0.1134],\n",
      "        [0.0000, 0.9046, 0.0000, 0.0839, 1.3139]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0834596157073975\n",
      "Batch 11, Loss: 3.0834596157073975\n",
      "Processing Batch 12/157\n",
      "Batch 12 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013246485963463783, Std: 0.08739543706178665\n",
      "z_j Mean: 0.012787386775016785, Std: 0.08746380358934402\n",
      "Similarities: tensor([[1.2457, 0.1312, 0.5414, 0.2160, 0.0000],\n",
      "        [0.5368, 1.4249, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2675, 0.0000, 1.4135, 0.0000, 0.0000],\n",
      "        [0.0922, 0.0000, 0.0000, 1.3960, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.2242]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0308597087860107\n",
      "Batch 12, Loss: 3.0308597087860107\n",
      "Processing Batch 13/157\n",
      "Batch 13 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.0132522601634264, Std: 0.08739456534385681\n",
      "z_j Mean: 0.013223034329712391, Std: 0.08669774234294891\n",
      "Similarities: tensor([[1.4119, 0.0000, 0.0000, 0.0683, 0.0000],\n",
      "        [0.0000, 1.4283, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3630, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4196, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.2002]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0383028984069824\n",
      "Batch 13, Loss: 3.0383028984069824\n",
      "Processing Batch 14/157\n",
      "Batch 14 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01315403077751398, Std: 0.08670824021100998\n",
      "z_j Mean: 0.013326337561011314, Std: 0.08668192476034164\n",
      "Similarities: tensor([[1.0742, 0.0000, 0.0000, 0.0000, 0.6669],\n",
      "        [0.0000, 1.3903, 0.4251, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2170, 1.3748, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3418, 0.9550],\n",
      "        [0.0000, 0.0000, 0.0000, 1.2440, 1.3632]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0726780891418457\n",
      "Batch 14, Loss: 3.0726780891418457\n",
      "Processing Batch 15/157\n",
      "Batch 15 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01244623214006424, Std: 0.08751300722360611\n",
      "z_j Mean: 0.012331262230873108, Std: 0.08752927929162979\n",
      "Similarities: tensor([[1.3728, 0.0000, 0.3792, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4143, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0396, 0.0000, 1.4102, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.1855, 1.4225, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3452]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0719830989837646\n",
      "Batch 15, Loss: 3.0719830989837646\n",
      "Processing Batch 16/157\n",
      "Batch 16 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01307007111608982, Std: 0.08742199838161469\n",
      "z_j Mean: 0.013104420155286789, Std: 0.08741685748100281\n",
      "Similarities: tensor([[1.4036, 0.0000, 0.0690, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4032, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0367, 0.0000, 1.3049, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4221, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.5806]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0159010887145996\n",
      "Batch 16, Loss: 3.0159010887145996\n",
      "Processing Batch 17/157\n",
      "Batch 17 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013012124225497246, Std: 0.08743064850568771\n",
      "z_j Mean: 0.012765148654580116, Std: 0.08676634728908539\n",
      "Similarities: tensor([[1.3874, 1.1599, 0.0000, 0.6952, 0.0300],\n",
      "        [1.1979, 1.4085, 0.0000, 0.0080, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4074, 0.0000, 0.0000],\n",
      "        [0.5361, 0.0209, 0.0000, 1.3173, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3703]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0643861293792725\n",
      "Batch 17, Loss: 3.0643861293792725\n",
      "Processing Batch 18/157\n",
      "Batch 18 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012531528249382973, Std: 0.08680040389299393\n",
      "z_j Mean: 0.012381944805383682, Std: 0.08682186901569366\n",
      "Similarities: tensor([[1.3842, 0.0468, 0.0677, 0.0000, 0.0000],\n",
      "        [0.0984, 1.4194, 0.0431, 0.0000, 0.0000],\n",
      "        [0.1882, 0.0541, 1.3072, 0.2318, 0.1937],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3449, 0.0000],\n",
      "        [0.0000, 0.0000, 0.2040, 0.0000, 1.1616]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0572516918182373\n",
      "Batch 18, Loss: 3.0572516918182373\n",
      "Processing Batch 19/157\n",
      "Batch 19 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01352118980139494, Std: 0.08735335618257523\n",
      "z_j Mean: 0.01388142816722393, Std: 0.08729682862758636\n",
      "Similarities: tensor([[1.1512e+00, 0.0000e+00, 8.9821e-04, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 1.3700e+00, 0.0000e+00, 1.4035e-01, 0.0000e+00],\n",
      "        [0.0000e+00, 1.1225e-02, 1.4197e+00, 9.5720e-02, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 2.3011e-01, 1.2083e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.4170e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0619490146636963\n",
      "Batch 19, Loss: 3.0619490146636963\n",
      "Processing Batch 20/157\n",
      "Batch 20 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012604136019945145, Std: 0.08749040216207504\n",
      "z_j Mean: 0.012655283324420452, Std: 0.08678244799375534\n",
      "Similarities: tensor([[1.3111, 0.0000, 0.1046, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4018, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3610, 0.0000, 1.4218, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3602, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4269]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.044649124145508\n",
      "Batch 20, Loss: 3.044649124145508\n",
      "Processing Batch 21/157\n",
      "Batch 21 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012522446922957897, Std: 0.0875021293759346\n",
      "z_j Mean: 0.012358558364212513, Std: 0.08682520687580109\n",
      "Similarities: tensor([[1.4283, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4240, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4205, 0.6793, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3448, 0.8866, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.8091]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.080366611480713\n",
      "Batch 21, Loss: 3.080366611480713\n",
      "Processing Batch 22/157\n",
      "Batch 22 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013582744635641575, Std: 0.08734380453824997\n",
      "z_j Mean: 0.013392672874033451, Std: 0.08737315237522125\n",
      "Similarities: tensor([[1.3980, 0.0000, 0.0000, 0.0000, 0.2616],\n",
      "        [0.0000, 1.3995, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4286, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3357, 0.0000],\n",
      "        [0.1560, 0.0000, 0.0000, 0.0000, 1.4227]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.021744728088379\n",
      "Batch 22, Loss: 3.021744728088379\n",
      "Processing Batch 23/157\n",
      "Batch 23 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013235720805823803, Std: 0.08739706873893738\n",
      "z_j Mean: 0.013295670971274376, Std: 0.08738797158002853\n",
      "Similarities: tensor([[1.0368, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3122, 0.0000, 0.0000, 0.7649],\n",
      "        [0.0000, 0.0179, 1.3856, 0.0000, 0.3631],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3277, 0.0000],\n",
      "        [0.0000, 1.0646, 0.1292, 0.0000, 1.2992]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0497894287109375\n",
      "Batch 23, Loss: 3.0497894287109375\n",
      "Processing Batch 24/157\n",
      "Batch 24 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012372327968478203, Std: 0.08752349019050598\n",
      "z_j Mean: 0.012548331171274185, Std: 0.08679797500371933\n",
      "Similarities: tensor([[1.2377, 0.1218, 0.0000, 0.0000, 0.3843],\n",
      "        [0.0128, 1.3710, 0.0530, 0.0000, 0.0037],\n",
      "        [0.0000, 0.1595, 1.4250, 0.0000, 0.0273],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4189, 0.0000],\n",
      "        [0.4345, 0.0314, 0.0470, 0.0000, 1.4244]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0829968452453613\n",
      "Batch 24, Loss: 3.0829968452453613\n",
      "Processing Batch 25/157\n",
      "Batch 25 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012664148584008217, Std: 0.08748172968626022\n",
      "z_j Mean: 0.012514881789684296, Std: 0.08680280297994614\n",
      "Similarities: tensor([[1.3595, 0.0835, 0.0000, 0.1621, 0.0000],\n",
      "        [0.2654, 1.1111, 0.0000, 0.1216, 0.0000],\n",
      "        [0.0000, 0.0000, 0.6772, 0.0000, 1.1778],\n",
      "        [0.2454, 0.0393, 0.0000, 1.3514, 0.0000],\n",
      "        [0.0000, 0.0000, 0.3241, 0.0000, 1.3944]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0686206817626953\n",
      "Batch 25, Loss: 3.0686206817626953\n",
      "Processing Batch 26/157\n",
      "Batch 26 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.0125836580991745, Std: 0.08749334514141083\n",
      "z_j Mean: 0.012729376554489136, Std: 0.08747226744890213\n",
      "Similarities: tensor([[1.2654, 0.0000, 0.0000, 0.0734, 0.0000],\n",
      "        [0.0000, 1.4174, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.2161, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3798, 0.2218],\n",
      "        [0.0000, 0.0000, 0.0000, 0.2098, 1.3961]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.052687168121338\n",
      "Batch 26, Loss: 3.052687168121338\n",
      "Processing Batch 27/157\n",
      "Batch 27 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012946192175149918, Std: 0.08744043111801147\n",
      "z_j Mean: 0.012585514225065708, Std: 0.08679259568452835\n",
      "Similarities: tensor([[1.3233, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4158, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4252, 0.2637, 0.0000],\n",
      "        [0.0000, 0.0000, 0.5676, 1.3912, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.2799]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.051823377609253\n",
      "Batch 27, Loss: 3.051823377609253\n",
      "Processing Batch 28/157\n",
      "Batch 28 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013125388883054256, Std: 0.0874137133359909\n",
      "z_j Mean: 0.013367364183068275, Std: 0.08737703412771225\n",
      "Similarities: tensor([[1.1528, 0.0830, 0.0881, 0.0000, 0.0556],\n",
      "        [0.0000, 1.3530, 0.7424, 0.0000, 0.9225],\n",
      "        [0.0000, 0.9552, 1.3963, 0.0146, 1.3651],\n",
      "        [0.0000, 0.0000, 0.2197, 1.3692, 0.2390],\n",
      "        [0.0000, 0.9612, 1.2718, 0.0170, 1.4185]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4283043146133423]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0118579864501953\n",
      "Batch 28, Loss: 3.0118579864501953\n",
      "Processing Batch 29/157\n",
      "Batch 29 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012922567315399647, Std: 0.08744392544031143\n",
      "z_j Mean: 0.012919312343001366, Std: 0.08744440972805023\n",
      "Similarities: tensor([[1.4242, 0.0000, 0.0000, 0.6083, 0.0000],\n",
      "        [0.0000, 1.4088, 0.3354, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2334, 1.4237, 0.0000, 0.0000],\n",
      "        [0.7063, 0.0334, 0.0000, 1.3906, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4083]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4284584522247314]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0789403915405273\n",
      "Batch 29, Loss: 3.0789403915405273\n",
      "Processing Batch 30/157\n",
      "Batch 30 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013083676807582378, Std: 0.08671888709068298\n",
      "z_j Mean: 0.012781989760696888, Std: 0.08676387369632721\n",
      "Similarities: tensor([[1.2959, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4286, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4068, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3407, 0.6227],\n",
      "        [0.0000, 0.0000, 0.0000, 0.8188, 1.4094]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0723698139190674\n",
      "Batch 30, Loss: 3.0723698139190674\n",
      "Processing Batch 31/157\n",
      "Batch 31 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012818098068237305, Std: 0.08745930343866348\n",
      "z_j Mean: 0.012665794230997562, Std: 0.08748149871826172\n",
      "Similarities: tensor([[1.2545, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3560, 0.3871, 0.4878, 0.0000],\n",
      "        [0.0000, 0.2224, 1.3928, 0.1321, 0.0000],\n",
      "        [0.0000, 0.0702, 0.0723, 1.2569, 0.0000],\n",
      "        [0.0554, 0.0000, 0.0000, 0.0000, 1.4227]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1054277420043945\n",
      "Batch 31, Loss: 3.1054277420043945\n",
      "Processing Batch 32/157\n",
      "Batch 32 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012926043942570686, Std: 0.08744341135025024\n",
      "z_j Mean: 0.013150283135473728, Std: 0.08740997314453125\n",
      "Similarities: tensor([[1.3891, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4202, 0.4843, 0.0209, 0.1318],\n",
      "        [0.0000, 0.5916, 1.3658, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0388, 0.1018, 1.4150, 0.3352],\n",
      "        [0.0000, 0.0427, 0.0000, 0.2201, 1.2436]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.030297040939331\n",
      "Batch 32, Loss: 3.030297040939331\n",
      "Processing Batch 33/157\n",
      "Batch 33 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012528379447758198, Std: 0.08680085837841034\n",
      "z_j Mean: 0.013143320567905903, Std: 0.08741101622581482\n",
      "Similarities: tensor([[1.4272, 0.0000, 0.2512, 0.0000, 0.3731],\n",
      "        [0.0000, 1.4234, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1809, 0.0000, 1.1779, 0.0000, 0.2102],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3465, 0.0000],\n",
      "        [0.3350, 0.0000, 0.3285, 0.0000, 1.3868]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4275481700897217]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0518999099731445\n",
      "Batch 33, Loss: 3.0518999099731445\n",
      "Processing Batch 34/157\n",
      "Batch 34 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013039780780673027, Std: 0.08742652088403702\n",
      "z_j Mean: 0.01334559079259634, Std: 0.08738036453723907\n",
      "Similarities: tensor([[1.4230, 1.1040, 0.0000, 0.1255, 0.9768],\n",
      "        [1.1298, 1.2890, 0.0000, 0.0000, 0.9058],\n",
      "        [0.0000, 0.0000, 1.4181, 0.0000, 0.0000],\n",
      "        [0.0467, 0.0000, 0.0000, 1.4271, 0.0033],\n",
      "        [1.0085, 0.6668, 0.0000, 0.0000, 1.4253]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.046066999435425\n",
      "Batch 34, Loss: 3.046066999435425\n",
      "Processing Batch 35/157\n",
      "Batch 35 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.0129285529255867, Std: 0.0874430388212204\n",
      "z_j Mean: 0.013049375265836716, Std: 0.08742509037256241\n",
      "Similarities: tensor([[1.3874, 0.0000, 0.0000, 0.2859, 0.0000],\n",
      "        [0.0000, 1.4176, 0.0000, 0.2069, 1.3672],\n",
      "        [0.0000, 0.0000, 1.3666, 0.0000, 0.0000],\n",
      "        [0.3461, 0.1115, 0.0000, 1.4108, 0.0369],\n",
      "        [0.0000, 1.3340, 0.0949, 0.0047, 1.3849]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.010554075241089\n",
      "Batch 35, Loss: 3.010554075241089\n",
      "Processing Batch 36/157\n",
      "Batch 36 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012758003547787666, Std: 0.08676740527153015\n",
      "z_j Mean: 0.012748431414365768, Std: 0.08746948838233948\n",
      "Similarities: tensor([[1.3061, 0.0000, 0.2738, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3919, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3342, 0.0000, 1.3883, 0.0000, 0.0000],\n",
      "        [0.3230, 0.0000, 0.0000, 1.3743, 0.4400],\n",
      "        [0.0000, 0.0000, 0.0000, 0.2673, 1.3489]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0945286750793457\n",
      "Batch 36, Loss: 3.0945286750793457\n",
      "Processing Batch 37/157\n",
      "Batch 37 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012942148372530937, Std: 0.08744102716445923\n",
      "z_j Mean: 0.012902885675430298, Std: 0.08744683116674423\n",
      "Similarities: tensor([[1.0399, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3808, 0.0000, 1.4055, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3432, 0.0000, 0.0556],\n",
      "        [0.0000, 1.3940, 0.0000, 1.3979, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0395, 0.0000, 1.3195]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.04349946975708\n",
      "Batch 37, Loss: 3.04349946975708\n",
      "Processing Batch 38/157\n",
      "Batch 38 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01291378028690815, Std: 0.0874452218413353\n",
      "z_j Mean: 0.013196670450270176, Std: 0.08740298449993134\n",
      "Similarities: tensor([[1.3011, 0.7547, 0.0000, 1.3640, 0.0000],\n",
      "        [0.5971, 1.3470, 0.0000, 0.7336, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4286, 0.0000, 0.0000],\n",
      "        [1.1516, 0.8069, 0.0000, 1.4131, 0.0000],\n",
      "        [0.0045, 0.0000, 0.0000, 0.0000, 0.4549]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.077242851257324\n",
      "Batch 38, Loss: 3.077242851257324\n",
      "Processing Batch 39/157\n",
      "Batch 39 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012664916925132275, Std: 0.08748162537813187\n",
      "z_j Mean: 0.012814502231776714, Std: 0.08745983242988586\n",
      "Similarities: tensor([[1.3739, 0.0000, 0.0000, 0.4616, 0.0000],\n",
      "        [0.0000, 1.3657, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3896, 0.0142, 0.5133],\n",
      "        [0.1242, 0.0000, 0.0000, 1.4218, 0.0000],\n",
      "        [0.2101, 0.0000, 0.4507, 0.0000, 1.3865]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 2.997178554534912\n",
      "Batch 39, Loss: 2.997178554534912\n",
      "Processing Batch 40/157\n",
      "Batch 40 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012820715084671974, Std: 0.08745892345905304\n",
      "z_j Mean: 0.012961780652403831, Std: 0.08743812143802643\n",
      "Similarities: tensor([[1.3722, 0.2445, 0.2406, 0.0000, 0.1473],\n",
      "        [0.0000, 0.4179, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3911, 0.0000, 1.3932, 0.0000, 0.1689],\n",
      "        [0.0000, 0.0000, 0.0000, 1.1829, 0.0000],\n",
      "        [0.0027, 0.0000, 0.1344, 0.0000, 1.3896]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0358526706695557\n",
      "Batch 40, Loss: 3.0358526706695557\n",
      "Processing Batch 41/157\n",
      "Batch 41 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01269972138106823, Std: 0.08747657388448715\n",
      "z_j Mean: 0.01254225056618452, Std: 0.08749929815530777\n",
      "Similarities: tensor([[1.4269, 0.0000, 0.0000, 0.1687, 0.0000],\n",
      "        [0.0000, 1.3468, 0.0000, 0.0000, 0.8808],\n",
      "        [0.0000, 0.0000, 1.3408, 0.3586, 0.0273],\n",
      "        [0.2511, 0.0000, 0.4998, 1.4067, 0.0491],\n",
      "        [0.0000, 0.6387, 0.0709, 0.0000, 1.4065]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0173513889312744\n",
      "Batch 41, Loss: 3.0173513889312744\n",
      "Processing Batch 42/157\n",
      "Batch 42 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01259535364806652, Std: 0.08749166876077652\n",
      "z_j Mean: 0.012331470847129822, Std: 0.087529256939888\n",
      "Similarities: tensor([[1.3195, 0.0000, 0.7120, 0.7627, 0.0000],\n",
      "        [0.0000, 1.3821, 0.0000, 0.0000, 0.0687],\n",
      "        [0.6781, 0.0000, 1.3716, 1.2833, 0.0000],\n",
      "        [0.6403, 0.0000, 1.4052, 1.3327, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4282]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.008544683456421\n",
      "Batch 42, Loss: 3.008544683456421\n",
      "Processing Batch 43/157\n",
      "Batch 43 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013445206917822361, Std: 0.08736509084701538\n",
      "z_j Mean: 0.013360006734728813, Std: 0.08737815916538239\n",
      "Similarities: tensor([[1.3400, 0.0000, 0.0023, 0.0250, 0.0053],\n",
      "        [0.0000, 0.9302, 0.0000, 0.2247, 0.0000],\n",
      "        [0.0033, 0.0000, 1.4256, 0.0000, 1.3651],\n",
      "        [0.0145, 1.1367, 0.0000, 1.4209, 0.0000],\n",
      "        [0.0076, 0.0000, 1.3788, 0.0000, 1.3848]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.051679849624634\n",
      "Batch 43, Loss: 3.051679849624634\n",
      "Processing Batch 44/157\n",
      "Batch 44 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012862753123044968, Std: 0.08745274692773819\n",
      "z_j Mean: 0.012824401259422302, Std: 0.08745837956666946\n",
      "Similarities: tensor([[1.2860e+00, 1.0751e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [1.6607e-02, 1.3772e+00, 0.0000e+00, 1.0933e-01, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 1.3478e+00, 0.0000e+00, 6.9308e-02],\n",
      "        [4.4359e-04, 7.3576e-02, 0.0000e+00, 1.4111e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.4158e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0146548748016357\n",
      "Batch 44, Loss: 3.0146548748016357\n",
      "Processing Batch 45/157\n",
      "Batch 45 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012679331004619598, Std: 0.08747953921556473\n",
      "z_j Mean: 0.01271229237318039, Std: 0.0874747484922409\n",
      "Similarities: tensor([[1.4015, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3905, 0.0000, 0.0000, 0.4406],\n",
      "        [0.0000, 0.0000, 1.3878, 0.0000, 0.0000],\n",
      "        [0.2186, 0.0000, 0.0000, 1.4133, 0.2724],\n",
      "        [0.0000, 0.4016, 0.0000, 0.3990, 1.3701]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.016746997833252\n",
      "Batch 45, Loss: 3.016746997833252\n",
      "Processing Batch 46/157\n",
      "Batch 46 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012390147894620895, Std: 0.08682069927453995\n",
      "z_j Mean: 0.012448849156498909, Std: 0.08751263469457626\n",
      "Similarities: tensor([[1.4174, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3447, 0.6704, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3178, 1.4022, 0.1265, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3549, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4097]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.04369854927063\n",
      "Batch 46, Loss: 3.04369854927063\n",
      "Processing Batch 47/157\n",
      "Batch 47 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01313475240021944, Std: 0.08741230517625809\n",
      "z_j Mean: 0.013476124033331871, Std: 0.08736032247543335\n",
      "Similarities: tensor([[1.2943, 0.0000, 0.0685, 0.0000, 0.0769],\n",
      "        [0.0000, 1.4159, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.2936, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.2941, 0.0000],\n",
      "        [0.0213, 0.0000, 0.0000, 0.0000, 1.2956]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.035585880279541\n",
      "Batch 47, Loss: 3.035585880279541\n",
      "Processing Batch 48/157\n",
      "Batch 48 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013022536411881447, Std: 0.08742909878492355\n",
      "z_j Mean: 0.012795349583029747, Std: 0.08676189929246902\n",
      "Similarities: tensor([[1.4245e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.4793e-03],\n",
      "        [0.0000e+00, 1.4254e+00, 0.0000e+00, 4.0144e-01, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 1.3249e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 2.7304e-01, 0.0000e+00, 1.3477e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 4.3216e-03, 0.0000e+00, 3.8914e-04, 1.3900e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0757699012756348\n",
      "Batch 48, Loss: 3.0757699012756348\n",
      "Processing Batch 49/157\n",
      "Batch 49 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013307549059391022, Std: 0.08738616853952408\n",
      "z_j Mean: 0.013119619339704514, Std: 0.08671345561742783\n",
      "Similarities: tensor([[1.3350e+00, 4.1993e-02, 0.0000e+00, 1.2444e-01, 1.0852e+00],\n",
      "        [3.3433e-04, 1.4268e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 1.3980e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [3.3083e-01, 0.0000e+00, 0.0000e+00, 1.3747e+00, 1.1549e-02],\n",
      "        [1.0219e+00, 8.4899e-03, 0.0000e+00, 0.0000e+00, 1.4225e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0377628803253174\n",
      "Batch 49, Loss: 3.0377628803253174\n",
      "Processing Batch 50/157\n",
      "Batch 50 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013475204817950726, Std: 0.0873604565858841\n",
      "z_j Mean: 0.013921597972512245, Std: 0.08729042857885361\n",
      "Similarities: tensor([[1.3182, 0.0000, 0.0460, 0.4356, 0.0000],\n",
      "        [0.0000, 1.3675, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3922, 0.0000, 1.2958, 0.0000, 0.0000],\n",
      "        [0.3243, 0.0000, 0.0000, 1.4045, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3457]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0384726524353027\n",
      "Batch 50, Loss: 3.0384726524353027\n",
      "Processing Batch 51/157\n",
      "Batch 51 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012714670971035957, Std: 0.08677376806735992\n",
      "z_j Mean: 0.01279055792838335, Std: 0.08676260709762573\n",
      "Similarities: tensor([[1.4262, 0.0000, 0.0000, 0.4544, 0.0000],\n",
      "        [0.0000, 1.4102, 0.0297, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0135, 1.3597, 0.2595, 0.0000],\n",
      "        [0.5217, 0.0000, 0.1459, 1.3379, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3727]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.048753261566162\n",
      "Batch 51, Loss: 3.048753261566162\n",
      "Processing Batch 52/157\n",
      "Batch 52 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013255407102406025, Std: 0.08739408850669861\n",
      "z_j Mean: 0.012833841145038605, Std: 0.08745699375867844\n",
      "Similarities: tensor([[1.4065, 0.0000, 0.3532, 0.0042, 0.3143],\n",
      "        [0.0000, 1.2137, 0.0000, 0.1648, 0.1662],\n",
      "        [0.8194, 0.0000, 1.0193, 0.0102, 0.3854],\n",
      "        [0.0638, 0.4311, 0.0392, 1.3438, 1.1601],\n",
      "        [0.2063, 0.5475, 0.1268, 1.1694, 1.3875]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.040276050567627\n",
      "Batch 52, Loss: 3.040276050567627\n",
      "Processing Batch 53/157\n",
      "Batch 53 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012735595926642418, Std: 0.0867706909775734\n",
      "z_j Mean: 0.012932216748595238, Std: 0.08603499084711075\n",
      "Similarities: tensor([[1.3133, 0.3741, 0.1435, 0.0000, 0.0000],\n",
      "        [0.2469, 1.4020, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2163, 0.0000, 1.3396, 0.0000, 0.6292],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4285, 0.0000],\n",
      "        [0.0000, 0.0000, 0.7561, 0.0000, 1.4253]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0751047134399414\n",
      "Batch 53, Loss: 3.0751047134399414\n",
      "Processing Batch 54/157\n",
      "Batch 54 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013131413608789444, Std: 0.08741280436515808\n",
      "z_j Mean: 0.012829636223614216, Std: 0.08675684034824371\n",
      "Similarities: tensor([[1.4247, 0.0000, 0.0000, 0.2670, 0.9411],\n",
      "        [0.0000, 1.3284, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.3046, 0.0000, 0.0000],\n",
      "        [0.4010, 0.0000, 0.5034, 1.3604, 0.2550],\n",
      "        [1.0624, 0.0000, 0.0000, 0.1707, 1.4174]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0850751399993896\n",
      "Batch 54, Loss: 3.0850751399993896\n",
      "Processing Batch 55/157\n",
      "Batch 55 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013624178245663643, Std: 0.08733735233545303\n",
      "z_j Mean: 0.013665270991623402, Std: 0.08733092993497849\n",
      "Similarities: tensor([[1.4286, 1.4189, 0.0000, 0.0000, 0.8102],\n",
      "        [1.4248, 1.4238, 0.0000, 0.0623, 0.8080],\n",
      "        [0.0000, 0.0000, 1.2718, 0.7854, 0.0000],\n",
      "        [0.0000, 0.0593, 0.4587, 1.1394, 0.0000],\n",
      "        [0.8402, 0.8345, 0.0000, 0.0000, 1.3897]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.083420515060425\n",
      "Batch 55, Loss: 3.083420515060425\n",
      "Processing Batch 56/157\n",
      "Batch 56 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01279181893914938, Std: 0.0874631479382515\n",
      "z_j Mean: 0.01255936548113823, Std: 0.08749683946371078\n",
      "Similarities: tensor([[1.3953, 0.0652, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0272, 1.4120, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4166, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.2115, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3354]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.03120756149292\n",
      "Batch 56, Loss: 3.03120756149292\n",
      "Processing Batch 57/157\n",
      "Batch 57 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013304973021149635, Std: 0.08738655596971512\n",
      "z_j Mean: 0.0132489874958992, Std: 0.08739505708217621\n",
      "Similarities: tensor([[1.3942, 0.0000, 0.8895, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3582, 0.0000, 0.0000, 0.0000],\n",
      "        [0.6892, 0.0000, 1.4009, 0.4467, 0.0000],\n",
      "        [0.0000, 0.0000, 0.2336, 1.2030, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.2214]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0163657665252686\n",
      "Batch 57, Loss: 3.0163657665252686\n",
      "Processing Batch 58/157\n",
      "Batch 58 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013406186364591122, Std: 0.08737108111381531\n",
      "z_j Mean: 0.013447316363453865, Std: 0.08736476302146912\n",
      "Similarities: tensor([[1.3484, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4003, 0.0000, 0.0000, 0.0783],\n",
      "        [0.0000, 0.0000, 1.3977, 0.7850, 0.4945],\n",
      "        [0.0000, 0.0000, 1.0637, 1.2486, 0.0000],\n",
      "        [0.0000, 0.0000, 0.3866, 0.0446, 1.3124]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.428131103515625]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0541598796844482\n",
      "Batch 58, Loss: 3.0541598796844482\n",
      "Processing Batch 59/157\n",
      "Batch 59 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012745317071676254, Std: 0.08746994286775589\n",
      "z_j Mean: 0.012781776487827301, Std: 0.08746461570262909\n",
      "Similarities: tensor([[1.3408, 0.0000, 0.0000, 0.0000, 0.6887],\n",
      "        [0.4261, 1.3227, 0.0000, 0.0000, 0.2459],\n",
      "        [0.0000, 0.0000, 1.3419, 0.7962, 0.0000],\n",
      "        [0.0000, 0.0000, 1.1548, 1.4286, 0.0000],\n",
      "        [0.6129, 0.0000, 0.0000, 0.0000, 0.0000]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0554802417755127\n",
      "Batch 59, Loss: 3.0554802417755127\n",
      "Processing Batch 60/157\n",
      "Batch 60 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012390286661684513, Std: 0.08682068437337875\n",
      "z_j Mean: 0.012206710875034332, Std: 0.086140938103199\n",
      "Similarities: tensor([[1.3756, 0.0000, 0.0000, 0.0553, 0.0000],\n",
      "        [0.0000, 1.3960, 0.0000, 0.0000, 0.2272],\n",
      "        [0.0000, 0.0000, 1.4023, 0.0000, 0.0000],\n",
      "        [0.0213, 0.0028, 0.0000, 1.3716, 0.1240],\n",
      "        [0.0000, 0.0250, 0.0000, 0.0000, 1.3681]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0700390338897705\n",
      "Batch 60, Loss: 3.0700390338897705\n",
      "Processing Batch 61/157\n",
      "Batch 61 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012625161558389664, Std: 0.0874873697757721\n",
      "z_j Mean: 0.01267513632774353, Std: 0.08748014271259308\n",
      "Similarities: tensor([[1.4236, 0.7342, 0.0000, 0.0000, 1.3447],\n",
      "        [0.6917, 1.3206, 0.0000, 0.0000, 0.3690],\n",
      "        [0.0000, 0.0000, 1.3385, 0.0150, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4163, 0.0000],\n",
      "        [1.2737, 0.3591, 0.0000, 0.0000, 1.4217]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.011152505874634\n",
      "Batch 61, Loss: 3.011152505874634\n",
      "Processing Batch 62/157\n",
      "Batch 62 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013084381818771362, Std: 0.08741986006498337\n",
      "z_j Mean: 0.013040995225310326, Std: 0.08742634952068329\n",
      "Similarities: tensor([[1.3546, 0.0139, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0020, 1.4030, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4010, 0.0000, 0.1938],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4149, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0486, 0.0000, 1.3839]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0589332580566406\n",
      "Batch 62, Loss: 3.0589332580566406\n",
      "Processing Batch 63/157\n",
      "Batch 63 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013800697401165962, Std: 0.08730962127447128\n",
      "z_j Mean: 0.013567142188549042, Std: 0.08734623342752457\n",
      "Similarities: tensor([[1.4284, 0.0000, 0.0000, 0.0000, 1.0641],\n",
      "        [0.0000, 1.4188, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.2663, 0.2919, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0535, 1.3469, 0.0000],\n",
      "        [0.7232, 0.0000, 0.0000, 0.0000, 1.1907]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0303072929382324\n",
      "Batch 63, Loss: 3.0303072929382324\n",
      "Processing Batch 64/157\n",
      "Batch 64 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012920033186674118, Std: 0.08744429796934128\n",
      "z_j Mean: 0.012971192598342896, Std: 0.0874367281794548\n",
      "Similarities: tensor([[1.3133, 0.4894, 0.3156, 0.5762, 0.0000],\n",
      "        [0.2768, 1.3525, 0.8400, 0.6470, 0.0000],\n",
      "        [0.1613, 0.5476, 1.3362, 0.0000, 0.3391],\n",
      "        [0.9711, 0.8302, 0.0000, 1.3868, 0.0000],\n",
      "        [0.0000, 0.0000, 0.4756, 0.0000, 1.4215]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285491704940796]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0327088832855225\n",
      "Batch 64, Loss: 3.0327088832855225\n",
      "Processing Batch 65/157\n",
      "Batch 65 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01228617038577795, Std: 0.08753561973571777\n",
      "z_j Mean: 0.0116333132609725, Std: 0.08692535012960434\n",
      "Similarities: tensor([[1.3537e+00, 2.4629e-03, 0.0000e+00, 0.0000e+00, 4.6252e-01],\n",
      "        [1.2508e-03, 1.4273e+00, 0.0000e+00, 6.2803e-01, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 1.4231e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 8.6890e-01, 0.0000e+00, 1.1813e+00, 0.0000e+00],\n",
      "        [5.6713e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.2279e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.079766273498535\n",
      "Batch 65, Loss: 3.079766273498535\n",
      "Processing Batch 66/157\n",
      "Batch 66 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012153005227446556, Std: 0.0875542163848877\n",
      "z_j Mean: 0.012161686085164547, Std: 0.0875530019402504\n",
      "Similarities: tensor([[1.3272, 0.1114, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.2508, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4121, 1.1709, 0.0000],\n",
      "        [0.0000, 0.0000, 1.1774, 1.4114, 0.0018],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0124, 1.4264]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.023638963699341\n",
      "Batch 66, Loss: 3.023638963699341\n",
      "Processing Batch 67/157\n",
      "Batch 67 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012300996109843254, Std: 0.08753354102373123\n",
      "z_j Mean: 0.01251444686204195, Std: 0.08750327676534653\n",
      "Similarities: tensor([[0.6919, 0.0000, 0.0000, 0.0852, 0.0000],\n",
      "        [0.0000, 0.5119, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0313, 0.0000, 1.4268, 0.0000, 0.0000],\n",
      "        [0.0110, 0.0000, 0.0000, 1.0227, 1.0251],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0160, 0.9946]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.111126661300659\n",
      "Batch 67, Loss: 3.111126661300659\n",
      "Processing Batch 68/157\n",
      "Batch 68 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013340452685952187, Std: 0.08738114684820175\n",
      "z_j Mean: 0.013426796533167362, Std: 0.08736792206764221\n",
      "Similarities: tensor([[1.1923, 0.0000, 0.1366, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3996, 0.0000, 0.0091, 0.0000],\n",
      "        [0.6009, 0.0000, 1.4131, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0014, 0.0000, 1.4271, 0.0539],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0207, 1.3632]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0237557888031006\n",
      "Batch 68, Loss: 3.0237557888031006\n",
      "Processing Batch 69/157\n",
      "Batch 69 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012866427190601826, Std: 0.08745221048593521\n",
      "z_j Mean: 0.012662393972277641, Std: 0.08748199045658112\n",
      "Similarities: tensor([[1.3182, 0.9267, 0.0000, 0.0000, 0.8196],\n",
      "        [0.8793, 1.1557, 0.0000, 0.0000, 0.5001],\n",
      "        [0.0000, 0.0000, 1.4185, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.8969, 0.0000],\n",
      "        [0.6356, 0.3050, 0.0000, 0.0000, 1.3242]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.427709937095642]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0606930255889893\n",
      "Batch 69, Loss: 3.0606930255889893\n",
      "Processing Batch 70/157\n",
      "Batch 70 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01303976122289896, Std: 0.08742652833461761\n",
      "z_j Mean: 0.012609418481588364, Std: 0.08748964220285416\n",
      "Similarities: tensor([[1.3484, 0.1167, 0.5699, 0.0000, 0.0000],\n",
      "        [0.0873, 1.4084, 0.3617, 0.0000, 0.0000],\n",
      "        [0.6918, 0.5102, 1.3971, 0.0000, 0.0248],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4133, 0.2654],\n",
      "        [0.0000, 0.0000, 0.0000, 0.2934, 1.4062]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.031418561935425\n",
      "Batch 70, Loss: 3.031418561935425\n",
      "Processing Batch 71/157\n",
      "Batch 71 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012898959219455719, Std: 0.08744741231203079\n",
      "z_j Mean: 0.01301801111549139, Std: 0.08742976933717728\n",
      "Similarities: tensor([[1.4279, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4201, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.0385, 0.7752, 0.0070],\n",
      "        [0.0000, 0.0000, 0.6960, 1.0613, 0.0000],\n",
      "        [0.0000, 0.0000, 0.5702, 0.0000, 1.0258]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.017317533493042\n",
      "Batch 71, Loss: 3.017317533493042\n",
      "Processing Batch 72/157\n",
      "Batch 72 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01269952766597271, Std: 0.08747660368680954\n",
      "z_j Mean: 0.012917445972561836, Std: 0.08744468539953232\n",
      "Similarities: tensor([[1.0320, 0.0000, 0.3279, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4028, 0.8389, 0.0000, 1.0767],\n",
      "        [0.1874, 0.5083, 1.3540, 0.0000, 0.3944],\n",
      "        [0.0000, 0.0000, 0.0000, 1.1865, 0.0000],\n",
      "        [0.0000, 1.1773, 0.7118, 0.0000, 1.4200]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0466575622558594\n",
      "Batch 72, Loss: 3.0466575622558594\n",
      "Processing Batch 73/157\n",
      "Batch 73 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012820861302316189, Std: 0.08745890110731125\n",
      "z_j Mean: 0.012322520837187767, Std: 0.08683032542467117\n",
      "Similarities: tensor([[0.0000, 0.8407, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.2666, 0.0000, 0.0000, 0.0532],\n",
      "        [0.0000, 0.0000, 1.4180, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.1435, 1.3746, 0.0000],\n",
      "        [0.0000, 0.2842, 0.0000, 0.0000, 1.3777]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0481820106506348\n",
      "Batch 73, Loss: 3.0481820106506348\n",
      "Processing Batch 74/157\n",
      "Batch 74 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012167450040578842, Std: 0.08755220472812653\n",
      "z_j Mean: 0.012120907194912434, Std: 0.08755866438150406\n",
      "Similarities: tensor([[1.2126, 0.0000, 0.0000, 0.6839, 0.0000],\n",
      "        [0.0000, 1.4242, 0.0000, 0.0000, 1.2901],\n",
      "        [0.0000, 0.0000, 1.4215, 0.0000, 0.0000],\n",
      "        [0.0759, 0.0000, 0.0000, 1.4201, 0.0000],\n",
      "        [0.0000, 1.2772, 0.0000, 0.0000, 1.4145]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.014260768890381\n",
      "Batch 74, Loss: 3.014260768890381\n",
      "Processing Batch 75/157\n",
      "Batch 75 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013086928054690361, Std: 0.08741947263479233\n",
      "z_j Mean: 0.013107853010296822, Std: 0.08741634339094162\n",
      "Similarities: tensor([[1.4097, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.8766, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.2714, 0.7319, 0.0000],\n",
      "        [0.0000, 0.0000, 1.1763, 1.3296, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4256]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0922324657440186\n",
      "Batch 75, Loss: 3.0922324657440186\n",
      "Processing Batch 76/157\n",
      "Batch 76 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01301899366080761, Std: 0.08742962032556534\n",
      "z_j Mean: 0.013308746740221977, Std: 0.08738598227500916\n",
      "Similarities: tensor([[1.4131, 0.0000, 0.5533, 0.0000, 0.1826],\n",
      "        [0.0000, 1.4139, 0.0327, 0.0524, 0.0000],\n",
      "        [0.6184, 0.0000, 1.2035, 0.0000, 0.0168],\n",
      "        [0.0000, 0.0893, 0.0000, 1.3461, 0.0000],\n",
      "        [0.3130, 0.0000, 0.0000, 0.0000, 1.4160]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.070142984390259\n",
      "Batch 76, Loss: 3.070142984390259\n",
      "Processing Batch 77/157\n",
      "Batch 77 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012907684780657291, Std: 0.08744613081216812\n",
      "z_j Mean: 0.012981697916984558, Std: 0.08673421293497086\n",
      "Similarities: tensor([[1.4206, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.1803, 0.2078, 0.0000, 0.1615],\n",
      "        [0.0000, 0.0115, 1.4259, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.2966, 0.0971],\n",
      "        [0.0000, 0.2401, 0.0000, 0.0354, 1.4203]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.428477168083191]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.066664218902588\n",
      "Batch 77, Loss: 3.066664218902588\n",
      "Processing Batch 78/157\n",
      "Batch 78 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.0130230113863945, Std: 0.08742903172969818\n",
      "z_j Mean: 0.01266330573707819, Std: 0.08748185634613037\n",
      "Similarities: tensor([[1.3404, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3656, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0174, 0.0000, 1.2736, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3439, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4050]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0643885135650635\n",
      "Batch 78, Loss: 3.0643885135650635\n",
      "Processing Batch 79/157\n",
      "Batch 79 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012927526608109474, Std: 0.08744319528341293\n",
      "z_j Mean: 0.012833382934331894, Std: 0.0874570682644844\n",
      "Similarities: tensor([[1.0090, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3078, 0.0000, 0.0325, 0.0000],\n",
      "        [0.0000, 0.0000, 0.8577, 0.0000, 0.6558],\n",
      "        [0.0000, 0.0402, 0.0000, 1.3801, 0.0000],\n",
      "        [0.0000, 0.0000, 0.2646, 0.0000, 1.4001]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.1136834621429443\n",
      "Batch 79, Loss: 3.1136834621429443\n",
      "Processing Batch 80/157\n",
      "Batch 80 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013221779838204384, Std: 0.0873991847038269\n",
      "z_j Mean: 0.012821734882891178, Std: 0.08675800263881683\n",
      "Similarities: tensor([[1.3711, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3639, 0.8782, 0.1721, 0.0867],\n",
      "        [0.0000, 0.7427, 1.3095, 0.0121, 0.0000],\n",
      "        [0.0000, 0.1970, 0.0000, 1.3950, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3418]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.06508731842041\n",
      "Batch 80, Loss: 3.06508731842041\n",
      "Processing Batch 81/157\n",
      "Batch 81 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013155713677406311, Std: 0.08740915358066559\n",
      "z_j Mean: 0.0134220439940691, Std: 0.08736864477396011\n",
      "Similarities: tensor([[1.2851, 0.2265, 0.2828, 0.2587, 0.0000],\n",
      "        [0.6202, 1.3561, 1.2330, 0.0000, 0.0000],\n",
      "        [0.5348, 1.3499, 1.4269, 0.0000, 0.0000],\n",
      "        [0.1115, 0.0000, 0.0000, 1.3809, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4143]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.428568959236145]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0273892879486084\n",
      "Batch 81, Loss: 3.0273892879486084\n",
      "Processing Batch 82/157\n",
      "Batch 82 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013100451789796352, Std: 0.08671635389328003\n",
      "z_j Mean: 0.013099614530801773, Std: 0.08741758018732071\n",
      "Similarities: tensor([[1.4274, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4082, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.2891, 0.8307, 0.0000],\n",
      "        [0.0000, 0.0000, 0.6541, 0.8991, 0.0000],\n",
      "        [0.0000, 0.6174, 0.0000, 0.0000, 1.1620]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0550665855407715\n",
      "Batch 82, Loss: 3.0550665855407715\n",
      "Processing Batch 83/157\n",
      "Batch 83 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012449711561203003, Std: 0.08681218326091766\n",
      "z_j Mean: 0.0128106027841568, Std: 0.08675964921712875\n",
      "Similarities: tensor([[1.4250, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4212, 0.0000, 1.4237, 0.6449],\n",
      "        [0.0000, 0.0000, 1.4140, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4044, 0.0000, 1.4286, 0.5415],\n",
      "        [0.0000, 0.6865, 0.0000, 0.4452, 1.3831]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.04304575920105\n",
      "Batch 83, Loss: 3.04304575920105\n",
      "Processing Batch 84/157\n",
      "Batch 84 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013020927086472511, Std: 0.08742932975292206\n",
      "z_j Mean: 0.012885120697319508, Std: 0.08744946122169495\n",
      "Similarities: tensor([[1.4031, 0.0000, 0.0000, 0.1343, 1.2039],\n",
      "        [0.0000, 1.1285, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3643, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.2031, 0.0000],\n",
      "        [1.4271, 0.0000, 0.0000, 0.0000, 1.2580]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0716235637664795\n",
      "Batch 84, Loss: 3.0716235637664795\n",
      "Processing Batch 85/157\n",
      "Batch 85 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013096447102725506, Std: 0.08741804957389832\n",
      "z_j Mean: 0.013160746544599533, Std: 0.0874083936214447\n",
      "Similarities: tensor([[1.3010, 0.0000, 0.9993, 0.2135, 0.0000],\n",
      "        [0.0000, 1.4190, 0.0000, 0.0000, 0.0426],\n",
      "        [1.2712, 0.0000, 1.4285, 0.2930, 0.0000],\n",
      "        [0.6574, 0.0000, 0.6038, 1.3609, 0.0000],\n",
      "        [0.0000, 0.0465, 0.0000, 0.0000, 1.3869]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285707473754883]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0731892585754395\n",
      "Batch 85, Loss: 3.0731892585754395\n",
      "Processing Batch 86/157\n",
      "Batch 86 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01369846984744072, Std: 0.08732572942972183\n",
      "z_j Mean: 0.013648941181600094, Std: 0.08663170784711838\n",
      "Similarities: tensor([[1.0651, 0.0000, 0.0000, 0.0000, 0.1767],\n",
      "        [0.0000, 1.4085, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1021, 0.0000, 1.3989, 0.0000, 0.2480],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3560, 0.0000],\n",
      "        [0.0604, 0.0000, 0.1947, 0.0000, 1.4179]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0950751304626465\n",
      "Batch 86, Loss: 3.0950751304626465\n",
      "Processing Batch 87/157\n",
      "Batch 87 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.0130553487688303, Std: 0.08742420375347137\n",
      "z_j Mean: 0.01274564117193222, Std: 0.08746989816427231\n",
      "Similarities: tensor([[1.2574, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3189, 0.0414, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2652, 1.3973, 0.3812, 0.0000],\n",
      "        [0.0000, 0.0000, 0.4022, 1.3578, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0920, 1.3845]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0311460494995117\n",
      "Batch 87, Loss: 3.0311460494995117\n",
      "Processing Batch 88/157\n",
      "Batch 88 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012590629048645496, Std: 0.08749234676361084\n",
      "z_j Mean: 0.012140912935137749, Std: 0.08685590326786041\n",
      "Similarities: tensor([[1.2438e+00, 0.0000e+00, 1.1444e+00, 0.0000e+00, 1.7609e-04],\n",
      "        [0.0000e+00, 1.3941e+00, 0.0000e+00, 1.3858e-01, 0.0000e+00],\n",
      "        [5.8172e-01, 0.0000e+00, 1.4286e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 1.3788e-01, 0.0000e+00, 1.4096e+00, 0.0000e+00],\n",
      "        [6.3941e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.4286e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.058256149291992\n",
      "Batch 88, Loss: 3.058256149291992\n",
      "Processing Batch 89/157\n",
      "Batch 89 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013258836232125759, Std: 0.08739356696605682\n",
      "z_j Mean: 0.013675769791007042, Std: 0.08732928335666656\n",
      "Similarities: tensor([[1.3670, 0.0000, 0.4164, 0.0000, 0.8491],\n",
      "        [0.0000, 1.3631, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3696, 0.0000, 1.4135, 0.0000, 1.0838],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4260, 0.0000],\n",
      "        [1.2847, 0.0000, 0.5717, 0.0000, 1.1604]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0516374111175537\n",
      "Batch 89, Loss: 3.0516374111175537\n",
      "Processing Batch 90/157\n",
      "Batch 90 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013059461489319801, Std: 0.08742358535528183\n",
      "z_j Mean: 0.013252293691039085, Std: 0.08739455789327621\n",
      "Similarities: tensor([[1.4261e+00, 0.0000e+00, 3.2465e-02, 0.0000e+00, 1.6689e-02],\n",
      "        [0.0000e+00, 1.4275e+00, 5.5071e-01, 0.0000e+00, 2.9373e-04],\n",
      "        [4.3289e-02, 2.6828e-01, 1.3444e+00, 0.0000e+00, 1.0324e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 1.4286e+00, 0.0000e+00],\n",
      "        [2.7435e-02, 1.9070e-02, 1.0698e+00, 0.0000e+00, 1.3725e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0760562419891357\n",
      "Batch 90, Loss: 3.0760562419891357\n",
      "Processing Batch 91/157\n",
      "Batch 91 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013527533039450645, Std: 0.08735237270593643\n",
      "z_j Mean: 0.013509129174053669, Std: 0.08665362000465393\n",
      "Similarities: tensor([[1.3461, 0.0000, 0.0120, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4273, 0.2030, 0.0000, 0.0000],\n",
      "        [0.0156, 0.0687, 1.3014, 0.0000, 0.3996],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.4360, 0.0000, 1.0289]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0283055305480957\n",
      "Batch 91, Loss: 3.0283055305480957\n",
      "Processing Batch 92/157\n",
      "Batch 92 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012985805980861187, Std: 0.0874345600605011\n",
      "z_j Mean: 0.013176102191209793, Std: 0.08740608394145966\n",
      "Similarities: tensor([[1.2162e+00, 7.7437e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [2.7469e-04, 1.4282e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 1.4281e+00, 0.0000e+00, 1.2748e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 1.4281e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 1.2231e+00, 0.0000e+00, 1.4265e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4284355640411377]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.057013988494873\n",
      "Batch 92, Loss: 3.057013988494873\n",
      "Processing Batch 93/157\n",
      "Batch 93 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013070672750473022, Std: 0.08742190897464752\n",
      "z_j Mean: 0.012842556461691856, Std: 0.08745571970939636\n",
      "Similarities: tensor([[1.3711, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4278, 0.0000, 0.5322, 1.3704],\n",
      "        [0.0000, 0.0000, 1.3641, 0.0109, 0.0000],\n",
      "        [0.0000, 0.6833, 0.0000, 1.4106, 0.6052],\n",
      "        [0.0000, 1.4209, 0.0000, 0.4731, 1.4001]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0349199771881104\n",
      "Batch 93, Loss: 3.0349199771881104\n",
      "Processing Batch 94/157\n",
      "Batch 94 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012669268064200878, Std: 0.08748099207878113\n",
      "z_j Mean: 0.01231810636818409, Std: 0.08683095127344131\n",
      "Similarities: tensor([[1.2607, 0.0000, 0.0717, 0.0000, 0.0580],\n",
      "        [0.0000, 0.9605, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0792, 0.0000, 1.4090, 0.0000, 1.0116],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4285, 0.0000],\n",
      "        [0.0931, 0.0000, 1.0528, 0.0000, 1.4286]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.072930335998535\n",
      "Batch 94, Loss: 3.072930335998535\n",
      "Processing Batch 95/157\n",
      "Batch 95 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013353800401091576, Std: 0.0873791053891182\n",
      "z_j Mean: 0.01322253979742527, Std: 0.08739906549453735\n",
      "Similarities: tensor([[1.2776e+00, 7.5242e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [2.3510e-01, 1.4169e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 1.4151e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [3.6464e-03, 0.0000e+00, 0.0000e+00, 1.3928e+00, 3.4900e-01],\n",
      "        [1.0947e-03, 0.0000e+00, 0.0000e+00, 2.3643e-01, 1.1892e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0471036434173584\n",
      "Batch 95, Loss: 3.0471036434173584\n",
      "Processing Batch 96/157\n",
      "Batch 96 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013028687797486782, Std: 0.08742817491292953\n",
      "z_j Mean: 0.012618513777852058, Std: 0.0874883309006691\n",
      "Similarities: tensor([[1.3045, 0.0000, 1.1256, 0.0000, 0.0000],\n",
      "        [0.0000, 1.2449, 0.0000, 0.0000, 0.0339],\n",
      "        [0.5805, 0.0000, 1.0428, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3908, 0.1097],\n",
      "        [0.0000, 0.0122, 0.0000, 0.0977, 1.2389]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.036501407623291\n",
      "Batch 96, Loss: 3.036501407623291\n",
      "Processing Batch 97/157\n",
      "Batch 97 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01273924671113491, Std: 0.08747082948684692\n",
      "z_j Mean: 0.0125972218811512, Std: 0.08679088950157166\n",
      "Similarities: tensor([[1.3875e+00, 0.0000e+00, 0.0000e+00, 4.0256e-01, 8.7747e-05],\n",
      "        [0.0000e+00, 1.3939e+00, 1.8020e-01, 1.9335e-03, 0.0000e+00],\n",
      "        [0.0000e+00, 1.9577e-02, 1.3825e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [3.8581e-01, 1.0003e-04, 0.0000e+00, 1.3743e+00, 2.2975e-03],\n",
      "        [4.2804e-04, 0.0000e+00, 0.0000e+00, 5.9931e-02, 1.3862e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.046860456466675\n",
      "Batch 97, Loss: 3.046860456466675\n",
      "Processing Batch 98/157\n",
      "Batch 98 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013529524207115173, Std: 0.08735205978155136\n",
      "z_j Mean: 0.01363872829824686, Std: 0.08663331717252731\n",
      "Similarities: tensor([[1.3607, 0.0159, 0.0000, 0.2988, 0.0746],\n",
      "        [0.0200, 1.4271, 0.0000, 0.0182, 0.6536],\n",
      "        [0.0000, 0.0000, 1.4263, 0.0000, 0.0000],\n",
      "        [0.3469, 0.0292, 0.0000, 1.2787, 0.4054],\n",
      "        [0.0938, 1.1219, 0.0000, 0.0942, 1.1837]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0218288898468018\n",
      "Batch 98, Loss: 3.0218288898468018\n",
      "Processing Batch 99/157\n",
      "Batch 99 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012736223638057709, Std: 0.08747126907110214\n",
      "z_j Mean: 0.012511937879025936, Std: 0.08750363439321518\n",
      "Similarities: tensor([[1.4045, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4253, 0.0550, 0.0485, 0.0089],\n",
      "        [0.0000, 0.0476, 1.3823, 1.3393, 0.4729],\n",
      "        [0.0000, 0.0661, 1.1844, 1.3770, 0.3981],\n",
      "        [0.0000, 0.0114, 0.4966, 0.4461, 1.4259]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0467870235443115\n",
      "Batch 99, Loss: 3.0467870235443115\n",
      "Processing Batch 100/157\n",
      "Batch 100 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013009216636419296, Std: 0.08743108063936234\n",
      "z_j Mean: 0.013193542137742043, Std: 0.08740344643592834\n",
      "Similarities: tensor([[1.4107, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3480, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.3164, 0.1461, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4285, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.2521]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.018597364425659\n",
      "Batch 100, Loss: 3.018597364425659\n",
      "Processing Batch 101/157\n",
      "Batch 101 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012817170470952988, Std: 0.08745943754911423\n",
      "z_j Mean: 0.012843535281717777, Std: 0.08745556324720383\n",
      "Similarities: tensor([[1.4244, 1.1766, 0.0000, 0.0000, 0.0300],\n",
      "        [1.1787, 1.2974, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3117, 0.2344, 0.0000],\n",
      "        [0.0000, 0.0000, 0.2202, 1.3536, 0.0000],\n",
      "        [0.0164, 0.0000, 0.0057, 0.0000, 1.4194]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.032210111618042\n",
      "Batch 101, Loss: 3.032210111618042\n",
      "Processing Batch 102/157\n",
      "Batch 102 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012902561575174332, Std: 0.08744688332080841\n",
      "z_j Mean: 0.012835182249546051, Std: 0.08745680004358292\n",
      "Similarities: tensor([[1.4225, 0.0000, 0.2257, 0.0136, 0.0000],\n",
      "        [0.0000, 1.4243, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2377, 0.0000, 1.3850, 0.0881, 0.6523],\n",
      "        [0.0041, 0.0000, 0.3080, 1.4153, 0.0000],\n",
      "        [0.0000, 0.0000, 0.7431, 0.0000, 1.3490]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.047865152359009\n",
      "Batch 102, Loss: 3.047865152359009\n",
      "Processing Batch 103/157\n",
      "Batch 103 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012456750497221947, Std: 0.08751150965690613\n",
      "z_j Mean: 0.012409398332238197, Std: 0.08751823753118515\n",
      "Similarities: tensor([[1.3717, 0.1716, 1.2200, 0.0000, 0.0178],\n",
      "        [0.0936, 1.3420, 0.0000, 1.3689, 0.6249],\n",
      "        [1.0785, 0.0000, 1.4153, 0.0000, 0.0000],\n",
      "        [0.0000, 1.1616, 0.0000, 1.3543, 0.4719],\n",
      "        [0.0000, 0.8209, 0.0000, 0.5202, 1.3881]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.037081003189087\n",
      "Batch 103, Loss: 3.037081003189087\n",
      "Processing Batch 104/157\n",
      "Batch 104 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012954859994351864, Std: 0.0874391421675682\n",
      "z_j Mean: 0.01293067168444395, Std: 0.08744272589683533\n",
      "Similarities: tensor([[1.2637, 0.0211, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0398, 1.4242, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3535, 0.0000, 0.2143],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4007, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0328, 0.0000, 1.4178]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0318171977996826\n",
      "Batch 104, Loss: 3.0318171977996826\n",
      "Processing Batch 105/157\n",
      "Batch 105 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012484517879784107, Std: 0.08680717647075653\n",
      "z_j Mean: 0.012518880888819695, Std: 0.08680222928524017\n",
      "Similarities: tensor([[1.3981, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.9484, 0.1615, 0.0000, 0.0425],\n",
      "        [0.0000, 0.4686, 1.3973, 0.1844, 0.0000],\n",
      "        [0.0000, 0.0000, 0.1473, 1.3828, 0.0000],\n",
      "        [0.0195, 0.0154, 0.0000, 0.0000, 1.4120]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.038119316101074\n",
      "Batch 105, Loss: 3.038119316101074\n",
      "Processing Batch 106/157\n",
      "Batch 106 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012292668223381042, Std: 0.08753471076488495\n",
      "z_j Mean: 0.01239178515970707, Std: 0.08752073347568512\n",
      "Similarities: tensor([[1.3728, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3838, 0.1540, 0.0000, 0.0146],\n",
      "        [0.0000, 0.0000, 1.2030, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3224, 0.0000],\n",
      "        [0.0058, 0.0000, 0.0000, 0.0000, 1.4219]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0270073413848877\n",
      "Batch 106, Loss: 3.0270073413848877\n",
      "Processing Batch 107/157\n",
      "Batch 107 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012451153248548508, Std: 0.08751230686903\n",
      "z_j Mean: 0.01250117551535368, Std: 0.08750517666339874\n",
      "Similarities: tensor([[1.4147, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.1991, 0.0000, 1.2541, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4202, 0.0000, 0.0000],\n",
      "        [0.0000, 0.9385, 0.0000, 1.3874, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.9905]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0265145301818848\n",
      "Batch 107, Loss: 3.0265145301818848\n",
      "Processing Batch 108/157\n",
      "Batch 108 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013198040425777435, Std: 0.08740276843309402\n",
      "z_j Mean: 0.013504818081855774, Std: 0.08735588937997818\n",
      "Similarities: tensor([[1.1428, 0.3772, 0.0000, 0.1518, 0.5012],\n",
      "        [0.2181, 1.2759, 0.0000, 0.0000, 0.1336],\n",
      "        [0.0000, 0.0000, 1.3246, 0.0896, 0.0000],\n",
      "        [0.0230, 0.0000, 0.2024, 1.2264, 0.0090],\n",
      "        [0.9211, 0.0000, 0.0000, 0.0313, 1.4142]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.04917573928833\n",
      "Batch 108, Loss: 3.04917573928833\n",
      "Processing Batch 109/157\n",
      "Batch 109 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012416929006576538, Std: 0.0875171646475792\n",
      "z_j Mean: 0.012551863677799702, Std: 0.08749791234731674\n",
      "Similarities: tensor([[1.3884, 0.4706, 0.0000, 0.2761, 0.0000],\n",
      "        [0.3415, 1.4285, 0.0000, 0.1421, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3994, 0.0000, 0.4791],\n",
      "        [0.5578, 0.1824, 0.0000, 1.4004, 0.4953],\n",
      "        [0.0000, 0.0000, 0.7257, 0.5876, 1.3829]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0379838943481445\n",
      "Batch 109, Loss: 3.0379838943481445\n",
      "Processing Batch 110/157\n",
      "Batch 110 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012419916689395905, Std: 0.08751674741506577\n",
      "z_j Mean: 0.012674061581492424, Std: 0.08748029172420502\n",
      "Similarities: tensor([[1.3980, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4285, 0.0000, 1.2166, 0.0000],\n",
      "        [0.0081, 0.0000, 1.1152, 0.0000, 0.0000],\n",
      "        [0.0000, 1.1757, 0.0000, 1.4221, 0.0000],\n",
      "        [0.0000, 0.0000, 0.4850, 0.0000, 1.4241]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0344197750091553\n",
      "Batch 110, Loss: 3.0344197750091553\n",
      "Processing Batch 111/157\n",
      "Batch 111 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012018619105219841, Std: 0.08757276833057404\n",
      "z_j Mean: 0.012250090949237347, Std: 0.08754067867994308\n",
      "Similarities: tensor([[1.4234, 0.0000, 0.0000, 0.0000, 0.0015],\n",
      "        [0.0000, 1.4097, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3550, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4286, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.1684]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.004791259765625\n",
      "Batch 111, Loss: 3.004791259765625\n",
      "Processing Batch 112/157\n",
      "Batch 112 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012961210682988167, Std: 0.08743821084499359\n",
      "z_j Mean: 0.0132286436855793, Std: 0.08739814162254333\n",
      "Similarities: tensor([[1.1732, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3922, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3938, 0.0388, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0664, 1.1155, 0.0000],\n",
      "        [0.0294, 0.0000, 0.0000, 0.0000, 1.3843]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285389184951782]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.055802345275879\n",
      "Batch 112, Loss: 3.055802345275879\n",
      "Processing Batch 113/157\n",
      "Batch 113 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012246739119291306, Std: 0.08613525331020355\n",
      "z_j Mean: 0.012323682196438313, Std: 0.08683016151189804\n",
      "Similarities: tensor([[1.4272, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3969, 0.0000, 0.0000, 1.0274],\n",
      "        [0.0000, 0.0000, 1.2141, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1560, 0.0000, 0.6296, 0.2989],\n",
      "        [0.0000, 0.7805, 0.0000, 0.0000, 1.4163]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0764718055725098\n",
      "Batch 113, Loss: 3.0764718055725098\n",
      "Processing Batch 114/157\n",
      "Batch 114 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013476247899234295, Std: 0.08736030012369156\n",
      "z_j Mean: 0.013368620537221432, Std: 0.08737684041261673\n",
      "Similarities: tensor([[1.1702, 0.0000, 0.2171, 0.0000, 0.0468],\n",
      "        [0.0000, 1.3839, 0.5976, 0.0000, 0.0000],\n",
      "        [0.1224, 0.4291, 1.3761, 0.0556, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0609, 1.4203, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3152]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0389721393585205\n",
      "Batch 114, Loss: 3.0389721393585205\n",
      "Processing Batch 115/157\n",
      "Batch 115 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013229466043412685, Std: 0.08739801496267319\n",
      "z_j Mean: 0.012921575456857681, Std: 0.08674319088459015\n",
      "Similarities: tensor([[1.3777, 0.1061, 0.0000, 1.1105, 0.0000],\n",
      "        [0.0000, 1.2294, 0.0000, 0.2103, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3354, 0.0000, 0.9232],\n",
      "        [1.1459, 0.2026, 0.0000, 1.3740, 0.0000],\n",
      "        [0.0000, 0.0000, 1.1782, 0.0000, 1.4271]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.049237012863159\n",
      "Batch 115, Loss: 3.049237012863159\n",
      "Processing Batch 116/157\n",
      "Batch 116 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013390351086854935, Std: 0.0873735100030899\n",
      "z_j Mean: 0.013173835352063179, Std: 0.08740642666816711\n",
      "Similarities: tensor([[1.3627, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4116, 0.4642, 0.0000, 0.0000],\n",
      "        [0.0000, 0.5312, 1.4280, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3093, 1.4286],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3093, 1.4286]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.066737413406372\n",
      "Batch 116, Loss: 3.066737413406372\n",
      "Processing Batch 117/157\n",
      "Batch 117 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012704834342002869, Std: 0.08747582882642746\n",
      "z_j Mean: 0.012947539798915386, Std: 0.08744023740291595\n",
      "Similarities: tensor([[1.2730, 0.1134, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2790, 1.4244, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3118, 0.7607, 0.0000],\n",
      "        [0.0000, 0.0000, 0.5436, 1.3029, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3840]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4283355474472046]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.051170587539673\n",
      "Batch 117, Loss: 3.051170587539673\n",
      "Processing Batch 118/157\n",
      "Batch 118 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01297624222934246, Std: 0.08743597567081451\n",
      "z_j Mean: 0.012880425900220871, Std: 0.08745014667510986\n",
      "Similarities: tensor([[1.4260, 0.0000, 1.3281, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3892, 0.0000, 0.0000, 0.0283],\n",
      "        [1.4054, 0.0000, 1.3346, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3097, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3130]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0317225456237793\n",
      "Batch 118, Loss: 3.0317225456237793\n",
      "Processing Batch 119/157\n",
      "Batch 119 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012603256851434708, Std: 0.08749052882194519\n",
      "z_j Mean: 0.01263858750462532, Std: 0.0874854251742363\n",
      "Similarities: tensor([[1.4116, 0.0000, 0.0000, 1.3519, 0.0628],\n",
      "        [0.0000, 1.4121, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4121, 0.0000, 0.0000],\n",
      "        [1.1558, 0.0000, 0.0000, 1.3803, 0.0320],\n",
      "        [0.7858, 0.0000, 0.0000, 0.7241, 1.0455]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 2.98655104637146\n",
      "Batch 119, Loss: 2.98655104637146\n",
      "Processing Batch 120/157\n",
      "Batch 120 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013325916603207588, Std: 0.08668198436498642\n",
      "z_j Mean: 0.013536695390939713, Std: 0.08735094964504242\n",
      "Similarities: tensor([[1.4021, 0.9029, 0.0000, 0.0365, 0.0000],\n",
      "        [1.1649, 1.1732, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.0429, 0.6823, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0051, 1.3709, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0024, 0.0000, 1.4132]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0855095386505127\n",
      "Batch 120, Loss: 3.0855095386505127\n",
      "Processing Batch 121/157\n",
      "Batch 121 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012865118682384491, Std: 0.08745240420103073\n",
      "z_j Mean: 0.013128135353326797, Std: 0.08741329610347748\n",
      "Similarities: tensor([[1.4151, 0.0000, 0.0884, 0.0168, 1.0544],\n",
      "        [0.0000, 1.3140, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0729, 0.0000, 1.3436, 0.1184, 0.4754],\n",
      "        [0.0549, 0.0000, 0.5798, 1.3698, 0.0934],\n",
      "        [0.8862, 0.0000, 0.3481, 0.0207, 1.3871]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.025623083114624\n",
      "Batch 121, Loss: 3.025623083114624\n",
      "Processing Batch 122/157\n",
      "Batch 122 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012487348169088364, Std: 0.08750715106725693\n",
      "z_j Mean: 0.01261328998953104, Std: 0.08678855746984482\n",
      "Similarities: tensor([[1.3074, 1.3074, 0.5863, 0.0000, 0.0000],\n",
      "        [1.4286, 1.4286, 0.0117, 0.0000, 0.0000],\n",
      "        [0.0226, 0.0226, 1.4283, 0.0000, 0.0000],\n",
      "        [1.4286, 1.4286, 0.0117, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3361]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0573408603668213\n",
      "Batch 122, Loss: 3.0573408603668213\n",
      "Processing Batch 123/157\n",
      "Batch 123 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01308667566627264, Std: 0.08741951733827591\n",
      "z_j Mean: 0.013350672088563442, Std: 0.0873795822262764\n",
      "Similarities: tensor([[1.4064e+00, 1.1592e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [9.6540e-01, 1.3363e+00, 0.0000e+00, 5.6687e-01, 0.0000e+00],\n",
      "        [0.0000e+00, 6.2332e-03, 1.4094e+00, 2.7850e-03, 1.9880e-03],\n",
      "        [0.0000e+00, 1.2451e-01, 0.0000e+00, 1.4231e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 1.1776e-03, 0.0000e+00, 1.4254e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.00632381439209\n",
      "Batch 123, Loss: 3.00632381439209\n",
      "Processing Batch 124/157\n",
      "Batch 124 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012656263075768948, Std: 0.08748286962509155\n",
      "z_j Mean: 0.012615910731256008, Std: 0.08748870342969894\n",
      "Similarities: tensor([[1.3472e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 1.1733e+00, 9.0452e-01, 9.8069e-01, 0.0000e+00],\n",
      "        [4.9061e-02, 8.0430e-01, 1.3847e+00, 4.5369e-01, 0.0000e+00],\n",
      "        [5.5405e-04, 1.0409e+00, 2.9211e-01, 1.2172e+00, 2.4472e-02],\n",
      "        [0.0000e+00, 1.9870e-02, 0.0000e+00, 4.3017e-02, 1.4231e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4284917116165161]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0114586353302\n",
      "Batch 124, Loss: 3.0114586353302\n",
      "Processing Batch 125/157\n",
      "Batch 125 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01239948719739914, Std: 0.08751963824033737\n",
      "z_j Mean: 0.01263347826898098, Std: 0.0874861627817154\n",
      "Similarities: tensor([[1.2653, 0.0000, 0.2030, 0.1694, 0.0000],\n",
      "        [0.0000, 1.4184, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0874, 0.0000, 1.3118, 0.0000, 1.0377],\n",
      "        [0.4630, 0.0000, 0.0265, 1.3907, 0.0000],\n",
      "        [0.0000, 0.0000, 0.9307, 0.0000, 1.3966]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0166258811950684\n",
      "Batch 125, Loss: 3.0166258811950684\n",
      "Processing Batch 126/157\n",
      "Batch 126 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012146968394517899, Std: 0.08755505084991455\n",
      "z_j Mean: 0.01237538829445839, Std: 0.08752305805683136\n",
      "Similarities: tensor([[1.3844, 0.9246, 0.0000, 0.0000, 0.0000],\n",
      "        [1.2011, 1.0571, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4286, 0.0000, 0.5066],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3972, 0.0000],\n",
      "        [0.0000, 0.0000, 0.7738, 0.0000, 1.3970]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.020946979522705\n",
      "Batch 126, Loss: 3.020946979522705\n",
      "Processing Batch 127/157\n",
      "Batch 127 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013469086959958076, Std: 0.0873614102602005\n",
      "z_j Mean: 0.01332809031009674, Std: 0.08738303184509277\n",
      "Similarities: tensor([[1.4252, 0.1775, 0.0000, 0.0000, 0.7743],\n",
      "        [0.2634, 1.4117, 0.0000, 0.0000, 0.1212],\n",
      "        [0.0000, 0.0000, 0.9106, 0.6898, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0585, 1.4235, 0.0000],\n",
      "        [0.7414, 0.0683, 0.0000, 0.0000, 1.4098]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0329556465148926\n",
      "Batch 127, Loss: 3.0329556465148926\n",
      "Processing Batch 128/157\n",
      "Batch 128 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013139775954186916, Std: 0.08671040087938309\n",
      "z_j Mean: 0.013079989701509476, Std: 0.0874205157160759\n",
      "Similarities: tensor([[1.3188, 0.0000, 1.0635, 0.0000, 0.4250],\n",
      "        [0.0000, 1.3668, 0.0000, 0.0000, 0.0000],\n",
      "        [0.7889, 0.0000, 1.2972, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3862, 0.0000],\n",
      "        [0.8459, 0.0000, 0.0000, 0.0000, 1.3245]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0361404418945312\n",
      "Batch 128, Loss: 3.0361404418945312\n",
      "Processing Batch 129/157\n",
      "Batch 129 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013522421941161156, Std: 0.0873531699180603\n",
      "z_j Mean: 0.013170573860406876, Std: 0.08740691840648651\n",
      "Similarities: tensor([[1.3702, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3257, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.4737, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3941, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3335, 0.0000, 1.4286]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.074843406677246\n",
      "Batch 129, Loss: 3.074843406677246\n",
      "Processing Batch 130/157\n",
      "Batch 130 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013324493542313576, Std: 0.08668220788240433\n",
      "z_j Mean: 0.013613041490316391, Std: 0.08663734793663025\n",
      "Similarities: tensor([[1.2560, 0.4868, 0.0000, 0.0000, 0.1104],\n",
      "        [0.3122, 1.0883, 0.0000, 0.0000, 0.0991],\n",
      "        [0.0000, 0.0000, 1.4263, 0.0000, 0.0000],\n",
      "        [0.1067, 0.0000, 0.0000, 0.9465, 1.1906],\n",
      "        [0.3009, 0.1099, 0.0000, 0.5756, 1.3898]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.045175552368164\n",
      "Batch 130, Loss: 3.045175552368164\n",
      "Processing Batch 131/157\n",
      "Batch 131 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013381389901041985, Std: 0.08737488090991974\n",
      "z_j Mean: 0.01345997117459774, Std: 0.08736281096935272\n",
      "Similarities: tensor([[1.3910, 0.1735, 0.0782, 0.0000, 0.5168],\n",
      "        [0.2095, 1.3599, 0.6832, 0.0000, 0.3498],\n",
      "        [0.1834, 0.3664, 1.4265, 0.0000, 0.2678],\n",
      "        [0.0000, 0.0196, 0.0000, 1.4125, 0.0000],\n",
      "        [0.3377, 0.6217, 0.2183, 0.0000, 1.2920]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285544157028198]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0208041667938232\n",
      "Batch 131, Loss: 3.0208041667938232\n",
      "Processing Batch 132/157\n",
      "Batch 132 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01268013659864664, Std: 0.08747941255569458\n",
      "z_j Mean: 0.0124885905534029, Std: 0.087506964802742\n",
      "Similarities: tensor([[1.3526, 0.6092, 0.8808, 0.0000, 0.0000],\n",
      "        [0.3394, 1.4286, 0.0000, 0.0000, 0.0000],\n",
      "        [1.0379, 0.0000, 1.4054, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3412, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.8693]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.031548023223877\n",
      "Batch 132, Loss: 3.031548023223877\n",
      "Processing Batch 133/157\n",
      "Batch 133 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01205955259501934, Std: 0.08756713569164276\n",
      "z_j Mean: 0.012304719537496567, Std: 0.08753301948308945\n",
      "Similarities: tensor([[1.3414, 0.1252, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4767, 1.3124, 0.0000, 0.0108, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3915, 1.2697, 0.0000],\n",
      "        [0.0000, 0.0591, 0.8453, 1.3524, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4022]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0119616985321045\n",
      "Batch 133, Loss: 3.0119616985321045\n",
      "Processing Batch 134/157\n",
      "Batch 134 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013326364569365978, Std: 0.08668192476034164\n",
      "z_j Mean: 0.013210684061050415, Std: 0.08669962733983994\n",
      "Similarities: tensor([[1.4026, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4156, 0.0000, 0.1184, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4077, 0.7514, 0.9138],\n",
      "        [0.0000, 0.1202, 0.7052, 1.3087, 0.6897],\n",
      "        [0.0000, 0.0000, 1.1947, 0.9168, 1.3788]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0282046794891357\n",
      "Batch 134, Loss: 3.0282046794891357\n",
      "Processing Batch 135/157\n",
      "Batch 135 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01272273063659668, Std: 0.08747322857379913\n",
      "z_j Mean: 0.01299243699759245, Std: 0.0874335765838623\n",
      "Similarities: tensor([[1.4191, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4075, 0.0164, 0.0081, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4093, 0.7203, 0.1059],\n",
      "        [0.0000, 0.0000, 0.5092, 1.2908, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0483, 0.0000, 1.3558]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0088915824890137\n",
      "Batch 135, Loss: 3.0088915824890137\n",
      "Processing Batch 136/157\n",
      "Batch 136 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012030405923724174, Std: 0.0875711441040039\n",
      "z_j Mean: 0.012058192864060402, Std: 0.08756732195615768\n",
      "Similarities: tensor([[1.3964, 0.0000, 1.4165, 0.4038, 0.0000],\n",
      "        [0.0000, 1.3477, 0.0000, 0.0000, 0.0000],\n",
      "        [1.3822, 0.0000, 1.4272, 0.4071, 0.0000],\n",
      "        [1.1612, 0.0000, 1.0116, 1.1653, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3936]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.007845163345337\n",
      "Batch 136, Loss: 3.007845163345337\n",
      "Processing Batch 137/157\n",
      "Batch 137 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012809313833713531, Std: 0.08746059238910675\n",
      "z_j Mean: 0.012449641712009907, Std: 0.08681219071149826\n",
      "Similarities: tensor([[1.3635, 0.9603, 0.0000, 0.0000, 0.0000],\n",
      "        [0.9653, 1.4167, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1819, 1.4152, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4252, 0.0000],\n",
      "        [0.7813, 0.5990, 0.0000, 0.2625, 0.0000]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0524582862854004\n",
      "Batch 137, Loss: 3.0524582862854004\n",
      "Processing Batch 138/157\n",
      "Batch 138 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012429468333721161, Std: 0.08751539140939713\n",
      "z_j Mean: 0.012060925364494324, Std: 0.08686704933643341\n",
      "Similarities: tensor([[1.2448, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4269, 0.2058, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3334, 1.4060, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4286, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4286]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.030550479888916\n",
      "Batch 138, Loss: 3.030550479888916\n",
      "Processing Batch 139/157\n",
      "Batch 139 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013042679987847805, Std: 0.0874260887503624\n",
      "z_j Mean: 0.013429786078631878, Std: 0.0873674601316452\n",
      "Similarities: tensor([[1.4257, 0.2180, 0.0000, 0.0750, 0.0000],\n",
      "        [0.0161, 1.2006, 0.0000, 0.4436, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3632, 0.0000, 0.0000],\n",
      "        [0.0312, 0.2916, 0.0000, 1.3704, 0.4932],\n",
      "        [0.0000, 0.0000, 0.0000, 0.5052, 1.4258]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.055689573287964\n",
      "Batch 139, Loss: 3.055689573287964\n",
      "Processing Batch 140/157\n",
      "Batch 140 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013338794931769371, Std: 0.08738140016794205\n",
      "z_j Mean: 0.013127028942108154, Std: 0.08671233057975769\n",
      "Similarities: tensor([[1.2738, 0.6602, 0.2275, 0.0539, 0.0000],\n",
      "        [1.2154, 1.1066, 0.3040, 0.0000, 0.0000],\n",
      "        [0.2331, 0.1076, 1.4263, 0.0000, 0.0000],\n",
      "        [0.0764, 0.0000, 0.0000, 1.3066, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4252]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.083890676498413\n",
      "Batch 140, Loss: 3.083890676498413\n",
      "Processing Batch 141/157\n",
      "Batch 141 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013008324429392815, Std: 0.08743121474981308\n",
      "z_j Mean: 0.01260845735669136, Std: 0.08678926527500153\n",
      "Similarities: tensor([[1.4196e+00, 0.0000e+00, 2.1434e-01, 2.4562e-01, 3.7487e-03],\n",
      "        [0.0000e+00, 1.3634e+00, 0.0000e+00, 0.0000e+00, 1.0005e-02],\n",
      "        [9.5658e-01, 0.0000e+00, 9.0831e-01, 8.4516e-02, 0.0000e+00],\n",
      "        [2.8187e-01, 0.0000e+00, 0.0000e+00, 1.4155e+00, 5.7450e-04],\n",
      "        [0.0000e+00, 1.0632e-02, 0.0000e+00, 0.0000e+00, 1.3927e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.033463716506958\n",
      "Batch 141, Loss: 3.033463716506958\n",
      "Processing Batch 142/157\n",
      "Batch 142 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013958962634205818, Std: 0.08728446066379547\n",
      "z_j Mean: 0.013983462005853653, Std: 0.08728053420782089\n",
      "Similarities: tensor([[1.4088, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3960, 0.0000, 1.2696, 0.0000],\n",
      "        [0.0000, 0.0000, 0.9167, 0.0000, 0.0000],\n",
      "        [0.0000, 1.2445, 0.0000, 1.4206, 0.0000],\n",
      "        [0.0000, 0.0226, 0.0000, 0.0000, 1.4280]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0356550216674805\n",
      "Batch 142, Loss: 3.0356550216674805\n",
      "Processing Batch 143/157\n",
      "Batch 143 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013177457265555859, Std: 0.08740587532520294\n",
      "z_j Mean: 0.013179527595639229, Std: 0.08740556985139847\n",
      "Similarities: tensor([[0.8070, 0.3652, 0.0243, 0.0000, 0.2369],\n",
      "        [1.3947, 1.3705, 0.0000, 0.0000, 1.2885],\n",
      "        [0.1095, 0.0000, 1.4205, 0.0000, 0.0218],\n",
      "        [0.0080, 0.0017, 0.0000, 1.3923, 0.0000],\n",
      "        [1.2447, 1.3596, 0.0000, 0.0000, 1.4248]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0296096801757812\n",
      "Batch 143, Loss: 3.0296096801757812\n",
      "Processing Batch 144/157\n",
      "Batch 144 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012518448755145073, Std: 0.08680228888988495\n",
      "z_j Mean: 0.012546161189675331, Std: 0.0867982879281044\n",
      "Similarities: tensor([[1.4119, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0483, 0.0941, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4175, 0.2600, 0.0000],\n",
      "        [0.0000, 0.2694, 0.4654, 1.2994, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.3140]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0590715408325195\n",
      "Batch 144, Loss: 3.0590715408325195\n",
      "Processing Batch 145/157\n",
      "Batch 145 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012510176748037338, Std: 0.08750388026237488\n",
      "z_j Mean: 0.012783790938556194, Std: 0.08746432512998581\n",
      "Similarities: tensor([[1.4285, 0.0000, 0.2537, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3812, 0.0000, 0.1360, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3464, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3422, 0.0000, 1.4239, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4234]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.029299736022949\n",
      "Batch 145, Loss: 3.029299736022949\n",
      "Processing Batch 146/157\n",
      "Batch 146 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013144483789801598, Std: 0.08741084486246109\n",
      "z_j Mean: 0.013039284385740757, Std: 0.08742660284042358\n",
      "Similarities: tensor([[1.3751e+00, 2.1131e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 1.4250e+00, 0.0000e+00, 2.2650e-01, 1.3730e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 1.4276e+00, 2.8290e-04, 0.0000e+00],\n",
      "        [0.0000e+00, 1.6386e-01, 0.0000e+00, 1.3336e+00, 3.5465e-01],\n",
      "        [0.0000e+00, 1.2598e+00, 0.0000e+00, 6.1516e-01, 1.3870e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4283106327056885]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0443074703216553\n",
      "Batch 146, Loss: 3.0443074703216553\n",
      "Processing Batch 147/157\n",
      "Batch 147 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011868789792060852, Std: 0.08618815243244171\n",
      "z_j Mean: 0.012084640562534332, Std: 0.08756367862224579\n",
      "Similarities: tensor([[1.4199, 0.0000, 0.1898, 0.0000, 0.0454],\n",
      "        [0.0000, 1.4021, 0.0000, 1.2853, 0.0000],\n",
      "        [0.1036, 0.0000, 1.4221, 0.0000, 0.6054],\n",
      "        [0.0000, 1.2993, 0.0000, 1.3954, 0.0000],\n",
      "        [0.0451, 0.0000, 0.6970, 0.0000, 1.4241]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.054945230484009\n",
      "Batch 147, Loss: 3.054945230484009\n",
      "Processing Batch 148/157\n",
      "Batch 148 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.01277199573814869, Std: 0.0867653414607048\n",
      "z_j Mean: 0.012830555438995361, Std: 0.08745747804641724\n",
      "Similarities: tensor([[1.4271, 0.0000, 0.0000, 0.0000, 0.0278],\n",
      "        [0.0000, 1.3995, 0.0040, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.2925, 0.0000, 1.1438],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4150, 0.0000],\n",
      "        [0.0051, 0.0000, 1.0318, 0.0000, 1.3985]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.069830894470215\n",
      "Batch 148, Loss: 3.069830894470215\n",
      "Processing Batch 149/157\n",
      "Batch 149 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.013249186798930168, Std: 0.08739502727985382\n",
      "z_j Mean: 0.013051124289631844, Std: 0.08742482960224152\n",
      "Similarities: tensor([[1.4281, 0.0000, 0.0000, 0.0000, 0.3336],\n",
      "        [0.0000, 1.3169, 0.0000, 0.0000, 0.3588],\n",
      "        [0.0000, 0.0000, 1.4153, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4275, 0.0000],\n",
      "        [0.2758, 0.0558, 0.0000, 0.0000, 1.4038]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.041372776031494\n",
      "Batch 149, Loss: 3.041372776031494\n",
      "Processing Batch 150/157\n",
      "Batch 150 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012491235509514809, Std: 0.08750659227371216\n",
      "z_j Mean: 0.012778088450431824, Std: 0.08746515959501266\n",
      "Similarities: tensor([[1.4241, 0.0000, 0.0000, 0.0000, 0.9545],\n",
      "        [0.0000, 1.4249, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4282, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2799, 0.0000, 1.3710, 0.0000],\n",
      "        [0.8126, 0.0000, 0.0000, 0.0000, 1.3275]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.038696765899658\n",
      "Batch 150, Loss: 3.038696765899658\n",
      "Processing Batch 151/157\n",
      "Batch 151 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012823725119233131, Std: 0.08745847642421722\n",
      "z_j Mean: 0.012678234837949276, Std: 0.08747969567775726\n",
      "Similarities: tensor([[1.4201, 0.2577, 0.0000, 0.0000, 0.3355],\n",
      "        [0.3494, 1.2817, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.3981, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.4286, 0.5073],\n",
      "        [0.5851, 0.0000, 0.0000, 0.6807, 1.3373]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.060840368270874\n",
      "Batch 151, Loss: 3.060840368270874\n",
      "Processing Batch 152/157\n",
      "Batch 152 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012396503239870071, Std: 0.0875200629234314\n",
      "z_j Mean: 0.01253032311797142, Std: 0.08750101178884506\n",
      "Similarities: tensor([[1.2174, 0.0000, 0.0000, 0.0000, 0.1998],\n",
      "        [0.0000, 1.4286, 0.0000, 0.4706, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4245, 0.7102, 0.0000],\n",
      "        [0.0000, 0.8621, 0.6047, 1.3382, 0.0000],\n",
      "        [0.0145, 0.0000, 0.0000, 0.0000, 0.2877]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0741655826568604\n",
      "Batch 152, Loss: 3.0741655826568604\n",
      "Processing Batch 153/157\n",
      "Batch 153 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012610334903001785, Std: 0.08678898960351944\n",
      "z_j Mean: 0.012661268934607506, Std: 0.08748214691877365\n",
      "Similarities: tensor([[1.3661, 0.0469, 0.5988, 0.0000, 0.0000],\n",
      "        [0.1752, 1.2848, 0.0672, 0.0000, 0.0082],\n",
      "        [0.3793, 0.0897, 1.4017, 0.0000, 0.1390],\n",
      "        [0.0000, 0.0000, 0.0000, 1.3993, 0.0000],\n",
      "        [0.0000, 0.0000, 0.1286, 0.0000, 1.4263]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0687716007232666\n",
      "Batch 153, Loss: 3.0687716007232666\n",
      "Processing Batch 154/157\n",
      "Batch 154 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012745526619255543, Std: 0.086769238114357\n",
      "z_j Mean: 0.012717165984213352, Std: 0.08677339553833008\n",
      "Similarities: tensor([[1.4256e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 1.4049e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 1.4283e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 1.4286e+00, 0.0000e+00],\n",
      "        [1.8199e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.3267e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.031836748123169\n",
      "Batch 154, Loss: 3.031836748123169\n",
      "Processing Batch 155/157\n",
      "Batch 155 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.011798204854130745, Std: 0.08690313249826431\n",
      "z_j Mean: 0.012179414741694927, Std: 0.08685051649808884\n",
      "Similarities: tensor([[1.4255, 0.0000, 0.0000, 1.2109, 0.0000],\n",
      "        [0.0000, 1.2988, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1171, 0.0000, 1.2090, 0.3480, 0.0000],\n",
      "        [1.2454, 0.0000, 0.4653, 1.4211, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.4286]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.021955966949463\n",
      "Batch 155, Loss: 3.021955966949463\n",
      "Processing Batch 156/157\n",
      "Batch 156 loaded: x.shape=torch.Size([64, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([64, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([64, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([64, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([64, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([64, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012724584899842739, Std: 0.08606596291065216\n",
      "z_j Mean: 0.012837734073400497, Std: 0.08675564080476761\n",
      "Similarities: tensor([[1.4001, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4256, 0.0000, 1.1626, 0.2457],\n",
      "        [0.0000, 0.0000, 1.3612, 0.1102, 0.1245],\n",
      "        [0.0000, 1.2148, 0.0294, 1.4171, 0.5376],\n",
      "        [0.0000, 0.0141, 0.0391, 0.5121, 1.3404]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4281600713729858]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 3.0707128047943115\n",
      "Batch 156, Loss: 3.0707128047943115\n",
      "Processing Batch 157/157\n",
      "Batch 157 loaded: x.shape=torch.Size([16, 62, 30]), metadata keys = ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Entering try block\n",
      "Inside try block\n",
      "Entered stratified_normalization_minibatch\n",
      "x.shape: torch.Size([16, 62, 30])\n",
      "Metadata keys: ['start_at', 'end_at', 'clip_id', 'subject_id', 'trial_id', 'emotion', 'date', '_record_id', 'segment_start', 'segment_end']\n",
      "Exit stratified_normalization_minibatch\n",
      "Normalized Input Shape: torch.Size([16, 62, 30])\n",
      "Entered BaseEncoder\n",
      "Exit BaseEncoder\n",
      "Encoded Output Shape: torch.Size([16, 16, 31])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_i Shape: torch.Size([16, 64, 2])\n",
      "Entered Projector\n",
      "Exit Projector\n",
      "Projected z_j Shape: torch.Size([16, 64, 2])\n",
      "Entered ContrastiveLoss\n",
      "z_i Mean: 0.012457677163183689, Std: 0.08752740919589996\n",
      "z_j Mean: 0.012606104835867882, Std: 0.08750614523887634\n",
      "Similarities: tensor([[1.4163, 0.3986, 0.0000, 0.1260, 0.0000],\n",
      "        [0.5187, 1.3843, 1.1387, 0.0938, 0.0000],\n",
      "        [0.0036, 1.2043, 1.3969, 0.0000, 0.0000],\n",
      "        [0.0302, 0.1019, 0.0000, 1.3940, 0.0181],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0561, 1.0828]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Similarities Range: [0.0, 1.4285714626312256]\n",
      "Labels: tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
      "Exit ContrastiveLoss with Loss: 1.7010525465011597\n",
      "Batch 157, Loss: 1.7010525465011597\n",
      "Epoch 10/10, Loss: 3.0382\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Initialize models\n",
    "base_encoder = BaseEncoder(input_channels=62, temporal_filter_length=48, spatial_filters=16, temporal_filters=16).to(device)\n",
    "projector = Projector(spatial_filters=16, pooling_kernel=24, temporal_filter_size=4, c=2).to(device)\n",
    "contrastive_loss = ContrastiveLoss(temperature=0.7).to(device)\n",
    "\n",
    "# Train with stratified normalization\n",
    "train_model_with_stratified_normalization(\n",
    "    train_data[:10000], base_encoder, projector, contrastive_loss, epochs=10, batch_size=256, lr=0.0007, patience=30 , checkpoint_path=\"./Checkpoints/check/\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test file created: ./Checkpoints/test.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "checkpoint_dir = \"./Checkpoints/\"\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "    \n",
    "test_file = os.path.join(checkpoint_dir, \"test.txt\")\n",
    "with open(test_file, \"w\") as f:\n",
    "    f.write(\"Testing permissions.\")\n",
    "\n",
    "print(f\"Test file created: {test_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
